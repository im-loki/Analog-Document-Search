[('Marble cutting with single point cutting tool and diamond segments\nAn investigation has been undertaken into the frame sawing with diamond blades.\nThe kinematic behaviour of the frame sawing process is discussed. Under\ndifferent cutting conditions, cutting and indenting-cutting tests are\ncarried out by single point cutting tools and single diamond segments.\nThe results indicate that the depth of cut per diamond grit increases\nas the blades move forward. Only a few grits per segment can remove the\nmaterial in the cutting process. When the direction of the stroke\nchanges, the cutting forces do not decrease to zero because of the\nresidual plastic deformation beneath the diamond grits. The plastic\ndeformation and fracture chipping of material are the dominant removal\nprocesses, which can be explained by the fracture theory of brittle\nmaterial indentation\n', ['marble cutting', 'single point cutting tool', 'diamond segments', 'frame sawing', 'kinematic behaviour', 'cutting tests', 'indenting-cutting tests', 'residual plastic deformation', 'fracture chipping', 'removal processes', 'fracture theory', 'brittle material indentation', 'computerised numerical control', 'cutting', 'diamond', 'kinematics', 'machining', 'plastic deformation']), ('On trajectory and force tracking control of constrained mobile manipulators\nwith parameter uncertainty\nStudies the trajectory and force tracking control problem of mobile\nmanipulators subject to holonomic and nonholonomic constraints with\nunknown inertia parameters. Adaptive controllers are proposed based on\na suitable reduced dynamic model, the defined reference signals and the\nmixed tracking errors. The proposed controllers not only ensure the\nentire state of the system to asymptotically converge to the desired\ntrajectory but also ensure the constraint force to asymptotically\nconverge to the desired force. A detailed numerical example is\npresented to illustrate the developed methods\n', ['trajectory control', 'force tracking control', 'constrained mobile manipulators', 'parameter uncertainty', 'holonomic constraints', 'nonholonomic constraints', 'adaptive controllers', 'reduced dynamic model', 'mixed tracking errors', 'asymptotic convergence', 'position control', 'mobile robots', 'adaptive control', 'convergence', 'force control', 'manipulator dynamics', 'mobile robots', 'position control', 'reduced order systems', 'uncertain systems']), ('Model checking games for branching time logics\nThis paper defines and examines model checking games for the branching time\ntemporal logic CTL*. The games employ a technique called focus which\nenriches sets by picking out one distinguished element. This is\nnecessary to avoid ambiguities in the regeneration of temporal\noperators. The correctness of these games is proved, and optimizations\nare considered to obtain model checking games for important fragments\nof CTL*. A game based model checking algorithm that matches the known\nlower and upper complexity bounds is sketched\n', ['model checking games', 'branching time logics', 'temporal logic', 'temporal operators', 'complexity bounds', 'computability', 'computational complexity', 'temporal logic']), ('Multispectral color image capture using a liquid crystal tunable filter\nWe describe the experimental setup of a multispectral color image acquisition\nsystem consisting of a professional monochrome CCD camera and a tunable\nfilter in which the spectral transmittance can be controlled\nelectronically. We perform a spectral characterization of the\nacquisition system taking into account the acquisition noise. To\nconvert the camera output signals to device-independent color data, two\nmain approaches are proposed and evaluated. One consists in applying\nregression methods to convert from the K camera outputs to a\ndevice-independent color space such as CIEXYZ or CIELAB. Another method\nis based on a spectral model of the acquisition system. By inverting\nthe model using a principal eigenvector approach, we estimate the\nspectral reflectance of each pixel of the imaged surface\n', ['multispectral color image capture', 'liquid crystal tunable filter', 'multispectral color image acquisition system', 'monochrome CCD camera', 'tunable filter', 'spectral transmittance', 'spectral characterization', 'acquisition system', 'acquisition noise', 'camera output signals', 'device-independent color data', 'regression methods', 'camera outputs', 'independent color space', 'CIEXYZ', 'CIELAB', 'spectral model', 'principal eigenvector approach', 'spectral reflectance', 'imaged surface', 'pixel', 'CCD image sensors', 'data acquisition', 'eigenvalues and eigenfunctions', 'image colour analysis', 'optical filters', 'optical tuning', 'reflectivity']), ('System embedding. Polynomial equations\nThe class of solutions of the polynomial equations including their\ngeneralizations in the form of the Bezout matrix identities was\nconstructed analytically using the technology of constructive system\nembedding. The structure of a solution depends on the number of steps\nof the Euclidean algorithm and is obtained explicitly by appropriate\nsubstitutions. Illustrative and descriptive examples are presented\n', ['determinate systems', 'polynomial equations', 'Bezout matrix identities', 'constructive system embedding', 'Euclidean algorithm', 'differential equations', 'matrix algebra', 'polynomials']), ('A geometric process equivalent model for a multistate degenerative system\nIn this paper, a monotone process model for a one-component degenerative system\nwith k+1 states (k failure states and one working state) is studied. We\nshow that this model is equivalent to a geometric process (GP) model\nfor a two-state one component system such that both systems have the\nsame long-run average cost per unit time and the same optimal policy.\nFurthermore, an explicit expression for the determination of an optimal\npolicy is derived\n', ['multistate degenerative system', 'geometric process equivalent model', 'monotone process model', 'one-component degenerative system', 'failure states', 'working state', 'two-state one component system', 'long-run average cost', 'optimal policy', 'replacement policy', 'renewal reward process', 'maintenance engineering', 'optimisation', 'reliability theory']), ('Open hypermedia for product support\nAs industrial systems become increasingly more complex, the maintenance and\noperating information increases both in volume and complexity. With the\ncurrent pressures on manufacturing, the management of information\nresources has become a critical issue. In particular, ensuring that\npersonnel can access current information quickly and effectively when\nundertaking a specific task. This paper discusses some of the issues\ninvolved in, and the benefits of using, open hypermedia to manage and\ndeliver a diverse range of information. While the paper concentrates on\nthe problems specifically associated with manufacturing organizations,\nthe problems are generic across other business sectors such as\nhealthcare, defence and finance. The open hypermedia approach to\ninformation management and delivery allows a multimedia resource base\nto be used for a range of applications and it permits a user to have\ncontrolled access to the required information in an easily accessible\nand structured manner. Recent advancement in hypermedia also permits\njust-in-time support in the most appropriate format for all users. Our\napproach is illustrated by the discussion of a case study in which an\nopen hypermedia system delivers maintenance and process information to\nfactory-floor users to support the maintenance and operation of a very\nlarge manufacturing cell\n', ['open hypermedia', 'maintenance', 'operating information', 'information resources', 'just-in-time support', 'product support', 'hypermedia', 'information resources', 'multimedia systems', 'production control']), ('Numerical representation of binary relations with a multiplicative error\nfunction\nThis paper studies the case of the representation of a binary relation via a\nnumerical function with threshold (error) depending on both compared\nalternatives. The error is considered to be multiplicative, its value\nbeing either directly or inversely proportional to the values of the\nnumerical function. For the first case, it is proved that a binary\nrelation is a semiorder. Moreover, any semiorder can be represented in\nthis form. In the second case, the corresponding binary relation is an\ninterval order\n', ['numerical representation', 'binary relations', 'multiplicative error function', 'numerical function', 'threshold', 'error', 'semiorder', 'interval order', 'decision theory', 'error analysis']), ("Valuing corporate debt: the effect of cross-holdings of stock and debt\nWe have developed a simple approach to valuing risky corporate debt when\ncorporations own securities issued by other corporations. We assume\nthat corporate debt can be valued as an option on corporate business\nasset value, and derive payoff functions when there exist\ncross-holdings of stock or debt between two firms. Next we show that\npayoff functions with multiple cross-holdings can be solved by the\ncontraction principle. The payoff functions which we derive provide a\nnumber of insights about the risk structure of company cross-holdings.\nFirst, the Modigliani-Miller theorem can obtain when there exist\ncross-holdings between firms. Second, by establishing\ncross-shareholdings each of stock holders distributes a part of its\npayoff values to the bond holder of the other's firm, so that both\nfirms can decrease credit risks by cross-shareholdings. In the\nnumerical examples, we show that the correlation in firms can be a\ncritical condition for reducing credit risk by cross-holdings of stock\nusing Monte Carlo simulation. Moreover, we show we can calculate the\ndefault spread easily when complicated cross-holdings exist, and find\nwhich shares are beneficial or disadvantageous\n", ['risky corporate debt valuation', 'stock holdings', 'debt holdings', 'securities', 'option', 'corporate business asset value', 'payoff functions', 'multiple cross-holdings', 'Modigliani-Miller theorem', 'cross-shareholdings', 'bond holder', 'credit risks', 'correlation', 'Monte Carlo simulation', 'corporate modelling', 'correlation methods', 'Monte Carlo methods', 'stock markets']), ('Modeling self-consistent multi-class dynamic traffic flow\nIn this study, we present a systematic self-consistent multiclass multilane\ntraffic model derived from the vehicular Boltzmann equation and the\ntraffic dispersion model. The multilane domain is considered as a\ntwo-dimensional space and the interaction among vehicles in the domain\nis described by a dispersion model. The reason we consider a multilane\ndomain as a two-dimensional space is that the driving behavior of road\nusers may not be restricted by lanes, especially motorcyclists. The\ndispersion model, which is a nonlinear Poisson equation, is derived\nfrom the car-following theory and the equilibrium assumption. Under the\nconcept that all kinds of users share the finite section, the density\nis distributed on a road by the dispersion model. In addition, the\ndynamic evolution of the traffic flow is determined by the systematic\ngas-kinetic model derived from the Boltzmann equation. Multiplying\nBoltzmann equation by the zeroth, first- and second-order moment\nfunctions, integrating both side of the equation and using chain rules,\nwe can derive continuity, motion and variance equation, respectively.\nHowever, the second-order moment function, which is the square of the\nindividual velocity, is employed by previous researches does not have\nphysical meaning in traffic flow\n', ['self-consistent multiclass dynamic traffic flow modeling', 'multilane traffic model', 'vehicular Boltzmann equation', 'traffic dispersion model', 'road users', 'nonlinear Poisson equation', 'car-following theory', 'dynamic evolution', 'variance equation', 'motion equation', 'Poisson equation', 'Boltzmann equation', 'flow simulation', 'multiphase flow', 'nonlinear differential equations', 'Poisson equation', 'road traffic']), ('A fuzzy logic approach to accommodate thermal stress and improve the start-up\nphase in combined cycle power plants\nUse of combined cycle power generation plant has increased dramatically over\nthe last decade. A supervisory control approach based on a dynamic\nmodel is developed, which makes use of proportional-integral-derivative\n(PID), fuzzy logic and fuzzy PID schemes. The aim is to minimize the\nsteam turbine plant start-up time, without violating maximum thermal\nstress limits. An existing start-up schedule provides the benchmark by\nwhich the performance of candidate controllers is assessed.\nImprovements regarding possible reduced start-up times and satisfaction\nof maximum thermal stress restrictions have been realized using the\nproposed control scheme\n', ['combined cycle power plants', 'supervisory control', 'fuzzy logic approach', 'dynamic model', 'PID control', 'fuzzy PID schemes', 'steam turbine plant start-up time minimization', 'maximum thermal stress limits', 'start-up schedule', 'combined cycle power stations', 'control system synthesis', 'fuzzy control', 'power station control', 'steam turbines', 'thermal stresses', 'three-term control']), ('Enhancing the reliability of modular medium-voltage drives\nA method to increase the reliability of modular medium-voltage induction motor\ndrives is discussed, by providing means to bypass a failed module. The\nimpact on reliability is shown. A control, which maximizes the output\nvoltage available after bypass, is described, and experimental results\nare given\n', ['modular medium-voltage induction motor drives', 'reliability enhancement', 'failed module bypass', 'available output voltage control', 'failure analysis', 'fault tolerance', 'induction motor drives', 'machine control', 'reliability', 'voltage control']), ('Hybrid fuzzy modeling of chemical processes\nFuzzy models have been proved to have the ability of modeling all plants\nwithout any priori information. However, the performance of\nconventional fuzzy models can be very poor in the case of insufficient\ntraining data due to their poor extrapolation capacity. In order to\novercome this problem, a hybrid grey-box fuzzy modeling approach is\nproposed in this paper to combine expert experience, local linear\nmodels and historical data into a uniform framework. It consists of two\nlayers. The expert fuzzy model constructed from linguistic information,\nthe local linear model and the T-S type fuzzy model constructed from\ndata are all put in the first layer. Layer 2 is a fuzzy decision module\nthat is used to decide which model in the first layer should be\nemployed to make the final prediction. The output of the second layer\nis the output of the hybrid fuzzy model. With the help of the\nlinguistic information, the poor extrapolation capacity problem caused\nby sparse training data for conventional fuzzy models can be overcome.\nSimulation result for pH neutralization process demonstrates its\nmodeling ability over the linear models, the expert fuzzy model and the\nconventional fuzzy model\n', ['fuzzy modeling', 'chemical processes', 'expert fuzzy model', 'fuzzy decision module', 'process modeling', 'chemical engineering computing', 'fuzzy set theory', 'modelling', 'process control']), ("The heat is on [building automation systems]\nIntegrating building automation systems (BASs) can result in systems that have\nthe ability to sense changes in the air temperature through a\nbuilding's heating, ventilation, and air conditioning (HVAC) systems.\nTaking advantages of the Internet, using remote monitoring, and\nbuilding interoperability through open protocol systems are some of the\nissues discussed throughout the BAS/HVAC community. By putting\ninformation over the Internet, facility managers get real-time data on\nenergy usage and performance issues\n", ['building automation systems', 'HVAC', 'heating', 'ventilation', 'air conditioning', 'remote monitoring', 'interoperability', 'Internet', 'real-time data', 'building management systems', 'computerised monitoring', 'HVAC', 'Internet', 'open systems']), ("Revisiting Hardy's paradox: Counterfactual statements, real measurements,\nentanglement and weak values\nHardy's (1992) paradox is revisited. Usually the paradox is dismissed on\ngrounds of counterfactuality, i.e., because the paradoxical effects\nappear only when one considers results of experiments which do not\nactually take place. We suggest a new set of measurements in connection\nwith Hardy's scheme, and show that when they are actually performed,\nthey yield strange and surprising outcomes. More generally, we claim\nthat counterfactual paradoxes point to a deeper structure inherent to\nquantum mechanics\n", ['Hardy paradox', 'counterfactual statements', 'real measurements', 'entanglement', 'gedanken-experiments', 'weak values', 'paradoxical effects', 'quantum mechanics', 'bound states', 'information theory', 'measurement theory', 'probability', 'quantum theory']), ('Open courseware and shared knowledge in higher education\nMost college and university campuses in the United States and much of the\ndeveloped world today maintain one, two, or several learning management\nsystems (LMSs), which are courseware products that provide students and\nfaculty with Web-based tools to manage course-related applications.\nSince the mid-1990s, two predominant models of Web courseware\nmanagement systems have emerged: commercial and noncommercial. Some of\nthe commercial products available today were created in academia as\nnoncommercial but have since become commercially encumbered. Other\nproducts remain noncommercial but are struggling to survive in a world\nof fierce commercial competition. This article argues for an ethics of\npedagogy in higher education that would be based on the guiding\nassumptions of the non-proprietary, peer-to-peer, open-source software\nmovement\n', ['open courseware', 'shared knowledge', 'higher education', 'learning management systems', 'college', 'university', 'Web courseware management systems', 'commercial products', 'ethics', 'Internet', 'open-source software', 'courseware', 'information resources', 'Internet', 'public domain software']), ('Prospective on computer applications in power\nThe so-called "deregulation" and restructuring of the electric power industry\nhave made it very difficult to keep up with industry changes and have\nmade it much more difficult to envision the future. In this article,\ncurrent key issues and major developments of the past few years are\nreviewed to provide perspective, and prospects for future computer\napplications in power are suggested. Technology changes are occurring\nat an exponential rate. The interconnected bulk electric systems are\nbecoming integrated with vast networked information systems. This\narticle discusses the skills that will be needed by future power\nengineers to keep pace with these developments and trends\n', ['electric power industry deregulation', 'computer applications', 'electricity industry restructuring', 'technology changes', 'interconnected bulk electric systems', 'networked information systems', 'electricity supply industry', 'management information systems', 'power engineering computing']), ("Learning nonregular languages: a comparison of simple recurrent networks and\nLSTM\nRodriguez (2001) examined the learning ability of simple recurrent nets (SRNs)\n(Elman, 1990) on simple context-sensitive and context-free languages.\nIn response to Rodriguez's (2001) article, we compare the performance\nof simple recurrent nets and long short-term memory recurrent nets on\ncontext-free and context-sensitive languages\n", ['nonregular language learning', 'recurrent neural networks', 'LSTM', 'context-sensitive languages', 'context-free languages', 'performance', 'short-term memory recurrent nets', 'context-free languages', 'context-sensitive languages', 'learning (artificial intelligence)', 'performance evaluation', 'recurrent neural nets']), ('Aim for the enterprise: Microsoft Project 2002\nA long-time favorite of project managers, Microsoft Project 2002 is making its\nenterprise debut. Its new Web-based collaboration tools and improved\nscalability with OLAP support make it much easier to manage multiple\nWeb projects with disparate workgroups and budgets\n', ['Microsoft Project 2002', 'Web-based collaboration tools', 'scalability', 'OLAP support', 'multiple Web project management', 'workgroups', 'budgets', 'business data processing', 'data mining', 'Internet', 'project management', 'software reviews']), ('Ventilation-perfusion ratio of signal intensity in human lung using\noxygen-enhanced and arterial spin labeling techniques\nThis study investigates the distribution of ventilation-perfusion (V/Q) signal\nintensity (SI) ratios using oxygen-enhanced and arterial spin labeling\n(ASL) techniques in the lungs of 10 healthy volunteers. Ventilation and\nperfusion images were simultaneously acquired using the flow-sensitive\nalternating inversion recovery (FAIR) method as volunteers alternately\ninhaled room air and 100% oxygen. Images of the T/sub 1/ distribution\nwere calculated for five volunteers for both selective (T/sub 1f/) and\nnonselective (T/sub 1/) inversion. The average T/sub 1/ was 1360\nms+or-116 ms, and the average T/sub 1f/ was 1012 ms+or-112 ms, yielding\na difference that is statistically significant (P<0.002). Excluding\nlarge pulmonary vessels, the average V/Q SI ratios were 0.355+or-0.073\nfor the left lung and 0.371+or-0.093 for the right lung, which are in\nagreement with the theoretical V/Q SI ratio. Plots of the WO SI ratio\nare similar to the logarithmic normal distribution obtained by multiple\ninert gas elimination techniques, with a range of ratios matching\nventilation and perfusion. This MRI V/Q technique is completely\nnoninvasive and does not involve ionized radiation. A limitation of\nthis method is the nonsimultaneous acquisition of perfusion and\nventilation data, with oxygen administered only for the ventilation\ndata\n', ['human lung', 'ventilation-perfusion ratio', 'signal intensity', 'oxygen-enhanced techniques', 'arterial spin labeling techniques', 'ventilation images', 'perfusion images', 'flow-sensitive alternating inversion recovery', 'logarithmic normal distribution', 'nonsimultaneous acquisition', 'gas exchange efficiency', 'pathomechanisms', 'MRI', 'time delay', 'pixel-by-pixel maps', 'multiple inert gas elimination', 'pulmonary embolism', 'chronic obstructive pulmonary disease', 'biomedical MRI', 'log normal distribution', 'lung', 'medical image processing', 'pneumodynamics']), ('Quantitative speed control for SRM drive using fuzzy adapted inverse model\nQuantitative and robust speed control for a switched reluctance motor (SRM)\ndrive is considered to be rather difficult and challenging owing to its\nhighly nonlinear dynamic behavior. A speed control scheme having\ntwo-degree-of-freedom (2DOF) structure is developed here to improve the\nspeed dynamic response of an SRM drive. In the proposed control scheme,\nthe feedback controller is quantitatively designed to meet the desired\nregulation control requirements first. Then a reference model and a\ncommand feedforward controller based on an inverse plant model are\nemployed to yield the desired tracking response at nominal case. As the\nvariations of system parameters and operating conditions occur, the\nprescribed control specifications may not be satisfied any more. To\nimprove this, the inverse model is adaptively tuned by a fuzzy control\nscheme so that the model-following tracking error is significantly\nreduced. In addition, a simple disturbance cancellation robust\ncontroller is added to improve the tracking and regulation control\nperformances further\n', ['quantitative speed control', 'SRM drive', 'fuzzy adapted inverse model', 'switched reluctance motor', 'nonlinear dynamic behavior', 'two-degree-of-freedom structure', 'speed dynamic response', 'regulation control requirements', 'reference model', 'command feedforward controller', 'inverse plant model', 'tracking response', 'system parameters', 'operating conditions', 'control specifications', 'fuzzy control scheme', 'model-following tracking error', 'disturbance cancellation controller', 'feedforward', 'fuzzy control', 'model reference adaptive control systems', 'reluctance motor drives', 'velocity control']), ("A partial converse to Hadamard's theorem on homeomorphisms\nA theorem by Hadamard gives a two-part condition under which a map from one\nBanach space to another is a homeomorphism. The theorem, while often\nvery useful, is incomplete in the sense that it does not explicitly\nspecify the family of maps for which the condition is met. Here, under\na typically weak additional assumption on the map, we show that\nHadamard's condition is met if, and only if, the map is a homeomorphism\nwith a Lipschitz continuous inverse. An application is given concerning\nthe relation between the stability of a nonlinear system and the\nstability of related linear systems\n", ['partial converse', 'Hadamard theorem', 'homeomorphisms', 'Banach space', 'Lipschitz continuous inverse', 'nonlinear system stability', 'linear system stability', 'linearization', 'nonlinear feedback systems', 'nonlinear networks', 'Banach spaces', 'feedback', 'linear systems', 'linearisation techniques', 'nonlinear network analysis', 'nonlinear systems', 'numerical stability']), ('Flexible air-jet tooling for vibratory bowl feeder systems\nVibratory bowl feeders (VBFs) are machines that feed various small parts in\nlarge volume automatic assembly systems. Their shortcomings, like\ninflexibility and the propensity to jam, stem from the use of\nmechanical orienting devices. Air jet based orienting devices can be\nimplemented to overcome these limitations. Applications of passive and\nactive air jet based orienting devices that replace conventional\ndevices for the VBF are discussed. Passive devices, which reject\nincorrectly oriented parts, are discussed first. Active air jet based\norienting devices are then introduced to further improve the\nflexibility of VBFs. Since active devices reorient parts into a desired\norientation, the part motion under their influence is analyzed. A\nnumber of tests demonstrate the feasibility and advantages of these new\norienting devices\n', ['vibratory bowl feeders', 'automatic assembly systems', 'passive air jet', 'active air jet', 'orienting devices', 'parts feeding', 'assembling', 'computerised control', 'materials handling']), ('Modeling dynamic objects in distributed systems with nested Petri nets\nNested Petri nets (NP-nets) is a Petri net extension, allowing tokens in a net\nmarking to be represented by marked nets themselves. The paper\ndiscusses applicability of NP-nets for modeling task planning systems,\nmulti-agent systems and recursive-parallel systems. A comparison of\nNP-nets with some other formalisms, such as OPNs of R. Valk (2000),\nrecursive parallel programs of O. Kushnarenko and Ph. Schnoebelen\n(1997) and process algebras is given. Some aspects of decidability for\nobject-oriented Petri net extensions are also discussed\n', ['dynamic objects modelling', 'distributed systems', 'nested Petri nets', 'multi-agent systems', 'recursive-parallel systems', 'process algebras', 'decidability', 'object-oriented Petri net', 'decidability', 'formal specification', 'multi-agent systems', 'parallel programming', 'Petri nets', 'process algebra', 'synchronisation']), ('Toward a formalism for conversation protocols using joint intention theory\nConversation protocols are used to achieve certain goals or to bring about\ncertain states in the world. Therefore, one may identify the landmarks\nor the states that must be brought about during the goal-directed\nexecution of a protocol. Accordingly, the landmarks, characterized by\npropositions that are true in the state represented by that landmark,\nare the most important aspect of a protocol. Families of conversation\nprotocols can be expressed formally as partially ordered landmarks\nafter the landmarks necessary to achieve a goal have been identified.\nConcrete protocols represented as joint action expressions can, then,\nbe derived from the partially ordered landmarks and executed directly\nby joint intention interpreters. This approach of applying Joint\nIntention theory to protocols also supports flexibility in the actions\nused to get to landmarks, shortcutting protocol execution, automatic\nexception handling, and correctness criterion for protocols and\nprotocol compositions\n', ['conversation protocols', 'protocol execution', 'automatic exception handling', 'correctness criterion', 'multi-agent interactions', 'finite state machines', 'finite state machines', 'multi-agent systems', 'protocols']), ('Hand-held digital video-camera for eye examination and follow-up\nWe developed a hand-held digital colour video-camera for eye examination in\nprimary care. The device weighed 550 g. It featured a charge-coupled\ndevice (CCD) and corrective optics. Both colour video and digital still\nimages could be taken. The video-camera was connected to a PC with\nsoftware for database storage, image processing and telecommunication.\nWe studied 88 normal subjects (38 male, 50 female), aged 7-62 years. It\nwas not necessary to use mydriatic eye drops for pupillary dilation.\nSatisfactory digital images of the whole face and the anterior eye were\nobtained. The optic disc and the central part of the ocular fundus\ncould also be recorded. Image quality of the face and the anterior eye\nwere excellent; image quality of the optic disc and macula were good\nenough for tele-ophthalmology. Further studies are needed to evaluate\nthe usefulness of the equipment in different clinical conditions\n', ['hand-held digital colour video camera', 'eye examination', 'primary care', 'charge-coupled device', 'corrective optics', 'digital still images', 'colour video images', 'PC', 'software', 'database storage', 'image processing', 'telecommunication', 'normal subjects', 'whole face', 'anterior eye', 'optic disc', 'ocular fundus', 'image quality', 'tele-ophthalmology', 'clinical conditions', 'follow-up', 'eye', 'medical image processing', 'microcomputer applications', 'telemedicine', 'video cameras', 'video databases', 'video signal processing']), ('The MAGNeT toolkit: design, implementation and evaluation\nThe current trend in constructing high-performance computing systems is to\nconnect a large number of machines via a fast interconnect or a\nlarge-scale network such as the Internet. This approach relies on the\nperformance of the interconnect (or Internet) to enable fast,\nlarge-scale distributed computing. A detailed understanding of the\ncommunication traffic is required in order to optimize the operation of\nthe entire system. Network researchers traditionally monitor traffic in\nthe network to gain the insight necessary to optimize network\noperations. Recent work suggests additional insight can be obtained by\nalso monitoring traffic at the application level. The Monitor for\nApplication-Generated Network Traffic toolkit (MAGNeT) we describe here\nmonitors application traffic patterns in production systems, thus\nenabling more highly optimized networks and interconnects for the next\ngeneration of high-performance computing systems\n', ['high-performance computing systems', 'Internet', 'Monitor for Application-Generated Network Traffic toolkit', 'optimized networks', 'interconnects', 'high-performance computing', 'network protocol', 'traffic characterization', 'MAGNeT', 'virtual supercomputing', 'computational grids', 'computer networks', 'protocols', 'telecommunication traffic recording']), ('Fuzzy polynomial neural networks: hybrid architectures of fuzzy modeling\nWe introduce a concept of fuzzy polynomial neural networks (FPNNs), a hybrid\nmodeling architecture combining polynomial neural networks (PNNs) and\nfuzzy neural networks (FNNs). The development of the FPNNs dwells on\nthe technologies of computational intelligence (CI), namely fuzzy sets,\nneural networks, and genetic algorithms. The structure of the FPNN\nresults from a synergistic usage of FNN and PNN. FNNs contribute to the\nformation of the premise part of the rule-based structure of the FPNN.\nThe consequence part of the FPNN is designed using PNNs. The structure\nof the PNN is not fixed in advance as it usually takes place in the\ncase of conventional neural networks, but becomes organized dynamically\nto meet the required approximation error. We exploit a group method of\ndata handling (GMDH) to produce this dynamic topology of the network.\nThe performance of the FPNN is quantified through experimentation that\nexploits standard data already used in fuzzy modeling. The obtained\nexperimental results reveal that the proposed networks exhibit high\naccuracy and generalization capabilities in comparison to other similar\nfuzzy models\n', ['fuzzy polynomial neural networks', 'hybrid architectures', 'fuzzy modeling', 'highly nonlinear rule-based models', 'computational intelligence', 'fuzzy sets', 'genetic algorithms', 'group method of data handling', 'GMDH', 'dynamic topology', 'fuzzy inference method', 'learning', 'standard backpropagation', 'membership functions', 'learning rates', 'momentum coefficients', 'genetic optimization', 'backpropagation', 'fuzzy logic', 'fuzzy neural nets', 'genetic algorithms', 'identification', 'inference mechanisms', 'modelling']), ('Playing for time [3G networks]\nThe delays in rolling out 3G networks across Europe should not always be seen\nwith a negative slant\n', ['3G networks', 'delays', 'Europe', 'mobile operators', 'cellular radio', 'mobile computing']), ("The impact and implementation of XML on business-to-business commerce\nThis paper discusses the impact analysis of the Extensible Markup Language\n(XML). Each business partner within a supply chain will be allowed to\ngenerate its own data exchange format by adopting an XML meta-data\nmanagement system in the local side. Followed after a brief\nintroduction of the information technology for Business to Customer\n(B2C) and Business to Business (B2B) Electronic Commerce (EC), the\nimpact of XML on the tomorrow business world is discussed. A real case\nstudy for impact analysis on information exchange platform, Microsoft's\nBizTalk platform which is actually an XML schema builder and the\nimplementation of XML commerce application will provide an interest\ninsight for users' future implementation\n", ['Extensible Markup Language', 'XML', 'Business to Customer', 'BizTalk', 'XML schema builder', 'Business to Business', 'electronic commerce', 'Electronic Data Interchange', 'Enterprise Resources Planning', 'electronic commerce', 'electronic data interchange', 'hypermedia markup languages']), ('Innovative phase unwrapping algorithm: hybrid approach\nWe present a novel algorithm based on a hybrid of the global and local\ntreatment of a wrapped map. The proposed algorithm is especially\neffective for the unwrapping of speckle-coded interferogram contour\nmaps. In contrast to earlier unwrapping algorithms by region, we\npropose a local discontinuity-restoring criterion to serve as the\npreprocessor or postprocessor of our hybrid algorithm, which makes the\nunwrapping by region much easier and more efficient. With this hybrid\nalgorithm, a robust, stable, and especially time effective phase\nunwrapping can be achieved. Additionally, the criterion and limitation\nof this hybrid algorithm are fully described. The robustness,\nstability, and speed of this hybrid algorithm are also studied. The\nproposed algorithm can be easily upgraded with minor modifications to\nsolve the unwrapping problem of maps with phase inconsistency. Both\nnumerical simulation and experimental applications demonstrate the\neffectiveness of the proposed algorithm\n', ['phase unwrapping algorithm', 'global treatment', 'local treatment', 'wrapped map', 'speckle-coded interferogram contour maps', 'unwrapping algorithms', 'local discontinuity-restoring criterion', 'postprocessor', 'hybrid algorithm', 'robust stable time effective phase unwrapping', 'unwrapping problem', 'phase inconsistency', 'numerical simulation', 'interferogram analysis', 'light interferometry', 'light interferometry', 'optical information processing', 'speckle', 'stability']), ('MRP in a job shop environment using a resource constrained project scheduling\nmodel\nOne of the most difficult tasks in a job shop manufacturing environment is to\nbalance schedule and capacity in an ongoing basis. MRP systems are\ncommonly used for scheduling, although their inability to deal with\ncapacity constraints adequately is a severe drawback. In this study, we\nshow that material requirements planning can be done more effectively\nin a job shop environment using a resource constrained project\nscheduling model. The proposed model augments MRP models by\nincorporating capacity constraints and using variable lead time\nlengths. The efficacy of this approach is tested on MRP systems by\ncomparing the inventory carrying costs and resource allocation of the\nsolutions obtained by the proposed model to those obtained by using a\ntraditional MRP model. In general, it is concluded that the proposed\nmodel provides improved schedules with considerable reductions in\ninventory carrying costs\n', ['job shop environment', 'MRP', 'resource constrained project scheduling model', 'material requirements planning', 'scheduling', 'capacity constraints', 'variable lead time lengths', 'inventory carrying costs', 'resource allocation', 'project management', 'manufacturing resources planning', 'production control', 'project management', 'resource allocation', 'stock control']), ('Production capacity of flexible manufacturing systems with fixed production\nratios\nDetermining the production capacity of flexible manufacturing systems is a very\nimportant issue in the design of such systems. We propose an approach\nfor determining the production capacity (i.e. the maximum production\nrate) of a flexible manufacturing system with several part types,\ndedicated pallets, and fixed production ratios among the different part\ntypes. We show that the problem reduces to the determination of a\nsingle parameter for which we propose an iterative procedure.\nSimulation or approximate analytical techniques can be used as the\nbuilding block performance evaluation technique in the iterative\nprocedure\n', ['flexible manufacturing systems', 'fixed production ratios', 'production capacity', 'maximum production rate', 'multiple part type', 'dedicated pallets', 'single parameter determination', 'iterative procedure', 'simulation', 'approximate analytical techniques', 'building block performance evaluation technique', 'stability condition', 'numerical experiments', 'flexible manufacturing systems', 'iterative methods', 'production control']), ('An entanglement measure based on the capacity of dense coding\nAn asymptotic entanglement measure for any bipartite states is derived in the\nlight of the dense coding capacity optimized with respect to local\nquantum operations and classical communications. General properties and\nsome examples with explicit forms of this entanglement measure are\ninvestigated\n', ['entanglement measure', 'dense coding capacity', 'asymptotic entanglement measure', 'bipartite states', 'local quantum operations', 'classical communications', 'optimization', 'encoding', 'optimisation', 'quantum communication']), ('The plot thins: thin-client computer systems and academic libraries\nThe few libraries that have tried thin client architectures have noted a number\nof compelling reasons to do so. For starters, thin client devices are\nfar less expensive than most PCs. More importantly, thin client\ncomputing devices are believed to be far less expensive to manage and\nsupport than traditional PCs\n', ['academic libraries', 'thin-client computer systems', 'academic libraries', 'library automation', 'network computers']), ('Women in computing: what brings them to it, what keeps them in it?\nCareer stereotyping and misperceptions about the nature of computing are\nsubstantive reasons for the under representation of women in\nprofessional computing careers. In this study, 15 women who have work\nexperience in several aspects of computing were asked about their\nreasons for entering computing, what they liked about working in\ncomputing, and what they disliked. While there are many common threads,\nthere are also individual differences. Common reasons for choosing\ncomputing as a career included: exposure to computing in a setting\nwhich enabled them to see the versatility of computers; the influence\nof someone close to them; personal abilities which they perceived to be\nappropriate for a career in computing; and characteristics of such\ncareers which appealed to them. Generally, women working in the field\nenjoy the work they are doing. Dislikes arising from their work\nexperiences are more likely to be associated with people and politics\nthan with the work they do-and they would like to have more female\ncolleagues\n', ['career stereotyping', 'personal abilities', 'politics', 'misperceptions', 'women', 'professional computing careers', 'computer science education', 'employment', 'gender issues', 'professional aspects']), ('Use of extra degrees of freedom in multilevel drives\nMultilevel converters with series connection of semiconductors allow power\nelectronics to reach medium voltages (1-10 kV) with relatively standard\ncomponents. The increase of the number of semiconductors provides extra\ndegrees of freedom, which can be used to improve different\ncharacteristics. This paper is focused on variable-speed drives and it\nis shown that with the proposed multilevel direct torque control\nstrategy (DiCoIF) the tradeoff between the performances of the drive\n(harmonic distortions, torque dynamics, voltage step gradients, etc.)\nand the switching frequency of the semiconductors is improved. Then, a\nslightly modified strategy reducing common-mode voltage and bearing\ncurrents is presented\n', ['degrees of freedom', 'series connection', 'semiconductors', 'power electronics', 'medium voltages', 'variable-speed drives', 'multilevel direct torque control strategy', 'harmonic distortions', 'torque dynamics', 'voltage step gradients', 'switching frequency', 'common-mode voltage reduction', 'bearing currents', 'delay estimation', 'industrial power systems', 'insulated gate bipolar transistors', 'state estimation', 'multilevel drives', 'fixed-frequency dynamic control', '1 to 10 kV', 'harmonic distortion', 'induction motor drives', 'insulated gate bipolar transistors', 'machine bearings', 'machine control', 'power convertors', 'state estimation', 'switching circuits', 'torque control']), ("Utilizing Web-based case studies for cutting-edge information services issues\nThis article reports on a pilot study conducted by the Academic Libraries of\nthe 21st Century project team to determine whether the benefits of the\ncase study method as a training framework for change initiatives could\nsuccessfully transfer from the traditional face-to-face format to a\nvirtual format. Methods of developing the training framework, as well\nas the benefits, challenges, and recommendations for future strategies\ngained from participant feedback are outlined. The results of a survey\nadministered to chat session registrants are presented in three\nsections: (1) evaluation of the training framework; (2) evaluation of\nparticipants' experiences in the virtual environment; and (3) a\ncomparison of participants' preference of format. The overall\nparticipant feedback regarding the utilization of the case study method\nin a virtual environment for professional development and collaborative\nproblem solving is very positive\n", ['Web-based case studies', 'cutting-edge information services', 'academic libraries', 'training', 'change initiatives', 'survey', 'virtual environment', 'professional development', 'Internet', 'collaborative problem solving', 'academic libraries', 'computer based training', 'information resources', 'Internet', 'library automation']), ('E-learning on the college campus: a help or hindrance to students learning\nobjectives: a case study\nIf you know how to surf the World Wide Web, have used email before, and can\nlearn how to send an email attachment, then learning how to interact in\nan online course should not be difficult at all. In a way to find out,\nI decided to offer two identical courses, one of which would be offered\nonline and the other the "traditional way". I wanted to see how\nstudents would fare with identical material provided in each course. I\nwanted their anonymous feedback, when the course was over\n', ['distance education', 'William Paterson University', 'e-learning', 'computer aided instruction', 'distance learning']), ('Advanced aerostatic stability analysis of cable-stayed bridges using\nfinite-element method\nBased on the concept of limit point instability, an advanced nonlinear\nfinite-element method that can be used to analyze the aerostatic\nstability of cable-stayed bridges is proposed. Both geometric\nnonlinearity and three components of wind loads are considered in this\nmethod. The example bridge is the second Santou Bay cable-stayed bridge\nwith a main span length of 518 m built in China. Aerostatic stability\nof the example bridge is investigated using linear and proposed\nmethods. The effect of pitch moment coefficient on the aerostatic\nstability of the bridge has been studied. The results show that the\naerostatic instability analyses of cable-stayed bridges based on the\nlinear method considerably overestimate the wind-resisting capacity of\ncable-stayed bridges. The proposed method is highly accurate and\nefficient. Pitch moment coefficient has a major effect on the\naerostatic stability of cable-stayed bridges. Finally, the aerostatic\nfailure mechanism of cable-stayed bridges is explained by tracing the\naerostatic instability path\n', ['limit point instability', 'advanced nonlinear finite element method', 'advanced aerostatic stability analysis', 'cable-stayed bridges', 'geometric nonlinearity', 'wind loads', 'Santou Bay cable-stayed bridge', 'China', 'pitch moment coefficient', 'aerostatic failure mechanism', 'civil engineering computing', 'finite element analysis', 'mechanical stability', 'wind']), ('An efficient DIPIE algorithm for CAD of electrostatically actuated MEMS devices\nPull-in parameters are important properties of electrostatic actuators.\nEfficient and accurate analysis tools that can capture these parameters\nfor different design geometries, are therefore essential. Current\nsimulation tools approach the pull-in state by iteratively adjusting\nthe voltage applied across the actuator electrodes. The convergence\nrate of this scheme gradually deteriorates as the pull-in state is\napproached. Moreover, the convergence is inconsistent and requires many\nmesh and accuracy refinements to assure reliable predictions. As a\nresult, the design procedure of electrostatically actuated MEMS devices\ncan be time-consuming. In this paper a novel Displacement Iteration\nPull-In Extraction (DIPIE) scheme is presented. The DIPIE scheme is\nshown to converge consistently and far more rapidly than the Voltage\nIterations (VI) scheme (>100 times faster!). The DIPIE scheme\nrequires separate mechanical and electrostatic field solvers.\nTherefore, it can be easily implemented in existing MOEMS CAD packages.\nMoreover, using the DIPIE scheme, the pull-in parameters extraction can\nbe performed in a fully automated mode, and no user input for search\nbounds is required\n', ['DIPIE algorithm', 'MOEMS CAD packages', 'electrostatically actuated MEMS devices', 'pull-in parameters', 'electrostatic actuators', 'design geometries', 'convergence rate', 'displacement iteration pull-in extraction scheme', 'mechanical field solver', 'electrostatic field solver', 'computer-aided design', 'displacement iteration', 'convergence', 'electronic design automation', 'electrostatic actuators', 'iterative methods', 'mechanical engineering computing', 'micro-optics', 'micromechanical devices', 'relaxation theory']), ('Monoids all polygons over which are omega -stable: proof of the Mustafin-Poizat\nconjecture\nA monoid S is called an omega -stabilizer (superstabilizer, or stabilizer) if\nevery S-polygon has an omega -stable (superstable, or stable) theory.\nIt is proved that every omega -stabilizer is a regular monoid. This\nconfirms the Mustafin-Poizat conjecture and allows us to end up the\ndescription of omega -stabilizers\n', ['monoids all polygons', 'omega -stabilizer', 'S-polygon', 'regular monoid', 'Mustafin-Poizat conjecture', 'computational geometry', 'formal logic']), ('Chaotic phenomena and fractional-order dynamics in the trajectory control of\nredundant manipulators\nRedundant manipulators have some advantages when compared with classical arms\nbecause they allow the trajectory optimization, both on the free space\nand on the presence of obstacles, and the resolution of singularities.\nFor this type of arms the proposed kinematic control algorithms adopt\ngeneralized inverse matrices but, in general, the corresponding\ntrajectory planning schemes show important limitations. Motivated by\nthese problems this paper studies the chaos revealed by the\npseudoinverse-based trajectory planning algorithms, using the theory of\nfractional calculus\n', ['chaotic phenomena', 'fractional-order dynamics', 'trajectory control', 'redundant manipulators', 'classical arms', 'trajectory optimization', 'kinematic control algorithms', 'generalized inverse matrices', 'trajectory planning schemes', 'fractional calculus', 'calculus', 'chaos', 'inverse problems', 'matrix algebra', 'nonlinear dynamical systems', 'optimisation', 'position control', 'redundant manipulators']), ('Quantitative analysis of reconstructed 3-D coronary arterial tree and\nintracoronary devices\nTraditional quantitative coronary angiography is performed on two-dimensional\n(2-D) projection views. These views are chosen by the angiographer to\nminimize vessel overlap and foreshortening. With 2-D projection views\nthat are acquired in this nonstandardized fashion, however, there is no\nway to know or estimate how much error occurs in the QCA process.\nFurthermore, coronary arteries possess a curvilinear shape and undergo\na cyclical deformation due to their attachment to the myocardium.\nTherefore, it is necessary to obtain three-dimensional (3-D)\ninformation to best describe and quantify the dynamic curvilinear\nnature of the human coronary artery. Using a patient-specific 3-D\ncoronary reconstruction algorithm and routine angiographic images, a\nnew technique is proposed to describe: (1) the curvilinear nature of\n3-D coronary arteries and intracoronary devices; (2) the magnitude of\nthe arterial deformation caused by intracoronary devices and due to\nheart motion; and (3) optimal view(s) with respect to the desired\n"pathway" for delivering intracoronary devices\n', ['medical diagnostic imaging', 'cyclical deformation', 'myocardium', 'dynamic curvilinear nature quantification', 'patient-specific 3-D coronary reconstruction algorithm', 'routine angiographic images', 'arterial deformation magnitude', 'intracoronary devices delivery pathway', 'human coronary artery', 'angiocardiography', 'image reconstruction', 'medical image processing', 'prosthetics']), ('The changing landscape for multi access portals\nDiscusses the factors that have made life difficult for consumer portal\noperators in recent years causing them, like others in the\ntelecommunications, media and technology sector, to take a close look\nat their business models following the dot.com crash and the consequent\nreassessment of Internet-related project financing by the venture\ncapital community. While the pressure is on to generate income from\nexisting customers and users, portal operators must reach new markets\nand find realistic revenue streams. This search for real revenues has\nled to a move towards charging for content, a strategy being pursued by\na large number of horizontal portal players, including MSN and Terra\nLycos. This trend is particularly noticeable in China, where\nChinadotcom operates a mainland portal and plans a range of fee-based\nservices, including electronic mail. The nature of advertising itself\nis changing, with portals seeking blue-chip sponsorship and marketing\ndeals that span a number of years. Players are struggling to redefine\nand reinvent themselves as a result of the changing environment and\neven the term "portal" is believed to be obsolete, partly due to its\ndot.com crash associations. Multi-access portals are expected to\ndominate the consumer sector, becoming bigger and better overall than\ntheir predecessors and playing a more powerful role in the consumer\nenvironment\n', ['multi-access portals', 'consumer portal operators', 'revenue streams', 'fee-based services', 'advertising', 'blue-chip sponsorship', 'information industry', 'information resources', 'Internet', 'management of change', 'online front-ends', 'search engines']), ("Effects of white space in learning via the Web\nThis study measured the effect of specific white space features on learning\nfrom instructional Web materials. The study also measured learners'\nbeliefs regarding Web-based instruction. Prior research indicated that\nsmall changes in the handling of presentation elements can affect\nlearning. Achievement results from this study indicated that in on-line\nmaterials, when content and overall structure are sound, minor\ndifferences regarding table borders and vertical spacing in text do not\nhinder learning. Beliefs regarding Web-based instruction and\ninstructors who use it did not differ significantly between treatment\ngroups. Implications of the study and cautions regarding generalizing\nfrom the results are discussed\n", ['white space features', 'Web-based instruction', 'presentation', 'online educational materials', 'table borders', 'text vertical spacing', 'Internet', 'educational computing', 'human factors', 'information resources', 'Internet', 'user interfaces']), ('There is no optimal routing policy for the torus\nA routing policy is the method used to select a specific output channel for a\nmessage from among a number of acceptable output channels. An optimal\nrouting policy is a policy that maximizes the probability of a message\nreaching its destination without delays. Optimal routing policies have\nbeen proposed for several regular networks, including the mesh and the\nhypercube. An open problem in interconnection network research has been\nthe identification of an optimal routing policy for the torus. In this\npaper, we show that there is no optimal routing policy for the torus.\nOur result is demonstrated by presenting a detailed example in which\nthe best choice of output channel is dependent on the probability of\neach channel being available. This result settles, in the negative, a\nconjecture by J. Wu (1996) concerning an optimal routing policy for the\ntorus\n', ['optimal routing policy', 'torus', 'hypercube', 'hypercube networks', 'network routing']), ('Human face detection in visual scenes using neural networks\nThis paper presents a neural network based face detection system. Our objective\nis to design a system that can detect human faces in visual scenes at\nhigh searching speed and accuracy. We used a neural network with a\nsimple structure but trained using face and non-face samples\npreprocessed by several methods (position normalization, histogram\nequalization, etc.) to attain high accuracy, then pruned the size of\nthe neural network so that it could run faster and reduced the total\nsearch area of a target visual scene using the skin color detector.\nSkin color detection assumes that faces reside only in skin color\nregions. The system design is made up of two parts: the face detecting\nsystem that detects the faces, and the searching speed improving\nsystem. Speed improvement is achieved by reducing the face locator\nnetwork size using the structural learning with knowledge and by\nreducing the face search area using the skin color detection system.\nFaster training of the neural networks was also achieved using variable\nstep sizes\n', ['backpropagation', 'self organizing maps', 'down-sampling', 'neural network', 'human face recognition', 'visual scene', 'skin color detector', 'face detecting system', 'merging overlapping detections', 'backpropagation', 'computer vision', 'face recognition', 'image colour analysis', 'neural nets']), ('Updating systems for monitoring and controlling power equipment on the basis of\nthe firmware system SARGON\nThe economic difficulties experienced by the power industry of Russia has\nconsiderably retarded the speed of commissioning new capacities and\nreconstructing equipment in service. The increasing deterioration of\nthe equipment at power stations makes the problem of its updating very\nacute. The main efforts of organizations working in the power industry\nare now focused on updating all kinds of equipment installed at power\ninstallations. The necessary condition for the efficient operation of\npower equipment is to carry out serious modernization of systems for\nmonitoring and control (SMC) of technological processes. The\nspecialists at ZAO NVT-Avtomatika have developed efficient technology\nfor updating the SMC on the basis of the firmware system SARGON which\nensures the fast introduction of high-quality systems of automation\nwith a minimal payback time of the capital outlay. This paper discusses\nthe updating of equipment using SARGON\n', ['power industry', 'Russia', 'SARGON firmware system', 'ZAO NVT-Avtomatika', 'monitoring systems', 'control systems', 'power equipment monitoring', 'power equipment control', 'computerised control', 'computerised monitoring', 'firmware', 'power apparatus', 'power engineering computing']), ('An adaptive sphere-fitting method for sequential tolerance control\nThe machining of complex parts typically involves a logical and chronological\nsequence of n operations on m machine tools. Because manufacturing\ndatums cannot always match design constraints, some of the design\nspecifications imposed on the part are usually satisfied by distinct\nsubsets of the n operations prescribed in the process plan.\nConventional tolerance control specifies a fixed set point for each\noperation and a permissible variation about this set point to insure\ncompliance with the specifications, whereas sequential tolerance\ncontrol (STC) uses real-time measurement information at the completion\nof one stage to reposition the set point for subsequent operations.\nHowever, it has been shown that earlier sphere-fitting methods for STC\ncan lead to inferior solutions when the process distributions are\nskewed. This paper introduces an extension of STC that uses an adaptive\nsphere-fitting method that significantly improves the yield in the\npresence of skewed distributions as well as significantly reducing the\ncomputational effort required by earlier probabilistic search methods\n', ['machine tools', 'sequential tolerance control', 'adaptive sphere-fitting method', 'design constraints', 'compliance', 'real-time measurement information', 'yield improvement', 'skewed distributions', 'computational effort', 'constraint theory', 'machine tools', 'optimisation', 'production control', 'tolerance analysis']), ('A universal decomposition of the integration range for exponential functions\nThe problem of determining the independent constants for decomposition of the\nintegration range of exponential functions was solved on the basis of a\nsimilar approach to polynomials. The constants obtained enable one to\ndecompose the integration range in two so that the integrals over them\nare equal independently of the function parameters. For the\nnontrigonometrical polynomials of even functions, an alternative\napproach was presented\n', ['integration range universal decomposition', 'exponential functions', 'polynomials', 'integration range decomposition', 'nontrigonometrical polynomials', 'even functions', 'exponential distribution', 'integration']), ('Product development: using a 3D computer model to optimize the stability of the\nRocket TM powered wheelchair\nA three-dimensional (3D) lumped-parameter model of a powered wheelchair was\ncreated to aid the development of the Rocket prototype wheelchair and\nto help explore the effect of innovative design features on its\nstability. The model was developed using simulation software,\nspecifically Working Model 3D. The accuracy of the model was determined\nby comparing both its static stability angles and dynamic behavior as\nit passed down a 4.8-cm (1.9") road curb at a heading of 45 degrees\nwith the performance of the actual wheelchair. The model\'s predictions\nof the static stability angles in the forward, rearward, and lateral\ndirections were within 9.3, 7.1, and 3.8% of the measured values,\nrespectively. The average absolute error in the predicted position of\nthe wheelchair as it moved down the curb was 2.2 cm/m (0.9" per 3\'3")\ntraveled. The accuracy was limited by the inability to model soft\nbodies, the inherent difficulties in modeling a statically\nindeterminate system, and the computing time. Nevertheless, it was\nfound to be useful in investigating the effect of eight design\nalterations on the lateral stability of the wheelchair. Stability was\nquantified by determining the static lateral stability angles and the\nmaximum height of a road curb over which the wheelchair could\nsuccessfully drive on a diagonal heading. The model predicted that the\nstability was more dependent on the configuration of the suspension\nsystem than on the dimensions and weight distribution of the\nwheelchair. Furthermore, for the situations and design alterations\nstudied, predicted improvements in static stability were not correlated\nwith improvements in dynamic stability\n', ['3D computer model', 'product development', 'innovative design features', 'suspension system configuration', 'dynamic stability improvements', 'average absolute error', 'predicted position', 'soft bodies modeling', 'statically indeterminate system', 'computing time', 'design alterations effect', 'diagonal heading', 'weight distribution', 'Rocket TM powered wheelchair', '4.8 cm', 'digital simulation', 'handicapped aids', 'mechanical stability', 'medical computing']), ('Using latent semantic analysis to assess reader strategies\nWe tested a computer-based procedure for assessing reader strategies that was\nbased on verbal protocols that utilized latent semantic analysis (LSA).\nStudents were given self-explanation-reading training (SERT), which\nteaches strategies that facilitate self-explanation during reading,\nsuch as elaboration based on world knowledge and bridging between text\nsentences. During a computerized version of SERT practice, students\nread texts and typed self-explanations into a computer after each\nsentence. The use of SERT strategies during this practice was assessed\nby determining the extent to which students used the information in the\ncurrent sentence versus the prior text or world knowledge in their\nself-explanations. This assessment was made on the basis of human\njudgments and LSA. Both human judgments and LSA were remarkably similar\nand indicated that students who were not complying with SERT tended to\nparaphrase the text sentences, whereas students who were compliant with\nSERT tended to explain the sentences in terms of what they knew about\nthe world and of information provided in the prior text context. The\nsimilarity between human judgments and LSA indicates that LSA will be\nuseful in accounting for reading strategies in a Web-based version of\nSERT\n', ['latent semantic analysis', 'reader strategy assessment', 'computer-based procedure', 'verbal protocols', 'self-explanation-reading training', 'elaboration', 'world knowledge', 'text sentence bridging', 'human judgments', 'computer aided instruction', 'psychology']), ('Even unimodular Gaussian lattices of rank 12\nWe classify even unimodular Gaussian lattices of rank 12, that is, even\nunimodular integral lattices of rank 12 over the ring of Gaussian\nintegers. This is equivalent to the classification of the automorphisms\ntau with tau /sup 2/=-1 in the automorphism groups of all the Niemeier\nlattices, which are even unimodular (real) integral lattices of rank\n24. There are 28 even unimodular Gaussian lattices of rank 12 up to\nequivalence\n', ['even unimodular Gaussian lattices', 'even unimodular integral lattices', 'Gaussian integers', 'automorphisms', 'Niemeier lattices', 'equivalence classes', 'number theory']), ('Attribute generation based on association rules\nA decision tree is considered to be appropriate (1) if the tree can classify\nthe unseen data accurately, and (2) if the size of the tree is small.\nOne of the approaches to induce such a good decision tree is to add new\nattributes and their values to enhance the expressiveness of the\ntraining data at the data pre-processing stage. There are many existing\nmethods for attribute extraction and construction, but constructing new\nattributes is still an art. These methods are very time consuming, and\nsome of them need a priori knowledge of the data domain. They are not\nsuitable for data mining dealing with large volumes of data. We propose\na novel approach that the knowledge on attributes relevant to the class\nis extracted as association rules from the training data. The new\nattributes and the values are generated from the association rules\namong the originally given attributes. We elaborate on the method and\ninvestigate its feature. The effectiveness of our approach is\ndemonstrated through some experiments\n', ['attribute generation', 'association rules', 'decision tree', 'training data', 'attribute extraction', 'data mining', 'large database', 'experiments', 'data mining', 'decision trees', 'learning (artificial intelligence)', 'very large databases']), ('OS porting and application development for SoC\nTo deliver improved usability in high-end portable consumer products, the use\nof an appropriate consumer operating system (OS) is becoming far more\nwidespread. Using a commercially supported OS also vastly increases the\navailability of supported applications. For the device developer, this\ntrend adds major complexity to the problem of system implementation.\nPorting a complete operating system to a new hardware design adds\nsignificantly to the development burden, increasing both time-to-market\nand expense. Even for those familiar with the integration of a\nreal-time OS, the porting, validation and support of a complex platform\nOS is a formidable task\n', ['OS porting', 'application development', 'consumer operating system', 'hardware design', 'operating systems (computers)', 'software portability']), ("The XML typechecking problem\nWhen an XML document conforms to a given type (e.g. a DTD or an XML schema\ntype) it is called a valid document. Checking if a given XML document\nis valid is called the validation problem, and is typically performed\nby a parser (hence, validating parser), more precisely it is performed\nright after parsing, by the same program module. In practice however,\nXML documents are often generated dynamically, by some program:\nchecking whether all XML documents generated by the program are valid\nWRT a given type is called the typechecking problem. While a validation\nanalyzes an XML document, a type checker analyzes a program, and the\nproblem's difficulty is a function of the language in which that\nprogram is expressed. The XML typechecking problem has been\ninvestigated recently and the XQuery Working Group adopted some of\nthese techniques for typechecking XQuery. All these techniques,\nhowever, have limitations which need to be understood and further\nexplored and investigated. We define the XML typechecking problem, and\npresent current approaches to typechecking, discussing their\nlimitations\n", ['XML typechecking problem', 'XML document', 'validation', 'XQuery', 'hypermedia markup languages']), ('Control of combustion processes in an internal combustion engine by\nlow-temperature plasma\nA new method of operation of internal combustion engines enhances power and\nreduces fuel consumption and exhaust toxicity. Low-temperature plasma\ncontrol combines working processes of thermal engines and steam\nmachines into a single process\n', ['combustion processes', 'internal combustion engine', 'low-temperature plasma', 'fuel consumption', 'exhaust toxicity', 'working processes', 'thermal engines', 'steam machines', 'heat transfer', 'internal combustion engines']), ('Too much middleware\nThe movement from client-server computing to multi-tier computing has created a\npotpourri of so-called middleware systems, including application\nservers, workflow products, EAI systems, ETL systems and federated data\nsystems. We argue that the explosion in middleware has created a myriad\nof poorly integrated systems with overlapping functionality. The world\nwould be well served by considerable consolidation, and we present some\nof the ways this might happen. Some of the points covered in the\narticle have been previously explored by P. Bernstein (1996)\n', ['client-server computing', 'multi-tier computing', 'middleware systems', 'application servers', 'workflow products', 'EAI systems', 'ETL systems', 'federated data systems', 'poorly integrated systems', 'overlapping functionality', 'application program interfaces', 'client-server systems', 'distributed databases', 'distributed object management', 'workflow management software']), ("The year of the racehorse [China Telecom]\nDoes China really offer the telecoms industry a route out of the telecoms\nslump? According to the Chinese government it has yet to receive a\nsingle application from foreign companies looking to invest in the\ncountry's domestic telecoms sector since the country joined the World\nTrade Organisation\n", ['China', 'telecoms industry', 'foreign investment', 'China Telecom', 'China Netcom', 'China Unicorn', 'telecommunication']), ('STEM: Secure Telephony Enabled Middlebox\nDynamic applications, including IP telephony, have not seen wide acceptance\nwithin enterprises because of problems caused by the existing network\ninfrastructure. Static elements, including firewalls and network\naddress translation devices, are not capable of allowing dynamic\napplications to operate properly. The Secure Telephony Enabled\nMiddlebox (STEM) architecture is an enhancement of the existing network\ndesign to remove the issues surrounding static devices. The\narchitecture incorporates an improved firewall that can interpret and\nutilize information in the application layer of packets to ensure\nproper functionality. In addition to allowing dynamic applications to\nfunction normally, the STEM architecture also incorporates several\ndetection and response mechanisms for well-known network-based\nvulnerabilities. This article describes the key components of the\narchitecture with respect to the SIP protocol\n', ['Secure Telephony Enabled Middlebox', 'STEM', 'IP telephony', 'network infrastructure', 'firewalls', 'network address translation devices', 'dynamic applications', 'STEM architecture', 'network design', 'static devices', 'application layer', 'detection mechanisms', 'response mechanisms', 'network-based vulnerabilities', 'SIP protocol', 'authorisation', 'Internet telephony', 'protocols', 'telecommunication security']), ('Optical recognition of three-dimensional objects with scale invariance using a\nclassical convergent correlator\nWe present a real-time method for recognizing three-dimensional (3-D) objects\nwith scale invariance. The 3-D information of the objects is codified\nin deformed fringe patterns using the Fourier transform profilometry\ntechnique and is correlated using a classical convergent correlator.\nThe scale invariance property is achieved using two different\napproaches: the Mellin radial harmonic decomposition and the\nlogarithmic radial harmonic filter. Thus, the method is invariant for\nchanges in the scale of the 3-D target within a defined interval of\nscale factors. Experimental results show the utility of the proposed\nmethod\n', ['optical recognition', '3D object recognition', 'scale invariance', 'classical convergent correlator', 'real-time method', '3-D information', 'deformed fringe patterns', 'Fourier transform profilometry technique', 'scale invariance property', 'Mellin radial harmonic decomposition', 'logarithmic radial harmonic filter', 'invariant', 'scale factors', 'CCD image sensors', 'Fourier transform optics', 'invariance', 'object recognition', 'optical correlation', 'spatial filters', 'spatial light modulators', 'surface topography measurement']), ('In medias res [DVD formats]\nFour years in the making, the DVD format war rages on, no winner insight.\nmeanwhile, the spoils of war abound, and DVD media manufacturers stand\npoised to profit\n', ['DVD media manufacturers', 'DVD format war', 'DVD-RAM', 'DVD+RW', 'DVD+R', 'DVD-RW', 'DVD-R', 'compatibility', 'writable DVD', 'optical disc storage', 'video discs']), ('Binocular model for figure-ground segmentation in translucent and occluding\nimages\nA Fourier-based solution to the problem of figure-ground segmentation in short\nbaseline binocular image pairs is presented. Each image is modeled as\nan additive composite of two component images that exhibit a spatial\nshift due to the binocular parallax. The segmentation is accomplished\nby decoupling each Fourier component in one of the resultant additive\nimages into its two constituent phasors, allocating each to its\nappropriate object-specific spectrum, and then reconstructing the\nforeground and background using the inverse Fourier transform. It is\nshown that the foreground and background shifts can be computed from\nthe differences of the magnitudes and phases of the Fourier transform\nof the binocular image pair. While the model is based on translucent\nobjects, it also works with occluding objects\n', ['binocular model', 'figure-ground segmentation', 'translucent images', 'occluding images', 'images', 'image segmentation', 'Fourier-based solution', 'short baseline binocular image pairs', 'component images', 'spatial shift', 'binocular parallax', 'Fourier component decoupling', 'phasors', 'object-specific spectrum', 'foreground', 'background', 'inverse Fourier transform', 'binocular image pair', 'translucent objects', 'occluding objects', 'Fourier transform optics', 'image segmentation', 'inverse problems']), ('Processing of complexly shaped multiply connected domains in finite element\nmesh generation\nLarge number of finite element models in modern materials science and\nengineering is defined on complexly shaped domains, quite often\nmultiply connected. Generation of quality finite element meshes on such\ndomains, especially in cases when the mesh must be 100% quadrilateral,\nis highly problematic. This paper describes mathematical fundamentals\nand practical -implementation of a powerful method and algorithm\nallowing transformation of multiply connected domains of arbitrary\ngeometrical complexity into a set of simple domains; the latter can\nthen be processed by broadly available finite element mesh generators.\nThe developed method was applied to a number of complex geometries,\nincluding those arising in analysis of parasitic inductances and\ncapacitances in printed circuit boards. The quality of practical\nresults produced by the method and its programming implementation\nprovide evidence that the algorithm can be applied to other finite\nelement models with various physical backgrounds\n', ['finite element mesh generation', 'complexly shaped multiply connected domains', 'finite element models', 'quadrilateral mesh', 'domains transformation', 'set of simple domains', 'parasitic inductances', 'parasitic capacitances', 'printed circuit boards', 'programming implementation', 'arbitrary geometrical complexity', 'metal forming processes', 'structural engineering models', 'iterative basis', 'general domain subdivision algorithm', 'artificial cut', 'automatic step calculation', 'computational complexity', 'iterative methods', 'mesh generation', 'modelling']), ('Optical encoding of color three-dimensional correlation\nThree-dimensional (3D) correlation of color images, considering the color\ndistribution as the third dimension, has been shown to be useful for\ncolor pattern recognition tasks. Nevertheless, 3D correlation cannot be\ndirectly performed on an optical correlator, that can only process\ntwo-dimensional (2D) signals. We propose a method to encode 3D\nfunctions onto 2D ones in such a way that the Fourier transform and\ncorrelation of these signals, that can be optically performed, encode\nthe 3D Fourier transform and correlation of the 3D signals. The theory\nfor the encoding is given and experimental results obtained in an\noptical correlator are shown\n', ['optical encoding', 'color three-dimensional correlation', '3D correlation', 'color images', 'color distribution', 'color pattern recognition tasks', 'optical correlator', '3D function encoding', 'Fourier transform', '3D Fourier transform', 'encoding', 'Fourier transform optics', 'image coding', 'image colour analysis', 'image recognition', 'optical correlation']), ('Current-mode fully-programmable piece-wise-linear block for neuro-fuzzy\napplications\nA new method to implement an arbitrary piece-wise-linear characteristic in\ncurrent mode is presented. Each of the breaking points and each slope\nis separately controllable. As an example a block that implements an\nN-shaped piece-wise-linearity has been designed. The N-shaped block\noperates in the subthreshold region and uses only ten transistors.\nThese characteristics make it especially suitable for large arrays of\nneuro-fuzzy systems where the number of transistors and power\nconsumption per cell is an important concern. A prototype of this block\nhas been fabricated in a 0.35 mu m CMOS technology. The functionality\nand programmability of this circuit has been verified through\nexperimental results\n', ['arbitrary piece-wise-linear characteristic', 'current mode', 'breaking points', 'separately controllable', 'N-shaped piece-wise-linearity', 'VLSI', 'subthreshold region', 'neuro-fuzzy systems', 'power consumption', 'CMOS', '0.35 micron', 'CMOS analogue integrated circuits', 'current-mode circuits', 'fuzzy neural nets', 'low-power electronics', 'neural chips', 'piecewise linear techniques', 'VLSI']), ('On deciding stability of constrained homogeneous random walks and queueing\nsystems\nWe investigate stability of scheduling policies in queueing systems. To this\nday no algorithmic characterization exists for checking stability of a\ngiven policy in a given queueing system. In this paper we introduce a\ncertain generalized priority policy and prove that the stability of\nthis policy is algorithmically undecidable. We also prove that\nstability of a homogeneous random walk in L/sub +//sup d/ is\nundecidable. Finally, we show that the problem of computing a fluid\nlimit of a queueing system or of a constrained homogeneous random walk\nis undecidable. To the best of our knowledge these are the first\nundecidability results in the area of stability of queueing systems and\nrandom walks in L/sub +//sup d/. We conjecture that stability of common\npolicies like First-In-First-Out and priority policy is also an\nundecidable problem\n', ['queueing systems', 'scheduling policy stability', 'constrained homogeneous random walks', 'generalized priority policy', 'homogeneous random walk stability', 'fluid limit computation', 'undecidability results', 'first-in-first-out policy', 'priority policy', 'undecidable problem', 'decidability', 'queueing theory', 'scheduling']), ('Acquisitions in the James Ford Bell Library\nThis article presents basic acquisitions philosophy and approaches in a noted\nspecial collection, with commentary on "just saying no" and on how the\nelectronic revolution has changed the acquisition of special\ncollections materials\n', ['James Ford Bell Library', 'library acquisitions philosophy', 'out-of-print books', 'University library', 'special collections', 'electronic revolution', 'academic libraries', 'library automation']), ('Application of ultrasonic sensors in the process industry\nContinuous process monitoring in gaseous, liquid or molten media is a\nfundamental requirement for process control. Besides temperature and\npressure other process parameters such as level, flow, concentration\nand conversion are of special interest. More qualified information\nobtained from new or better sensors can significantly enhance the\nprocess quality and thereby product properties. Ultrasonic sensors or\nsensor systems can contribute to this development. The state of the art\nof ultrasonic sensors and their advantages and disadvantages will be\ndiscussed. Commercial examples will be presented. Among others,\napplications in the food, chemical and pharmaceutical industries are\ndescribed. Possibilities and limitations of ultrasonic process sensors\nare discussed\n', ['ultrasonic sensors application', 'process industry', 'continuous process monitoring', 'process control', 'process quality', 'food industries', 'chemical industries', 'pharmaceutical industries', 'acoustic microsensors', 'ultrasonic measurements', 'ultrasonic attenuation', 'acoustic impedance', 'temperature measurement', 'pressure measurement', 'level measurement', 'distance measurement', 'flow measurement', 'chemical industry', 'distance measurement', 'flow measurement', 'flowmeters', 'food processing industry', 'level measurement', 'microsensors', 'pharmaceutical industry', 'pressure measurement', 'process control', 'process monitoring', 'reviews', 'temperature measurement', 'ultrasonic applications', 'ultrasonic measurement']), ('Resonant controllers for smart structures\nIn this paper we propose a special type of colocated feedback controller for\nsmart structures. The controller is a parallel combination of high-Q\nresonant circuits. Each of the resonant circuits is tuned to a pole (or\nthe resonant frequency) of the smart structure. It is proven that the\nparallel combination of resonant controllers is stable with an infinite\ngain margin. Only one set of actuator-sensor can damp multiple resonant\nmodes with the resonant controllers. Experimental results are presented\nto show the robustness of the proposed controller in damping multimode\nresonances\n', ['feedback controller', 'smart structures', 'high-Q resonant circuits', 'resonant frequency', 'smart structure', 'actuator-sensor', 'multiple resonant modes', 'damping', 'multimode resonances', 'laminate beam', 'circuit feedback', 'closed loop systems', 'control system synthesis', 'controllers', 'damping', 'intelligent actuators', 'intelligent control', 'intelligent structures', 'piezoelectric actuators', 'resonators', 'stability']), ('A stochastic averaging approach for feedback control design of nonlinear\nsystems under random excitations\nThis paper presents a method for designing and quantifying the performance of\nfeedback stochastic controls for nonlinear systems. The design makes\nuse of the method of stochastic averaging to reduce the dimension of\nthe state space and to derive the Ito stochastic differential equation\nfor the response amplitude process. The moment equation of the\namplitude process closed by the Rayleigh approximation is used as a\nmeans to characterize the transient performance of the feedback\ncontrol. The steady state and transient response of the amplitude\nprocess are used as the design criteria for choosing the feedback\ncontrol gains. Numerical examples are studied to demonstrate the\nperformance of the control\n', ['stochastic averaging', 'feedback control', 'nonlinear systems', 'steady state', 'transient response', 'random excitations', 'feedback stochastic controls', 'Ito stochastic differential equation', 'Rayleigh approximation', 'nonlinear control systems', 'state feedback', 'stochastic systems']), ('Supersampling multiframe blind deconvolution resolution enhancement of adaptive\noptics compensated imagery of low earth orbit satellites\nWe describe a postprocessing methodology for reconstructing undersampled image\nsequences with randomly varying blur that can provide image enhancement\nbeyond the sampling resolution of the sensor. This method is\ndemonstrated on simulated imagery and on\nadaptive-optics-(AO)-compensated imagery taken by the Starfire Optical\nRange 3.5-m telescope that has been artificially undersampled. Also\nshown are the results of multiframe blind deconvolution of some of the\nhighest quality optical imagery of low earth orbit satellites collected\nwith a ground-based telescope to date. The algorithm used is a\ngeneralization of multiframe blind deconvolution techniques that\ninclude a representation of spatial sampling by the focal plane array\nelements based on a forward stochastic model. This generalization\nenables the random shifts and shape of the AO-compensated point spread\nfunction (PSF) to be used to partially eliminate the aliasing effects\nassociated with sub-Nyquist sampling of the image by the focal plane\narray. The method could be used to reduce resolution loss that occurs\nwhen imaging in wide-field-of-view (FOV) modes\n', ['supersampling multiframe blind deconvolution resolution enhancement', 'adaptive optics compensated imagery', 'low earth orbit satellites', 'postprocessing methodology', 'undersampled image sequence reconstruction', 'randomly varying blur', 'image enhancement', 'sensor sampling resolution', 'simulated imagery', 'Starfire Optical Range telescope', 'multiframe blind deconvolution', 'ground-based telescope', 'spatial sampling', 'focal plane array elements', 'forward stochastic model', 'random shifts', 'AO-compensated point spread function', 'aliasing effects', 'sub-Nyquist sampling', 'resolution loss', 'wide-field-of-view modes', '3.5 m', 'adaptive optics', 'astronomical techniques', 'astronomical telescopes', 'astronomy computing', 'deconvolution', 'focal planes', 'image enhancement', 'image reconstruction', 'image resolution', 'image sampling', 'image sequences', 'optical transfer function']), ('Multidimensional data visualization\nHistorically, data visualization has been limited primarily to two dimensions\n(e.g., histograms or scatter plots). Available software packages (e.g.,\nData Desk 6.1, MatLab 6.1, SAS-JMP 4.04, SPSS 10.0) are capable of\nproducing three-dimensional scatter plots with (varying degrees of)\nuser interactivity. We constructed our own data visualization\napplication with the Visualization Toolkit (Schroeder et al., 1998) and\nTcl/Tk to display multivariate data through the application of glyphs\n(Ware, 2000). A glyph is a visual object onto which many data\nparameters may be mapped, each with a different visual attribute (e.g.,\nsize or color). We used our multi-dimensional data viewer to explore\ndata from several psycholinguistic experiments. The graphical interface\nprovides flexibility when users dynamically explore the\nmultidimensional image rendered from raw experimental data. We\nhighlight advantages of multidimensional data visualization and\nconsider some potential limitations\n', ['multidimensional data visualization', '3D scatter plots', 'user interactivity', 'Visualization Toolkit', 'Tcl/Tk', 'multivariate data display', 'glyphs', 'visual object', 'data parameters', 'visual attribute', 'multi-dimensional data viewer', 'psycholinguistic experiments', 'graphical interface', 'multidimensional image rendering', 'data visualisation', 'psychology', 'rendering (computer graphics)']), ('A scalable model of cerebellar adaptive timing and sequencing: the recurrent\nslide and latch (RSL) model\nFrom the dawn of modern neural network theory, the mammalian cerebellum has\nbeen a favored object of mathematical modeling studies. Early studies\nfocused on the fanout, convergence, thresholding, and learned weighting\nof perceptual-motor signals within the cerebellar cortex. This led to\nthe still viable idea that the granule cell stage in the cerebellar\ncortex performs a sparse expansive recoding of the time-varying input\nvector. This recoding reveals and emphasizes combinations in a\ndistributed representation that serves as a basis for the learned,\nstate-dependent control actions engendered by cerebellar outputs to\nmovement related centers. To make optimal use of available signals, the\ncerebellum must be able to sift the evolving state representation for\nthe most reliable predictors of the need for control actions, and to\nuse those predictors even if they appear only transiently and well in\nadvance of the optimal time for initiating the control action. The\npaper proposes a modification to prior, population, models for\ncerebellar adaptive timing and sequencing. Since it replaces a\npopulation with a single element, the proposed RSL model is in one\nsense maximally efficient, and therefore optimal from the perspective\nof scalability\n', ['scalable model', 'cerebellar adaptive timing', 'cerebellar sequencing', 'recurrent slide and latch model', 'neural network theory', 'mammalian cerebellum', 'granule cell stage', 'sparse expansive recoding', 'time-varying input vector', 'distributed representation', 'recurrent network', 'brain models', 'recurrent neural nets', 'timing']), ('Establishing the discipline of physics-based CMP modeling\nFor the past decade, a physically based comprehensive process model for\nchemical mechanical polishing has eluded the semiconductor industry.\nHowever, a long-term collaborative effort has now resulted in a\nworkable version of that approach. The highly fundamental model is\nbased on advanced finite element analysis and is beginning to show\npromise in CMP process development\n', ['chemical mechanical polishing', 'CMP', 'physically based process model', 'finite element analysis', 'CMP process development', 'chemical mechanical polishing', 'finite element analysis', 'semiconductor process modelling']), ("Mount Sinai Hospital uses integer programming to allocate operating room time\nAn integer-programming model and a post-solution heuristic allocates operating\nroom time to the five surgical divisions at Toronto's Mount Sinai\nHospital. The hospital has used this approach for several years and\ncredits it with both administrative savings and the ability to produce\nquickly an equitable master surgical schedule\n", ['Mount Sinai Hospital', 'integer programming', 'operating room time allocation', 'Toronto', 'Ontario', 'Canada', 'post-solution heuristic', 'health care', 'heuristic programming', 'integer programming', 'resource allocation', 'surgery']), ('Efficient transitive closure reasoning in a combined class/part/containment\nhierarchy\nClass hierarchies form the backbone of many implemented knowledge\nrepresentation and reasoning systems. They are used for inheritance,\nclassification and transitive closure reasoning. Part hierarchies are\nalso important in artificial intelligence. Other hierarchies, e.g.\ncontainment hierarchies, have received less attention in artificial\nintelligence. This paper presents an architecture and an implementation\nof a hierarchy reasoner that integrates a class hierarchy, a part\nhierarchy, and a containment hierarchy into one structure. In order to\nmake an implemented reasoner useful, it needs to operate at least at\nspeeds comparable to human reasoning. As real-world hierarchies are\nalways large, special techniques need to be used to achieve this. We\nhave developed a set of parallel algorithms and a data representation\ncalled maximally reduced tree cover for that purpose. The maximally\nreduced tree cover is an improvement of a materialized transitive\nclosure representation which has appeared in the literature. Our\nexperiments with a medical vocabulary show that transitive closure\nreasoning for combined class/part/containment hierarchies in near\nconstant time is possible for a fixed hardware configuration\n', ['transitive closure reasoning', 'knowledge representation', 'class hierarchy', 'part hierarchy', 'containment hierarchy', 'parallel algorithms', 'data representation', 'maximally reduced tree cover', 'materialized transitive closure representation', 'experiments', 'medical vocabulary', 'fixed hardware configuration', 'parallel reasoning', 'inheritance', 'part hierarchies', 'artificial intelligence', 'classification', 'inference mechanisms', 'knowledge representation', 'medical computing', 'parallel algorithms', 'tree data structures', 'vocabulary']), ("Practice management goes remote [accounting]\nThere's a lot of life in accounting practice management software, a valuable\ncategory that has been subject to much change in the last few years.\nWeb-based time tracking grows in popularity. Looks at CCH ProSystem fx\nPractice, CMS Open Solutions 6, Creative Solutions Practice, Time\nMatters, CPASoftware Visual Practice Management, and Abak\n", ['accounting practice management software', 'Web-based time tracking', 'CCH ProSystem fx Practice', 'CMS Open Solutions 6', 'Creative Solutions Practice', 'Time Matters', 'CPASoftware Visual Practice Management', 'Abak', 'accounting', "buyer's guides", 'invoicing']), ("Collective action in the age of the Internet: mass communication and online\nmobilization\nThis article examines how the Internet transforms collective action. Current\npractices on the Web bear witness to thriving collective action ranging\nfrom persuasive to confrontational, individual to collective,\nundertakings. Even more influential than direct calls for action is the\nindirect mobilizing influence of the Internet's powers of mass\ncommunication, which is boosted by an antiauthoritarian ideology on the\nWeb. Theoretically, collective action through the otherwise socially\nisolating computer is possible because people rely on internalized\ngroup memberships and social identities to achieve social involvement.\nEmpirical evidence from an online survey among environmental activists\nand nonactivists confirms that online action is considered an\nequivalent alternative to offline action by activists and nonactivists\nalike. However, the Internet may slightly alter the motives underlying\ncollective action and thereby alter the nature of collective action and\nsocial movements. Perhaps more fundamental is the reverse influence\nthat successful collective action will have on the nature and function\nof the Internet\n", ['Internet', 'mass communication', 'online mobilization', 'collective action', 'World Wide Web', 'antiauthoritarian ideology', 'group memberships', 'social identities', 'online survey', 'anonymity', 'politics', 'human factors', 'information resources', 'Internet', 'politics', 'psychology', 'social aspects of automation']), ('To commit or not to commit: modeling agent conversations for action\nConversations are sequences of messages exchanged among interacting agents. For\nconversations to be meaningful, agents ought to follow commonly known\nspecifications limiting the types of messages that can be exchanged at\nany point in the conversation. These specifications are usually\nimplemented using conversation policies (which are rules of inference)\nor conversation protocols (which are predefined conversation\ntemplates). In this article we present a semantic model for specifying\nconversations using conversation policies. This model is based on the\nprinciples that the negotiation and uptake of shared social commitments\nentail the adoption of obligations to action, which indicate the\nactions that agents have agreed to perform. In the same way,\nobligations are retracted based on the negotiation to discharge their\ncorresponding shared social commitments. Based on these principles,\nconversations are specified as interaction specifications that model\nthe ideal sequencing of agent participations negotiating the execution\nof actions in a joint activity. These specifications not only specify\nthe adoption and discharge of shared commitments and obligations during\nan activity, but also indicate the commitments and obligations that are\nrequired (as preconditions) or that outlive a joint activity (as\npostconditions). We model the Contract Net Protocol as an example of\nthe specification of conversations in a joint activity\n', ['interacting agents', 'specifications', 'rules of inference', 'conversation protocols', 'autonomous agents', 'social commitments', 'speech acts', 'software agents', 'conversation templates', 'formal languages', 'software agents']), ('Intelligent optimal sieving method for FACTS device control in multi-machine\nsystems\nA multi-target oriented optimal control strategy for FACTS devices installed in\nmulti-machine power systems is presented in this paper, which is named\nthe intelligent optimal sieving control (IOSC) method. This new method\ndivides the FACTS device output region into several parts and selects\none typical value from each part, which is called output candidate.\nThen, an intelligent optimal sieve is constructed, which predicts the\nimpacts of each output candidate on a power system and sieves out an\noptimal output from all of the candidates. The artificial neural\nnetwork technologies and fuzzy methods are applied to build the\nintelligent sieve. Finally, the real control signal of FACTS devices is\ncalculated according to the selected optimal output through inverse\nsystem method. Simulation has been done on a three-machine power system\nand the results show that the proposed IOSC controller can effectively\nattenuate system oscillations and enhance the power system transient\nstability\n', ['FACTS', 'intelligent control', 'intelligent optimal sieving method', 'FACTS device control', 'multi-machine systems', 'multi-target oriented optimal control strategy', 'intelligent optimal sieve', 'artificial neural network technologies', 'fuzzy methods', 'control signal', 'selected optimal output', 'inverse system method', 'three-machine power system', 'system oscillations attenuation', 'power system transient stability enhancement', 'control system synthesis', 'flexible AC transmission systems', 'fuzzy set theory', 'intelligent control', 'neural nets', 'optimal control', 'power system control', 'predictive control']), ('Trading exchanges: online marketplaces evolve\nLooks at how trading exchanges are evolving rapidly to help manufacturers keep\nup with customer demand\n', ['online marketplaces', 'trading exchanges', 'manufacturers', 'customer demand', 'enterprise platforms', 'supply chain management', 'enterprise resource planning', 'core software platform', 'private exchanges', 'integration technology', 'middleware', 'XML standards', 'content management capabilities', 'manufacturing industries', 'marketing', 'workflow management software']), ('Cane railway scheduling via constraint logic programming: labelling order and\nconstraints in a real-life application\nIn Australia, cane transport is the largest unit cost in the manufacturing of\nraw sugar, making up around 35% of the total manufacturing costs.\nProducing efficient schedules for the cane railways can result in\nsignificant cost savings. The paper presents a study using constraint\nlogic programming (CLP) to solve the cane transport scheduling problem.\nTailored heuristic labelling order and constraints strategies are\nproposed and encouraging results of application to several test\nproblems and one real-life case are presented. The preliminary results\ndemonstrate that CLP can be used as an effective tool for solving the\ncane transport scheduling problem, with a potential decrease in\ndevelopment costs of the scheduling system. It can also be used as an\nefficient tool for rescheduling tasks which the existing cane transport\nscheduling system cannot perform well\n', ['cane railway scheduling', 'constraint logic programming', 'cane transport', 'raw sugar', 'total manufacturing costs', 'cost savings', 'heuristic labelling order', 'constraints strategies', 'agriculture', 'constraint handling', 'constraint theory', 'goods distribution', 'locomotives', 'scheduling']), ("Hilbert modular threefolds of arithmetic genus one\nD. Weisser (1981) proved that there are exactly four Galois cubic number fields\nwith Hilbert modular threefolds of arithmetic genus one. In this paper,\nwe extend Weisser's work to cover all cubic number fields. Our main\nresult is that there are exactly 33 fields with Hilbert modular\nthreefolds of arithmetic genus one. These fields are enumerated\nexplicitly\n", ['Hilbert modular threefolds', 'arithmetic genus one', 'Galois cubic number fields', 'Hilbert transforms', 'number theory']), ('Recognition of finite simple groups S/sub 4/(q) by their element orders\nIt is proved that among simple groups S/sub 4/(q) in the class of\nfinite-groups, only the groups S/sub 4/(3/sup n/), where n is an odd\nnumber greater than unity, are recognizable by a set of their element\norders. It is also shown that simple groups U/sub 3/(9), /sup 3/D/sub\n4/(2), G/sub 2/(4), S/sub 6/(3), F/sub 4/(2), and /sup 2/E/sub 6/(2)\nare recognizable, but L/sub 3/(3) is not\n', ['finite simple groups recognition', 'divisibility relation', 'element orders', 'group theory', 'process algebra']), ("Trusted...or...trustworthy: the search for a new paradigm for computer and\nnetwork security\nThis paper sets out a number of major questions and challenges which include:\n(a) just what is meant by `trusted' or `trustworthy' systems after 20\nyears of experience, or more likely, lack of business level experience,\nwith the 'trusted computer system' criteria anyway; (b) does anyone\nreally care about the adoption of international standards for computer\nsystem security evaluation by IT product and system manufacturers and\nsuppliers (IS 15408) and, if so, how does it all relate to business\nrisk management anyway (IS 17799); (c) with the explosion of adoption\nof the microcomputer and personal computer some 20 years ago, has the\nindustry abandoned all that it learnt about security during the\n`mainframe era'; or - `whatever happened to MULTICS' and its lessons;\n(d) has education kept up with security requirements by industry and\ngovernment alike in the need for safe and secure operation of large\nscale and networked information systems on national and international\nbases, particularly where Web or Internet-based information services\nare being proposed as the major `next best thing' in the IT industry;\n(e) has the `fourth generation' of computer professionals inherited the\nspirit of information systems management and control that resided by\nnecessity with the last `generation', the professionals who developed\nand created the applications for shared mainframe and minicomputer\nsystems?\n", ['computer security', 'network security', 'trusted systems', 'trustworthy systems', 'international standards', 'IS 15408', 'business risk management', 'IS 17799', 'IT manufacturers', 'microcomputer', 'personal computer', 'MULTICS', 'education', 'large scale information systems', 'Web', 'Internet-based information services', 'fourth generation computer professionals', 'information systems management', 'information systems control', 'computer network management', 'computer science education', 'information systems', 'Internet', 'microcomputers', 'risk management', 'security of data', 'standards', 'telecommunication security']), ('Non-nested multi-level solvers for finite element discretisations of mixed\nproblems\nWe consider a general framework for analysing the convergence of multi-grid\nsolvers applied to finite element discretisations of mixed problems,\nboth of conforming and nonconforming type. As a basic new feature. our\napproach allows to use different finite element discretisations on each\nlevel of the multi-grid hierarchy. Thus, in our multi-level approach,\naccurate higher order finite element discretisations can be combined\nwith fast multi-level solvers based on lower order (nonconforming)\nfinite element discretisations. This leads to the design of efficient\nmulti-level solvers for higher order finite element discretisations\n', ['non-nested multi-level solvers', 'finite element discretisations', 'mixed problems', 'multi-grid solvers', 'higher order finite element discretisations', 'multi-level solvers', 'differential equations', 'finite element analysis']), ('ERP systems implementation: Best practices in Canadian government organizations\nERP (Enterprise resource planning) systems implementation is a complex exercise\nin organizational innovation and change management. Government\norganizations are increasing their adoption of these systems for\nvarious benefits such as integrated real-time information, better\nadministration, and result-based management. Government organizations,\ndue to their social obligations, higher legislative and public\naccountability, and unique culture face many specific challenges in the\ntransition to enterprise systems. This motivated the authors to explore\nthe key considerations and typical activities in government\norganizations adopting ERP systems. The article adopts the innovation\nprocess theory framework as well as the (Markus & Tanis, 2000) model as\na basis to delineate the ERP adoption process. Although, each adopting\norganization has a distinct set of objectives for its systems, the\nstudy found many similarities in motivations, concerns, and strategies\nacross organizations\n', ['ERP systems implementation', 'Canadian government organizations', 'best practices', 'enterprise resource planning', 'integrated real-time information', 'administration', 'result-based management', 'social obligations', 'higher legislative accountability', 'public accountability', 'innovation process theory framework', 'client-server systems', 'government data processing', 'relational databases', 'systems re-engineering']), ('Modeling cutting temperatures for turning inserts with various tool geometries\nand materials\nTemperatures are of interest in machining because cutting tools often fail by\nthermal softening or temperature-activated wear. Many models for\ncutting temperatures have been developed, but these models consider\nonly simple tool geometries such as a rectangular slab with a sharp\ncorner. This report describes a finite element study of tool\ntemperatures in cutting that accounts for tool nose radius and included\nangle effects. A temperature correction factor model that can be used\nin the design and selection of inserts is developed to account for\nthese effects. A parametric mesh generator is used to generate the\nfinite element models of tool and inserts of varying geometries. The\nsteady-state temperature response is calculated using NASTRAN solver.\nSeveral finite element analysis (FEA) runs are performed to quantify\nthe effects of inserts included angle, nose radius, and materials for\nthe insert and the tool holder on the cutting temperature at the insert\nrake face. The FEA results are then utilized to develop a temperature\ncorrection factor model that accounts for these effects. The\ntemperature correction factor model is integrated with an analytical\ntemperature model for rectangular inserts to predict cutting\ntemperatures for contour turning with inserts of various shapes and\nnose radii. Finally, experimental measurements of cutting temperature\nusing the tool-work thermocouple technique are performed and compared\nwith the predictions of the new temperature model. The comparisons show\ngood agreement\n', ['cutting temperature model', 'turning inserts', 'machining', 'tool nose radius', 'parametric mesh generator', 'finite element models', 'temperature correction factor', 'insert shape effects', 'tool geometries', 'cutting', 'machine tools', 'machining', 'mesh generation', 'temperature distribution']), ('CRONE control: principles and extension to time-variant plants with\nasymptotically constant coefficients\nThe principles of CRONE control, a frequency-domain robust control design\nmethodology based on fractional differentiation, are presented.\nContinuous time-variant plants with asymptotically constant\ncoefficients are analysed in the frequency domain, through their\nrepresentation using time-variant frequency responses. A stability\ntheorem for feedback systems including time-variant plants with\nasymptotically constant coefficients is proposed. Finally, CRONE\ncontrol is extended to robust control of these plants\n', ['CRONE control', 'time-variant plants', 'asymptotically constant coefficients', 'frequency-domain robust control design', 'fractional differentiation', 'time-variant frequency responses', 'stability theorem', 'feedback systems', 'robust control', 'automatic control', 'asymptotic stability', 'differentiation', 'frequency response', 'robust control', 'time-varying systems']), ("A building block approach to automated engineering\nShenandoah Valley Electric Cooperative (SVEC, Mt. Crawford, Virginia, US)\nrecognized the need to automate engineering functions and create an\ninteractive model of its distribution system in the early 1990s. It had\nused Milsoft's DA software for more than 10 years to make engineering\nstudies, and had a Landis and Gyr SCADA system and a hybrid load\nmanagement system for controlling water heater switches. With the\ndevelopment of GIS and facilities management (FM) applications, SVEC\ndecided this should be the basis for an information system that would\nmodel its physical plant and interface with its accounting and billing\nsystems. It could add applications such as outage management, staking,\nline design and metering to use this information and interface with\nthese databases. However, based on SVEC's size it was not feasible to\nimplement a sophisticated and expensive GIS/FM system. Over the past\nnine years, SVEC has had success with a building block approach, and\nits customers and employees are realizing the benefits of the automated\napplications. This building block approach is discussed in this article\nincluding the GIS, outage management system, MapViewer and a staking\npackage. The lessons learned and future expansion are discussed\n", ['Shenandoah Valley Electric Cooperative', 'engineering functions automation', 'interactive model', 'distribution system', 'building block approach', 'billing systems', 'outage management', 'staking', 'line design', 'metering', 'databases', 'MapViewer', 'GIS', 'geographic information systems', 'management', 'power distribution control', 'power engineering computing']), ("Factors contributing to preservice teachers' discomfort in a Web-based course\nstructured as an inquiry\nA report is given of a qualitative emergent design study of a Science,\nTechnology, Society Interaction (STS) Web-enhanced course. Students'\ndiscomfort during the pilot test provided insight into the intellectual\nscaffolding that preservice secondary science teachers needed to\noptimize their performance when required to develop understanding\nthrough open-ended inquiry in a Web environment. Eight factors\nidentified contributed to student discomfort: computer skills, paradigm\nshifts, trust, time management, thinking about their own thinking,\nsystematic inquiry, self-assessment, and scientific discourse. These\nfactors suggested developing understanding through inquiry by\nconducting a self-designed, open-ended, systematic inquiry required\nautonomous learning involving metacognitive skills and time management\nskills. To the extent in which students either came into the course\nwith this scaffolding, or developed it during the course, they were\nsuccessful in learning about STS and its relationship to science\nteaching. Changes in the Web site made to accommodate learners' needs\nas they surfaced are described\n", ['preservice teacher discomfort', 'Web-based course', 'qualitative emergent design study', 'science technology society interaction course', 'Web-enhanced course', 'student discomfort', 'intellectual scaffolding', 'preservice secondary science teachers', 'open-ended inquiry', 'Web environment', 'computer skills', 'paradigm shifts', 'trust', 'time management', 'thinking', 'systematic inquiry', 'self-assessment', 'scientific discourse', 'autonomous learning', 'metacognitive skills', 'time management skills', 'STS', 'science teaching', 'educational computing', 'educational courses', 'human factors', 'information resources', 'natural sciences computing', 'teacher training', 'teaching', 'user interfaces']), ('Autofocus system for microscope\nA technique is developed for microscope autofocusing, which is called the\neccentric light beam approach with high resolution, wide focusing\nrange, and compact construction. The principle is described. The\ntheoretical formula of the eccentric light beam approach deduced can be\napplied not only to an object lens whose objective plane is just at the\nfocal plane, but also to an object lens whose objective plane is not at\nthe focal plane. The experimental setup uses a semiconductor laser\ndevice as the light source. The laser beam that enters into the\nmicroscope is eccentric with the main light axis. A defocused signal is\nacquired by a symmetrical silicon photocell for the change of the\nreflected light position caused by differential amplification and\nprocessed by a microprocessor. Then the electric signal is\npower-amplified and drives a dc motor, which moves a fine working\nplatform to an automatic focus of the microscope. The result of the\nexperiments shows a +or-0.1- mu m precision of autofocusing for a range\nof +or-500- mu m defocusing. The system has high reliability and can\nmeet the requirements of various accurate micro measurement systems\n', ['autofocus system', 'microscope autofocusing', 'eccentric light beam approach', 'object lens', 'objective plane', 'semiconductor laser', 'main light axis', 'defocused signal', 'symmetrical silicon photocell', 'reflected light position', 'differential amplification', 'microprocessor', 'power-amplified electric signal', 'dc motor', 'fine working platform', 'high reliability', 'micro measurement systems', 'lenses', 'micropositioning', 'optical focusing', 'optical microscopes']), ('Automated breath detection on long-duration signals using feedforward\nbackpropagation artificial neural networks\nA new breath-detection algorithm is presented, intended to automate the\nanalysis of respiratory data acquired during sleep. The algorithm is\nbased on two independent artificial neural networks (ANN/sub insp/ and\nANN/sub expi/) that recognize, in the original signal, windows of\ninterest where the onset of inspiration and expiration occurs.\nPostprocessing consists in finding inside each of these windows of\ninterest minimum and maximum corresponding to each inspiration and\nexpiration. The ANN/sub insp/ and ANN/sub expi/ correctly determine\nrespectively 98.0% and 98.7% of the desired windows, when compared with\n29 820 inspirations and 29 819 expirations detected by a human expert,\nobtained from three entire-night recordings. Postprocessing allowed\ndetermination of inspiration and expiration onsets with a mean\ndifference with respect to the same human expert of (mean +or- SD) 34\n+or- 71 ms for inspiration and 5 +or- 46 ms for expiration. The method\nproved to be effective in detecting the onset of inspiration and\nexpiration in full night continuous recordings. A comparison of five\nhuman experts performing the same classification task yielded that the\nautomated algorithm was undifferentiable from these human experts,\nfailing within the distribution of human expert results. Besides being\napplicable to adult respiratory volume data, the presented algorithm\nwas also successfully applied to infant sleep data, consisting of\nuncalibrated rib cage and abdominal movement recordings. A comparison\nwith two previously published algorithms for breath detection in\nrespiratory volume signal shows that the presented algorithm has a\nhigher specificity, while presenting similar or higher positive\npredictive values\n', ['respiratory movements', 'automated breath detection', 'postprocessing', 'inspiration', 'expiration', 'automated algorithm', 'human experts', 'entire-night recordings', 'uncalibrated rib cage', 'abdominal movement recordings', 'infant sleep data', 'adult respiratory volume data', 'long-duration signals', 'feedforward backpropagation artificial neural networks', '34 ms', '5 ms', 'backpropagation', 'feedforward neural nets', 'medical signal detection', 'pneumodynamics', 'sleep']), ('Tax forms: CD or not CD?\nThe move from CD to the Web looks unstoppable. Besides counting how many\nthousands of electronic tax forms they offer, vendors are rapidly\nmoving those documents to the Web\n', ['electronic tax forms', 'Web', 'ATX Forms Zillion Forms', 'CCH Perform Plus H', 'Kleinrock Forms Library Plus', 'Nelco LaserLibrarian II', 'RIA eForm', 'STF Services Superform', 'Universal Tax Systems Forms Complete', 'accounting', 'tax preparation']), ('Ant colony optimization and stochastic gradient descent\nWe study the relationship between the two techniques known as ant colony\noptimization (ACO) and stochastic gradient descent. More precisely, we\nshow that some empirical ACO algorithms approximate stochastic gradient\ndescent in the space of pheromones, and we propose an implementation of\nstochastic gradient descent that belongs to the family of ACO\nalgorithms. We then use this insight to explore the mutual\ncontributions of the two techniques\n', ['ant colony optimization', 'stochastic gradient descent', 'empirical ACO algorithms', 'pheromones', 'combinatorial optimization', 'heuristic', 'reinforcement learning', 'social insects', 'swarm intelligence', 'artificial life', 'local search algorithms', 'artificial life', 'heuristic programming', 'learning (artificial intelligence)', 'optimisation', 'search problems']), ('On fuzzy and probabilistic control charts\nIn this article, different procedures of constructing control charts for\nlinguistic data, based on fuzzy and probability theory, are discussed.\nThree sets of membership functions, with different degrees of\nfuzziness, are proposed for fuzzy approaches. A comparison between\nfuzzy and probability approaches, based on the Average Run Length and\nsamples under control, is conducted for real data. Contrary to the\nconclusions of Raz and Wang (1990) the choice of degree of fuzziness\naffected the sensitivity of control charts\n', ['linguistic data', 'fuzzy control charts', 'probabilistic control charts', 'control chart construction', 'membership functions', 'fuzziness degree', 'average run length', 'sensitivity', 'fuzzy subsets', 'porcelain products', 'fuzzy control', 'fuzzy set theory', 'probability', 'production control', 'sensitivity analysis']), ("Computational capacity of an odorant discriminator: the linear separability of\ncurves\nWe introduce and study an artificial neural network inspired by the\nprobabilistic receptor affinity distribution model of olfaction. Our\nsystem consists of N sensory neurons whose outputs converge on a single\nprocessing linear threshold element. The system's aim is to model\ndiscrimination of a single target odorant from a large number p of\nbackground odorants within a range of odorant concentrations. We show\nthat this is possible provided p does not exceed a critical value p/sub\nc/ and calculate the critical capacity alpha c=p/sub c//N. The critical\ncapacity depends on the range of concentrations in which the\ndiscrimination is to be accomplished. If the olfactory bulb may be\nthought of as a collection of such processing elements, each\nresponsible for the discrimination of a single odorant, our study\nprovides a quantitative analysis of the potential computational\nproperties of the olfactory bulb. The mathematical formulation of the\nproblem we consider is one of determining the capacity for linear\nseparability of continuous curves, embedded in a large-dimensional\nspace. This is accomplished here by a numerical study, using a method\nthat signals whether the discrimination task is realizable, together\nwith a finite-size scaling analysis\n", ['artificial neural network', 'receptor affinity distribution', 'olfaction', 'linear threshold element', 'sensory neurons', 'linear separability', 'odorant discriminator', 'chemioception', 'neural nets']), ("CRM: approaching zenith\nLooks at how manufacturers are starting to warm up to the concept of customer\nrelationship management. CRM has matured into what is expected to be\nbig business. As CRM software evolves to its second, some say third,\ngeneration, it's likely to be more valuable to holdouts in\nmanufacturing and other sectors\n", ['manufacturers', 'customer relationship management', 'CRM', 'manufacturing', 'management']), ('Robust output-feedback control for linear continuous uncertain state delayed\nsystems with unknown time delay\nThe state-delayed time often is unknown and independent of other variables in\nmost real physical systems. A new stability criterion for uncertain\nsystems with a state time-varying delay is proposed. Then, a robust\nobserver-based control law based on this criterion is constructed via\nthe sequential quadratic programming method. We also develop a\nseparation property so that the state feedback control law and observer\ncan be independently designed and maintain closed-loop system\nstability. An example illustrates the availability of the proposed\ndesign method\n', ['robust control', 'output-feedback control', 'linear continuous systems', 'uncertain systems', 'state delayed systems', 'time delay', 'state time-varying delay', 'observer-based control law', 'sequential quadratic programming', 'state feedback control law', 'closed-loop system stability', 'closed loop systems', 'continuous time systems', 'control theory', 'delays', 'linear systems', 'observers', 'quadratic programming', 'robust control', 'state feedback', 'time-varying systems', 'uncertain systems']), ('Chemical information based scaling of molecular descriptors: a universal\nchemical scale for library design and analysis\nScaling is a difficult issue for any analysis of chemical properties or\nmolecular topology when disparate descriptors are involved. To compare\nproperties across different data sets, a common scale must be defined.\nUsing several publicly available databases (ACD, CMC, MDDR, and NCI) as\na basis, we propose to define chemically meaningful scales for a number\nof molecular properties and topology descriptors. These chemically\nderived scaling functions have several advantages. First, it is\npossible to define chemically relevant scales, greatly simplifying\nsimilarity and diversity analyses across data sets. Second, this\napproach provides a convenient method for setting descriptor boundaries\nthat define chemically reasonable topology spaces. For example,\ndescriptors can be scaled so that compounds with little potential for\nbiological activity, bioavailability, or other drug-like\ncharacteristics are easily identified as outliers. We have compiled\nscaling values for 314 molecular descriptors. In addition the 10th and\n90th percentile values for each descriptor have been calculated for use\nin outlier filtering\n', ['universal chemical scale', 'library design', 'library analysis', 'chemical information based scaling', 'molecular descriptors', 'molecular topology', 'chemical properties', 'databases', 'diversity analyses', 'similarity analyses', 'data sets', 'descriptor boundaries', 'drug-like characteristics', 'biological activity', 'bioavailability', 'outliers', 'biology computing', 'chemistry computing', 'medical computing', 'medical information systems', 'molecular biophysics', 'pharmaceutical industry', 'scientific information systems']), ('Model predictive control helps to regulate slow processes-robust barrel\ntemperature control\nSlow temperature control is a challenging control problem. The problem becomes\neven more challenging when multiple zones are involved, such as in\nbarrel temperature control for extruders. Often, strict closed-loop\nperformance requirements (such as fast startup with no overshoot and\nmaintaining tight temperature control during production) are given for\nsuch applications. When characteristics of the system are examined, it\nbecomes clear that a commonly used proportional plus integral plus\nderivative (PID) controller cannot meet such performance specifications\nfor this kind of system. The system either will overshoot or not\nmaintain the temperature within the specified range during the\nproduction run. In order to achieve the required performance, a control\nstrategy that utilizes techniques such as model predictive control,\nautotuning, and multiple parameter PID is formulated. This control\nstrategy proves to be very effective in achieving the desired\nspecifications, and is very robust\n', ['model predictive control', 'slow processes regulation', 'robust barrel temperature control', 'extruders', 'autotuning', 'multiple parameter PID', 'extrusion', 'moulding', 'predictive control', 'robust control', 'temperature control', 'three-term control', 'tuning']), ('Variable structure intelligent control for PM synchronous servo motor drive\nThe variable structure control (VSC) of discrete time systems based on\nintelligent control is presented in this paper. A novel approach is\nproposed for the state estimation. A linear observer is firstly\ndesigned. Then a neural network is used for compensating uncertainty.\nThe parameter of the VSC scheme is adjusted online by a neural network.\nPractical operating results from a PM synchronous motor (PMSM)\nillustrate the effectiveness and practicability of the proposed\napproach\n', ['PM synchronous servo motor drive', 'variable structure intelligent control', 'control design', 'discrete time systems', 'state estimation', 'linear observer', 'neural network', 'uncertainty compensation', 'control performance', 'compensation', 'control system synthesis', 'discrete time systems', 'intelligent control', 'machine control', 'machine testing', 'neurocontrollers', 'permanent magnet motors', 'servomotors', 'state estimation', 'synchronous motor drives', 'uncertain systems', 'variable structure systems']), ('Internet infrastructure and the emerging information society: an appraisal of\nthe Internet backbone industry\nThis paper examines the real constraints to the expansion of all encumbering\nand all pervasive information technology in our contemporary society.\nPerhaps the U.S. Internet infrastructure is the most appropriate to\nexamine since it is U.S. technology that has led the world into the\nInternet age. In this context, this paper reviews the state of the U.S.\nInternet backbone that will lead us into information society of the\nfuture by facilitating massive data transmission\n', ['Internet infrastructure', 'Internet service providers', 'users', 'backbone companies', 'local telephone companies', 'Internet']), ("Isogenous of the elliptic curves over the rationals\nAn elliptic curve is a pair (E, O), where E is a smooth projective curve of\ngenus 1 and O is a point of E, called the point at infinity. Every\nelliptic curve can be given by a Weierstrass equation E : y/sup 2/ +\na/sub 1/xy + a/sub 3/y = x/sup 3/ + a/sub 2/x/sup 2/ + a/sub 4/x +\na/sub 6/. Let Q be the set of rationals. E is said to be defined over Q\nif the coefficients a/sub i/, i = 1, 2, 3, 4, 6 are rationals and O is\ndefined over Q. Let E/Q be an elliptic curve and let E(Q)/sub tors/ be\nthe torsion group of points of E defined over Q. The theorem of Mazur\nasserts that E(Q)/sub tors/ is one of the following 15 groups E(Q)/sub\ntors/ {Z/mZ, Z/mZ * Z/2mZ, m, = 1, 2, ..., 10, 12, m = 1, 2, 3, 4. We\nsay that an elliptic curve E'/Q is isogenous to the elliptic curve E if\nthere is an isogeny, i.e. a morphism phi : E to E' such that phi (O) =\nO, where O is the point at infinity. We give an explicit model of all\nelliptic curves for which E(Q)/sub tors/ is in the form Z/mZ where m =\n9,10,12 or Z/2Z * Z/2mZ where m = 4, according to Mazur's theorem.\nMoreover, for every family of such elliptic curves, we give an explicit\nmodel of all their isogenous curves with cyclic kernels consisting of\nrational points\n", ['elliptic curves isogenous', 'rationals', 'smooth projective curve', 'Weierstrass equation', 'explicit model', "Mazur's theorem", 'cyclic kernels', 'computational geometry', 'graph theory']), ("Three-dimensional global MHD simulation code for the Earth's magnetosphere\nusing HPF/JA\nWe have translated a three-dimensional magnetohydrodynamic (MHD) simulation\ncode of the Earth's magnetosphere from VPP Fortran to HPF/JA on the\nFujitsu VPP5000/56 vector-parallel supercomputer and the MHD code was\nfully vectorized and fully parallelized in VPP Fortran. The entire\nperformance and capability of the HPF MHD code could be shown to be\nalmost comparable to that of VPP Fortran. A three-dimensional global\nMHD simulation of the Earth's magnetosphere was performed at a speed of\nover 400 Gflops with an efficiency of 76.5% using 56 processing\nelements of the Fujitsu VPP5000/56 in vector and parallel computation\nthat permitted comparison with catalog values. We have concluded that\nfluid and MHD codes that are fully vectorized and fully parallelized in\nVPP Fortran can be translated with relative ease to HPF/JA, and a code\nin HPF/JA may be expected to perform comparably to the same code\nwritten in VPP Fortran\n", ['magnetohydrodynamic simulation', 'vector-parallel supercomputer', 'Fujitsu VPP5000/56', 'HPF MHD code', 'MHD simulation', 'parallel computation', 'digital simulation', 'FORTRAN', 'geophysics computing', 'magnetohydrodynamics', 'parallel programming', 'solar wind']), ("The semi-algebraic theory of stochastic games\nThe asymptotic behavior of the min-max value of a finite-state zero-sum\ndiscounted stochastic game, as the discount rate approaches 0, has been\nstudied in the past using the theory of real-closed fields. We use the\ntheory of semi-algebraic sets and mappings to prove some asymptotic\nproperties of the min-max value, which hold uniformly for all\nstochastic games in which the number of states and players' actions are\npredetermined to some fixed values. As a corollary, we prove a uniform\npolynomial convergence rate of the value of the N-stage game to the\nvalue of the nondiscount game, over a bounded set of payoffs\n", ['finite-state zero-sum discounted stochastic game', 'semi-algebraic set theory', 'asymptotic behavior', 'min-max value', 'discount rate', 'uniform polynomial convergence rate', 'N-stage game', 'two-player zero-sum finite-state stochastic games', 'convergence', 'dynamic programming', 'minimax techniques', 'polynomials', 'set theory', 'stochastic games']), ('Observer-based strict positive real (SPR) feedback control system design\nPresents theory for stability analysis and design for a class of observer-based\nfeedback control systems. Relaxation of the controllability and\nobservability conditions imposed in the Yakubovich-Kalman-Popov lemma\ncan be made for a class of nonlinear systems described by a linear\ntime-invariant system with a feedback-connected cone-bounded nonlinear\nelement. It is shown how a circle-criterion approach can be used to\ndesign an observer-based state feedback control which yields a\nclosed-loop system with specified robustness characteristics. The\napproach is relevant for design with preservation of stability when a\ncone-bounded nonlinearity is introduced in the feedback loop. Important\napplications are to be found in nonlinear control with high robustness\nrequirements\n', ['observer-based strict positive real feedback control system', 'control system design', 'stability analysis', 'Yakubovich-Kalman-Popov lemma', 'nonlinear systems', 'linear time-invariant system', 'feedback-connected cone-bounded nonlinear element', 'circle-criterion approach', 'state feedback control', 'closed-loop system', 'robustness characteristics', 'cone-bounded nonlinearity', 'closed loop systems', 'control nonlinearities', 'control system synthesis', 'linear systems', 'nonlinear control systems', 'observers', 'robust control', 'stability criteria', 'state feedback']), ('An algorithm to generate all spanning trees with flow\nSpanning tree enumeration in undirected graphs is an important issue and task\nin many problems encountered in computer network and circuit analysis.\nThis paper discusses the spanning tree with flow for the case that\nthere are flow requirements between each node pair. An algorithm based\non minimal paths (MPs) is proposed to generate all spanning trees\nwithout flow. The proposed algorithm is a structured approach, which\nsplits the system into structural MPs first, and also all steps in it\nare easy to follow\n', ['undirected graphs', 'spanning trees', 'minimal paths', 'computer network analysis', 'circuit analysis', 'computer networks', 'graph theory', 'network analysis', 'trees (mathematics)']), ("Excess energy [cooling system]\nThe designers retrofitting a comfort cooling system to offices in Hertfordshire\nhave been able to make use of the waste heat rejected. what's more\nthey're now making it a standard solution for much larger projects\n", ['comfort cooling system', 'waste heat', 'Nationwide Trust', 'air conditioning', 'air conditioning', 'energy conservation']), ('Embeddings of planar graphs that minimize the number of long-face cycles\nWe consider the problem of finding embeddings of planar graphs that minimize\nthe number of long-face cycles. We prove that for any k >or= 4, it\nis NP-complete to find an embedding that minimizes the number of face\ncycles of length at least k\n', ['embeddings', 'planar graphs', 'long-face cycles', 'NP-complete problem', 'graph drawing', 'computational complexity', 'graph theory', 'minimisation']), ('Taming the paper tiger [paperwork organization]\nGenerally acknowledged as a critical problem for many information\nprofessionals, the massive flow of documents, paper trails, and\ninformation needs efficient and dependable approaches for processing\nand storing and finding items and information\n', ['paperwork organization', 'information professionals', 'information processing', 'information storage', 'information retrieval', 'information retrieval', 'information storage', 'library automation']), ('Mobile banking\'s tough sell\nBanks are having to put their mobile-commerce projects on hold because the\nessential technology to make the services usable, in particular GPRS\n(general packet radio service) hasn\'t become widely available. It is\nestimated that by the end of 2002, only 5 per cent of adults will have\nGPRS phones. This will have a knock-on effect for other technologies\nsuch as clickable icons and multimedia messaging. In fact banking via\nWAP (wireless application protocol) has proved to be a frustrating and\ntime-consuming process for the customer. Financial firms\' hopes for\nhigher mobile usage are stymied by the fact that improvements to the\nsystems won\'t happen as fast as they want and the inadequacies of the\nsystem go beyond immature technology. Financial services institutions\nshould not wait for customers to become au fait with their WAP. Instead\nthey should be the ones "driving the traffic"\n', ['banking', 'mobile-commerce', 'GPRS', 'wireless application protocol', 'banking', 'electronic commerce', 'mobile computing']), ('Business school research: bridging the gap between producers and consumers\nThere has been a great deal of continuing discussion concerning the seemingly\nunbridgeable gap between so much of the research produced by business\nschool professors and the needs of the business people who, ideally,\nwould use it. Here, we examine this gap and suggest a model for\nbridging it. We sample four groups of people, business school academics\n(professors), deans of business schools, executive MBA students/recent\ngraduates, and senior business executives. Each group rates 44\ndifferent (potential) properties of exemplary research. We analyze\nwithin-group differences, and more meaningfully, between-group\ndifferences. We then offer commentary on the results and use the\nresults to develop the aforementioned suggestions for bridging the gap\nwe find\n', ['business school', 'producers', 'consumers', 'professors', 'business people', 'academics', 'deans', 'students', 'recent graduates', 'senior business executives', 'exemplary research', 'within-group differences', 'between-group differences', 'ANOVA', 'coefficient of concordance', 'multiple comparison testing', 'education', 'management science', 'statistical analysis']), ("Writing the fulfillment RFP [publishing]\nFor the uninitiated, writing a request for proposal can seem both mysterious\nand daunting. Here's a format that will make you look like a pro the\nfirst time out\n", ['request for proposal', 'fulfillment', 'publisher', 'publishing']), ('Technology CAD of SiGe-heterojunction field effect transistors\nA 2D virtual wafer fabrication simulation suite has been employed for the\ntechnology CAD of SiGe channel heterojunction field effect transistors\n(HFETs). Complete fabrication process of SiGe p-HFETs has been\nsimulated. The SiGe material parameters and mobility model were\nincorporated to simulate Si/SiGe p-HFETs with a uniform germanium\nchannel having an L/sub eff/ of 0.5 mu m. A significant improvement in\nlinear transconductance is observed when compared to control-silicon\np-MOSFETs\n', ['technology CAD', 'heterojunction field effect transistors', 'SiGe', 'fabrication process', 'material parameters', 'mobility model', 'uniform channel', 'linear transconductance', '0.5 micron', 'carrier mobility', 'field effect transistors', 'Ge-Si alloys', 'semiconductor device models', 'semiconductor materials', 'technology CAD (electronics)']), ("A model for choosing an electronic reserves system: a pre-implementation study\nat the library of Long Island University's Brooklyn campus\nThis study explores the nature of electronic reserves (e-reserves) and\ninvestigates the possibilities of implementing the e-reserves at the\nLong Island University/Brooklyn Campus Library (LIU/BCL)\n", ['electronic reserves system', 'Long Island University Brooklyn Campus Library', 'academic libraries', 'library automation', 'software selection']), ('New voice over Internet protocol technique with hierarchical data security\nprotection\nThe authors propose a voice over Internet protocol (VoIP) technique with a new\nhierarchical data security protection (HDSP) scheme. The proposed HDSP\nscheme can maintain the voice quality degraded from packet loss and\npreserve high data security. It performs both the data inter-leaving on\nthe inter-frame of voice for achieving better error recovery of voices\nsuffering from continuous packet loss, and the data encryption on the\nintra-frame of voice for achieving high data security, which are\ncontrolled by a random bit-string sequence generated from a chaotic\nsystem. To demonstrate the performance of the proposed HDSP scheme, we\nhave successfully verified and analysed the proposed approach through\nsoftware simulation and statistical measures on several test voices\n', ['voice over Internet protocol', 'hierarchical data security protection', 'VoIP', 'HDSP scheme', 'packet loss', 'high data security', 'data interleaving', 'data encryption', 'random bit-string sequence', 'chaotic system', 'software simulation', 'statistical measures', 'packet voice communications', 'cryptography', 'Internet telephony', 'protocols', 'security of data']), ('Computing the frequency response of systems affinely depending on uncertain\nparameters\nThe computation of the frequency response of systems depending affinely on\nuncertain parameters can be reduced to that of all its one-dimensional\nedge plants while the image of such an edge plant at a fixed frequency\nis an arc or a line segment in the complex plane. Based on this\nconclusion, four computational formulas of the maximal and minimal\n(maxi-mini) magnitudes and phases of an edge plant at a fixed frequency\nare given. The formulas, besides sharing a simpler form of expression,\nconcretely display how the extrema of the frequency response of the\nedge plant relate to the typical characteristics of the arc and line\nsegment such as the centre, radius and tangent points of the arc, the\ndistance from the origin to the line segment etc. The direct\napplication of the results is to compute the Bode-, Nichols- and\nNyquist-plot collections of the systems which are needed in robustness\nanalysis and design\n', ['frequency response', 'uncertain parameters', 'affine systems', 'one-dimensional edge plants', 'arc', 'line segment', 'Bode-plot', 'Nichols-plot', 'Nyquist-plot', 'robustness analysis', 'robustness design', 'frequency-domain design methods', 'Bode diagrams', 'frequency response', 'frequency-domain analysis', 'frequency-domain synthesis', 'Nyquist diagrams', 'polynomials', 'robust control', 'uncertain systems']), ("Integrate-and-fire neurons driven by correlated stochastic input\nNeurons are sensitive to correlations among synaptic inputs. However,\nanalytical models that explicitly include correlations are hard to\nsolve analytically, so their influence on a neuron's response has been\ndifficult to ascertain. To gain some intuition on this problem, we\nstudied the firing times of two simple integrate-and-fire model neurons\ndriven by a correlated binary variable that represents the total input\ncurrent. Analytic expressions were obtained for the average firing rate\nand coefficient of variation (a measure of spike-train variability) as\nfunctions of the mean, variance, and correlation time of the stochastic\ninput. The results of computer simulations were in excellent agreement\nwith these expressions. In these models, an increase in correlation\ntime in general produces an increase in both the average firing rate\nand the variability of the output spike trains. However, the magnitude\nof the changes depends differentially on the relative values of the\ninput mean and variance: the increase in firing rate is higher when the\nvariance is large relative to the mean, whereas the increase in\nvariability is higher when the variance is relatively small. In\naddition, the firing rate always tends to a finite limit value as the\ncorrelation time increases toward infinity, whereas the coefficient of\nvariation typically diverges. These results suggest that temporal\ncorrelations may play a major role in determining the variability as\nwell as the intensity of neuronal spike trains\n", ['integrate-and-fire neurons', 'correlated stochastic input', 'synaptic input correlations', 'firing times', 'correlated binary variable', 'coefficient of variation', 'spike-train variability', 'computer simulation', 'output spike trains', 'temporal correlations', 'bioelectric potentials', 'neural nets', 'neurophysiology']), ('CyberEthics bibliography 2002: a select list of recent works\nIncluded in the 2002 annual bibliography update is a select list of recent\nbooks and conference proceedings that have been published since 2000.\nAlso included is a select list of special issues of journals and\nperiodicals that were recently published. For additional lists of\nrecently published books and articles, see ibid. (June 2000, June 2001)\n', ['CyberEthics bibliography', '2002 annual bibliography', 'recent books', 'conference proceedings', 'special issues', 'journals', 'periodicals', 'bibliographies', 'social aspects of automation']), ('Meeting of minds\nTechnical specialists need to think about their role in IT projects and how\nthey communicate with end-users and other participants to ensure they\ncontribute fully as team members. It is especially important to\ncommunicate and document trade-offs that may have to be made, including\nthe rationale behind them, so that if requirements change, the impact\nand decisions can be readily communicated to the stakeholders\n', ['technical specialists', 'IT projects', 'communication', 'end-users', 'DP management', 'professional communication', 'project management']), ("Neighborhood operator systems and approximations\nThis paper presents a framework for the study of generalizing the standard\nnotion of equivalence relation in rough set approximation space with\nvarious categories of k-step neighborhood systems. Based on a binary\nrelation on a finite universe, six families of binary relations are\nobtained, and the corresponding six classes of k-step neighborhood\nsystems are derived. Extensions of Pawlak's (1982) rough set\napproximation operators based on such neighborhood systems are\nproposed. Properties of neighborhood operator systems and rough set\napproximation operators are investigated, and their connections are\nexamined\n", ['neighborhood operator systems', 'equivalence relation', 'rough set approximation space', 'k-step neighborhood systems', 'binary relation', 'finite universe', 'approximation theory', 'mathematical operators', 'rough set theory']), ('Records role in e-business\nRecords management standards are now playing a key role in e-business strategy\n', ['e-business strategy', 'records management', 'electronic commerce', 'records management']), ('Heuristics for single-pass welding task sequencing\nWelding task sequencing is a prerequisite in the offline programming of robot\narc welding. Single-pass welding task sequencing can be modelled as a\nmodified travelling salesman problem. Owing to the difficulty of the\nresulting arc-routing problems, effective local search heuristics are\ndeveloped. Computational speed becomes important because robot arc\nwelding is often part of an automated process-planning procedure.\nGenerating a reasonable solution in an acceptable time is necessary for\neffective automated process planning. Several different heuristics are\nproposed for solving the welding task-sequencing problem considering\nboth productivity and the potential for welding distortion.\nConstructive heuristics based on the nearest neighbour concept and tabu\nsearch heuristics are developed and enhanced using improvement\nprocedures. The effectiveness of the heuristics developed is tested and\nverified on actual welded structure problems and random problems\n', ['single-pass welding task sequencing', 'constructive heuristics', 'offline programming', 'robot arc welding', 'modified travelling salesman problem', 'local search heuristics', 'computational speed', 'automated process-planning procedure', 'productivity', 'welding distortion', 'nearest neighbour concept', 'tabu search heuristics', 'random problems', 'welded structure problems', 'arc welding', 'assembly planning', 'computer aided production planning', 'heuristic programming', 'industrial robots', 'robot programming', 'search problems', 'travelling salesman problems']), ('Control of a heavy-duty robotic excavator using time delay control with\nintegral sliding surface\nThe control of a robotic excavator is difficult from the standpoint of the\nfollowing problems: parameter variations in mechanical structures,\nvarious nonlinearities in hydraulic actuators and disturbance due to\nthe contact with the ground. In addition, the more the size of robotic\nexcavators increases, the more the length and mass of the excavator\nlinks; the more the parameters of a heavy-duty excavator vary. A\ntime-delay control with switching action (TDCSA) using an integral\nsliding surface is proposed in this paper for the control of a 21-ton\nrobotic excavator. Through analysis and experiments, we show that using\nan integral sliding surface for the switching action of TDCSA is better\nthan using a PD-type sliding surface. The proposed controller is\napplied to straight-line motions of a 21-ton robotic excavator with a\nspeed level at which skillful operators work. Experiments, which were\ndesigned for surfaces with various inclinations and over broad ranges\nof joint motions, show that the proposed controller exhibits good\nperformance\n', ['time-delay control', 'robust control', 'robotic excavator', 'integral sliding surface', 'motion control', 'trajectory control', 'dynamics', 'tracking', 'pressure control', 'delay systems', 'dynamics', 'excavators', 'industrial robots', 'motion control', 'pressure control', 'robust control', 'tracking', 'variable structure systems']), ('Robust speech recognition using probabilistic union models\nThis paper introduces a new statistical approach, namely the probabilistic\nunion model, for speech recognition involving partial, unknown\nfrequency-band corruption. Partial frequency-band corruption accounts\nfor the effect of a family of real-world noises. Previous methods based\non the missing feature theory usually require the identity of the noisy\nbands. This identification can be difficult for unexpected noise with\nunknown, time-varying band characteristics. The new model combines the\nlocal frequency-band information based on the union of random events,\nto reduce the dependence of the model on information about the noise.\nThis model partially accomplishes the target: offering robustness to\npartial frequency-band corruption, while requiring no information about\nthe noise. This paper introduces the theory and implementation of the\nunion model, and is focused on several important advances. These new\ndevelopments include a new algorithm for automatic order selection, a\ngeneralization of the modeling principle to accommodate partial feature\nstream corruption, and a combination of the union model with\nconventional noise reduction techniques to deal with a mixture of\nstationary noise and unknown, nonstationary noise. For the evaluation,\nwe used the TIDIGITS database for speaker-independent connected digit\nrecognition. The utterances were corrupted by various types of additive\nnoise, stationary or time-varying, assuming no knowledge about the\nnoise characteristics. The results indicate that the new model offers\nsignificantly improved robustness in comparison to other models\n', ['robust speech recognition', 'probabilistic union models', 'partial real-world noise', 'automatic order selection', 'modeling', 'partial feature stream corruption', 'noise reduction techniques', 'stationary noise', 'nonstationary noise', 'TIDIGITS database', 'speaker-independent connected digit recognition', 'additive noise', 'noise characteristics', 'missing feature theory', 'noisy bands', 'time-varying band characteristics', 'local frequency-band information', 'partial frequency-band corruption', 'noise', 'probability', 'speech recognition']), ('Impact of aviation highway-in-the-sky displays on pilot situation awareness\nThirty-six pilots (31 men, 5 women) were tested in a flight simulator on their\nability to intercept a pathway depicted on a highway-in-the-sky (HITS)\ndisplay. While intercepting and flying the pathway, pilots were\nrequired to watch for traffic outside the cockpit. Additionally, pilots\nwere tested on their awareness of speed, altitude, and heading during\nthe flight. Results indicated that the presence of a flight guidance\ncue significantly improved flight path awareness while intercepting the\npathway, but significant practice effects suggest that a guidance cue\nmight be unnecessary if pilots are given proper training. The amount of\ntime spent looking outside the cockpit while using the HITS display was\nsignificantly less than when using conventional aircraft instruments.\nAdditionally, awareness of flight information present on the HITS\ndisplay was poor. Actual or potential applications of this research\ninclude guidance for the development of perspective flight display\nstandards and as a basis for flight training requirements\n', ['flight simulator', 'pilots', 'highway-in-the-sky display', 'cockpit', 'flight guidance', 'human factors', 'situation awareness', 'flight path awareness', 'aircraft display', 'aircraft displays', 'human factors', 'man-machine systems']), ('Recording quantum properties of light in a long-lived atomic spin state:\ntowards quantum memory\nWe report an experiment on mapping a quantum state of light onto the ground\nstate spin of an ensemble of Cs atoms with the lifetime of 2 ms.\nRecording of one of the two quadrature phase operators of light is\ndemonstrated with vacuum and squeezed states of light. The sensitivity\nof the mapping procedure at the level of approximately 1 photon/sec per\nHz is shown. The results pave the road towards complete (storing both\nquadrature phase observables) quantum memory for Gaussian states of\nlight. The experiment also sheds new light on fundamental limits of\nsensitivity of the magneto-optical resonance method\n', ['light quantum properties recording', 'long-lived atomic spin state', 'quantum memory', 'ground state spin', 'ensemble', 'two quadrature phase operators', 'vacuum states', 'squeezed states', 'mapping procedure', 'magnetooptical resonance method', '2 ms', 'Cs', 'ground states', 'magneto-optical effects', 'optical squeezing', 'quantum computing']), ('Conceptual modeling and specification generation for B2B business processes\nbased on ebXML\nIn order to support dynamic setup of business processes among independent\norganizations, a formal standard schema for describing the business\nprocesses is basically required. The ebXML framework provides such a\nspecification schema called BPSS (Business Process Specification\nSchema) which is available in two standalone representations: a UML\nversion, and an XML version. The former, however, is not intended for\nthe direct creation of business process specifications, but for\ndefining specification elements and their relationships required for\ncreating an ebXML-compliant business process specification. For this\nreason, it is very important to support conceptual modeling that is\nwell organized and directly matched with major modeling concepts. This\npaper deals with how to represent and manage B2B business processes\nusing UML-compliant diagrams. The major challenge is to organize UML\ndiagrams in a natural way that is well suited to the business process\nmeta-model and then to transform the diagrams into an XML version. This\npaper demonstrates the usefulness of conceptually modeling business\nprocesses by prototyping a business process editor tool called\nebDesigner\n', ['B2B business processes', 'ebXML', 'conceptual modeling', 'specification generation', 'formal standard schema', 'Business Process Specification Schema', 'UML-compliant diagrams', 'meta model', 'ebDesigner', 'business process editor', 'electronic commerce', 'formal specification', 'hypermedia markup languages', 'specification languages']), ('IT: Utilities\nA look at five utilities to make your PCs more, efficient, effective, and\nefficacious\n', ['utilities', 'PCs', 'MobileMessenger', 'Post-it software', 'EasyNotes', 'Print Shop Pro', 'Download Accelerator Plus', 'microcomputers', 'software packages']), ('Computational complexity of probabilistic disambiguation\nRecent models of natural language processing employ statistical reasoning for\ndealing with the ambiguity of formal grammars. In this approach,\nstatistics, concerning the various linguistic phenomena of interest,\nare gathered from actual linguistic data and used to estimate the\nprobabilities of the various entities that are generated by a given\ngrammar, e.g., derivations, parse-trees and sentences. The extension of\ngrammars with probabilities makes it possible to state ambiguity\nresolution as a constrained optimization formula, which aims at\nmaximizing the probability of some entity that the grammar generates\ngiven the input (e.g., maximum probability parse-tree given some input\nsentence). The implementation of these optimization formulae in\nefficient algorithms, however, does not always proceed smoothly. In\nthis paper, we address the computational complexity of ambiguity\nresolution under various kinds of probabilistic models. We provide\nproofs that some, frequently occurring problems of ambiguity resolution\nare NP-complete. These problems are encountered in various\napplications, e.g., language understanding for textand speech-based\napplications. Assuming the common model of computation, this result\nimplies that, for many existing probabilistic models it is not possible\nto devise tractable algorithms for solving these optimization problems\n', ['natural language processing', 'statistical reasoning', 'formal grammars', 'statistics', 'computational complexity', 'probabilistic disambiguation', 'NP-completeness results', 'parsing problems', 'speech processing', 'state ambiguity resolution', 'constrained optimization formula', 'probabilistic models', 'language understanding', 'computational complexity', 'grammars']), ('Dynamics and control of initialized fractional-order systems\nDue to the importance of historical effects in fractional-order systems, this\npaper presents a general fractional-order system and control theory\nthat includes the time-varying initialization response. Previous\nstudies have not properly accounted for these historical effects. The\ninitialization response, along with the forced response, for\nfractional-order systems is determined. The scalar fractional-order\nimpulse response is determined, and is a generalization of the\nexponential function. Stability properties of fractional-order systems\nare presented in the complex w-plane, which is a transformation of the\ns-plane. Time responses are discussed with respect to pole positions in\nthe complex w-plane and frequency response behavior is included. A\nfractional-order vector space representation, which is a generalization\nof the state space concept, is presented including the initialization\nresponse. Control methods for vector representations of initialized\nfractional-order systems are shown. Finally, the fractional-order\ndifferintegral is generalized to continuous order-distributions which\nhave the possibility of including all fractional orders in a transfer\nfunction\n', ['initialized fractional-order systems', 'dynamics', 'control', 'initialization response', 'forced response', 'impulse response', 'exponential function', 'vector space representation', 'state space concept', 'fractional-order differintegral', 'transfer function', 'differential equations', 'integral equations', 'nonlinear control systems', 'nonlinear dynamical systems', 'numerical stability', 'transfer functions', 'transient response']), ('Oxygen-enhanced MRI of the brain\nBlood oxygenation level-dependent (BOLD) contrast MRI is a potential method for\na physiological characterization of tissue beyond mere morphological\nrepresentation. The purpose of this study was to develop evaluation\ntechniques for such examinations using a hyperoxia challenge.\nAdministration of pure oxygen was applied to test these techniques, as\npure oxygen can be expected to induce relatively small signal intensity\n(SI) changes compared to CO/sub 2/-containing gases and thus requires\nvery sensitive evaluation methods. Fourteen volunteers were\ninvestigated by alternating between breathing 100% O/sub 2/ and normal\nair, using two different paradigms of administration. Changes ranged\nfrom >30% in large veins to 1.71%+or-0.14% in basal ganglia and\n0.82%+or-0.08% in white matter. To account for a slow physiological\nresponse function, a reference for correlation analysis was derived\nfrom the venous reaction. An objective method is presented that allows\nthe adaptation of the significance threshold to the complexity of the\nparadigm used. Reference signal characteristics in representative brain\ntissue regions were established. As the presented evaluation scheme\nproved its applicability to small SI changes induced by pure oxygen, it\ncan readily be used for similar experiments with other gases\n', ['BOLD contrast MRI', 'oxygen-enhanced MRI', 'hyperoxia', 'brain', 'oxygen breathing', 'normal air breathing', 'physiological response function', 'correlation analysis', 'venous reaction', 'significance threshold', 'paradigm complexity', 'MRI contrast agent', 'functional imaging', 'Fourier transform Analysis', 'biomedical MRI', 'brain', 'haemodynamics', 'medical image processing']), ('Geometrically invariant watermarking using feature points\nThis paper presents a new approach for watermarking of digital images providing\nrobustness to geometrical distortions. The weaknesses of classical\nwatermarking methods to geometrical distortions are outlined first.\nGeometrical distortions can be decomposed into two classes: global\ntransformations such as rotations and translations and local\ntransformations such as the StirMark attack. An overview of existing\nself-synchronizing schemes is then presented. Theses schemes can use\nperiodical properties of the mark, invariant properties of transforms,\ntemplate insertion, or information provided by the original image to\ncounter geometrical distortions. Thereafter, a new class of\nwatermarking schemes using the image content is presented. We propose\nan embedding and detection scheme where the mark is bound with a\ncontent descriptor defined by salient points. Three different types of\nfeature points are studied and their robustness to geometrical\ntransformations is evaluated to develop an enhanced detector. The\nembedding of the signature is done by extracting feature points of the\nimage and performing a Delaunay tessellation on the set of points. The\nmark is embedded using a classical additive scheme inside each triangle\nof the tessellation. The detection is done using correlation properties\non the different triangles. The performance of the presented scheme is\nevaluated after JPEG compression, geometrical attack and\ntransformations. Results show that the fact that the scheme is robust\nto these different manipulations. Finally, in our concluding remarks,\nwe analyze the different perspectives of such content-based\nwatermarking scheme\n', ['geometrically invariant watermarking', 'feature points', 'digital images', 'geometrical distortions', 'global transformations', 'rotations', 'translations', 'local transformations', 'StirMark attack', 'self-synchronizing schemes', 'periodical properties', 'invariant properties', 'transforms', 'template insertion', 'image content', 'embedding', 'detection scheme', 'content descriptor', 'feature extraction', 'Delaunay tessellation', 'additive scheme', 'correlation properties', 'JPEG compression', 'geometrical attack', 'copy protection', 'data encapsulation', 'feature extraction', 'image coding', 'image recognition', 'mesh generation', 'transforms']), ('Development of visual design steering as an aid in large-scale\nmultidisciplinary design optimization. I. Method development\nA modified paradigm of computational steering (CS), termed visual design\nsteering (VDS), is developed in this paper. The VDS paradigm is applied\nto optimal design problems to provide a means for capturing and\nenabling designer insights. VDS allows a designer to make decisions\nbefore, during or after an analysis or optimization via a visual\nenvironment, in order to effectively steer the solution process. The\nobjective of VDS is to obtain a better solution in less time through\nthe use of designer knowledge and expertise. Using visual\nrepresentations of complex systems in this manner enables human\nexperience and judgement to be incorporated into the optimal design\nprocess at appropriate steps, rather than having traditional black box\nsolvers return solutions from a prescribed input set. Part I of this\npaper focuses on the research issues pertaining to the Graph Morphing\nvisualization method created to represent an n-dimensional optimization\nproblem using 2-dimensional and 3-dimensional visualizations. Part II\ninvestigates the implementation of the VDS paradigm, using the graph\nmorphing approach, to improve an optimal design process. Specifically,\nthe following issues are addressed: impact of design variable changes\non the optimal design space; identification of possible constraint\nredundancies; impact of constraint tolerances on the optimal solution:\nand smoothness of the objective function contours. It is demonstrated\nthat graph morphing can effectively reduce the complexity and\ncomputational time associated with some optimization problems\n', ['visual design steering', 'large-scale multidisciplinary design optimization', 'computational steering', 'optimal design problems', 'designer decision making', 'visual representations', 'complex systems', 'graph morphing visualization method', '3D visualizations', '2D visualizations', 'n-dimensional optimization', 'computational time', 'complexity', 'design variable changes', 'constraint redundancies', 'constraint tolerances', 'objective function contour smoothness', 'CAD', 'computational complexity', 'data visualisation', 'engineering graphics', 'graph theory', 'image morphing', 'optimisation']), ('Pool halls, chips, and war games: women in the culture of computing\nComputers are becoming ubiquitous in our society and they offer superb\nopportunities for people in jobs and everyday life. But there is a\nnoticeable sex difference in use of computers among children. This\narticle asks why computers are more attractive to boys than to girls\nand offers a cultural framework for explaining the apparent sex\ndifferences. Although the data are fragmentary, the world of computing\nseems to be more consistent with male adolescent culture than with\nfeminine values and goals. Furthermore, both arcade and educational\nsoftware is designed with boys in mind. These observations lead us to\nspeculate that computing is neither inherently difficult nor\nuninteresting to girls, but rather that computer games and other\nsoftware might have to be designed differently for girls. Programs to\nhelp teachers instill computer efficacy in all children also need to be\ndeveloped\n', ['women', 'culture of computing', 'sex difference', 'children', 'male adolescent culture', 'educational software', 'computer games', 'teachers', 'computer games', 'computer literacy', 'educational computing', 'gender issues', 'social aspects of automation', 'teaching']), ('Finally! some sensible European legislation on software\nThe European Commission has formally tabled a draft Directive on the Protection\nby Patents of Computer-Implemented Inventions. The aim of this very\nimportant Directive is to harmonise national patent laws relating to\ninventions using software. It follows an extensive consultation\nlaunched by the Commission in October 2000. The impetus behind the\nDirective was the recognition at EU level of a total lack of unity\nbetween the European Patent Office and European national courts in\ndeciding what was or was not deemed patentable when it came to the\nsubject of computer programs\n', ['European Commission', 'Directive on the Protection by Patents of Computer-Implemented Inventions', 'national patent laws', 'law harmonisation', 'EU', 'European Patent Office', 'national courts', 'computer programs', 'computer software', 'government policies', 'legislation', 'patents']), ("Examining children's reading performance and preference for different\ncomputer-displayed text\nThis study investigated how common online text affects reading performance of\nelementary school-age children by examining the actual and perceived\nreadability of four computer-displayed typefaces at 12- and 14-point\nsizes. Twenty-seven children, ages 9 to 11, were asked to read eight\nchildren's passages and identify erroneous/substituted words while\nreading. Comic Sans MS, Arial and Times New Roman typefaces, regardless\nof size, were found to be more readable (as measured by a reading\nefficiency score) than Courier New. No differences in reading speed\nwere found for any of the typeface combinations. In general, the\n14-point size and the examined sans serif typefaces were perceived as\nbeing the easiest to read, fastest, most attractive, and most desirable\nfor school-related material. In addition, participants significantly\npreferred Comic Sans MS and 14-point Arial to 12-point Courier.\nRecommendations for appropriate typeface combinations for children\nreading on computers are discussed\n", ['child reading performance', 'computer-displayed text', 'online text', 'elementary school-age children', 'computer-displayed typefaces', 'fonts', 'user interface', 'human factors', 'educational computing', 'character sets', 'educational computing', 'human factors', 'text analysis', 'user interfaces']), ('LAN-based building maintenance and surveillance robot\nThe building and construction industry is the major industry of Hong Kong as in\nmany developed countries around the world. After the commissioning of a\nhigh-rise building or a large estate, substantial manpower, both inside\nthe management centre under a standby manner, as well as surveillance\nfor security purposes around the whole building, is required for daily\noperation to ensure a quality environment for the occupants. If the\nsurveillance job can be done by robots, the efficiency can be highly\nenhanced, resulting in a great saving of manpower and the improved\nsafety of the management staff as a by-product. Furthermore, if the\nrobot can retrieve commands from the building management system via a\nlocal area network (LAN), further savings in manpower can be achieved\nin terms of first-line fault attendance by human management staff. This\npaper describes the development of a robot prototype here in Hong Kong,\nwhich can handle some daily routine maintenance works and surveillance\nresponsibilities. The hardware structure of the robot and its on-board\ndevices are described. Real-time images captured by a camera on the\nrobot with pan/tilt/zoom functions can be transmitted back to the\ncentral management office via a local area network. The interface\nbetween the robot and the building automation system (BAS) of the\nbuilding is discussed. This is the first key achievement of this\nproject with a strong implication on reducing the number of human staff\nto manage a modem building. Teleoperation of the robot via the Internet\nor intranet is also possible, which is the second achievement of this\nproject. Finally, the robot can identify its physical position inside\nthe building by a landmark recognition method based on standard CAD\ndrawings, which is the third achievement of this project. The main goal\nof this paper is not the description of some groundbreaking technology\nin robotic development. It is mainly intended to convince building\ndesigners and managers to incorporate robotic systems when they are\nmanaging modem buildings to save manpower and improve efficiency\n', ['LAN-based building maintenance and surveillance robot', 'high-rise building', 'security purposes', 'building management system', 'local area network', 'first-line fault attendance', 'hardware structure', 'pan/tilt/zoom functions', 'teleoperation', 'landmark recognition method', 'building management systems', 'local area networks', 'mobile robots', 'security', 'surveillance']), ('How to avoid merger pitfalls\nPaul Diamond of consultancy KPMG explains why careful IT asset management is\ncrucial to the success of mergers\n', ['consultancy', 'KPMG', 'IT asset management', 'mergers', 'DP management']), ("A winning combination [wireless health care]\nThree years ago, the Institute of Medicine (IOM) reported that medical errors\nresult in at least 44,000 deaths each year-more than deaths from\nhighway accidents, breast cancer or AIDS. That report, and others which\nplaced serious errors as high as 98,000 annually, served as a wake-up\ncall for healthcare providers such as the CareGroup Healthcare System\nInc., a Boston-area healthcare network that is the second largest\nintegrated delivery system in the northeastern United States. With\nannual revenues of $1.2B, CareGroup provides primary care and specialty\nservices to more than 1,000,000 patients. CareGroup combined wireless\ntechnology with the Web to create a provider order entry (POE) system\ndesigned to reduce the frequency of costly medical mistakes. The POE\ninfrastructure includes InterSystems Corporation's CACHE database, Dell\nComputer C600 laptops and Cisco Systems' Aironet 350 wireless networks\n", ['CareGroup Healthcare System', 'healthcare network', 'wireless', 'medical errors', 'provider order entry', 'InterSystems Corporation CACHE database', 'Cisco Systems Aironet 350 wireless networks', 'Dell Computer C600 laptops', 'health care', 'mobile computing']), ('Source/channel coding of still images using lapped transforms and block\nclassification\nA novel scheme for joint source/channel coding of still images is proposed. By\nusing efficient lapped transforms, channel-optimised robust quantisers\nand classification methods it is shown that significant improvements\nover traditional source/channel coding of images can be obtained while\nkeeping the complexity low\n', ['joint source-channel coding', 'still images', 'lapped transforms', 'block classification', 'image coding', 'channel-optimised robust quantisers', 'low complexity', 'combined source-channel coding', 'image classification', 'image coding', 'transform coding', 'transforms']), ("Sliding mode control of chaos in the cubic Chua's circuit system\nIn this paper, a sliding mode controller is applied to control the cubic Chua's\ncircuit system. The sliding surface of this paper used is one dimension\nhigher than the traditional surface and guarantees its passage through\nthe initial states of the controlled system. Therefore, using the\ncharacteristic of this sliding mode we aim to design a controller that\ncan meet the desired specification and use less control energy by\ncomparing with the result in the current existing literature. The\nresults show that the proposed controller can steer Chua's circuit\nsystem to the desired state without the chattering phenomenon and\nabrupt state change\n", ['sliding mode control', 'chaos', 'cubic Chua circuit system', 'sliding surface', 'chattering', 'state change', 'match disturbance', 'mismatch disturbance', 'bifurcation', 'chaos', "Chua's circuit", 'nonlinear control systems', 'nonlinear dynamical systems', 'state feedback', 'variable structure systems']), ('Approximation theory of fuzzy systems based upon genuine many-valued\nimplications - MIMO cases\nIt is constructively proved that the multi-input-multi-output fuzzy systems\nbased upon genuine many-valued implications are universal approximators\n(they are called Boolean type fuzzy systems in this paper). The general\napproach to construct such fuzzy systems is given, that is, through the\npartition of the output region (by the given accuracy). Two examples\nare provided to demonstrate the way in which fuzzy systems are designed\nto approximate given functions with a given required approximation\naccuracy\n', ['multi-input-multi-output fuzzy systems', 'Boolean type fuzzy systems', 'fuzzy systems', 'many-valued implication', 'universal approximator', 'approximation theory', 'fuzzy systems', 'MIMO systems']), ('Power electronics spark new simulation challenges\nThis article discusses some of the changes that have taken place in power\nsystems and explores some of the inherent requirements for simulation\ntechnologies in order to keep up with this rapidly changing\nenvironment. The authors describe how energy utilities are realizing\nthat, with the appropriate tools, they can train and sustain engineers\nwho can maintain a great insight into system dynamics\n', ['power system computer simulation', 'power electronics', 'simulation challenges', 'simulation technologies', 'electric utilities', 'circuit simulation', 'electricity supply industry', 'power electronics', 'power system simulation']), ("Contrast sensitivity in a dynamic environment: effects of target conditions and\nvisual impairment\nContrast sensitivity was determined as a function of target velocity (0 degrees\n-120 degrees /s) over a variety of viewing conditions. In Experiment 1,\nmeasurements of dynamic contrast sensitivity were determined for\nobservers as a function of target velocity for letter stimuli.\nSignificant main effects were found for target velocity, target size,\nand target duration, but significant interactions among the variables\nindicated especially pronounced adverse effects of increasing target\nvelocity for small targets and brief durations. In Experiment 2, the\neffects of simulated cataracts were determined. Although the simulated\nimpairment had no effect on traditional acuity scores, dynamic contrast\nsensitivity was markedly reduced. Results are discussed in terms of\ndynamic contrast sensitivity as a useful composite measure of visual\nfunctioning that may provide a better overall picture of an\nindividual's visual functioning than does traditional static acuity,\ndynamic acuity, or contrast sensitivity alone. The measure of dynamic\ncontrast sensitivity may increase understanding of the practical\neffects of various conditions, such as aging or disease, on the visual\nsystem, or it may allow improved prediction of individuals' performance\nin visually dynamic situations\n", ['contrast sensitivity', 'dynamic environment', 'target conditions', 'visual impairment', 'dynamic contrast sensitivity', 'target velocity', 'target size', 'target duration', 'acuity scores', 'aging', 'disease', 'human factors', 'vision defects', 'visual perception']), ('Generalized spatio-chromatic diffusion\nA framework for diffusion of color images is presented. The method is based on\nthe theory of thermodynamics of irreversible transformations which\nprovides a suitable basis for designing correlations between the\ndifferent color channels. More precisely, we derive an equation for\ncolor evolution which comprises a purely spatial diffusive term and a\nnonlinear term that depends on the interactions among color channels\nover space. We apply the proposed equation to images represented in\nseveral color spaces, such as RGB, CIELAB, Opponent colors, and IHS\n', ['generalized spatio-chromatic diffusion', 'color images', 'diffusion', 'thermodynamics', 'irreversible transformations', 'color channels', 'color evolution', 'spatial diffusive term', 'nonlinear term', 'vector-valued diffusion', 'scale-space', 'RGB', 'CIELAB', 'Opponent colors', 'IHS', 'diffusion', 'image colour analysis', 'image representation', 'thermodynamics']), ('Edit distance of run-length encoded strings\nLet X and Y be two run-length encoded strings, of encoded lengths k and l,\nrespectively. We present a simple O(|X|l+|Y|k) time algorithm that\ncomputes their edit distance\n', ['run-length encoded strings', 'encoded lengths', 'algorithm', 'edit distance', 'computation time', 'computational complexity', 'string matching']), ('Reply to Carreira-Perpinan and Goodhill [mathematics in biology]\nIn a paper by Carreira-Perpinan and Goodhill (see ibid., vol.14, no.7,\np.1545-60, 2002) the authors apply mathematical arguments to biology.\nSwindale et al. think it is inappropriate to apply the standards of\nproof required in mathematics to the acceptance or rejection of\nscientific hypotheses. To give some examples, showing that data are\nwell described by a linear model does not rule out an infinity of other\npossible models that might give better descriptions of the data.\nProving in a mathematical sense that the linear model was correct would\nrequire ruling out all other possible models, a hopeless task.\nSimilarly, to demonstrate that two DNA samples come from the same\nindividual, it is sufficient to show a match between only a few regions\nof the genome, even though there remains a very large number of\nadditional comparisons that could be done, any one of which might\npotentially disprove the match. This is unacceptable in mathematics,\nbut in the real world, it is a perfectly reasonable basis for belief\n', ['mathematical arguments', 'biology', 'scientific hypotheses', 'linear model', 'DNA', 'genome', 'hypothesis testing', 'cortical maps', 'neural nets', 'biology', 'neural nets']), ('A dataflow computer which accelerates execution of sequential programs by\nprecedent firing instructions\nIn the dataflow machine, it is important to avoid degradation of performance in\nsequential processing, and it is important from the viewpoint of\nhardware scale to reduce the number of waiting operands. This paper\ndemonstrates that processing performance is degraded by sequential\nprocessing in the switching process, and presents a method of remedy.\nPrecedent firing control is proposed as a means of remedy, and it is\nshown by a simulation that the execution time and the total number of\nwaiting operands can be reduced by the precedent firing control. Then\nthe hardware scale is examined as an evaluation of precedent firing\ncontrol\n', ['dataflow computer', 'execution acceleration', 'sequential programs', 'precedent firing instructions', 'precedent firing control', 'execution time', 'waiting operands', 'hardware scale', 'parallel processing', 'computer architecture', 'processing performance', 'switching process', 'data flow computing', 'parallel architectures', 'performance evaluation']), ('Server safeguards tax service\nPeterborough-based tax consultancy IE Taxguard wanted real-time failover\nprotection for important Windows-based applications. Its solution was\nto implement a powerful failover server from UK supplier Neverfail in\norder to provide real-time backup for three core production servers\n', ['tax consultancy', 'IE Taxguard', 'failover server', 'Neverfail', 'backup', 'back-up procedures', 'network servers', 'system recovery', 'tax preparation']), ('Application of normal possibility decision rule to silence\nThe paper presents the way of combining two decision problems concerning a\nsingle (or a common) dimension, so that an effective fuzzy decision\nrule can be obtained. Normality of the possibility distribution is\nassumed, leading to possibility of fusing the respective functions\nrelated to the two decision problems and their characteristics\n(decisions, states of nature, utility functions, etc.). The approach\nproposed can be applied in cases when the statement of the problem\nrequires making of more refined distinctions rather than considering\nsimply a bi-criterion or bi-utility two-decision problem\n', ['normal possibility decision rule', 'silence', 'conflicting objectives', 'conflicting utilities', 'cool head', 'warm heart', 'decision problems', 'two-dimensional fuzzy events', 'decision theory', 'fuzzy set theory', 'possibility theory']), ('Evolving robust asynchronous cellular automata for the density task\nIn this paper the evolution of three kinds of asynchronous cellular automata\nare studied for the density task. Results are compared with those\nobtained for synchronous automata and the influence of various\nasynchronous update policies on the computational strategy is\ndescribed. How synchronous and asynchronous cellular automata behave is\ninvestigated when the update policy is gradually changed, showing that\nasynchronous cellular automata are more adaptable. The behavior of\nsynchronous and asynchronous evolved automata are studied under the\npresence of random noise of two kinds and it is shown that asynchronous\ncellular automata implicitly offer superior fault tolerance\n', ['asynchronous cellular automata', 'cellular automata', 'fault tolerance', 'discrete dynamical systems', 'random noise', 'synchronous automata', 'cellular automata']), ('Using fractional order adjustment rules and fractional order reference models\nin model-reference adaptive control\nThis paper investigates the use of Fractional Order Calculus (FOC) in\nconventional Model Reference Adaptive Control (MRAC) systems. Two\nmodifications to the conventional MRAC are presented, i.e., the use of\nfractional order parameter adjustment rule and the employment of\nfractional order reference model. Through examples, benefits from the\nuse of FOC are illustrated together with some remarks for further\nresearch\n', ['fractional order adjustment rules', 'fractional order reference models', 'model-reference adaptive control', 'MRAC', 'FOC', 'fractional calculus', 'model reference adaptive control systems', 'nonlinear control systems', 'nonlinear dynamical systems', 'parameter estimation', 'variational techniques']), ('Why your Web strategy is, err, wrong\nAn awkward look at a few standard views from the author, who thinks that most\npeople have got it, err, wrong. Like every other investment, when the\ntime comes to sign the contract, the question that should be asked is\nnot whether it is a good investment, but whether it is the best\ninvestment the firm can make with the money. the author argues that he\nwould be surprised if any law firm Web site he has seen yet would jump\nthat particular hurdle\n', ['Web strategy', 'law firm Web site', 'DP management', 'information resources', 'Internet', 'legislation']), ('Information interaction: providing a framework for information architecture\nInformation interaction is the process that people use in interacting with the\ncontent of an information system. Information architecture is a\nblueprint and navigational aid to the content of information-rich\nsystems. As such information architecture performs an important\nsupporting role in information interactivity. This article elaborates\non a model of information interactivity that crosses the "no-man\'s\nland" between user and computer articulating a model that includes\nuser, content and system, illustrating the context for information\narchitecture\n', ['information interaction', 'navigational aid', 'information-rich systems', 'information interactivity', 'electronic publishing', 'hypermedia', 'information resources', 'information retrieval', 'Internet']), ('Natural language from artificial life\nThis article aims to show that linguistics, in particular the study of the\nlexico-syntactic aspects of language, provides fertile ground for\nartificial life modeling. A survey of the models that have been\ndeveloped over the last decade and a half is presented to demonstrate\nthat ALife techniques have a lot to offer an explanatory theory of\nlanguage. It is argued that this is because much of the structure of\nlanguage is determined by the interaction of three complex adaptive\nsystems: learning, culture, and biological evolution. Computational\nsimulation, informed by theoretical linguistics, is an appropriate\nresponse to the challenge of explaining real linguistic data in terms\nof the processes that underpin human language\n', ['natural language', 'linguistics', 'lexico-syntactic aspects', 'ALife', 'adaptive systems', 'learning', 'culture', 'biological evolution', 'computational simulation', 'artificial life', 'artificial life', 'computational linguistics', 'natural languages']), ('Development of computer-mediated teaching resources for tourism distance\neducation: the University of Otago model\nThis article presents a qualitative account of the development of\ncomputer-mediated tourism distance learning resources. A distance\nlearning model was developed at the Centre for Tourism, University of\nOtago (New Zealand) in 1998-1999. The article reviews the development\nof this Internet-based learning resource explaining the design and\ndevelopment of programme links (providing study information for\nstudents) and paper links (course material and learning features). The\ndesign of course material is reviewed with emphasis given to\nconsistency of presentation between papers. The template for course\nmaterial is described and illustrated and the article concludes with an\noverview of important design considerations\n', ['computer-mediated tourism distance learning resources', 'University of Otago', 'Internet-based learning resource', 'paper links', 'programme links', 'computer aided instruction', 'distance learning', 'Internet', 'teaching', 'travel industry']), ('Quantum market games\nWe propose a quantum-like description of markets and economics. The approach\nhas roots in the recently developed quantum game theory\n', ['quantum market games', 'economics', 'quantum game theory', 'quantum strategies', 'financial markets', 'economics', 'game theory', 'quantum statistical mechanics', 'quantum theory']), ('A Web-accessible database of characteristics of the 1,945 basic Japanese kanji\nIn 1981, the Japanese government published a list of the 1,945 basic Japanese\nkanji (Jooyoo Kanji-hyo), including specifications of pronunciation.\nThis list was established as the standard for kanji usage in print. The\ndatabase for 1,945 basic Japanese kanji provides 30 cells that explain\nin detail the various characteristics of kanji. Means, standard\ndeviations, distributions, and information related to previous research\nconcerning these kanji are provided in this paper. The database is\nsaved as a Microsoft Excel 2000 file for Windows. This kanji database\nis accessible on the Web site of the Oxford Text Archive, Oxford\nUniversity (http://ota.ahds.ac.uk). Using this database, researchers\nand educators will be able to conduct planned experiments and organize\nclassroom instruction on the basis of the known characteristics of\nselected kanji\n', ['Web-accessible database', 'basic Japanese kanji', 'Jooyoo Kanji-hyo', 'pronunciation', 'kanji usage print', 'cells', 'means', 'standard deviations', 'distributions', 'Microsoft Excel 2000 file for Windows', 'Oxford Text Archive Web site', 'classroom instruction', 'behavioural sciences computing', 'character sets', 'information resources']), ('Stabilization of global invariant sets for chaotic systems: an energy based\ncontrol approach\nThis paper presents a new control approach for steering trajectories of\nthree-dimensional nonlinear chaotic systems towards stable stationary\nstates or time-periodic orbits. The proposed method mainly consists in\na sliding mode-based control design that is extended by an explicit\nconsideration of system energy as basis for both controller design and\nsystem stabilization. The control objective is then to regulate the\nenergy with respect to a shaped nominal representation implicitly\nrelated to system trajectories. In this paper, we establish some\ntheoretical results to introduce the control design approach referred\nto as energy based sliding mode control. Then, some capabilities of the\nproposed approach are illustrated through examples related to the\nchaotic circuit of Chua\n', ['three-dimensional nonlinear chaotic systems', 'stable stationary states', 'time-periodic orbits', 'sliding mode-based control', 'energy based sliding mode control', "Chua's circuit", 'global invariant sets', 'chaos', "Chua's circuit", 'set theory', 'variable structure systems']), ('Moving into the mainstream [product lifecycle management]\nProduct lifecycle management (PLM) is widely recognised by most manufacturing\ncompanies, as manufacturers begin to identify and implement targeted\nprojects intended to deliver return-on investment in a timely fashion.\nVendors are also releasing second-generation PLM products that are\npackaged, out-of-the-box solutions\n', ['product lifecycle management', 'manufacturing companies', 'product data management', 'product development', 'enterprise resource planning', 'manufacturing data processing', 'manufacturing resources planning', 'product development']), ('Two-layer model for the formation of states of the hidden Markov chains\nProcedures for the formation of states of the hidden Markov models are\ndescribed. Formant amplitudes and frequencies are used as state\nfeatures. The training strategy is presented that allows one to\ncalculate the parameters of conditional probabilities of the generation\nof a given formant set by a given hidden state with the help of the\nmaximum likelihood method\n', ['hidden Markov models', 'formant amplitudes', 'formant frequencies', 'state features', 'conditional probabilities', 'hidden state', 'maximum likelihood method', 'hidden Markov models', 'maximum likelihood estimation', 'speech recognition']), ("How to drive strategic innovation [law firms]\nInnovation. It has everything to do with organization and attitude. Marginal\nimprovement isn't enough anymore. Convert your problem-solving skills\ninto a new value for the entire firm. 10 initiatives\n", ['law firms', 'strategic innovation', 'management', 'change', 'clients', 'experiments', 'law administration']), ('Exploring the sabbatical or other leave as a means of energizing a career\nThis article challenges librarians to create leaves that will not only inspire\nprofessional growth but also renewal. It presents a framework for\ndeveloping a successful leave, incorporating useful advice from\nlibrarians at Concordia University (Montreal). As food for thought, the\narticle offers examples of specific options meant to encourage\nprofessionals to explore their own creative ideas. Finally, a central\ntheme of this article is that a midlife leave provides one with the\nperfect opportunity to take stock of oneself in order to define future\ncareer directions. Midlife is a time when rebel forces, feisty\nprotestors from within, often insist on being heard. It is a time, in\nother words, when professionals often long to break loose from the\nstress "to do far more, in less time" (Barner, 1994). Escaping from\ncurrent job constraints into a world of creative endeavor, when\nwell-executed, is a superb means of invigorating a career stuck in gear\nand discovering a fresh perspective from which to view one\'s\nprofession. To ignite renewal, midcareer is the perfect time to grant\none\'s imagination free reign\n', ['sabbatical leave', 'career', 'librarians', 'professional growth', 'library staff', 'midlife leave', 'employment', 'human resource management', 'information science', 'libraries', 'personnel', 'professional aspects']), ('More constructions for Boolean algebras\nWe construct Boolean algebras with prescribed behaviour concerning depth for\nthe free product of two Boolean algebras over a third, in ZFC using\npcf; assuming squares we get results on ultraproducts. We also deal\nwith the family of cardinalities and topological density of homomorphic\nimages of Boolean algebras (you can translate it to topology-on the\ncardinalities of closed subspaces); and lastly we deal with\ninequalities between cardinal invariants, mainly d(B)/sup kappa\n/<|B| implies ind(B)>/sup kappa /V Depth(B)>or=log(|B|)\n', ['Boolean algebras', 'prescribed behaviour', 'free product', 'ZFC', 'ultraproducts', 'homomorphic images', 'cardinal invariants', 'Boolean algebra', 'formal logic']), ('A feature-preserving volumetric technique to merge surface triangulations\nSeveral extensions and improvements to surface merging procedures based on the\nextraction of isosurfaces from a distance map defined on an adaptive\nbackground grid are presented. The main objective is to extend the\napplication of these algorithms to surfaces with sharp edges and\ncomers. In order to deal with objects of different length scales, the\ninitial background grids are created using a Delaunay triangulation\nmethod and local voxelizations. A point enrichment technique that\nintroduces points into the background grid along detected surface\nfeatures such as ridges is used to ensure that these features are\npreserved in the final merged surface. The surface merging methodology\nis extended to include other Boolean operations between surface\ntriangulations. The iso-surface extraction algorithms are modified to\nobtain the correct iso-surface for multi-component objects. The\nprocedures are demonstrated with various examples, ranging from simple\ngeometrical entities to complex engineering applications. The present\nalgorithms allow realistic modelling of a large number of complex\nengineering geometries using overlapping components defined discretely,\ni.e. via surface triangulations. This capability is very useful for\ngrid generation starting from data originated in measurements or images\n', ['feature-preserving volumetric technique', 'merge surface triangulations', 'surface merging procedures', 'iso-surfaces extraction', 'multi-component objects', 'simple geometrical entities', 'complex engineering applications', 'overlapping components', 'images', 'mesh generation', 'unstructured grids', 'discrete data', 'surface intersection', 'geometric modelling', 'adaptive background grid', 'sharp edges', 'sharp comers', 'Delaunay triangulation method', 'local voxelizations', 'point enrichment technique background grid', 'ridges', 'Boolean operations', 'surface triangulations', 'arterial surfaces', 'haemoglobin molecule', 'mesh generation', 'physiological models']), ('Bisimulation minimization and symbolic model checking\nState space minimization techniques are crucial for combating state explosion.\nA variety of explicit-state verification tools use bisimulation\nminimization to check equivalence between systems, to minimize\ncomponents before composition, or to reduce a state space prior to\nmodel checking. Experimental results on bisimulation minimization in\nsymbolic model checking contexts, however, are mixed. We explore\nbisimulation minimization as an optimization in symbolic model checking\nof invariance properties. We consider three bisimulation minimization\nalgorithms. From each, we produce a BDD-based model checker for\ninvariant properties and compare this model checker to a conventional\none based on backwards reachability. Our comparisons, both theoretical\nand experimental, suggest that bisimulation minimization is not viable\nin the context of invariance verification, because performing the\nminimization requires as many, if not more, computational resources as\nmodel checking the unminimized system through backwards reachability\n', ['bisimulation minimization', 'symbolic model checking', 'state space minimization techniques', 'state explosion', 'explicit-state verification tools', 'experimental results', 'optimization', 'invariance properties', 'BDD', 'binary decision diagram', 'backwards reachability', 'invariance verification', 'binary decision diagrams', 'bisimulation equivalence', 'formal verification', 'minimisation', 'reachability analysis']), ('Supporting global user profiles through trusted authorities\nPersonalization generally refers to making a Web site more responsive to the\nunique and individual needs of each user. We argue that for\npersonalization to work effectively, detailed and interoperable user\nprofiles should be globally available for authorized sites, and these\nprofiles should dynamically reflect changes in user interests. Creating\nuser profiles from user click-stream data seems to be an effective way\nof generating detailed and dynamic user profiles. However, a user\nprofile generated in this way is available only on the computer where\nthe user accesses his browser, and is inaccessible when the same user\nworks on a different computer. On the other hand, integration of the\nInternet with telecommunication networks has made it possible for the\nusers to connect to the Web with a variety of mobile devices as well as\ndesktops. This requires that user profiles should be available to any\ndesktop or mobile device on the Internet that users choose to work\nwith. In this paper, we address these problems through the concept of\n"trusted authority". A user agent at the client side that captures the\nuser click stream, dynamically generates a navigational history \'log\'\nfile in Extensible Markup Language (XML). This log file is then used to\nproduce \'user profiles\' in a resource description framework (RDF). A\nuser\'s right to privacy is provided through the Platform for Privacy\nPreferences (P3P) standard. User profiles are uploaded to the trusted\nauthority and served next time the user connects to the Web\n', ['global user profiles', 'trusted authorities', 'personalization', 'Web site', 'Internet', 'telecommunication networks', 'mobile device', 'user agent', 'user click stream', 'navigational history log file', 'XML', 'resource description framework', 'privacy', 'Platform for Privacy Preferences standard', 'namespace qualifier', 'globally unique user ID/password identification', 'client-server systems', 'data privacy', 'file servers', 'hypermedia markup languages', 'information needs', 'information resources', 'Internet', 'online front-ends']), ("Node-capacitated ring routing\nWe consider the node-capacitated routing problem in an undirected ring network\nalong with its fractional relaxation, the node-capacitated\nmulticommodity flow problem. For the feasibility problem, Farkas' lemma\nprovides a characterization for general undirected graphs, asserting\nroughly that there exists such a flow if and only if the so-called\ndistance inequality holds for every choice of distance functions\narising from nonnegative node weights. For rings, this\n(straightforward) result will be improved in two ways. We prove that,\nindependent of the integrality of node capacities, it suffices to\nrequire the distance inequality only for distances arising from\n(0-1-2)-valued node weights, a requirement that will be called the\ndouble-cut condition. Moreover, for integer-valued node capacities, the\ndouble-cut condition implies the existence of a half-integral\nmulticommodity flow. In this case there is even an integer-valued\nmulticommodity flow that violates each node capacity by at most one.\nOur approach gives rise to a combinatorial, strongly polynomial\nalgorithm to compute either a violating double-cut or a\nnode-capacitated multicommodity flow. A relation of the problem to its\nedge-capacitated counterpart will also be explained\n", ['node-capacitated routing problem', 'node-capacitated ring routing', 'undirected ring network', 'fractional relaxation', 'node-capacitated multicommodity flow problem', 'feasibility problem', 'Farkas lemma', 'undirected graphs', 'distance inequality', 'distance functions', 'nonnegative node weights', 'node capacity integrality', 'double-cut condition', 'integer-valued node capacities', 'half-integral multicommodity flow', 'integer-valued multicommodity flow', 'combinatorial strongly polynomial algorithm', 'violating double-cut', 'edge-cut criterion', 'graph theory', 'operations research', 'telecommunication network routing']), ("A conference's impact on undergraduate female students\nIn September of 2000, the 3rd Grace Hopper Celebration of Women in Computing\nwas held in Cape Cod, Massachusetts. Along with a colleague from a\nnearby university, we accompanied seven of our female undergraduate\nstudents to this conference. This paper reports on how the conference\nexperience immediately affected these students - what impressed them,\nwhat scared them, what it clarified for them. It also reports on how\nthe context in which these students currently evaluate their ability,\npotential and opportunity in computer science is different now from\nwhat it was before the conference. Hopefully, by understanding their\nexperience, we can gain some insight into things we can do for all of\nour undergraduate female students to better support their computer\nscience and engineering education\n", ['undergraduate female students', 'computer science education', 'engineering education', 'gender issues', 'conference', 'computer science education', 'gender issues', 'social aspects of automation']), ('Infrared-image classification using hidden Markov trees\nAn image of a three-dimensional target is generally characterized by the\nvisible target subcomponents, with these dictated by the target-sensor\norientation (target pose). An image often changes quickly with variable\npose. We define a class as a set of contiguous target-sensor\norientations over which the associated target image is relatively\nstationary with aspect. Each target is in general characterized by\nmultiple classes. A distinct set of Wiener filters are employed for\neach class of images, to identify the presence of target subcomponents.\nA Karhunen-Loeve representation is used to minimize the number of\nfilters (templates) associated with a given subcomponent. The\nstatistical relationships between the different target subcomponents\nare modeled via a hidden Markov tree (HMT). The HMT classifier is\ndiscussed and example results are presented for\nforward-looking-infrared (FLIR) imagery of several vehicles\n', ['IR image classification', 'infrared-image classification', 'hidden Markov trees', '3D target image', 'target-sensor orientation', 'target pose', 'contiguous target-sensor orientations', 'Wiener filters', 'Karhunen-Loeve representation', 'minimization', 'HMT', 'forward-looking-infrared imagery', 'FLIR imagery', 'vehicles', 'image classification', 'infrared imaging', 'Karhunen-Loeve transforms', 'Markov processes', 'minimisation', 'trees (mathematics)', 'Wiener filters']), ("Impact of user satisfaction and trust on virtual team members\nPressured by the growing need for fast response times, mass customization, and\nglobalization, many organizations are turning to flexible\norganizational forms, such as virtual teams. Virtual teams consist of\ncooperative relationships supported by information technology to\novercome limitations of time and/or location. Virtual teams require\ntheir members to rely heavily on the use of information technology and\ntrust in coworkers. This study investigates the impacts that the\nreliance on information technology (operationalized in our study via\nthe user satisfaction construct) and trust have on the job satisfaction\nof virtual team members. The study findings reveal that both user\nsatisfaction and trust are positively related to job satisfaction in\nvirtual teams, while system use was not found to play a significant\nrole. These findings emphasize that organizations seeking the benefits\nof flexible, IT-enabled virtual teams must consider both the level of\ntrust among colleagues, and the users' satisfaction with the\ninformation technology on which virtual teams rely\n", ['information technology', 'trust', 'IT', 'user satisfaction', 'job satisfaction', 'virtual team members', 'business data processing', 'groupware', 'human factors', 'information technology']), ('The decision procedure for profitability of investment projects using the\ninternal rate of return of single-period projects\nThe internal rate of return (IRR) criterion is often used to evaluate\nprofitability of investment projects. In this paper, we focus on a\nsingle-period project which consists of two types of cash flows; an\ninvestment at one period and a return at a succeeding period, and a\nfinancing at one period and a repayment at a succeeding period. We\ndecompose the given investment project into a series of the\nsingle-period projects. From the viewpoint of the single-period\nproject, we point out the applicability issue of the IRR criterion,\nnamely the IRR criterion cannot be applied in which a project is\ncomposed of both investment type and financing type. Investigating the\nproperties of a series of the single-period projects, we resolve the\napplicability issue of the IRR criterion and propose the decision\nprocedure for profitability judgment toward any type of investment\nproject based on the comparison between the IRR and the capital cost.\nWe develop a new algorithm to obtain the value of the project\ninvestment rate (PIR) for the given project, which is a function of the\ncapital cost, only using the standard IRR computing routine. This\noutcome is a theoretical breakthrough to widen the utilization of IRR\nin practical applications\n', ['decision procedure', 'investment project profitability', 'internal rate of return', 'single-period projects', 'profitability', 'cash flows', 'investment project decomposition', 'IRR criterion', 'project investment rate', 'PIR', 'decision theory', 'investment']), ('Bayesian nonstationary autoregressive models for biomedical signal analysis\nWe describe a variational Bayesian algorithm for the estimation of a\nmultivariate autoregressive model with time-varying coefficients that\nadapt according to a linear dynamical system. The algorithm allows for\ntime and frequency domain characterization of nonstationary\nmultivariate signals and is especially suited to the analysis of\nevent-related data. Results are presented on synthetic data and real\nelectroencephalogram data recorded in event-related desynchronization\nand photic synchronization scenarios\n', ['photic synchronization scenarios', 'event-related desynchronization', 'frequency domain characterization', 'time domain characterization', 'biomedical signal analysis', 'Kalman smoother', 'EEG analysis', 'Bayesian nonstationary autoregressive models', 'linear dynamical system', 'variational Bayesian algorithm', 'time-varying coefficients', 'autoregressive processes', 'Bayes methods', 'electroencephalography', 'frequency-domain analysis', 'medical signal processing', 'physiological models', 'time series', 'time-domain analysis']), ('Use of Bayesian Belief Networks when combining disparate sources of information\nin the safety assessment of software-based systems\nThe paper discusses how disparate sources of information can be combined in the\nsafety assessment of software-based systems. The emphasis is put on an\nemerging methodology, relevant for intelligent product-support systems,\nto combine information about disparate evidences systematically based\non Bayesian Belief Networks. The objective is to show the link between\nbasic information and the confidence one can have in a system. How one\ncombines the Bayesian Belief Net (BBN) method with a software safety\nstandard (RTCA/DO-178B,) for safety assessment of software-based\nsystems is also discussed. Finally, the applicability of the BBN\nmethodology and experiences from cooperative research work together\nwith Kongsberg Defence & Aerospace and Det Norske Veritas, and ongoing\nresearch with VTT Automation are presented\n', ['Bayesian belief networks', 'intelligent product-support systems', 'software safety standard', 'safety assessment', 'software-based systems', 'belief networks', 'safety-critical software', 'software reliability']), ('Online auctions: dynamic pricing and the lodging industry\nThe traditional channels of distribution for overnight accommodation are\nrapidly being displaced by Web site scripting, online intermediaries,\nand specialty brokers. Businesses that pioneered Internet usage relied\non it as a sales and marketing alternative to predecessor product\ndistribution channels. As such, Web sites replace the traditional\ntrading model to the Internet. Web-enabled companies are popular\nbecause the medium renders the process faster, less costly, highly\nreliable, and secure. Auction-based models impact business models by\nconverting the price setting mechanism from supplier-centric to\nmarket-centric and transforming the trading model from "one to many" to\n"many to many." Historically, pricing was based on the cost of\nproduction plus a margin of profit. Traditionally, as products and\nservices move through the supply chain, from the producer to the\nconsumer, various intermediaries added their share of profit to the\nprice. As Internet based mediums of distribution become more prevalent,\ntraditional pricing models are being supplanted with dynamic pricing. A\ndynamic pricing model represents a flexible system that changes prices\nnot only from product to product, but also from customer to customer\nand transaction to transaction. Many industry leaders are skeptical of\nthe long run impact of online auctions on lodging industry profit\nmargins, despite the fact pricing theory suggests that an increase in\nthe flow of information results in efficient market pricing. The future\nof such endeavors remains promising, but controversial\n', ['online auctions', 'dynamic pricing', 'lodging industry', 'overnight accommodations', 'Web site scripting', 'online intermediaries', 'specialty brokers', 'Internet usage', 'sales', 'marketing', 'trading model', 'business models', 'price setting mechanism', 'supply chain', 'costing', 'electronic commerce', 'information resources', 'Internet', 'travel industry']), ('Flow measurement - future directions\nInterest in the flow of liquids and its measurement can be traced back to early\nstudies by the Egyptians, the Chinese and the Romans. Since these early\ntimes the science of flow measurement has undergone a massive change\nbut during the last 25 years or so (1977-2002) it has matured\nenormously. One of the principal reasons for this is that higher\naccuracies and reliabilities have been demanded by industry in the\nmeasurement of fiscal transfers and today there is vigorous interest in\nthe subject from both the flowmeter manufacturer and user viewpoints.\nThis interest is coupled with the development of advanced computer\ntechniques in fluid mechanics together with the application of\nincreasingly sophisticated electronics\n', ['flow measurement', 'flow metering', 'signal processing', 'liquid flow', 'Egyptians', 'Chinese', 'Romans', 'fiscal transfers', 'flowmeter manufacturer', 'advanced computer techniques', 'fluid mechanics', 'electronics application', 'computational fluid dynamics', 'flow measurement', 'flowmeters', 'technological forecasting']), ('Developing a CD-ROM as a teaching and learning tool in food and beverage\nmanagement: a case study in hospitality education\nFood and beverage management is the traditional core of hospitality education\nbut, in its laboratory manifestation, has come under increasing\npressure in recent years. It is an area that, arguably, presents the\ngreatest challenges in adaptation to contemporary learning technologies\nbut, at the same time, stands to benefit most from the potential of the\nWeb. This paper addresses the design and development of a CD-ROM\nlearning resource for food and beverage. It is a learning resource\nwhich is designed to integrate with rather than to replace existing\nconventional classroom and laboratory learning methods and, thus,\ncompensate for the decline in the resource base faced in food and\nbeverage education in recent years. The paper includes illustrative\nmaterial drawn from the CD-ROM which demonstrates its use in teaching\nand learning\n', ['food and beverage management', 'hospitality education', 'CD-ROM', 'learning tool', 'teaching tool', 'catering industry', 'courseware', 'hotel industry', 'management education', 'teaching']), ('Support vector machines model for classification of thermal error in machine\ntools\nThis paper addresses a change in the concept of machine tool thermal error\nprediction which has been hitherto carried out by directly mapping them\nwith the temperature of critical elements on the machine. The model\ndeveloped herein using support vector machines, a powerful\ndata-training algorithm, seeks to account for the impact of specific\noperating conditions, in addition to temperature variation, on the\neffective prediction of thermal errors. Several experiments were\nconducted to study the error pattern, which was found to change\nsignificantly with variation in operating conditions. This model\nattempts to classify the error based on operating conditions. Once\nclassified, the error is then predicted based on the temperature\nstates. This paper also briefly describes the concept of the\nimplementation of such a comprehensive model along with an on-line\nerror assessment and calibration system in a PC-based open-architecture\ncontroller environment, so that it could be employed in regular\nproduction for the purpose of periodic calibration of machine tools\n', ['SVM', 'support vector machines model', 'thermal error classification', 'machine tool thermal error prediction', 'critical element temperature', 'data-training algorithm', 'error pattern', 'online error assessment', 'online calibration system', 'PC-based open-architecture controller environment', 'calibration', 'errors', 'factory automation', 'learning automata', 'machine tools', 'mechanical engineering computing', 'microcomputer applications', 'pattern classification']), ('Deterministic calculations of photon spectra for clinical accelerator targets\nA method is proposed to compute photon energy spectra produced in clinical\nelectron accelerator targets, based on the deterministic solution of\nthe Boltzmann equation for coupled electron-photon transport in\none-dimensional (1-D) slab geometry. It is shown that the deterministic\nmethod gives similar results as Monte Carlo calculations over the\nangular range of interest for therapy applications. Relative energy\nspectra computed by deterministic and 3-D Monte Carlo methods,\nrespectively, are compared for several realistic target materials and\ndifferent electron beams, and are found to give similar photon energy\ndistributions and mean energies. The deterministic calculations\ntypically require 1-2 mins of execution time on a Sun Workstation,\ncompared to 2-36 h for the Monte Carlo runs\n', ['photon energy spectra', 'deterministic calculations', 'clinical electron accelerator targets', 'Boltzmann equation', 'coupled electron-photon transport', 'one-dimensional slab geometry', 'angular range of interest', 'therapy applications', 'relative energy spectra', '3-D Monte Carlo methods', 'linear accelerator', 'therapy planning', 'integrodifferential equation', 'pencil beam source representations', 'Boltzmann equation', 'determinants', 'dosimetry', 'medical computing', 'photon transport theory', 'radiation therapy']), ('A portable Auto Attendant System with sophisticated dialog structure\nAn attendant system connects the caller to the party he/she wants to talk to.\nTraditional systems require the caller to know the full name of the\nparty. If the caller forgets the name, the system fails to provide\nservice for the caller. In this paper we propose a portable Auto\nAttendant System (AAS) with sophisticated dialog structure that gives a\ncaller more flexibility while calling. The caller may interact with the\nsystem to request a phone number by providing just a work area,\nspecialty, surname, or title, etc. If the party is absent, the system\nmay provide extra information such as where he went, when he will be\nback, and what he is doing. The system is built modularly, with\ncomponents such as speech recognizer, language model, dialog manager\nand text-to-speech that can be replaced if necessary. By simply\nchanging the personnel record database, the system can easily be ported\nto other companies. The sophisticated dialog manager applies many\nstrategies to allow natural interaction between user and system.\nFunctions such as fuzzy request, user repairing, and extra information\nquery, which are not provided by other systems, are integrated into our\nsystem. Experimental results and comparisons to other systems show that\nour approach provides a more user friendly and natural interaction for\nauto attendant system\n', ['attendant system', 'Auto Attendant System', 'fuzzy request', 'clear request', 'semantic frame', 'dialog manager', 'spoken dialog systems', 'telephone', 'speech recognizer', 'telephone-based system', 'information systems', 'speech recognition', 'telephony']), ('Quantum sensitive dependence\nWave functions of bounded quantum systems with time-independent potentials,\nbeing almost periodic functions, cannot have time asymptotics as in\nclassical chaos. However, bounded quantum systems with time-dependent\ninteractions, as used in quantum control, may have continuous spectrum\nand the rate of growth of observables is an issue of both theoretical\nand practical concern. Rates of growth in quantum mechanics are\ndiscussed by constructing quantities with the same physical meaning as\nthose involved in the classical Lyapunov exponent. A generalized notion\nof quantum sensitive dependence is introduced and the mathematical\nstructure of the operator matrix elements that correspond to different\ntypes of growth is characterized\n', ['quantum sensitive dependence', 'wave functions', 'bounded quantum systems', 'time-independent potentials', 'periodic functions', 'time asymptotics', 'classical chaos', 'time-dependent interactions', 'quantum control', 'classical Lyapunov exponent', 'operator matrix elements', 'quantum complexity', 'bound states', 'chaos', 'Lyapunov methods', 'matrix algebra', 'nonlinear control systems', 'quantum theory', 'wave functions']), ('Convergence of finite element approximations and multilevel linearization for\nGinzburg-Landau model of d-wave superconductors\nIn this paper, we consider the finite element approximations of a recently\nproposed Ginzburg-Landau-type model for d-wave superconductors. In\ncontrast to the conventional Ginzburg-Landau model the scalar complex\nvalued order-parameter is replaced by a multicomponent complex\norder-parameter and the free energy is modified according to the d-wave\nparing symmetry. Convergence and optimal error estimates and some\nsuper-convergent estimates for the derivatives are derived.\nFurthermore, we propose a multilevel linearization procedure to solve\nthe nonlinear systems. It is proved that the optimal error estimates\nand super-convergence for the derivatives are preserved by the\nmulti-level linearization algorithm\n', ['Ginzburg-Landau model', 'd-wave', 'superconductivity', 'finite element method', 'nonlinear systems', 'error estimation', 'two-grid method', 'free energy', 'multilevel linearization', 'approximation theory', 'digital simulation', 'error analysis', 'finite element analysis', 'free energy', 'physics computing', 'superconductivity']), ('Baseball, optimization, and the World Wide Web\nThe competition for baseball play-off spots-the fabled pennant race-is one of\nthe most closely watched American sports traditions. While play-off\nrace statistics, such as games back and magic number, are informative,\nthey are overly conservative and do not account for the remaining\nschedule of games. Using optimization techniques, one can model\nschedule effects explicitly and determine precisely when a team has\nsecured a play-off spot or has been eliminated from contention. The\nRIOT Baseball Play-off Races Web site developed at the University of\nCalifornia, Berkeley, provides automatic updates of new,\noptimization-based play-off race statistics each day of the major\nleague baseball season. In developing the site, we found that we could\ndetermine the first-place elimination status of all teams in a division\nusing a single linear-programming formulation, since a minimum win\nthreshold for teams finishing in first place applies to all teams in a\ndivision. We identified a similar (but weaker) result for the problem\nof play-off elimination with wildcard teams\n', ['baseball play-off spot competition', 'optimization', 'World Wide Web', 'pennant race', 'play-off race statistics', 'games back', 'magic number', 'game schedule', 'RIOT Baseball Play-off Races Web site', 'linear programming', 'LP', 'minimum win threshold', 'administrative data processing', 'information resources', 'Internet', 'linear programming', 'sport']), ('Comments on "Frequency decomposition and computing of ultrasound medical images\nwith wavelet packets"\nIn this paper, errors and discrepancies in the subject paper [Cincotti et al.\n(2002)] are highlighted. A comment, concerning the axial resolution\nassociated to the adopted processing procedure is also reported\n', ['ultrasound medical images', 'wavelet packets', 'frequency decomposition', 'axial resolution', 'medical diagnostic imaging', 'biomedical ultrasonics', 'image resolution', 'medical image processing', 'wavelet transforms']), ('Digital rights (and wrongs)\nAttempting to grasp the many conflicts and proposed safeguards for intellectual\nproperty is extremely difficult. Legal, political, economic, and\ncultural issues-both domestic and international-loom large, almost\ndwarfing the daunting technological challenges. Solutions devised by\ncourts and legislatures and regulatory agencies are always late out of\nthe blocks and fall ever farther behind. Recently proposed legislation\nonly illustrates the depth and complexity of the problem\n', ['intellectual property', 'cultural issues', 'economic issues', 'political issues', 'legal issues', 'industrial property', 'legislation']), ("Arbortext: enabler of multichannel publishing\nA company has a document-say, dosage instructions for a prescription drug or a\ntroubleshooting sheet for a DVD drive. That document starts its life in\na predictable format, probably Microsoft Word or WordPerfect, but\nthen-to meet the needs of readers who nowadays demand access via\nmultiple devices-the material has to be translated into many more\nformats: HTML, PageMaker, or Quark, possibly RTF, almost certainly PDF,\nand nowadays, next-generation devices (cell phones, handheld computers)\nalso impose their own requirements. And what if, suddenly, the dosage\nlevels change or new workarounds emerge to handle DVD problems? That's\nwhen a company should put in a call to Arbortext, a 20-year-old Ann\nArbor, Michigan-based company that exists to solve a single problem:\nhelping clients automate multichannel publishing\n", ['document format', 'next-generation devices', 'Arbortext', 'multichannel publishing', 'content assets', 'document handling', 'electronic publishing']), ('The efficacy of electronic telecommunications in fostering interpersonal\nrelationships\nThe effectiveness of electronic telecommunications as a supplementary aid to\ninstruction and as a communication link between students, and between\nstudents and instructors in fostering interpersonal relationships was\nexplored in this study. More specifically, the impacts of e-mail, one\nof the most accessible, convenient, and easy to use computer-mediated\ncommunications, on student attitudes toward the instructor,\ngroup-mates, and other classmates were investigated. A posttest-only\nexperimental design was adopted. In total, 68 prospective teachers\nenrolling in a "Computers in Education" course participated in the\nstudy for a whole semester. Results from the study provided substantial\nevidence supporting e-mail\'s beneficial effects on student attitudes\ntoward the instructor and other classmates\n', ['interpersonal relationships', 'telecommunications', 'student communication link', 'e-mail', 'computer-mediated communications', 'student attitudes', 'Computers in Education course', 'educational technology', 'educational technology', 'electronic mail', 'human factors', 'social aspects of automation']), ("Measuring return: revealing ROI\nThe most critical part of the return-on-investment odyssey is to develop\nmetrics that matter to the business and to measure systems in terms of\ntheir ability to help achieve those business goals. Everything must\nflow from those key metrics. And don't forget to revisit those every\nnow and then, too. Since all systems wind down over time, it's\nimportant to keep tabs on how well your automation investment is\nmeeting the metrics established by your company. Manufacturers are\nclamoring for a tool to help quantify returns and analyze the results\n", ['technology purchases', 'return-on-investment', 'ROI', 'key metrics', 'automation investment', 'management', 'manufacturing industries']), ('Applied ethics in business information units\nThe primary thesis of this paper is that business information professionals\ncommonly overlook ethical dilemmas in the workplace. Although the\nthesis remains unproven, the author highlights, by way of real and\nhypothetical case studies, a number of situations in which ethical\ntensions can be identified, and suggests that information professionals\nneed to be more aware of the moral context of their actions. Resolving\nethical dilemmas should be one of the aims of competent information\nprofessionals and their managers, although it is recognized that\ndilemmas often cannot easily be resolved. A background to the main\ntheories of applied ethics forms the framework for later discussion\n', ['business information professionals', 'ethical dilemmas', 'moral context', 'applied ethics', 'business information units', 'information industry', 'information science', 'professional aspects']), ('Hamiltonian modelling and nonlinear disturbance attenuation control of TCSC for\nimproving power system stability\nTo tackle the obstacle of applying passivity-based control (PBC) to power\nsystems, an affine non-linear system widely existing in power systems\nis formulated as a standard Hamiltonian system using a pre-feedback\nmethod. The port controlled Hamiltonian with dissipation (PCHD) model\nof a thyristor controlled serial compensator (TCSC) is then established\ncorresponding with a revised Hamiltonian function. Furthermore,\nemploying the modified Hamiltonian function directly as the storage\nfunction, a non-linear adaptive L/sub 2/ gain control method is\nproposed to solve the problem of L/sub 2/ gain disturbance attenuation\nfor this Hamiltonian system with parametric perturbations. Finally,\nsimulation results are presented to verify the validity of the proposed\ncontroller\n', ['Hamiltonian modelling', 'thyristor controlled serial compensator', 'nonlinear disturbance attenuation control', 'power system stability', 'passivity-based control', 'affine nonlinear system', 'pre-feedback method', 'port controlled Hamiltonian with dissipation model', 'Hamiltonian function', 'storage function', 'nonlinear adaptive L/sub 2/ gain control method', 'parametric perturbations', 'adaptive control', 'closed loop systems', 'compensation', 'control system synthesis', 'feedback', 'nonlinear control systems', 'power system stability', 'thyristor applications']), ('Quadratic programming algorithms for large-scale model predictive control\nQuadratic programming (QP) methods are an important element in the application\nof model predictive control (MPC). As larger and more challenging MPC\napplications are considered, more attention needs to be focused on the\nconstruction and tailoring of efficient QP algorithms. In this study,\nwe tailor and apply a new QP method, called QPSchur, to large MPC\napplications, such as cross directional control problems in paper\nmachines. Written in C++, QPSchur is an object oriented implementation\nof a novel dual space, Schur complement algorithm. We compare this\napproach to three widely applied QP algorithms and show that QPSchur is\nsignificantly more efficient (up to two orders of magnitude) than the\nother algorithms. In addition, detailed simulations are considered that\ndemonstrate the importance of the flexible, object oriented\nconstruction of QPSchur, along with additional features for constraint\nhandling, warm starts and partial solution\n', ['large-scale model predictive control', 'quadratic programming algorithms', 'QPSchur', 'cross directional control problems', 'paper machines', 'object oriented implementation', 'dual space Schur complement algorithm', 'simulations', 'flexible object oriented construction', 'constraint handling', 'warm starts', 'partial solution', 'control system analysis computing', 'large-scale systems', 'object-oriented programming', 'paper industry', 'predictive control', 'quadratic programming']), ('A new voltage-vector selection algorithm in direct torque control of induction\nmotor drives\nAC drives based on direct torque control of induction machines allow high\ndynamic performance to be obtained with very simple control schemes.\nThe drive behavior, in terms of current, flux and torque ripple, is\ndependent on the utilised voltage vector selection strategy and the\noperating conditions. In this paper a new voltage vector selection\nalgorithm, which allows a sensible reduction of the RMS value of the\nstator current ripple without increasing the average value of the\ninverter switching frequency and without the need of a PWM pulse\ngenerator block is presented Numerical simulations have been carried\nout to validate the proposed method\n', ['voltage-vector selection algorithm', 'direct torque control', 'induction motor drives', 'AC drives', 'high dynamic performance', 'torque ripple', 'voltage vector selection strategy', 'operating conditions', 'RMS value', 'stator current ripple', 'inverter switching frequency', 'torque variations', 'flux variations', '4-poles induction motor', 'steady-state operation', 'dynamic behavior', 'torque step response', '220 V', '50 Hz', '4 kW', 'induction motor drives', 'invertors', 'machine control', 'stators', 'torque control']), ('How does attitude impact IT implementation: a study of small business owners\nAccording to previous studies, attitude towards information technology (IT)\namong small business owners appears to be a key factor in achieving\nhigh quality IT implementations. In an effort to extend this stream of\nresearch, we conducted case studies with small business owners and\nlearned that high quality IT implementations resulted with owners who\nhad positive or negative attitudes toward IT, but not with owners who\nhad uncertain attitudes. Owners with apolar attitude, either positive\nor negative, all took action to temper the uncertainty and risk\nsurrounding the use of new IT in their organization. In contrast,\nowners with uncertain attitudes did not make mitigating attempts to\nreduce uncertainty and risk. A consistent finding among those with high\nquality IT implementations was an entrepreneurial, or shared,\nmanagement style. It is proposed, based on case study data, that small\nbusiness owners with an uncertain attitude towards IT might experience\nhigher quality IT results in their organizations through practicing a\nmore entrepreneurial, or shared, management style. The study provides\ninsights for both computer specialists and small business owners\nplanning IT implementations\n', ['small business owners', 'information technology implementation', 'negative attitudes', 'positive attitudes', 'uncertain attitude', 'risk', 'organization', 'management style', 'computer specialists', 'planning', 'business data processing', 'human factors', 'information technology', 'personal computing', 'risk management']), ('SIA shelves T+1 decision till 2004\nThe Securities Industry Association has decided that a move to T+1 is more than\nthe industry can handle right now. STP, however, will remain a focus\n', ['Securities Industry Association', 'straight-through-processing', 'T+1', 'securities trading']), ('Control performance with three translational degrees of freedom\nFor multiple degree-of-freedom (DOF) systems, it is important to determine how\naccurately operators can control each DOF and what influence\nperceptual, information processing, and psychomotor components have on\nperformance. Sixteen right-handed male students participated in 2\nexperiments: 1 involving positioning and 1 involving tracking with 3\ntranslational DOFs. To separate perceptual and psychomotor effects, we\nused 2 control-display mappings that differed in the coupling of\nvertical and depth dimensions to the up-down and fore-aft control axes.\nWe observed information processing effects in the positioning task:\nInitial error correction on the vertical dimension lagged in time\nbehind the horizontal dimension. The depth dimension error correction\nlagged behind both, which was ascribed to the poorer perceptual\ninformation. We observed this perceptual effect also in the tracking\nexperiment. Motor effects were also present, with tracking errors along\nthe up-down axis of the hand controller being 1.1 times larger than\nalong the fore-aft axis. These results indicate that all 3 components\ncontribute to control performance. Actual applications of this research\ninclude interface design\n', ['control performance', 'multi-DOF systems', 'positioning', 'tracking', 'perceptual effects', 'psychomotor effects', 'initial error correction', 'depth dimension error correction', 'interface design', 'remote control', 'virtual reality', 'graphical user interfaces', 'human factors']), ('Real-time transmission of pediatric echocardiograms using a single ISDN line\nWe tested the adequacy of a videoconferencing system using a single integrated\nsystems digital network (ISDN) line (128 kilobits per second) for the\nremote diagnosis of children with suspected congenital heart disease\n(CHD). Real-time echocardiogram interpretation was compared to\nsubsequent videotape review in 401 studies with concordance in 383\n(95.5%) studies. A new diagnosis of CHD was made in 98 studies.\nImmediate patient transfer was arranged based upon a real-time\ndiagnosis in five studies. In 300 studies, a normal diagnosis obviated\nfurther evaluation. A single ISDN line is adequate for transmission of\npediatric echocardiograms and it allows for remote management of\npatients with CHD\n', ['real-time pediatric echocardiogram transmission', 'single ISDN line', 'videoconferencing system', 'remote diagnosis', 'children', 'suspected congenital heart disease', 'real-time echocardiogram interpretation', 'videotape review', 'immediate patient transfer', 'remote patient management', 'diseases', 'echocardiography', 'ISDN', 'medical signal processing', 'paediatrics', 'patient diagnosis', 'real-time systems', 'teleconferencing', 'telemedicine']), ("Semantic B2B integration: issues in ontology-based approaches\nSolving queries to support e-commerce transactions can involve retrieving and\nintegrating information from multiple information resources. Often,\nusers don't care which resources are used to answer their query. In\nsuch situations, the ideal solution would be to hide from the user the\ndetails of the resources involved in solving a particular query. An\nexample would be providing seamless access to a set of heterogeneous\nelectronic product catalogues. There are many problems that must be\naddressed before such a solution can be provided. In this paper, we\ndiscuss a number of these problems, indicate how we have addressed\nthese and go on to describe the proof-of-concept demonstration system\nwe have developed\n", ['e-commerce transactions', 'queries', 'information integration', 'information retrieval', 'multiple information resources', 'heterogeneous electronic product catalogues', 'ontology-based approaches', 'semantic B2B integration', 'client-server systems', 'distributed databases', 'electronic commerce', 'knowledge engineering', 'nomenclature', 'query processing']), ('Incremental motion control of linear synchronous motor\nIn this study a particular incremental motion control problem, which is\nspecified by the trapezoidal velocity profile using multisegment\nsliding mode control (MSSMC), is proposed to control a permanent magnet\nlinear synchronous motor (PMLSM) servo drive system. First, the\nstructure and operating principle of the PMLSM are described in detail.\nSecond, a field-oriented control PMLSM servo drive is introduced. Then,\neach segment of the multisegment switching surfaces is designed to\nmatch the corresponding part of the trapezoidal velocity profile, thus\nthe motor dynamics on the specified-segment switching surface have the\ndesired velocity or acceleration corresponding part of the trapezoidal\nvelocity profile. In addition, the proposed control system is\nimplemented in a PC-based computer control system. Finally, the\neffectiveness of the proposed PMLSM servo drive system is demonstrated\nby some simulated and experimental results\n', ['incremental motion control', 'linear synchronous motor', 'trapezoidal velocity profile', 'multisegment sliding mode control', 'permanent magnet motor', 'servo drive system', 'field-oriented control', 'multisegment switching surfaces', 'motor dynamics', 'linear synchronous motors', 'motion control', 'permanent magnet motors', 'servomotors', 'variable structure systems']), ("What's in a name? [mobile telephony branding]\nMobile operators are frantically consolidating businesses into single\ninternational brands\n", ['mobile telephony', 'branding', 'consolidating businesses', 'cellular radio']), ("Data assimilation of local model error forecasts in a deterministic model\nOne of the most popular data assimilation techniques in use today are of the\nKalman filter type, which provide an improved estimate of the state of\na system up to the current time level, based on actual measurements.\nFrom a forecasting viewpoint, this corresponds to an updating of the\ninitial conditions. The standard forecasting procedure is to then run\nthe model uncorrected into the future, driven by predicted boundary and\nforcing conditions. The problem with this methodology is that the\nupdated initial conditions quickly 'wash-out', thus, after a certain\nforecast horizon the model predictions are no better than from an\ninitially uncorrected model. This study demonstrates that through the\nassimilation of error forecasts (in the present case made using\nso-called local models) entire model domains can be corrected for\nextended forecast horizons (i.e. long after updated initial conditions\nhave become washed-out), thus demonstrating significant improvements\nover the conventional methodology. Some alternate uses of local models\nare also explored for the re-distribution of error forecasts over the\nentire model domain, which are then compared with more conventional\nKalman filter type schemes\n", ['data assimilation', 'local model error forecasts', 'deterministic model', 'Kalman filter', 'forcing conditions', 'forecast horizon', 'error prediction', 'hydrodynamic modelling', 'computational fluid dynamics', 'deterministic algorithms', 'error statistics', 'hydrodynamics', 'Kalman filters']), ('VPP Fortran and the design of HPF/JA extensions\nVPP Fortran is a data parallel language that has been designed for the VPP\nseries of supercomputers. In addition to pure data parallelism, it\ncontains certain low-level features that were designed to extract high\nperformance from user programs. A comparison of VPP Fortran and\nHigh-Performance Fortran (HPF) 2.0 shows that these low-level features\nare not available in HPF 2.0. The features include asynchronous\ninterprocessor communication, explicit shadow, and the LOCAL directive.\nThey were shown in VPP Fortran to be very useful in handling real-world\napplications, and they have been included in the HPF/JA extensions.\nThey are described in the paper. The HPF/JA Language Specification\nVersion 1.0 is an extension of HPF 2.0 to achieve practical performance\nfor real-world applications and is a result of collaboration in the\nJapan Association for HPF (JAHPF). Some practical programming and\ntuning procedures with the HPF/JA Language Specification are described,\nusing the NAS Parallel Benchmark BT as an example\n', ['VPP Fortran', 'data parallel language', 'data parallelism', 'high performance', 'asynchronous interprocessor communication', 'explicit shadow', 'benchmark', 'asynchronous communication', 'data locality', 'FORTRAN', 'parallel languages']), ('Robot trajectory control using neural networks\nThe use of a new type of neural network (NN) for controlling the trajectory of\na robot is discussed. A control system is described which comprises an\nNN-based controller and a fixed-gain feedback controller. The NN-based\ncontroller employs a modified recurrent NN, the weights of which are\nobtained by training another NN to identify online the inverse dynamics\nof the robot. The work has confirmed the superiority of the proposed\nNN-based control system in rejecting large disturbances\n', ['robot trajectory control', 'neural networks', 'neural network-based controller', 'control system', 'fixed-gain feedback controller', 'modified recurrent neural network', 'neural network training', 'robot inverse dynamics', 'large disturbance rejection', 'robot manipulators', 'time-varying nonlinear multivariable plant', 'fourth-order Runge-Kutta algorithm', 'feedback', 'learning (artificial intelligence)', 'manipulator dynamics', 'mobile robots', 'multivariable control systems', 'neurocontrollers', 'nonlinear control systems', 'position control', 'recurrent neural nets', 'Runge-Kutta methods', 'time-varying systems']), ('Numerical approximation of nonlinear BVPs by means of BVMs\nBoundary Value Methods (BVMs) would seem to be suitable candidates for the\nsolution of nonlinear Boundary Value Problems (BVPs). They have been\nsuccessfully used for solving linear BVPs together with a mesh\nselection strategy based on the conditioning of the linear systems. Our\naim is to extend this approach so as to use them for the numerical\napproximation of nonlinear problems. For this reason, we consider the\nquasi-linearization technique that is an application of the Newton\nmethod to the nonlinear differential equation. Consequently, each\niteration requires the solution of a linear BVP. In order to guarantee\nthe convergence to the solution of the continuous nonlinear problem, it\nis necessary to determine how accurately the linear BVPs must be\nsolved. For this goal, suitable stopping criteria on the residual and\non the error for each linear BVP are given. Numerical experiments on\nstiff problems give rather satisfactory results, showing that the\nexperimental code, called TOM, that uses a class of BVMs and the\nquasi-linearization technique, may be competitive with well known\nsolvers for BVPs\n', ['numerical approximation', 'nonlinear boundary value problems', 'boundary value methods', 'mesh selection strategy', 'quasi-linearization technique', 'Newton method', 'nonlinear differential equation', 'stopping criteria', 'stiff problems', 'BVMs', 'boundary-value problems', 'Newton method', 'nonlinear differential equations']), ("Nurturing clients' trust to encourage engagement success during the\ncustomization of ERP systems\nCustomization is a crucial, lengthy, and costly aspect in the successful\nimplementation of ERP systems, and has, accordingly, become a major\nspecialty of many vendors and consulting companies. The study examines\nhow such companies can increase their clients' perception of engagement\nsuccess through increased client trust that is brought about through\nresponsive and dependable customization. Survey data from ERP\ncustomization clients show that, as hypothesized, clients' trust\ninfluenced their perception of engagement success with the company. The\ndata also show that clients' trust in the customization company was\nincreased when the company behaved in accordance with client\nexpectations by being responsive, and decreased when the company\nbehaved in a manner that contradicted these expectations by not being\ndependable. Responses to an open-ended question addendum attached to\nthe survey corroborated the importance of responsiveness and\ndependability. Implications for customization companies and research on\ntrust are discussed\n", ['client trust', 'engagement success', 'customization', 'ERP systems', 'enterprise resource planning systems', 'vendors', 'consulting companies', 'perceived responsiveness', 'MRP II implementation', 'integrity', 'benevolence', 'dependability', 'behavioural sciences', 'business data processing', 'human factors', 'management science']), ('Statistical analysis of nonlinearly reconstructed near-infrared tomographic\nimages. I. Theory and simulations\nNear-infrared (NIR) diffuse tomography is an emerging method for imaging the\ninterior of tissues to quantify concentrations of hemoglobin and\nexogenous chromophores noninvasively in vivo. It often exploits an\noptical diffusion model-based image reconstruction algorithm to\nestimate spatial property values from measurements of the light flux at\nthe surface of the tissue. In this study, mean-squared error (MSE) over\nthe image is used to evaluate methods for regularizing the ill-posed\ninverse image reconstruction problem in NIR tomography. Estimates of\nimage bias and image standard deviation were calculated based upon 100\nrepeated reconstructions of a test image with randomly distributed\nnoise added to the light flux measurements. It was observed that the\nbias error dominates at high regularization parameter values while\nvariance dominates as the algorithm is allowed to approach the optimal\nsolution. This optimum does not necessarily correspond to the minimum\nprojection error solution, but typically requires further iteration\nwith a decreasing regularization parameter to reach the lowest image\nerror. Increasing measurement noise causes a need to constrain the\nminimum regularization parameter to higher values in order to achieve a\nminimum in the overall image MSE\n', ['medical diagnostic imaging', 'hemoglobin', 'oxygen saturation', 'photon migration', 'optical diffusion model-based image reconstruction algorithm', 'decreasing regularization parameter', 'lowest image error', 'minimum regularization parameter constraint', 'bias error', 'optimal solution', 'light flux', 'mean-squared error', 'ill-posed inverse image reconstruction problem regularization', 'spatial property values estimation', 'test image', 'randomly distributed noise', 'O/sub 2/', 'image reconstruction', 'infrared imaging', 'inverse problems', 'iterative methods', 'medical image processing', 'optical tomography', 'statistical analysis']), ('New tuning method for PID controller\nIn this paper, a tuning method for proportional-integral-derivative (PID)\ncontroller and the performance assessment formulas for this method are\nproposed. This tuning method is based on a genetic algorithm based PID\ncontroller design method. For deriving the tuning formula, the genetic\nalgorithm based design method is applied to design PID controllers for\na variety of processes. The relationship between the controller\nparameters and the parameters that characterize the process dynamics\nare determined and the tuning formula is then derived. Using simulation\nstudies, the rules for assessing the performance of a PID controller\ntuned by the proposed method are also given. This makes it possible to\nincorporate the capability to determine if the PID controller is well\ntuned or not into an autotuner. An autotuner based on this new tuning\nmethod and the corresponding performance assessment rules is also\nestablished. Simulations and real-time experimental results are given\nto demonstrate the effectiveness and usefulness of these formulas\n', ['tuning method', 'PID controller', 'proportional-integral-derivative controller', 'genetic algorithm', 'controller design method', 'process dynamics', 'autotuner', 'control system analysis', 'control system synthesis', 'genetic algorithms', 'three-term control', 'tuning']), ('Geotensity: combining motion and lighting for 3D surface reconstruction\nThis paper is about automatically reconstructing the full 3D surface of an\nobject observed in motion by a single static camera. Based on the two\nparadigms, structure from motion and linear intensity subspaces, we\nintroduce the geotensity constraint that governs the relationship\nbetween four or more images of a moving object. We show that it is\npossible in theory to solve for 3D Lambertian surface structure for the\ncase of a single point light source and propose that a solution exists\nfor an arbitrary number point light sources. The surface may or may not\nbe textured. We then give an example of automatic surface\nreconstruction of a face under a point light source using arbitrary\nunknown object motion and a single fixed camera\n', ['full 3D surface', 'single static camera', 'linear intensity subspaces', 'geotensity constraint', '3D Lambertian surface structure', 'single point light source', 'arbitrary number point light sources', 'automatic surface reconstruction', 'point light source', 'linear image subspaces', 'structure-from-motion', 'computer vision', 'image reconstruction']), ('SBC gets more serious on regulatory compliance\nWith one eye on the past and the other on its future, SBC Communications last\nweek created a unit it hopes will bring a cohesiveness and efficiency\nto its regulatory compliance efforts that previously had been lacking.\nThe carrier also hopes the new regulatory compliance unit will help it\naccomplish its short-term goal of landing FCC approval. to provide\nlong-distance service throughout its region, and its longer-term, goal\nof reducing the regulatory burdens under which it and currently operate\n', ['SBC Communications', 'regulatory compliance', 'telecom carrier', 'telecommunication']), ('Watermarking techniques for electronic delivery of remote sensing images\nEarth observation missions have recently attracted a growing interest, mainly\ndue to the large number of possible applications capable of exploiting\nremotely sensed data and images. Along with the increase of market\npotential, the need arises for the protection of the image products.\nSuch a need is a very crucial one, because the Internet and other\npublic/private networks have become preferred means of data exchange. A\ncritical issue arising when dealing with digital image distribution is\ncopyright protection. Such a problem has been largely addressed by\nresorting to watermarking technology. A question that obviously arises\nis whether the requirements imposed by remote sensing imagery are\ncompatible with existing watermarking techniques. On the basis of these\nmotivations, the contribution of this work is twofold: assessment of\nthe requirements imposed by remote sensing applications on\nwatermark-based copyright protection, and modification of two\nwell-established digital watermarking techniques to meet such\nconstraints. More specifically, the concept of near-lossless\nwatermarking is introduced and two possible algorithms matching such a\nrequirement are presented. Experimental results are shown to measure\nthe impact of watermark introduction on a typical remote sensing\napplication, i.e., unsupervised image classification\n', ['remote sensing images', 'electronic delivery', 'watermarking techniques', 'Earth observation missions', 'copyright protection', 'digital watermarking', 'near-lossless watermarking', 'digital image distribution', 'unsupervised image classification', 'copy protection', 'image classification', 'image coding', 'remote sensing']), ('Virtual Development Center\nThe Virtual Development Center of the Institute for Women and Technology seeks\nto significantly enhance the impact of women on technology. It\naddresses this goal by increasing the number of women who have input on\ncreated technology, enhancing the ways people teach and develop\ntechnology, and developing need-based technology that serves the\ncommunity. Through activities of the Virtual Development Center, a\npattern is emerging regarding how computing technologies do or do not\nsatisfy the needs of community groups, particularly those communities\nserving women. This paper describes the Virtual Development Center\nprogram and offers observations on the impact of computing technology\non non-technical communities\n', ['Virtual Development Center', 'women', 'information technology', 'teaching', 'community groups', 'gender issues', 'computer science education', 'computer science education', 'gender issues', 'information technology', 'social aspects of automation']), ('Teaching modeling in management science\nThis essay discusses how we can most effectively teach Management Science to\nstudents in MBA or similar programs who will be, at best, part-time\npractitioners of these arts. I take as a working hypothesis the radical\nproposition that the heart of Management Science itself is not the\nimpressive array of tools that have been built up over the years\n(optimization, simulation, decision analysis, queuing, and so on) but\nrather the art of reasoning logically with formal models. I believe it\nis necessary with this group of students to teach basic modeling\nskills, and in fact it is only when such students have these basic\nskills as a foundation that they are prepared to acquire the more\nsophisticated skills needed to employ Management Science. In this paper\nI present a hierarchy of modeling skills, from numeracy skills through\nsophisticated Management Science skills, as a framework within which to\nplan courses for the occasional practitioner\n', ['management science', 'modeling', 'numeracy skills', 'formal models', 'decision analysis', 'educational courses', 'management science', 'modelling']), ('A multimodal data collection tool using REALbasic and Mac OS X\nThis project uses REALbasic 3.5 in the Mac OS X environment for development of\na configuration tool that builds a data collection procedure for\ninvestigating the effectiveness of sonified graphs. The advantage of\nusing REALbasic with the Mac OS X system is that it provides rapid\ndevelopment of stimulus presentation, direct recording of data to\nfiles, and control over other procedural issues. The program can be\nmade to run natively on the new Mac OS X system, older Mac OS systems,\nand Windows (98SE, ME, 2000 PRO). With modification, similar programs\ncould be used to present any number of visual/auditory stimulus\ncombinations, complete with questions for each stimulus\n', ['multimodal data collection tool', 'REALbasic', 'Mac OS X environment', 'configuration tool', 'data collection', 'sonified graphs', 'visual data comprehension', 'psychology', 'visual stimulus', 'auditory stimulus', 'stimulus presentation', 'direct data recording', 'Windows', 'Apple computers', 'BASIC', 'graphs', 'operating systems (computers)', 'psychology', 'user interfaces']), ('An exactly solvable random satisfiability problem\nWe introduce a new model for the generation of random satisfiability problems.\nIt is an extension of the hyper-SAT model of Ricci-Tersenghi, Weigt and\nZecchina (2001), which is a variant of the famous K-SAT model: it is\nextended to q-state variables and relates to a different choice of the\nstatistical ensemble. The model has an exactly solvable statistic: the\ncritical exponents and scaling functions of the SAT/UNSAT transition\nare calculable at zero temperature, with no need of replicas, also with\nexact finite-size corrections. We also introduce an exact duality of\nthe model, and show an analogy of thermodynamic properties with the\nrandom energy model of disordered spin system theory. Relations with\nerror correcting codes are also discussed\n', ['exactly solvable random satisfiability problem', 'hyper-SAT model', 'q-state variables', 'statistical ensemble', 'exact finite-size corrections', 'exact duality', 'thermodynamic properties', 'random energy model', 'disordered spin system theory', 'error correcting codes', 'combinatorial mathematics', 'computability', 'computational complexity', 'optimisation']), ("Industrial/sup IT/ for performance buildings\nABB has taken a close look at how buildings are used and has come up with a\nradical solution for the technical infrastructure that places the\nend-user's processes at the center and integrates all the building's\nsystems around their needs. The new solution is based on the\nrealization that tasks like setting up an office meeting, registering a\nhotel guest or moving a patient in a hospital, can all benefit from the\nsame Industrial IT concepts employed by ABB to optimize manufacturing,\nfor example in the automotive industry\n", ['Industrial/sup IT/', 'ABB', 'building management system', 'technical infrastructure', 'building systems integration', 'industrial IT concepts', 'access control', 'building management systems', 'computer networks']), ('Formalising optimal feature weight setting in case based diagnosis as linear\nprogramming problems\nMany approaches to case based reasoning (CBR) exploit feature weight setting\nalgorithms to reduce the sensitivity to distance functions. We\ndemonstrate that optimal feature weight setting in a special kind of\nCBR problems can be formalised as linear programming problems.\nTherefore, the optimal weight settings can be calculated in polynomial\ntime instead of searching in exponential weight space using heuristics\nto get sub-optimal settings. We also demonstrate that our approach can\nbe used to solve classification problems\n', ['optimal feature weight setting', 'case based diagnosis', 'linear programming', 'case based reasoning', 'distance functions', 'polynomial time', 'searching', 'exponential weight space', 'classification', 'heuristics', 'case-based reasoning', 'classification', 'computational complexity', 'diagnostic reasoning', 'linear programming']), ('Dense coding in entangled states\nWe consider the dense coding of entangled qubits shared between two parties,\nAlice and Bob. The efficiency of classical information gain through\nquantum entangled qubits is also considered for the case of pairwise\nentangled qubits and maximally entangled qubits. We conclude that using\nthe pairwise entangled qubits can be more efficient when two parties\ncommunicate whereas using the maximally entangled qubits can be more\nefficient when the N parties communicate\n', ['dense coding', 'entangled states', 'pairwise entangled qubits', 'Alice', 'Bob', 'classical information gain efficiency', 'maximally entangled qubits', 'quantum information processing', 'quantum communication', 'quantum communication', 'quantum cryptography']), ('Hysteretic threshold logic and quasi-delay insensitive asynchronous design\nWe introduce the class of hysteretic linear-threshold (HLT) logic functions as\na novel extension of linear threshold logic, and prove their general\napplicability for constructing state-holding Boolean functions. We then\ndemonstrate a fusion of HLT logic with the quasi-delay insensitive\nstyle of asynchronous circuit design, complete with logical design\nexamples. Future research directions are also identified\n', ['hysteretic linear-threshold logic functions', 'state-holding Boolean functions', 'HLT logic', 'quasi-delay insensitive style', 'asynchronous circuit design', 'logic design', 'digital logic', 'CMOS implementation', 'asynchronous circuits', 'Boolean functions', 'CMOS logic circuits', 'delays', 'hysteresis', 'logic design', 'threshold logic', 'VLSI']), ('An automated irradiation device for use in cyclotrons\nTwo cyclotrons are being operated at IPEN-CNEN/SP: one model CV-28, capable of\naccelerating protons with energies up to 24 MeV and beam currents up to\n30 mu A, and three other particles; the other one, model Cyclone 30,\naccelerates protons with energy of 30 MeV and currents up to 350 mu A.\nBoth have the objective of irradiating targets both for radioisotope\nproduction for use in nuclear medicine and general research. The\ndevelopment of irradiating systems completely automatized was the\nobjective of this work, always aiming to reduce the radiation\nexposition dose to the workers and to increase the reliability of use\nof these systems\n', ['automated irradiation device', 'cyclotrons', 'CV-28', 'protons', 'Cyclone 30', 'radioisotope production', 'nuclear medicine', 'general research', 'radiation exposition dose', 'accelerator control systems', 'cyclotrons', 'nuclear bombardment targets', 'proton accelerators', 'radioactive tracers']), ('Robust control of nonlinear systems with parametric uncertainty\nProbabilistic robustness analysis and synthesis for nonlinear systems with\nuncertain parameters are presented. Monte Carlo simulation is used to\nestimate the likelihood of system instability and violation of\nperformance requirements subject to variations of the probabilistic\nsystem parameters. Stochastic robust control synthesis searches the\ncontroller design parameter space to minimize a cost that is a function\nof the probabilities that design criteria will not be satisfied. The\nrobust control design approach is illustrated by a simple nonlinear\nexample. A modified feedback linearization control is chosen as\ncontroller structure, and the design parameters are searched by a\ngenetic algorithm to achieve the tradeoff between stability and\nperformance robustness\n', ['robust control', 'nonlinear systems', 'parametric uncertainty', 'probabilistic robustness analysis', 'probabilistic robustness synthesis', 'uncertain parameters', 'Monte Carlo simulation', 'system instability', 'performance requirements violation', 'stochastic control synthesis', 'modified feedback linearization control', 'genetic algorithm', 'input-to-state stability', 'control system analysis', 'control system synthesis', 'feedback', 'genetic algorithms', 'linearisation techniques', 'Monte Carlo methods', 'nonlinear control systems', 'probability', 'robust control', 'search problems', 'stochastic systems', 'uncertain systems']), ('Access matters\nDiscusses accessibility needs of people with disabilities, both from the\nperspective of getting the information from I&R programs (including\naccessible Web sites, TTY access, Braille, and other mechanisms) and\nfrom the perspective of being aware of accessibility needs when\nreferring clients to resources. Includes information on ADA legislation\nrequiring accessibility to public places and recommends several\norganizations and Web sites for additional information\n', ['accessibility needs', 'disabled people', 'information and referral programs', 'accessible Web sites', 'TTY access', 'Braille', 'ADA legislation', 'public places', 'handicapped aids', 'information services', 'legislation']), ('Manufacturing data analysis of machine tool errors within a contemporary small\nmanufacturing enterprise\nThe main focus of the paper is directed at the determination of manufacturing\nerrors within the contemporary smaller manufacturing enterprise sector.\nThe manufacturing error diagnosis is achieved through the manufacturing\ndata analysis of the results obtained from the inspection of the\ncomponent on a co-ordinate measuring machine. This manufacturing data\nanalysis activity adopts a feature-based approach and is conducted\nthrough the application of a forward chaining expert system, called the\nproduct data analysis distributed diagnostic expert system, which forms\npart of a larger prototype feedback system entitled the production data\nanalysis framework. The paper introduces the manufacturing error\ncategorisations that are associated with milling type operations,\nknowledge acquisition and representation, conceptual structure and\noperating procedure of the prototype manufacturing data analysis\nfacility. The paper concludes with a brief evaluation of the logic\nemployed through the simulation of manufacturing error scenarios. This\nprototype manufacturing data analysis expert system provides a valuable\naid for the rapid diagnosis and elimination of manufacturing errors on\na 3-axis vertical machining centre in an environment where operator\nexpertise is limited\n', ['manufacturing data analysis', 'machine tool errors', 'contemporary small manufacturing enterprise', 'fixturing errors', 'programming errors', '2 1/2D components', '3-axis vertical machining centre', 'inspection', 'milling type operations', 'knowledge acquisition', 'knowledge representation', 'conceptual structure', 'operating procedure', 'co-ordinate measuring machine', 'feature-based approach', 'forward chaining expert system', 'product data analysis distributed diagnostic expert system', 'condition monitoring', 'data analysis', 'diagnostic expert systems', 'inspection', 'knowledge acquisition', 'knowledge representation', 'machine tools', 'machining', 'tolerance analysis']), ('Online coverage of the Olympic Games\nIn 1956 a new medium was evolving which helped shape not only the presentation\nof the Games to a worldwide audience, but created entirely new avenues\nfor marketing and sponsorship which changed the entire economic\nrelevance of the Games. The medium in 1956 was television, and the\nmedium now, of course, is the Internet. Not since 1956 has Olympic\ncoverage been so impacted by the onset of new technology as the current\nOlympiad has been. But now the IOC finds itself in another set of\ncircumstances not altogether different from 1956\n', ['Olympic Games', 'online coverage', 'marketing', 'sponsorship', 'economic relevance', 'Olympiad', 'IOC', 'online rights', 'e-broadcast', 'information resources', 'multimedia communication', 'sport']), ("New Jersey African American women writers and their publications: a study of\nidentification from written and oral sources\nThis study examines the use of written sources, and personal interviews and\ninformal conversations with individuals from New Jersey's religious,\npolitical, and educational community to identify African American women\nwriters in New Jersey and their intellectual output. The focus on\nrecognizing the community as an oral repository of history and then\ntapping these oral sources for collection development and acquisition\npurposes is supported by empirical and qualitative evidence. Findings\nindicate that written sources are so limited that information\nprofessionals must rely on oral sources to uncover local writers and\ntheir publications\n", ['New Jersey African American women writers', 'written sources', 'personal interviews', 'informal conversations', 'intellectual output', 'oral repository', 'history', 'collection development', 'local writers', 'special collections', 'bibliographies', 'gender issues', 'libraries', 'literature']), ("Where tech is cheap [servers]\nTalk, consultancy, support, not tech is the expensive part of network\ninstallations. It's a good job that small-scale servers can either be\nremotely managed, or require little actual management\n", ['small-scale servers', 'network installations', 'management', 'computer network management', 'network servers']), ("Developing Web-enhanced learning for information fluency-a liberal arts\ncollege's perspective\nLearning is likely to take a new form in the twenty-first century, and a\ntransformation is already in process. Under the framework of\ninformation fluency, efforts are being made at Rollins College to\ndevelop a Web-enhanced course that encompasses information literacy,\nbasic computer literacy, and critical thinking skills. Computer-based\neducation can be successful when librarians use technology effectively\nto enhance their integrated library teaching. In an online learning\nenvironment, students choose a time for learning that best suits their\nneeds and motivational levels. They can learn at their own pace, take a\nnonlinear approach to the subject, and maintain constant communication\nwith instructors and other students. The quality of a\ntechnology-facilitated course can be upheld if the educational\nobjectives and methods for achieving those objectives are carefully\nplanned and explored\n", ['Web-enhanced learning', 'information fluency', 'liberal arts college', 'information literacy', 'computer literacy', 'critical thinking skills', 'computer-based education', 'librarians', 'integrated library teaching', 'online learning', 'computer literacy', 'educational computing', 'information resources', 'information science', 'Internet']), ('Going electronic [auditing]\nA study group examines the issues auditors face in gathering electronic\ninformation as evidence and its impact on the audit\n', ['auditing', 'electronic information', 'assurance standards', 'audit evidence', 'auditing']), ('A fuzzy logic adaptation circuit for control systems of deformable space\nvehicles: its design\nA fuzzy-logic adaptation algorithm is designed for adjusting the discreteness\nperiod of a control system for ensuring the stability and quality of\ncontrol process with regard to the elastic structural vibrations of a\ndeformable space vehicle. Its performance is verified by digital\nmodeling of a discrete control system with two objects\n', ['fuzzy logic adaptation circuit', 'control systems', 'deformable space vehicles', 'discreteness period', 'stability', 'elastic structural vibrations', 'digital modeling', 'adaptive control', 'aerospace control', 'closed loop systems', 'discrete systems', 'fuzzy control', 'graph theory', 'space vehicles']), ('Optimization-based design of fixed-order controllers for command following\nFor discrete-time scalar systems, we propose an approach for designing feedback\ncontrollers of fixed order to minimize an upper bound on the peak\nmagnitude of the tracking error to a given command input. The work\nmakes use of linear programming to design over a class of closed-loop\nsystems proposed for the rejection of non-zero initial conditions and\nbounded disturbances. We incorporate performance robustness in the form\nof a guaranteed upper bound on the peak magnitude of the tracking error\nunder plant coprime factor uncertainty\n', ['optimization-based design', 'fixed-order controllers', 'command following', 'discrete-time scalar systems', 'feedback controllers', 'tracking error', 'linear programming', 'closed-loop systems', 'performance robustness', 'guaranteed upper bound', 'coprime factor uncertainty', 'closed loop systems', 'control system synthesis', 'discrete time systems', 'feedback', 'linear programming', 'minimisation', 'robust control']), ('The maximum possible EVPI\nIn this paper we calculate the maximum expected value of perfect information\n(EVPI) for any probability distribution for the states of the world.\nThis maximum EVPI is an upper bound for the EVPI with given\nprobabilities and thus an upper bound for any partial information about\nthe states of the world\n', ['decision analysis', 'expected value of perfect information', 'operations research', 'management science', 'probability distribution', 'optimisation', 'decision theory', 'management science', 'operations research', 'optimisation', 'probability']), ("Dedekind zeta-functions and Dedekind sums\nIn this paper we use Dedekind zeta functions of two real quadratic number\nfields at -1 to denote Dedekind sums of high rank. Our formula is\ndifferent from that of Siegel's (1969). As an application, we get a\npolynomial representation of zeta /sub K/(-1) = zeta /sub K/(-1) =\n1/45(26n/sup 3/ - 41n +or- 9), n identical to +or-2(mod 5), where K =\nQ( square root (5q)), prime q = 4n/sup 2/ + 1, and the class number of\nquadratic number field K/sub 2/ = Q( square root q) is 1\n", ['Dedekind zeta-functions', 'Dedekind sums', 'real quadratic number fields', 'polynomial representation', 'number theory', 'polynomials']), ("The development and evaluation of a fuzzy logic expert system for renal\ntransplantation assignment: Is this a useful tool?\nAllocating donor kidneys to patients is a complex, multicriteria\ndecision-making problem which involves not only medical, but also\nethical and political issues. In this paper, a fuzzy logic expert\nsystem approach was proposed as an innovative way to deal with the\nvagueness and complexity faced by medical doctors in kidney allocation\ndecision making. A pilot fuzzy logic expert system for kidney\nallocation was developed and evaluated in comparison with two existing\nallocation algorithms: a priority sorting system used by multiple organ\nretrieval and exchange (MORE) in Canada and a point scoring systems\nused by united network for organ sharing (UNOS) in US. Our simulated\nexperiment based on real data indicated that the fuzzy logic system can\nrepresent the expert's thinking well in handling complex tradeoffs, and\noverall, the fuzzy logic derived recommendations were more acceptable\nto the expert than those from the MORE and UNOS algorithms\n", ['renal transplantation assignment', 'fuzzy logic expert system', 'donor kidneys', 'multicriteria decision-making problem', 'kidney allocation decision making', 'priority sorting system', 'multiple organ retrieval exchange', 'point scoring systems', 'united network for organ sharing', 'simulated experiment', 'complex tradeoff handling', 'decision support systems', 'fuzzy logic', 'health care', 'kidney', 'medical expert systems', 'professional aspects']), ("'Virtual Family': an approach to introducing Java programming\nThis paper introduces and discusses Virtual Family (VF): a gender-neutral\ngame-based software that introduces Java programming. VF provides a\ncompletely functioning game that students extend and enhance via\nprogramming. We discuss the background and context within which Virtual\nFamily was developed and other available multimedia resources for\nteaching programming. The paper then goes on to describe Virtual\nFamily's concept and design. Finally, feedback received from Virtual\nFamily teaching workshops is related, as well as preliminary results\nfrom using VF in high-school teaching units. Virtual Family is under\ndevelopment in a research lab at the University of British Columbia and\nis an initiative of Supporting Women in Information Technology (SWIFT).\nSWIFT is a five-year research action and implementation project to\nincrease the participation of women in information technology\n", ['Virtual Family', 'gender-neutral game-based software', 'Java programming teaching', 'multimedia resources', 'teaching workshops', 'high-school teaching units', 'Supporting Women in Information Technology', 'computer games', 'computer science education', 'courseware', 'gender issues', 'Java', 'multimedia computing', 'object-oriented programming', 'teaching']), ('Comparison of non-stationary time series in the frequency domain\nIn this paper we compare two nonstationary time series using nonparametric\nprocedures. Evolutionary spectra are estimated for the two series.\nRandomization tests are performed on groups of spectral estimates for\nboth related and independent time series. Simulation studies show that\nin certain cases the tests perform reasonably well. The tests are\napplied to observed geological and financial time series\n', ['nonstationary time series', 'nonparametric procedures', 'evolutionary spectra estimation', 'randomization tests', 'spectral estimates', 'related time series', 'lag window', 'time window', 'independent time series', 'simulation', 'geological time series', 'financial time series', 'nonparametric statistics', 'spectral analysis', 'time series']), ('War games: The truth [network security]\nWith al Qaeda on the tip of tongues around the world, find out how terror\ngroups could target your network. What are the dangers and how do you\nfight them?\n', ['networks', 'malicious attacks', 'security', 'employees', 'computer crime', 'computer network management', 'security of data']), ("VoIP makeover transforms ugly duckling network\nSurrey County Council's Swan project is Europe's biggest implementation of\nvoice over IP. Six Wans and countless Lans are are being consolidated\ninto a single network covering 6,000 users at 200 sites. The contract\nwas signed in October 2001 for Pounds 13m over five years and rollout\nwill be completed in May 2003\n", ['Surrey County Council', 'Swan', 'voice over IP', 'WAN', 'LAN', 'Internet telephony', 'local area networks', 'public administration', 'wide area networks']), ("The development of virtual reality therapy (VRT) system for the treatment of\nacrophobia and therapeutic case\nVirtual reality therapy (VRT), based on this sophisticated technology, has been\nused in the treatment of subjects diagnosed with acrophobia, a disorder\nthat is characterized by marked anxiety upon exposure to heights and\navoidance of heights. Conventional VR systems for the treatment of\nacrophobia have limitations, over-costly devices or somewhat\nunrealistic graphic scenes. The goal of this study was to develop an\ninexpensive and more realistic virtual environment (VE) in which to\nperform exposure therapy for acrophobia. It is based on a personal\ncomputer, and a virtual scene of a bunge-jump tower in the middle of a\nlarge city. The virtual scenario includes an open lift surrounded by\nprops beside a tower, which allows the patient to feel a sense of\nheights. The effectiveness of the VE was evaluated through the clinical\ntreatment of a subject who was suffering from the fear of heights. As a\nresult, it was proved that this VR environment was effective and\nrealistic at overcoming acrophobia according not only to the comparison\nresults of a variety of questionnaires before and after treatment but\nalso to the subject's comments that the VE seemed to evoke more fearful\nfeelings than the real situation\n", ['virtual reality therapy system', 'acrophobia treatment', 'therapeutic case', 'patient anxiety', 'patient treatment', 'realistic virtual environment', 'exposure therapy', 'personal computer', 'virtual scene', 'clinical treatment', 'psychotherapy', 'heights phobia', 'medical computing', 'patient treatment', 'psychology', 'user interfaces', 'virtual reality']), ('Integration is LIMS inspiration\nFor software manufacturers, blessings come in the form of fast-moving\napplication areas. In the case of LIMS, biotechnology is still in the\ndriving seat, inspiring developers to maintain consistently rapid and\ncreative levels of innovation. Current advancements are no exception.\nIntegration and linking initiatives are still popular and much of the\nactivity appears to be coming from a very productive minority\n', ['software manufacturers', 'LIMS', 'biotechnology', 'biology computing', 'biotechnology', 'chemistry computing', 'laboratory techniques', 'software packages']), ('Place/Transition Petri net evolutions: recording ways, analysis and synthesis\nFour semantic domains for Place/Transition Petri nets and their relationships\nare considered. They are monoids of respectively: firing sequences,\nprocesses, traces and dependence graphs. For each of them the analysis\nand synthesis problem is stated and solved. The monoid of processes is\ndefined in a non-standard way, Nets under consideration involve weights\nof arrows and capacities (finite or infinite) of places. However, the\nanalysis and synthesis tasks require nets to be pure, i.e. each of\ntheir transition must have the pre-set and post-set disjoint\n', ['place/transition Petri net evolutions', 'semantic domains', 'monoids', 'firing sequences', 'dependence graphs', 'post-set disjoint', 'pre-set disjoint', 'formal specification', 'group theory', 'Petri nets']), ('Exploiting structure in adaptive dynamic programming algorithms for a\nstochastic batch service problem\nThe purpose of this paper is to illustrate the importance of using structural\nresults in dynamic programming algorithms. We consider the problem of\napproximating optimal strategies for the batch service of customers at\na service station. Customers stochastically arrive at the station and\nwait to be served, incurring a waiting cost and a service cost. Service\nof customers is performed in groups of a fixed service capacity. We\ninvestigate the structure of cost functions and establish some\ntheoretical results including monotonicity of the value functions.\nThen, we use our adaptive dynamic programming monotone algorithm that\nuses structure to preserve monotonicity of the estimates at each\niterations to approximate the value functions. Since the problem with\nhomogeneous customers can be solved optimally, we have a means of\ncomparison to evaluate our heuristic. Finally, we compare our algorithm\nto classical forward dynamic programming methods\n', ['stochastic batch service problem', 'adaptive dynamic programming algorithms', 'structural results', 'optimal strategy approximation', 'service station', 'waiting cost', 'service cost', 'fixed service capacity', 'cost function structure', 'value function monotonicity', 'inventory theory', 'dynamic programming', 'queueing theory', 'stochastic processes', 'stock control']), ('Nonlinear systems arising from nonisothermal, non-Newtonian Hele-Shaw flows in\nthe presence of body forces and sources\nIn this paper, we first give a formal derivation of several systems of\nequations for injection moulding. This is done starting from the basic\nequations for nonisothermal, non-Newtonian flows in a three-dimensional\ndomain. We derive systems for both (T/sup 0/, p/sup 0/) and (T/sup 1/,\np/sup 1/) in the presence of body forces and sources. We find that body\nforces and sources have a nonlinear effect on the systems. We also\nderive a nonlinear "Darcy law". Our formulation includes not only the\npressure gradient, but also body forces and sources, which play the\nrole of a nonlinearity. Later, we prove the existence of weak solutions\nto certain boundary value problems and initial-boundary value problems\nassociated with the resulting equations for (T/sup 0/, p/sup 0/) but in\na more general mathematical setting\n', ['injection moulding', 'body forces', 'sources', 'Darcy law', 'nonlinear systems', 'boundary value problems', 'Hele-Shaw flows', 'boundary-value problems', 'moulding', 'nonlinear systems']), ('Pitch post-processing technique based on robust statistics\nA novel pitch post-processing technique based on robust statistics is proposed.\nPerformances in terms of pitch error rates and pitch contours show the\nsuperiority of the proposed method compared with the median filtering\ntechnique. Further improvement is achieved through incorporating an\nuncertainty term in the robust statistics model\n', ['pitch post-processing technique', 'robust statistics', 'pitch error rates', 'pitch contours', 'median filtering', 'uncertainty term', 'speech quality', 'speech communications', 'speech processing', 'statistical analysis']), ("Statistical inference with partial prior information based on a Gauss-type\ninequality\nPotter and Anderson (1983) have developed a Bayesian decision procedure\nrequiring the specification of a class of prior distributions\nrestricted to have a minimal probability content for a given subset of\nthe parameter space. They do not, however, provide a method for the\nselection of that subset. We show how a generalization of Gauss'\ninequality can be used to determine the relevant parameter subset\n", ['Bayesian decision procedure', 'prior distributions', 'minimal probability content', 'parameter space', 'Gauss inequality', 'prior-to-posterior sensitivity', 'partial prior information', 'decision theory', 'inference mechanisms', 'probability']), ('Reconfigurable context-sensitive middleware for pervasive computing\nContext-sensitive applications need data from sensors, devices, and user\nactions, and might need ad hoc communication support to dynamically\ndiscover new devices and engage in spontaneous information exchange.\nReconfigurable Context-Sensitive Middleware facilitates the development\nand runtime operations of context-sensitive pervasive computing\nsoftware\n', ['pervasive computing', 'Reconfigurable Context-Sensitive Middleware', 'context-sensitive pervasive computing', 'middleware', 'context-sensitive applications', 'distributed object management', 'mobile computing', 'portable computers']), ("Cutting through the confusion [workflow & content management]\nInformation management vendors are rushing to re-position themselves and put a\nportal spin on their products, says ITNET's Graham Urquhart. The result\nis confusion, with a range of different definitions and claims clouding\nthe true picture\n", ['ITNET', 'portals', 'collaboratively', 'workflow', 'workflow management software']), ("A sufficient condition for optimality in nondifferentiable invex programming\nA sufficient optimality condition is established for a nonlinear programming\nproblem without differentiability assumption on the data wherein\nClarke's (1975) generalized gradient is used to define invexity\n", ['nondifferentiable invex programming', 'sufficient optimality condition', 'nonlinear programming problem', 'generalized gradient', 'invexity', 'locally Lipschitz function', 'semiconvex function', 'functions', 'nonlinear programming', 'set theory']), ('A simple etalon-stabilized visible laser diode\nVisible laser diodes (LDs) are inexpensively available with\nsingle-transverse-mode, single-longitudinal-mode operation with a\ncoherence length in the metre range. With constant current bias and\nconstant operating temperature, the optical output power and operating\nwavelength are stable. A simple and inexpensive way is developed to\nmaintain a constant LD temperature as the temperature of the local\nenvironment varies, by monitoring the initially changing wavelength\nwith an external etalon and using this information to apply a heating\ncorrection to the monitor photodiode commonly integral to the LD\npackage. The fractional wavelength stability achieved is limited by the\nsolid etalon to 7*10/sup -6/ degrees C/sup -1/\n', ['etalon-stabilized laser diode', 'visible laser diode', 'constant current bias', 'constant operating temperature', 'heating correction', 'monitor photodiode', 'fractional wavelength stability', 'single-transverse-mode', 'single-longitudinal-mode', 'index-guided multi-quantum-well', 'closed-loop operation', 'feedback loop', 'closed loop systems', 'laser cavity resonators', 'laser feedback', 'laser stability', 'light interferometers', 'optical control', 'quantum well lasers', 'temperature control']), ("The cataloger's workstation revisited: utilizing Cataloger's Desktop\nA few years into the development of Cataloger's Desktop, an electronic\ncataloging tool aggregator available through the Library of Congress,\nis an opportune time to assess its impact on cataloging operations. A\nsearch for online cataloging tools on the Internet indicates a\nproliferation of cataloging tool aggregators; which provide access to\nonline documentation related to cataloging practices and procedures.\nCataloger's Desktop stands out as a leader among these aggregators.\nResults of a survey to assess 159 academic ARL and large public\nlibraries' reasons for use or non-use of Cataloger's Desktop highlight\nthe necessity of developing strategies for its successful\nimplementation including training staff, providing documentation, and\nmanaging technical issues\n", ["Cataloger's Desktop", 'electronic cataloging tool', 'online cataloging tools', 'Internet', 'cataloging tool aggregators', 'online documentation', 'large public libraries', 'academic ARL', 'staff training', 'documentation', 'managing technical issues', "cataloger's workstation", 'academic libraries', 'cataloguing', 'library automation', 'public libraries', 'training']), ('DEVS simulation of distributed intrusion detection systems\nAn intrusion detection system (IDS) attempts to identify unauthorized use,\nmisuse, and abuse of computer and network systems. As intrusions become\nmore sophisticated, dealing with them moves beyond the scope of one\nIDS. The need arises for systems to cooperate with one another, to\nmanage diverse attacks across networks. The feature of recent attacks\nis that the packet delivery is moderately slow, and the attack sources\nand attack targets are distributed. These attacks are called "stealthy\nattacks." To detect these attacks, the deployment of distributed IDSs\nis needed. In such an environment, the ability of an IDS to share\nadvanced information about these attacks is especially important. In\nthis research, the IDS model exploits blacklist facts to detect the\nattacks that are based on either slow or highly distributed packets. To\nmaintain the valid blacklist facts in the knowledge base of each IDS,\nthe model should communicate with the other IDSs. When attack level\ngoes beyond the interaction threshold, ID agents send interaction\nmessages to ID agents in other hosts. Each agent model is developed as\nan interruptible atomic-expert model in which the expert system is\nembedded as a model component\n', ['intrusion detection system', 'IDS', 'intrusions', 'expert system', 'distributed intrusion detection system', 'cooperative intrusion detection', 'warning threshold', 'authorisation', 'computer network management', 'cooperative systems', 'discrete event simulation', 'expert systems']), ('On fractal dimension in information systems. Toward exact sets in infinite\ninformation systems\nThe notions of an exact as well as a rough set are well-grounded as basic\nnotions in rough set theory. They are however defined in the setting of\na finite information system i.e. an information system having finite\nnumbers of objects as well as attributes. In theoretical studies e.g.\nof topological properties of rough sets, one has to trespass this\nlimitation and to consider information systems with potentially unbound\nnumber of attributes. In such setting, the notions of rough and exact\nsets may be defined in terms of topological operators of interior and\nclosure with respect to an appropriate topology following the ideas\nfrom the finite case, where it is noticed that in the finite case\nrough-set-theoretic operators of lower and upper approximation are\nidentical with the interior, respectively, closure operators in\ntopology induced by equivalence classes of the indiscernibility\nrelation. Extensions of finite information systems are also desirable\nfrom application point of view in the area of knowledge discovery and\ndata mining, when demands of e.g. mass collaboration and/or huge\nexperimental data call for need of working with large data tables so\nthe sound theoretical generalization of these cases is an information\nsystem with the number of attributes not bound in advance by a fixed\ninteger i.e. an information system with countably but infinitely many\nattributes, In large information systems, a need arises for qualitative\nmeasures of complexity of concepts involved free of parameters, cf.\ne.g. applications for the Vapnik-Czervonenkis dimension. We study here\nin the theoretical setting of infinite information system a proposal to\napply fractal dimensions suitably modified as measures of concept\ncomplexity\n', ['fractal dimension', 'information systems', 'exact sets', 'infinite information systems', 'rough set', 'topological properties', 'closure operators', 'equivalence classes', 'knowledge discovery', 'data mining', 'qualitative measures', 'complexity', 'computational complexity', 'data mining', 'equivalence classes']), ('Noise effect on memory recall in dynamical neural network model of hippocampus\nWe investigate some noise effect on a neural network model proposed by Araki\nand Aihara (1998) for the memory recall of dynamical patterns in the\nhippocampus and the entorhinal cortex; the noise effect is important\nsince the release of transmitters at synaptic clefts, the operation of\ngate of ion channels and so on are known as stochastic phenomena. We\nconsider two kinds of noise effect due to a deterministic noise and a\nstochastic noise. By numerical simulations, we find that reasonable\nvalues of noise give better performance on the memory recall of\ndynamical patterns. Furthermore we investigate the effect of the\nstrength of external inputs on the memory recall\n', ['hippocampus', 'dynamical neural network model', 'noise effect', 'memory recall', 'dynamical patterns', 'entorhinal cortex', 'synaptic clefts', 'gate of ion channels', 'stochastic phenomena', 'deterministic noise', 'stochastic noise', 'brain functions', 'numerical simulations', 'synaptic strength', 'inhibitory connection', 'brain models', 'neural nets', 'neurophysiology', 'nonlinear dynamical systems', 'random noise', 'stochastic processes']), ('The use of visual search for knowledge gathering in image decision support\nThis paper presents a new method of knowledge gathering for decision support in\nimage understanding based on information extracted from the dynamics of\nsaccadic eye movements. The framework involves the construction of a\ngeneric image feature extraction library, from which the feature\nextractors that are most relevant to the visual assessment by domain\nexperts are determined automatically through factor analysis. The\ndynamics of the visual search are analyzed by using the Markov model\nfor providing training information to novices on how and where to look\nfor image features. The validity of the framework has been evaluated in\na clinical scenario whereby the pulmonary vascular distribution on\nComputed Tomography images was assessed by experienced radiologists as\na potential indicator of heart failure. The performance of the system\nhas been demonstrated by training four novices to follow the visual\nassessment behavior of two experienced observers. In all cases, the\naccuracy of the students improved from near random decision making\n(33%) to accuracies ranging from 50% to 68%\n', ['pulmonary vascular distribution', 'experienced radiologists', 'heart failure indicator', 'visual assessment behavior', 'experienced observers', 'student accuracy', 'Markov model', 'training information', 'image features', 'domain experts', 'saccadic eye movements dynamics', 'near random decision making', 'medical diagnostic imaging', 'biomechanics', 'cardiology', 'computerised tomography', 'decision support systems', 'eye', 'feature extraction', 'lung', 'Markov processes', 'medical image processing', 'visual perception']), ("Reaching strong consensus in a general network\nThe strong consensus (SC) problem is a variant of the conventional distributed\nconsensus problem (also known as the Byzantine agreement problem). The\nSC problem requires that the agreed value among fault-free processors\nbe one of the fault-free processor's initial values. Originally, the\nproblem was studied in a fully connected network with malicious faulty\nprocessors. In this paper, the SC problem is re-examined in a general\nnetwork, in which the components (processors and communication links)\nmay be subjected to different faulty types simultaneously (also called\nthe hybrid fault model or mixed faulty types) and the network topology\ndoes not have to be fully connected. The proposed protocol can tolerate\nthe maximum number of tolerable faulty components such that each\nfault-free processor obtains a common value for the SC problem in a\ngeneral network\n", ['strong consensus problem', 'distributed consensus problem', 'Byzantine agreement', 'fault-free processors', 'fully connected network', 'hybrid fault model', 'fault-tolerant distributed system', 'strong consensus', 'concurrency control', 'distributed processing', 'fault tolerant computing', 'protocols']), ("It's time to buy\nThere is an upside to a down economy: over-zealous suppliers are willing to\nmake deals that were unthinkable a few years ago. That's because\nvendors are experiencing the same money squeeze as manufacturers, which\nmakes the year 2002 the perfect time to invest in new technology. The\nauthor states that when negotiating the deal, provisions for unexpected\ncosts, an exit strategy, and even shared risk with the vendor should be\non the table\n", ['negotiation', 'unexpected costs', 'exit strategy', 'shared risk', 'vendor', 'suppliers', 'money squeeze', 'buyers market', 'bargaining power', 'costing', 'management', 'purchasing']), ('Separate accounts go mainstream [investment]\nNew entrants are shaking up the separate-account industry by supplying\nWeb-based platforms that give advisers the tools to pick independent\nmoney managers\n', ['separate-account industry', 'Web-based platforms', 'independent money managers', 'investment', 'financial advisors', 'investment']), ('Exploiting structure in quantified formulas\nWe study the computational problem "find the value of the quantified formula\nobtained by quantifying the variables in a sum of terms." The "sum" can\nbe based on any commutative monoid, the "quantifiers" need only satisfy\ntwo simple conditions, and the variables can have any finite domain.\nThis problem is a generalization of the problem "given a\nsum-of-products of terms, find the value of the sum" studied by R.E.\nStearns and H.B. Hunt III (1996). A data structure called a "structure\ntree" is defined which displays information about "subproblems" that\ncan be solved independently during the process of evaluating the\nformula. Some formulas have "good" structure trees which enable certain\ngeneric algorithms to evaluate the formulas in significantly less time\nthan by brute force evaluation. By "generic algorithm," we mean an\nalgorithm constructed from uninterpreted function symbols, quantifier\nsymbols, and monoid operations. The algebraic nature of the model\nfacilitates a formal treatment of "local reductions" based on the\n"local replacement" of terms. Such local reductions "preserve formula\nstructure" in the sense that structure trees with nice properties\ntransform into structure trees with similar properties. These local\nreductions can also be used to transform hierarchical specified\nproblems with useful structure into hierarchically specified problems\nhaving similar structure\n', ['quantified formulas', 'structure exploitation', 'commutative monoid', 'data structure', 'structure tree', 'satisfiability problems', 'constraint satisfaction problems', 'dynamic programming', 'computational complexity', 'generic algorithms', 'function symbols', 'quantifier symbols', 'monoid operations', 'hierarchically specified problems', 'computational complexity', 'constraint theory', 'data structures', 'dynamic programming', 'genetic algorithms']), ('Existence theorems for nonconvex problems of variational calculus\nA solution to a variational calculus problem is studied under the conditions of\nintegrant convexity. The existence theorem is proved. As an example, a\nfunction is given, which satisfies all the conditions of the theorem\nbut is not convex\n', ['existence theorems', 'nonconvex problems', 'variational calculus', 'integrant convexity', 'convex programming', 'variational techniques']), ('Williams nears end of Chapter 11 [telecom]\nLeucadia National Corp. comes through with a $330 million boost for Williams\nCommunications, which should keep the carrier afloat through the\nremainder of its bankruptcy\n', ['Leucadia National Corp', 'Williams Communications', 'bankruptcy', 'telecommunication']), ("Cultural differences in developers' perceptions of information systems success\nfactors: Japan vs. the United States\nThe study examined the perceptions of information systems (IS) developers from\nJapan and the United States regarding the strategies that are\nconsidered most important for successful implementation of an IS. The\nresults of principal component analysis revealed that the IS strategies\ncould be reduced to five components: (1) characteristics of the team\nmembers, (2) characteristics of the project leader, (3) management/user\ninput, (4) proper technology, and (5) communication. The results\nindicated that there was a significant difference in the perceptions of\nJapanese and US developers with respect to the importance of the five\ncomponents. Japanese developers perceived the project leader as the\nmost crucial component for determining the success of an IS project.\nTeam member characteristics was viewed as the least important by\nJapanese developers. On the other hand, developers from the US viewed\ncommunications as the most critical component. Project leader\ncharacteristics were perceived to be the least important by US\ndevelopers. The results were discussed in terms of cultural differences\n", ['cultural differences', 'information systems success factors', 'information systems developer perceptions', 'Japan', 'United States', 'principal component analysis', 'team member characteristics', 'project leader characteristics', 'management/user input', 'proper technology', 'communication', 'IS project', 'information systems', 'principal component analysis', 'project management', 'social aspects of automation', 'software development management', 'systems analysis']), ("Laptops zip to 2 GHz-plus\nIntel's Pentium 4-M processor has reached the coveted 2-GHz mark, and\nspeed-hungry mobile users will be tempted to buy a laptop with the\nchip. However, while our exclusive tests found 2-GHz P4-M notebooks\namong the fastest units we've tested, the new models failed to make\ndramatic gains compared with those based on Intel's 1.8-GHz mobile\nchip. Since 2-GHz notebooks carry a hefty price premium, buyers seeking\nboth good performance and a good price might prefer a 1.8-GHz unit\ninstead\n", ['Intel Pentium 4-M processor', 'mobile', 'laptop', 'notebooks', '2 GHz', 'computer evaluation', 'microprocessor chips', 'notebook computers']), ('The design and performance evaluation of alternative XML storage strategies\nThis paper studies five strategies for storing XML documents including one that\nleaves documents in the file system, three that use a relational\ndatabase system, and one that uses an object manager. We implement and\nevaluate each approach using a number of XQuery queries. A number of\ninteresting insights are gained from these experiments and a summary of\nthe advantages and disadvantages of the approaches is presented\n', ['XML document storage', 'file system', 'relational database system', 'object manager', 'performance evaluation', 'XQuery queries', 'hypermedia markup languages', 'object-oriented databases', 'query processing', 'relational databases', 'storage management']), ('Linear models of circuits based on the multivalued components\nLinearization and planarization of the circuit models is pivotal to the\nsubmicron technologies. On the other hand, the characteristics of the\nVLSI circuits can be sometimes improved by using the multivalued\ncomponents. It was shown that any l-level circuit based on the\nmultivalued components is representable as an algebraic model based on\nl linear arithmetic polynomials mapped correspondingly into l decision\ndiagrams that are linear and planar by nature. Complexity of\nrepresenting a circuit as the linear decision diagram was estimated as\nO(G) with G for the number of multivalued components in the circuit.\nThe results of testing the LinearDesignMV algorithm on circuits of more\nthan 8000 LGSynth 93 multivalued components were presented\n', ['linear circuit model', 'linearization', 'planarization', 'submicron technologies', 'VLSI circuits', 'linear arithmetic polynomials', 'linear planar decision diagrams', 'circuit representation complexity', 'LinearDesignMV algorithm', 'LGSynth 93 multivalued components', 'circuit analysis computing', 'circuit complexity', 'linearisation techniques', 'multivalued logic circuits', 'polynomials', 'VLSI']), ('Estimating populations for collective dose calculations\nThe collective dose provides an estimate of the effects of facility operations\non the public based on an estimate of the population in the area.\nGeographic information system software, electronic population data\nresources, and a personal computer were used to develop estimates of\npopulation within 80 km radii of two sites\n', ['collective dose calculations', 'facility operations', 'public', 'geographic information system software', 'electronic population data resources', 'personal computer', 'dosimetry', 'geographic information systems', 'medical computing']), ("The real story behind Calpoint [telecom]\nA former Qwest executive sheds light on the carrier's controversial deal with\nCalpoint. Discusses why Calpoint gets a monthly check from Quest,\nregardless of whether it provides services\n", ['Qwest', 'Calpoint', 'telecom carrier', 'telecommunication']), ('Becoming a computer scientist\nThe focus of this report is pipeline shrinkage for women in computer science.\nWe describe the situation for women at all stages of training in\ncomputer science, from the precollege level through graduate school.\nBecause many of the problems discussed are related to the lack of role\nmodels for women who are in the process of becoming computer\nscientists, we also concern ourselves with the status of women faculty\nmembers. We not only describe the problems, but also make specific\nrecommendations for change and encourage further study of those\nproblems whose solutions are not yet well understood\n', ['pipeline shrinkage', 'women', 'computer science', 'role models', 'women faculty members', 'computer science education', 'gender issues']), ('Molecular descriptor selection combining genetic algorithms and fuzzy logic:\napplication to database mining procedures\nA new algorithm, devoted to molecular descriptor selection in the context of\ndata mining problems, has been developed. This algorithm is based on\nthe concepts of genetic algorithms (GA) for descriptor hyperspace\nexploration and combined with a stepwise approach to get local\nconvergence. Its selection power was evaluated by a fitness function\nderived from a fuzzy clustering method. Different training and test\nsets were randomly generated at each GA generation. The fitness score\nwas derived by combining the scores of the training and test sets. The\nability of the proposed algorithm to select relevant subsets of\ndescriptors was tested on two data sets. The first one, an academic\nexample, corresponded to the artificial problem of Bullseye, the second\nwas a real data set including 114 olfactory compounds divided into\nthree odor categories. In both cases, the proposed method allowed to\nimprove the separation between the different data set classes\n', ['molecular descriptor selection', 'database mining', 'data mining', 'genetic algorithms', 'fuzzy clustering method', 'fuzzy logic', 'descriptor hyperspace exploration', 'local convergence', 'stepwise approach', 'fitness function', 'test sets', 'training sets', 'fitness score', 'Bullseye', 'olfactory compounds', 'odor categories', 'chemistry computing', 'convergence of numerical methods', 'data mining', 'fuzzy logic', 'genetic algorithms', 'pattern clustering', 'scientific information systems']), ('Cool and green [air conditioning]\nIn these days of global warming, air conditioning engineers need to specify not\njust for the needs of the occupants, but also to maximise energy\nefficiency. Julian Brunnock outlines the key areas to consider for\nenergy efficient air conditioning systems\n', ['air conditioning', 'energy efficiency', 'air conditioning']), ('The crossing number of P(N, 3)\nIt is proved that the crossing number of the generalized Petersen graph P(3k +\nh, 3) is k + h if h in {0, 2} and k + 3 if h = 1, for each k >or= 3,\nwith the single exception of P(9,3), whose crossing number is 2\n', ['crossing number', 'generalized Petersen graph', 'graph theory']), ('Knowledge management\nThe article defines knowledge management, discusses its role, and describes its\nfunctions. It also explains the principles of knowledge management,\nenumerates the strategies involved in knowledge management, and traces\nits history in brief. The focus is on its interdisciplinary nature. The\nsteps involved in knowledge management i.e. identifying, collecting and\ncapturing, selecting, organizing and storing, sharing, applying, and\ncreating, are explained. The pattern of knowledge management\ninitiatives is also considered\n', ['knowledge management', 'business data processing', 'data warehouses', 'management information systems', 'marketing data processing']), ('IT at the heart of joined-up policing\nPolice IT is to shift from application-focused to component-based technology.\nThe change of strategy, part of the Valiant Programme, will make\ninformation held by individual forces available on a national basis\n', ['Valiant Programme', 'police IT', 'UK', 'police']), ("Press shop. Industrial IT solutions for the press shop\nGlobalization of the world's markets is challenging the traditional limits of\nmanufacturing efficiency. The competitive advantage belongs to those\nwho understand the new requirements and opportunities, and who commit\nto integrated solutions that span the value chain all the way from\ndemand to production. ABB's automation and IT expertise and the process\nknow-how gained from its long involvement with the automotive industry,\nhave been brought together in new, state-of-the-art software solutions\nfor press shops. Integrated into Industrial IT architecture, they allow\nthe full potential of the shops to be realized, with advantages at\nevery step in the supply chain\n", ['press shops', 'industrial IT solutions', 'market globalisation', 'manufacturing efficiency', 'automation', 'state-of-the-art', 'software solutions', 'car manufacturing business', 'supply chain', 'automobile industry', 'industrial control', 'industrial robots']), ('Eliminating counterevidence with applications to accountable certificate\nmanagement\nThis paper presents a method to increase the accountability of certificate\nmanagement by making it intractable for the certification authority\n(CA) to create contradictory statements about the validity of a\ncertificate. The core of the method is a new primitive, undeniable\nattester, that allows someone to commit to some set S of bitstrings by\npublishing a short digest of S and to give attestations for any x that\nit is or is not a member of S. Such an attestation can be verified by\nobtaining in an authenticated way the published digest and applying a\nverification algorithm to the triple of the bitstring, the attestation\nand the digest. The most important feature of this primitive is the\nintractability of creating two contradictory proofs for the same\ncandidate element x and digest. We give an efficient construction for\nundeniable attesters based on authenticated search trees. We show that\nthe construction also applies to sets of more structured elements. We\nalso show that undeniable attesters exist iff collision-resistant hash\nfunctions exist\n', ['counterevidence elimination', 'accountable certificate management', 'accountability', 'certification authority', 'undeniable attester primitive', 'attestations', 'published digest', 'verification algorithm', 'bitstring', 'contradictory proofs', 'authenticated search trees', 'structured elements', 'collision-resistant hash functions', 'long-term authenticity', 'non repudiation', 'public-key infrastructure', 'time-stamping', 'certification', 'message authentication', 'public key cryptography', 'tree searching']), ('SRP rolls out reliability and asset management initiative\nReliability planning analysis at the Salt River Project (SRP, Tempe, Arizona,\nUS) prioritizes geographic areas for preventive inspections based on a\ncost benefit model. However, SRP wanted a new application system to\nprioritize inspections and to predict when direct buried cable would\nfail using the same cost benefit model. In the business cases, the\nrepresented type of kilowatt load-residential, commercial or critical\ncircuit-determines the cost benefit per circuit. The preferred solution\nwas to develop a geographical information system (GIS) application\nallowing for a circuit query for the specific geographic areas it\ncrosses and the density of load points of a given type within those\nareas. The query returns results based on the type of equipment\nanalysis execution: wood pole, preventive maintenance for a line or\ncable replacement. This differentiation insures that all the facilities\nrelevant to a specific analysis type influence prioritization of the\ngeographic areas\n', ['Salt River Project', 'Tempe', 'Arizona', 'USA', 'geographic areas', 'preventive inspections', 'reliability planning analysis', 'cost benefit model', 'direct buried cable', 'geographical information system', 'GIS', 'equipment analysis execution', 'wood pole', 'cable replacement', 'condition monitoring', 'condition monitoring', 'cost-benefit analysis', 'geographic information systems', 'inspection', 'maintenance engineering', 'planning', 'poles and towers', 'power cables', 'power engineering computing', 'power system reliability', 'underground cables']), ('Regional flux target with minimum energy\nAn extension of a gradient controllability problem to the case where the target\nsubregion is a part of the boundary of a parabolic system domain is\ndiscussed. A definition and some properties adapted to this case are\npresented. The focus is on the characterisation of the control\nachieving a regional boundary gradient target with minimum energy. An\napproach is developed that leads to a numerical algorithm for the\ncomputation of optimal control. Numerical illustrations show the\nefficiency of the approach and lead to conjectures\n', ['regional flux target', 'minimum energy', 'gradient controllability problem', 'target subregion', 'parabolic system domain boundary', 'regional boundary gradient target', 'numerical algorithm', 'optimal control', 'controllability', 'distributed parameter systems', 'gradient methods', 'minimisation', 'numerical analysis', 'optimal control']), ('Electromagnetics computations using the MPI parallel implementation of the\nsteepest descent fast multipole method (SDFMM)\nThe computational solution of large-scale linear systems of equations\nnecessitates the use of fast algorithms but is also greatly enhanced by\nemploying parallelization techniques. The objective of this work is to\ndemonstrate the speedup achieved by the MPI (message passing interface)\nparallel implementation of the steepest descent fast multipole method\n(SDFMM). Although this algorithm has already been optimized to take\nadvantage of the structure of the physics of scattering problems, there\nis still the opportunity to speed up the calculation by dividing tasks\ninto components using multiple processors and solve them in parallel.\nThe SDFMM has three bottlenecks ordered as (1) filling the sparse\nimpedance matrix associated with the near-field method of moments\ninteractions (MoM), (2) the matrix vector multiplications associated\nwith this sparse matrix and (3) the far field interactions associated\nwith the fast multipole method. The parallel implementation task is\naccomplished using a thirty-one node Intel Pentium Beowulf cluster and\nis also validated on a 4-processor Alpha workstation. The Beowulf\ncluster consists of thirty-one nodes of 350 MHz Intel Pentium IIs with\n256 MB of RAM and one node of a 4*450 MHz Intel Pentium II Xeon shared\nmemory processor with 2 GB of RAM with all nodes connected to a 100\nBaseTX Ethernet network. The Alpha workstation has a maximum of four\n667 MHz processors. Our numerical results show significant linear\nspeedup in filling the sparse impedance matrix. Using the 32-processors\non the Beowulf cluster lead to a 7.2 overall speedup while a 2.5\noverall speedup is gained using the 4-processors on the Alpha\nworkstation\n', ['electromagnetics computations', 'MPI parallel implementation', 'steepest descent fast multipole method', 'large-scale linear systems', 'fast algorithms', 'message passing interface', 'physics', 'multiple processors', 'sparse impedance matrix', 'near-field MoM', 'method of moments', 'scattering problems', 'matrix vector multiplications', 'Intel Pentium Beowulf cluster', '4-processor Alpha workstation', 'Intel Pentium II', 'RAM', 'Xeon shared memory processor', '100 BaseTX Ethernet network', 'scattered electric field', 'scattered magnetic field', '350 MHz', '256 MByte', '450 MHz', '2 GByte', '667 MHz', 'application program interfaces', 'electric fields', 'electromagnetic wave scattering', 'electromagnetism', 'impedance matrix', 'magnetic fields', 'matrix multiplication', 'message passing', 'method of moments', 'parallel algorithms', 'parallel architectures', 'physics computing', 'sparse matrices', 'workstations']), ('The Open Archives Initiative: realizing simple and effective digital library\ninteroperability\nThe Open Archives Initiative (OAI) is dedicated to solving problems of digital\nlibrary interoperability. Its focus has been on defining simple\nprotocols, most recently for the exchange of metadata from archives.\nThe OAI evolved out of a need to increase access to scholarly\npublications by supporting the creation of interoperable digital\nlibraries. As a first step towards such interoperability, a metadata\nharvesting protocol was developed to support the streaming of metadata\nfrom one repository to another, ultimately to a provider of user\nservices such as browsing, searching, or annotation. This article\nprovides an overview of the mission, philosophy, and technical\nframework of the OAI\n', ['Open Archives Initiative', 'digital library interoperability', 'protocols', 'exchange metadata', 'scholarly publications', 'metadata harvesting protocol', 'streaming metadata', 'annotation', 'searching', 'browsing', 'user services', 'digital libraries', 'electronic publishing', 'meta data', 'open systems', 'protocols']), ('Effect of multileaf collimator leaf width on physical dose distributions in the\ntreatment of CNS and head and neck neoplasms with intensity modulated\nradiation therapy\nThe purpose of this work is to examine physical radiation dose differences\nbetween two multileaf collimator (MLC) leaf widths (5 and 10 mm) in the\ntreatment of CNS and head and neck neoplasms with intensity modulated\nradiation therapy (IMRT). Three clinical patients with CNS tumors were\nplanned with two different MLC leaf sizes, 5 and 10 mm, representing\nVarian-120 and Varian-80 Millennium multileaf collimators,\nrespectively. Two sets of IMRT treatment plans were developed. The goal\nof the first set was radiation dose conformality in three dimensions.\nThe goal for the second set was organ avoidance of a nearby critical\nstructure while maintaining adequate coverage of the target volume.\nTreatment planning utilized the CadPlan/Helios system (Varian Medical\nSystems, Milpitas CA) for dynamic MLC treatment delivery. All beam\nparameters and optimization (cost function) parameters were identical\nfor the 5 and 10 mm plans. For all cases the number of beams, gantry\npositions, and table positions were taken from clinically treated\nthree-dimensional conformal radiotherapy plans. Conformality was\nmeasured by the ratio of the planning isodose volume to the target\nvolume. Organ avoidance was measured by the volume of the critical\nstructure receiving greater than 90% of the prescription dose (V/sub\n90/). For three patients with squamous cell carcinoma of the head and\nneck (T2-T4 N0-N2c M0) 5 and 10 mm leaf widths were compared for\nparotid preservation utilizing nine coplanar equally spaced beams\ndelivering a simultaneous integrated boost. Because modest differences\nin physical dose to the parotid were detected, a NTCP model based upon\nthe clinical parameters of Eisbruch et al. was then used for\ncomparisons. The conformality improved in all three CNS cases for the 5\nmm plans compared to the 10 mm plans. For the organ avoidance plans,\nV/sub 90/ also improved in two of the three cases when the 5 mm leaf\nwidth was utilized for IMRT treatment delivery. In the third case, both\nthe 5 and 10 mm plans were able to spare the critical structure with\nnone of the structure receiving more than 90% of the prescription dose,\nbut in the moderate dose range, less dose was delivered to the critical\nstructure with the 5 mm plan. For the head and neck cases both the 5\nand 10*2.5 mm beamlets dMLC sliding window techniques spared the\ncontralateral parotid gland while maintaining target volume coverage.\nThe mean parotid dose was modestly lower with the smaller beamlet size\n(21.04 Gy vs 22.36 Gy). The resulting average NTCP values were 13.72%\nfor 10 mm dMLC and 8.24% for 5 mm dMLC. In conclusion, five mm leaf\nwidth results in an improvement in physical dose distribution over 10\nmm leaf width that may be clinically relevant in some cases. These\ndifferences may be most pronounced for single fraction radiosurgery or\nin cases where the tolerance of the sensitive organ is less than or\nclose to the target volume prescription\n', ['multileaf collimator leaf width', 'physical dose distributions', 'head and neck neoplasms', 'intensity modulated radiation therapy', 'CNS neoplasms', 'CNS tumors', 'treatment planning', 'parotid preservation', 'optimization parameters', 'conformal radiotherapy', 'single fraction radiosurgery', 'acceptable tumor coverage', 'minimal toxicity', 'collimator rotation', 'beamlet size', '5 mm', '10 mm', '21.04 Gy', '22.36 Gy', 'dosimetry', 'intensity modulation', 'medical computing', 'neurophysiology', 'radiation therapy', 'tumours']), ("Development of an Internet-based intelligent design support system for rolling\nelement bearings\nThis paper presents a novel approach to developing an intelligent agile design\nsystem for rolling bearings based on artificial intelligence (AI),\nInternet and Web technologies and expertise. The underlying philosophy\nof the approach is to use AI technology and Web-based design support\nsystems as smart tools from which design customers can rapidly and\nresponsively access the systems' built-in design expertise. The\napproach is described in detail with a novel AI model and system\nimplementation issues. The major issues in implementing the approach\nare discussed with particular reference to using AI technologies,\nnetwork programming, client-server technology and open computing of\nbearing design and manufacturing requirements\n", ['Internet-based intelligent design support system', 'rolling element bearings', 'intelligent agile design system', 'artificial intelligence', 'Web technologies', 'Internet technologies', 'smart tools', 'network programming', 'client-server technology', 'manufacturing requirements', 'bearing design', 'artificial intelligence', 'CAD/CAM', 'knowledge based systems', 'rolling', 'software agents']), ('Design methodology for diagnostic strategies for industrial systems\nThis paper presents a method for the construction of diagnostic systems for\ncomplex industrial applications. The approach has been explicitely\ndeveloped to shorten the design cycle and meet some specific\nrequirements, such as modularity, flexibility, and the possibility of\nmerging many different sources of information. The method allows one to\nconsider multiple simultaneous failures and is specifically designed to\nmake easier the coordination and simplification of local diagnostic\nalgorithms developed by different teams\n', ['design methodology', 'modularity', 'local diagnostic algorithms', 'diagnostic strategies', 'industrial systems', 'industrial control', 'software maintenance', 'systems analysis']), ('Global comparison of stages of growth based on critical success factors\nWith increasing globalization of business, the management of IT in\ninternational organizations is faced with the complex task of dealing\nwith the difference between local and international IT needs. This\nstudy evaluates, and compares, the level of IT maturity and the\ncritical success factors (CSFs) in selected geographic regions, namely,\nNorway, Australia/New Zealand, North America, Europe, Asia/Pacific, and\nIndia. The results show that significant differences in the IT\nmanagement needs in these geographic regions exist, and that the IT\nmanagement operating in these regions must balance the multiple\ncritical success factors for achieving an optimal local-global mix for\nbusiness success\n', ['business globalization', 'IT management', 'international IT needs', 'local IT needs', 'IT maturity', 'critical success factors', 'Norway', 'Australia', 'New Zealand', 'North America', 'Europe', 'Asia/Pacific', 'India', 'optimal local-global mix', 'business success', 'DP management', 'international trade', 'management of change']), ('Exploring developments in Web based relationship marketing within the hotel\nindustry\nThis paper provides a content analysis study of the application of World Wide\nWeb marketing by the hotel industry. There is a lack of historical\nperspective on industry related Web marketing applications and this\npaper attempts to resolve this with a two-year follow-up case study of\nthe changing use of the Web to develop different types of\nrelationships. Specifically, the aims are: (1) to identify key changes\nin the way hotels are using the Web; (2) to look for evidence of the\nadoption of a relationship marketing (RM) model as a strategy for the\ndevelopment of hotel Web sites and the use of new technologies; and,\n(3) To investigate the use of multimedia in hotel Web sites. The\ndevelopment and strategic exploitation of the Internet has transformed\nthe basis of marketing. Using the evidence from a Web content survey\nthis study reveals the way relationships are being created and managed\nwithin the hotel industry by its use of the Web as a marketing tool.\nThe authors have collected evidence by means of a descriptive study on\nthe way hotels build and create relationships with their Web presence\ndelivering multimedia information as well as channel and interactive\nmeans of communication. In addition a strategic framework is offered as\nthe means to describe the mechanism and orientation of Web based\nmarketing by hotels. The study utilizes a model by Gilbert (1996) as a\nmeans of developing a measurement instrument to allow a content\nanalysis of the current approach by hotels to the development of Web\nsites. The results indicate hotels are aware of the new uses of Web\ntechnology and are promoting hotel products in the global electronic\nmarket in new and sophisticated ways\n', ['Web based relationship marketing', 'hotel industry', 'World Wide Web marketing', 'hotel Web sites', 'multimedia', 'Web content survey', 'global electronic market', 'hotel industry', 'information resources', 'Internet', 'marketing data processing', 'multimedia computing']), ('Solution of the reconstruction problem of a source function in the\ncoagulation-fragmentation equation\nWe study the problem of reconstructing a source function in the kinetic\ncoagulation-fragmentation equation. The study is based on optimal\ncontrol methods, the solvability theory of operator equations, and the\nuse of iteration algorithms\n', ['source function reconstruction', 'kinetic coagulation-fragmentation equation', 'optimal control methods', 'solvability', 'operator equations', 'iteration algorithms', 'coagulation', 'iterative methods', 'physics computing']), ('Homogenization in L/sup infinity /\nHomogenization of deterministic control problems with L/sup infinity / running\ncost is studied by viscosity solutions techniques. It is proved that\nthe value function of an L/sup infinity / problem in a medium with a\nperiodic micro-structure converges uniformly on the compact sets to the\nvalue function of the homogenized problem as the period shrinks to 0.\nOur main convergence result extends that of Ishii (Stochastic Analysis,\ncontrol, optimization and applications, pp. 305-324, Birkhauser Boston,\nBoston, MA, 1999.) to the case of a discontinuous Hamiltonian. The cell\nproblem is solved, but, as nonuniqueness occurs, the effective\nHamiltonian must be selected in a careful way. The paper also provides\na representation formula for the effective Hamiltonian and gives\nillustrations to calculus of variations, averaging and one-dimensional\nproblems\n', ['deterministic control', 'L/sup infinity / running cost', 'homogenization', 'value function', 'averaging', 'calculus of variations', 'convergence', 'optimal control', 'cell problem', 'convergence', 'optimal control', 'variational techniques']), ("p-Bezier curves, spirals, and sectrix curves\nWe elucidate the connection between Bezier curves in polar coordinates, also\ncalled p-Bezier or focal Bezier curves, and certain families of spirals\nand sectrix curves. p-Bezier curves are the analogue in polar\ncoordinates of nonparametric Bezier curves in Cartesian coordinates.\nSuch curves form a subset of rational Bezier curves characterized by\ncontrol points on radial directions regularly spaced with respect to\nthe polar angle, and weights equal to the inverse of the polar radius.\nWe show that this subset encompasses several classical sectrix curves,\nwhich solve geometrically the problem of dividing an angle into equal\nspans, and also spirals defining the trajectories of particles in\ncentral fields. First, we identify as p-Bezier curves a family of\nsinusoidal spirals that includes Tschirnhausen's cubic. Second, the\ntrisectrix of Maclaurin and their generalizations, called arachnidas.\nFinally, a special class of epi spirals that encompasses the trisectrix\nof Delanges\n", ['p-Bezier curves', 'spirals', 'sectrix curves', 'polar coordinates', 'focal Bezier curves', 'rational Bezier curves', 'control points', 'radial directions', 'polar angle', 'geometry', 'angle division', 'equal spans', 'particle trajectories', 'central fields', 'sinusoidal spirals', 'cubic', 'arachnidas', 'epi spirals', 'trisectrix', 'computational geometry']), ('Simulation and transient testing of numerical relays\nA hybrid and practical solution for relay evaluation is presented. Two main\nissues are taken into account: power system simulation and relay\nsimulation, both of which consist of different stages. System\nsimulation is carried out by means of EMTP and is complemented by\nadditional features, such as filtering for location and determination\nof fault parameters that allow comparing simulated and actual fault\nrecords to improve and guarantee a correct system simulation. Relay\nsimulation includes filtering algorithms, all the relaying units, and\nthe decision logic. Playing simulated or real faults over the actual\nrelay and comparing simulated and real responses can check for correct\nrelay simulation\n', ['relay evaluation', 'power system simulation', 'relay simulation', 'EMTP', 'numerical relays', 'filtering', 'fault location', 'filtering algorithms', 'decision logic', 'real faults', 'simulated faults', 'relay testing', 'digital simulation', 'EMTP', 'fault location', 'power system protection', 'power system simulation', 'relay protection', 'testing']), ('Single-phase shunt active power filter with harmonic detection\nAn advanced active power filter for the compensation of instantaneous harmonic\ncurrent components in nonlinear current loads is presented. A signal\nprocessing technique using an adaptive neural network algorithm is\napplied for the detection of harmonic components generated by nonlinear\ncurrent loads and it can efficiently determine the instantaneous\nharmonic components in real time. The validity of this active filtering\nprocessing system to compensate current harmonics is substantiated by\nsimulation results\n', ['single-phase shunt active power filter', 'harmonic detection', 'instantaneous harmonic current components compensation', 'nonlinear current loads', 'signal processing technique', 'adaptive neural network algorithm', 'instantaneous harmonic components', 'simulation', 'active filters', 'compensation', 'control system analysis', 'control system synthesis', 'harmonic distortion', 'load (electric)', 'neurocontrollers', 'power harmonic filters', 'power system control', 'power system harmonics']), ('Coarse-grained reduction and analysis of a network model of cortical response:\nI. Drifting grating stimuli\nWe present a reduction of a large-scale network model of visual cortex\ndeveloped by McLaughlin, Shapley, Shelley, and Wielaard. The reduction\nis from many integrate-and-fire neurons to a spatially coarse-grained\nsystem for firing rates of neuronal subpopulations. It accounts\nexplicitly for spatially varying architecture, ordered cortical maps\n(such as orientation preference) that vary regularly across the\ncortical layer, and disordered cortical maps (such as spatial phase\npreference or stochastic input conductances) that may vary widely from\ncortical neuron to cortical neuron. The result of the reduction is a\nset of nonlinear spatiotemporal integral equations for "phase-averaged"\nfiring rates of neuronal subpopulations across the model cortex,\nderived asymptotically from the full model without the addition of any\nextra phenomological constants. This reduced system is used to study\nthe response of the model to drifting grating stimuli - where it is\nshown to be useful for numerical investigations that reproduce, at far\nless computational cost, the salient features of the point-neuron\nnetwork and for analytical investigations that unveil cortical\nmechanisms behind the responses observed in the simulations of the\nlarge-scale computational model. For example, the reduced equations\nclearly show (1) phase averaging as the source of the time-invariance\nof cortico-cortical conductances, (2) the mechanisms in the model for\nhigher firing rates and better orientation selectivity of simple cells\nwhich are near pinwheel centers, (3) the effects of the length-scales\nof cortico-cortical coupling, and (4) the role of noise in improving\nthe contrast invariance of orientation selectivity\n', ['large-scale network model', 'visual cortex', 'neuronal networks', 'coarse-graining', 'point-neuron network', 'phase-averaged firing rates', 'nonlinear spatiotemporal integral equations', 'dynamics', 'orientation selectivity', 'brain models', 'neural nets']), ('P systems with symport/antiport rules: the traces of objects\nWe continue the study of those P systems where the computation is performed by\nthe communication of objects, that is, systems with symport and\nantiport rules. Instead of the (number of) objects collected in a\nspecified membrane, as the result of a computation we consider the\nitineraries of a certain object through membranes, during a halting\ncomputation, written as a coding of the string of labels of the visited\nmembranes. The family of languages generated in this way is\ninvestigated with respect to its place in the Chomsky hierarchy. When\nthe (symport and antiport) rules are applied in a conditional manner,\npromoted or inhibited by certain objects which should be present in the\nmembrane where a rule is applied, then a characterization of\nrecursively enumerable languages is obtained; the power of systems with\nthe rules applied freely is only partially described\n', ['P systems', 'object communication', 'object traces', 'antiport rules', 'symport rules', 'itineraries', 'halting computation', 'label string coding', 'languages', 'Chomsky hierarchy', 'recursively enumerable languages', 'computational linguistics', 'grammars']), ('Compatibility of systems of linear constraints over the set of natural numbers\nCriteria of compatibility of a system of linear Diophantine equations, strict\ninequations, and nonstrict inequations are considered. Upper bounds for\ncomponents of a minimal set of solutions and algorithms of construction\nof minimal generating sets of solutions for all types of systems are\ngiven. These criteria and the corresponding algorithms for constructing\na minimal supporting set of solutions can be used in solving all the\nconsidered types of systems and systems of mixed types\n', ['linear constraints', 'set of natural numbers', 'linear Diophantine equations', 'strict inequations', 'nonstrict inequations', 'upper bounds', 'minimal generating sets', 'linear programming']), ("A dynamic method for weighted linear least squares problems\nA new method for solving the weighted linear least squares problems with full\nrank is proposed. Based on the theory of Liapunov's stability, the\nmethod associates a dynamic system with a weighted linear least squares\nproblem, whose solution we are interested in and integrates the former\nnumerically by an A-stable numerical method. The numerical tests\nsuggest that the new method is more than comparative with current\nconventional techniques based on the normal equations\n", ['dynamic method', 'weighted linear least squares problems', 'Lyapunov stability', 'A-stable numerical method', 'eigenvalues and eigenfunctions', 'least squares approximations', 'Lyapunov methods']), ("Recruiting and retaining women in undergraduate computing majors\nThis paper recommends methods for increasing female participation in\nundergraduate computer science. The recommendations are based on recent\nand on-going research into the gender gap in computer science and\nrelated disciplines. They are intended to work in tandem with the\nComputing Research Association's recommendations for graduate programs\nto promote a general increase in women's participation in computing\nprofessions. Most of the suggestions offered could improve the\neducational environment for both male and female students. However,\ngeneral improvements are likely to be of particular benefit to women\nbecause women in our society do not generally receive the same level of\nsupport that men receive for entering and persisting in this field\n", ['undergraduate computing majors', 'women retention', 'women recruitment', 'female participation', 'gender gap', 'computer science', 'computer science education', 'gender issues']), ('Verifying concurrent systems with symbolic execution\nCurrent techniques for interactively proving temporal properties of concurrent\nsystems translate transition systems into temporal formulas by\nintroducing program counter variables. Proofs are not intuitive,\nbecause control flow is not explicitly considered. For sequential\nprograms symbolic execution is a very intuitive, interactive proof\nstrategy. In this paper we adopt this technique for parallel programs.\nProperties are formulated in interval temporal logic. An implementation\nin the interactive theorem prover KIV has shown that this technique\noffers a high degree of automation and allows simple, local invariants\n', ['concurrent systems verification', 'symbolic execution', 'temporal properties', 'concurrent systems', 'transition systems', 'temporal formulas', 'program counter variables', 'sequential programs', 'parallel programs', 'interactive theorem prover KIV', 'local invariants', 'parallel programming', 'program verification', 'temporal logic', 'theorem proving']), ('Bad pixel identification by means of principal components analysis\nBad pixels are defined as those pixels showing a temporal evolution of the\nsignal different from the rest of the pixels of a given array.\nPrincipal component analysis helps us to understand the definition of a\nstatistical distance associated with each pixels, and using this\ndistance it is possible to identify those pixels labeled as bad pixels.\nThe spatiality of a pixel is also calculated. An assumption about the\nnormality of the distribution of the distances of the pixels is\nrevised. Although the influence on the robustness of the identification\nalgorithm is negligible, the definition of a parameter related with\nthis nonnormality helps to identify those principal components and\neigenimages responsible for the departure from a multinormal\ndistribution. The method for identifying the bad pixels is successfully\napplied to a set of frames obtained from a CCD visible and a focal\nplane array (FPA) IR camera\n', ['bad pixel identification', 'principal components analysis', 'temporal evolution', 'statistical distance', 'robustness', 'identification algorithm', 'eigenimages', 'multinormal distribution', 'CCD visible camera', 'focal plane array', 'IR camera', 'CCD image sensors', 'focal planes', 'image recognition', 'principal component analysis']), ("An interactive self-replicator implemented in hardware\nSelf-replicating loops presented to date are essentially worlds unto\nthemselves, inaccessible to the observer once the replication process\nis launched. We present the design of an interactive self-replicating\nloop of arbitrary size, wherein the user can physically control the\nloop's replication and induce its destruction. After introducing the\nBioWall, a reconfigurable electronic wall for bio-inspired\napplications, we describe the design of our novel loop and delineate\nits hardware implementation in the wall\n", ['interactive self-replicator', 'interactive self-replicating loop', 'BioWall', 'reconfigurable electronic wall', 'bio-inspired applications', 'hardware implementation', 'self-replication', 'field programmable gate array', 'cellular automata', 'reconfigurable computing', 'artificial life', 'artificial life', 'cellular automata', 'field programmable gate arrays', 'reconfigurable architectures', 'self-reproducing automata']), ('Controls help harmonic spray do OK removing residues\nLooks at how innovative wafer-cleaning equipment hit the market in a timely\nfashion thanks in part to controls maker Rockwell Automation\n', ['harmonic spray', 'residues removal', 'wafer-cleaning equipment', 'Rockwell Automation', 'PSI machine', 'Allen-Bradley ControlLogix automation control platform', 'motion control', 'Allen-Bradley 1336 Plus II variable frequency ac drives', 'control systems', 'motion control', 'process control', 'semiconductor device manufacture', 'surface cleaning']), ("A conflict between language and atomistic information\nFred Dretske and Jerry Fodor are responsible for popularizing three well-known\ntheses in contemporary philosophy of mind: the thesis of\nInformation-Based Semantics (IBS), the thesis of Content Atomism\n(Atomism) and the thesis of the Language of Thought (LOT). LOT concerns\nthe semantically relevant structure of representations involved in\ncognitive states such as beliefs and desires. It maintains that all\nsuch representations must have syntactic structures mirroring the\nstructure of their contents. IBS is a thesis about the nature of the\nrelations that connect cognitive representations and their parts to\ntheir contents (semantic relations). It holds that these relations\nsupervene solely on relations of the kind that support information\ncontent, perhaps with some help from logical principles of combination.\nAtomism is a thesis about the nature of the content of simple symbols.\nIt holds that each substantive simple symbol possesses its content\nindependently of all other symbols in the representational system. I\nargue that Dretske's and Fodor's theories are false and that their\nfalsehood results from a conflict IBS and Atomism, on the one hand, and\nLOT, on the other\n", ['philosophy of mind', 'Information-Based Semantics', 'Content Atomism', 'IBS', 'Language of Thought', 'LOT', 'cognitive states', 'beliefs', 'desires', 'artificial intelligence', 'cognitive systems', 'philosophical aspects']), ('Necessary conditions of optimality for impulsive systems on Banach spaces\nWe present necessary conditions of optimality for optimal control problems\narising in systems governed by impulsive evolution equations on Banach\nspaces. Basic notations and terminologies are first presented and\nnecessary conditions of optimality are presented. Special cases are\ndiscussed and we present an application to the classical linear\nquadratic regulator problem\n', ['linear quadratic regulator', 'optimality', 'impulsive systems', 'optimal control', 'impulsive evolution equations', 'Banach spaces', 'necessary conditions', 'Banach spaces', 'differential equations', 'optimal control']), ("Optimal control using the transport equation: the Liouville machine\nTransport theory describes the scattering behavior of physical particles such\nas photons. Here we show how to connect this theory to optimal control\ntheory and to adaptive behavior of agents embedded in an environment.\nEnvironments and tasks are defined by physical boundary conditions.\nGiven some task, we compute a set of probability densities on\ncontinuous state and action and time. From these densities we derive an\noptimal policy such that for all states the most likely action\nmaximizes the probability of reaching a predefined goal state.\nLiouville's conservation theorem tells us that the conditional density\nat time t, state s, and action a must equal the density at t + dt, s +\nds, a + da. Discretization yields a linear system that can be solved\ndirectly and whose solution corresponds to an optimal policy.\nDiscounted reward schemes are incorporated naturally by taking the\nLaplace transform of the equations. The Liouville machine quickly\nsolves rather complex maze problems\n", ['optimal control', 'transport equation', 'Liouville machine', 'scattering behavior', 'physical particles', 'adaptive behavior', 'embedded agents', 'adaptive systems', 'difference equations', 'Laplace transforms', 'learning (artificial intelligence)', 'optimal control', 'probability']), ('A new approach to the problem of structural identification. II\nThe subject under discussion is a new approach to the problem of structural\nidentification, which relies on the recognition of a decisive role of\nthe human factor in the process of structural identification. Potential\npossibilities of the suggested approach are illustrated by the\nstatement of a new mathematical problem of structural identification\n', ['structural identification', 'human factor', 'mathematical equations', 'decision-maker', 'functional equations', 'identification', 'set theory']), ('Information architecture in JASIST: just where did we come from?\nThe emergence of Information Architecture within the information systems world\nhas been simultaneously drawn out yet rapid. Those with an eye on\nhistory are quick to point to Wurman\'s 1976 use of the term\n"architecture of information," but it has only been in the last 2 years\nthat IA has become the source of sufficient interest for people to\nlabel themselves professionally as Information Architects. The impetus\nfor this recent emergence of IA can be traced to a historical summit,\nsupported by ASIS&T in May 2000 at Boston. It was here that several\nhundred of us gathered to thrash out the questions of just what IA was\nand what this new field might become. At the time of the summit,\ninvited to present a short talk on my return journey from the annual\nACM SIGCHI conference, I entered the summit expecting little and\nconvinced that IA was nothing new. I left 2 days later refreshed, not\njust by the enthusiasm of the attendees for this term but by IA\'s\npotential to unify the disparate perspectives and orientations of\nprofessionals from a range of disciplines. It was at this summit that\nthe idea for the special issue took root. I proposed the idea to Don\nKraft, hoping he would find someone else to run with it. AS luck would\nhave it, I ended up taking charge of it myself, with initial support\nfrom David Blair. From the suggestion to the finished product-has been\nthe best part of 2 years, and in that time more than 50 volunteers\nreviewed over 20 submissions\n', ['information architecture', 'information systems', 'metadata fields', 'controlled vocabularies', 'Web sites', 'CD-ROM', 'qualified information architect', 'certification', 'hypermedia', 'information resources']), ('Voltage control methods with grid connected wind turbines: a tutorial review\nWithin electricity grid networks it is conventional for large-scale central\ngenerators to both provide power and control grid node voltage.\nTherefore when wind turbines replace conventional power stations on a\nsubstantial scale, they must not only generate power, but also control\ngrid node voltages. This paper reviews the basic principles of voltage\ncontrol for tutorial benefit and then considers application of\ngrid-connected wind turbines for voltage control. The most widely used\ncontemporary wind turbine types are considered and further detail is\ngiven for determining the range of variables that allow control\n', ['electricity grid networks', 'large-scale central generators', 'grid connected wind turbines', 'grid node voltages control', 'voltage control', 'reactive power', 'direct drive', 'variable speed', 'offshore wind park', 'squirrel cage induction generator', 'doubly fed induction generator', 'direct drive synchronous generator', 'weak grid', 'converter rating', 'asynchronous generators', 'power generation control', 'power system control', 'synchronous generators', 'voltage control', 'wind turbines']), ('Embedded Linux and the law\nThe rising popularity of Linux, combined with perceived cost savings, has\nspurred many embedded developers to consider a real-time Linux variant\nas an alternative to a traditional RTOS. The paper presents the legal\nimplications for the proprietary parts of firmware\n', ['embedded Linux', 'real-time Linux', 'legal implications', 'proprietary firmware', 'embedded systems', 'firmware', 'industrial property', 'operating systems (computers)', 'public domain software', 'real-time systems']), ('Quality image metrics for synthetic images based on perceptual color\ndifferences\nDue to the improvement of image rendering processes, and the increasing\nimportance of quantitative comparisons among synthetic color images, it\nis essential to define perceptually based metrics which enable to\nobjectively assess the visual quality of digital simulations. In\nresponse to this need, this paper proposes a new methodology for the\ndetermination of an objective image quality metric, and gives an answer\nto this problem through three metrics. This methodology is based on the\nLLAB color space for perception of color in complex images, a\nmodification of the CIELab1976 color space. The first metric proposed\nis a pixel by pixel metric which introduces a local distance map\nbetween two images. The second metric associates, to a pair of images,\na global value. Finally, the third metric uses a recursive subdivision\nof the images to obtain an adaptative distance map, rougher but less\nexpensive to compute than the first method\n', ['quality image metrics', 'synthetic images', 'perceptual color differences', 'image rendering', 'color images', 'perceptually based metrics', 'visual quality', 'digital simulations', 'LLAB color space', 'CIELab1976 color space', 'pixel by pixel metric', 'local distance map', 'global value', 'recursive subdivision', 'adaptative distance map', 'image colour analysis', 'rendering (computer graphics)']), ('Mammogram synthesis using a 3D simulation. I. Breast tissue model and image\nacquisition simulation\nA method is proposed for generating synthetic mammograms based upon simulations\nof breast tissue and the mammographic imaging process. A computer\nbreast model has been designed with a realistic distribution of large\nand medium scale tissue structures. Parameters controlling the size and\nplacement of simulated structures (adipose compartments and ducts)\nprovide a method for consistently modeling images of the same simulated\nbreast with modified position or acquisition parameters. The\nmammographic imaging process is simulated using a compression model and\na model of the X-ray image acquisition process. The compression model\nestimates breast deformation using tissue elasticity parameters found\nin the literature and clinical force values. The synthetic mammograms\nwere generated by a mammogram acquisition model using a monoenergetic\nparallel beam approximation applied to the synthetically compressed\nbreast phantom\n', ['mammogram synthesis', '3D simulation', 'breast tissue model', 'image acquisition simulation', 'mammographic compression', 'computer breast model', 'adipose compartments', 'ducts', 'X-ray image acquisition', 'tissue elasticity parameters', 'force values', 'monoenergetic parallel beam approximation', 'breast lesions', 'rectangular slice approximation', 'composite beam model', "linear Young's moduli", 'biological tissues', 'computer graphics', 'diagnostic radiography', 'image registration', 'mammography', 'medical image processing', 'physiological models', "Young's modulus"]), ("Virus hunting\nWe all appreciate the need for, and hopefully we have all deployed, anti-virus\nsoftware. The good news is that AV software has come a long way fast.\nFour or so years ago it was true to write that AV software could not\ndetect Trojan Horses and similar intrusion attempts. Now it can and\ndoes. McAfee's VirusScan, for example, goes one further; it detects\nviruses, worms and Trojan Horses and deploys itself as a firewall to\nfilter data packets, control access to Internet resources, activate\nrule sets for specific applications, in general to protect against\nhackers. But like so much software, we use it with little thought as to\nhow it came to do its job. Behind the scenes there is an army of top\nnotch programmers trying to stay ahead of the baddies who, at the last\ncount, had produced some 60,000 viruses\n", ['anti-virus software', 'programmers', 'worms', 'Trojan Horses', 'computer viruses']), ('Steinmetz system design under unbalanced conditions\nThis paper studies and develops general analytical expressions to obtain\nthree-phase current symmetrization under unbalanced voltage conditions.\nIt proposes two procedures for this symmetrization: the application of\nthe traditional expressions assuming symmetry conditions and the use of\noptimization methods based on the general analytical equations.\nSpecifically, the paper applies and evaluates these methods to analyze\nthe Steinmetz system design. Several graphics evaluating the error\nintroduced by assumption of balanced voltage in the design are plotted\nand an example is studied to compare both procedures. In the example\nthe necessity to apply the optimization techniques in highly unbalanced\nconditions is demonstrated\n', ['three-phase current symmetrization', 'unbalanced voltage conditions', 'Steinmetz system design', 'power system control design', 'optimization methods', 'general analytical equations', 'balanced voltage assumption', 'control system analysis', 'control system synthesis', 'electric current control', 'harmonic distortion', 'harmonics suppression', 'optimal control', 'power system control', 'power system harmonics']), ('Adaptive image denoising using scale and space consistency\nThis paper proposes a new method for image denoising with edge preservation,\nbased on image multiresolution decomposition by a redundant wavelet\ntransform. In our approach, edges are implicitly located and preserved\nin the wavelet domain, whilst image noise is filtered out. At each\nresolution level, the image edges are estimated by gradient magnitudes\n(obtained from the wavelet coefficients), which are modeled\nprobabilistically, and a shrinkage function is assembled based on the\nmodel obtained. Joint use of space and scale consistency is applied for\nbetter preservation of edges. The shrinkage functions are combined to\npreserve edges that appear simultaneously at several resolutions, and\ngeometric constraints are applied to preserve edges that are not\nisolated. The proposed technique produces a filtered version of the\noriginal image, where homogeneous regions appear separated by\nwell-defined edges. Possible applications include image\npresegmentation, and image denoising\n', ['adaptive image denoising', 'scale consistency', 'space consistency', 'edge preservation', 'image multiresolution decomposition', 'redundant wavelet transform', 'image edges', 'gradient magnitudes', 'shrinkage function', 'geometric constraints', 'edge enhancement', 'adaptive signal processing', 'edge detection', 'image enhancement', 'image resolution', 'image restoration', 'interference suppression', 'wavelet transforms']), ('Neural and neuro-fuzzy integration in a knowledge-based system for air quality\nprediction\nWe propose a unified approach for integrating implicit and explicit knowledge\nin neurosymbolic systems as a combination of neural and neuro-fuzzy\nmodules. In the developed hybrid system, a training data set is used\nfor building neuro-fuzzy modules, and represents implicit domain\nknowledge. The explicit domain knowledge on the other hand is\nrepresented by fuzzy rules, which are directly mapped into equivalent\nneural structures. The aim of this approach is to improve the abilities\nof modular neural structures, which are based on incomplete learning\ndata sets, since the knowledge acquired from human experts is taken\ninto account for adapting the general neural architecture. Three\nmethods to combine the explicit and implicit knowledge modules are\nproposed. The techniques used to extract fuzzy rules from neural\nimplicit knowledge modules are described. These techniques improve the\nstructure and the behavior of the entire system. The proposed\nmethodology has been applied in the field of air quality prediction\nwith very encouraging results. These experiments show that the method\nis worth further investigation\n', ['neuro-fuzzy integration', 'knowledge-based system', 'air quality prediction', 'neurosymbolic systems', 'hybrid system', 'training data set', 'implicit domain knowledge representation', 'fuzzy rules', 'incomplete learning', 'neural architecture', 'experiments', 'air pollution', 'air pollution', 'environmental science computing', 'fuzzy logic', 'fuzzy neural nets', 'geophysics computing', 'knowledge based systems', 'knowledge representation', 'learning (artificial intelligence)']), ('Virtual-reality-based multidimensional therapy for the treatment of body image\ndisturbances in binge eating disorders: a preliminary controlled study\nThe main goal of this paper is to preliminarily evaluate the efficacy of a\nvirtual-reality (VR)-based multidimensional approach in the treatment\nof body image attitudes and related constructs. The female binge eating\ndisorder (BED) patients (n=20), involved in a residential weight\ncontrol treatment including low-calorie diet (1200 cal/day) and\nphysical training, were randomly assigned either to the\nmultidimensional VR treatment or to psychonutritional groups based on\nthe cognitive-behavior approach. Patients were administered a battery\nof outcome measures assessing eating disorders symptomathology,\nattitudes toward food, body dissatisfaction, level of anxiety,\nmotivation for change, level of assertiveness, and general psychiatric\nsymptoms. In the short term, the VR treatment was more effective than\nthe traditional cognitive-behavioral psychonutritional groups in\nimproving the overall psychological state of the patients. In\nparticular, the therapy was more effective in improving body\nsatisfaction, self-efficacy, and motivation for change. No significant\ndifferences were found in the reduction of the binge eating behavior.\nThe possibility of inducing a significant change in body image and its\nassociated behaviors using a VR-based short-term therapy can be useful\nto improve the body satisfaction in traditional weight reduction\nprograms. However, given the nature of this research that does not\ninclude a followup study, the obtained results are preliminary only\n', ['virtual reality', 'multidimensional therapy', 'body image disturbances', 'binge eating disorders', 'obesity', 'patient therapy', 'residential weight control treatment', 'psychonutritional groups', 'cognitive-behavior approach', 'anxiety', 'psychiatric symptoms', 'medical computing', 'patient treatment', 'psychology', 'user interfaces', 'virtual reality']), ("An efficient and stable ray tracing algorithm for parametric surfaces\nIn this paper, we propose an efficient and stable algorithm for finding\nray-surface intersections. Newton's method and Bezier clipping are\nadapted to form the core of our algorithm. Ray coherence is used to\nfind starting points for Newton iteration. We introduce an obstruction\ndetection technique to verify whether an intersection point found using\nNewton's method is the closest. When Newton's method fails to achieve\nconvergence, we use Bezier clipping substitution to find the\nintersection points. This combination achieves a significant\nimprovement in tracing primary rays. A similar approach successfully\nimproves the performance of tracing secondary rays\n", ['efficient stable ray tracing algorithm', 'parametric surfaces', 'ray-surface intersections', 'Newton method', 'Bezier clipping', 'ray coherence', 'Newton iteration', 'obstruction detection technique', 'convergence', 'primary ray tracing', 'secondary ray tracing', 'computational geometry', 'Newton method', 'numerical stability', 'ray tracing', 'realistic images']), ('On quasi-linear PDAEs with convection: applications, indices, numerical\nsolution\nFor a class of partial differential algebraic equations (PDAEs) of quasi-linear\ntype which include nonlinear terms of convection type, a possibility to\ndetermine a time and spatial index is considered. As a typical example\nwe investigate an application from plasma physics. Especially we\ndiscuss the numerical solution of initial boundary value problems by\nmeans of a corresponding finite difference splitting procedure which is\na modification of a well-known fractional step method coupled with a\nmatrix factorization. The convergence of the numerical solution towards\nthe exact solution of the corresponding initial boundary value problem\nis investigated. Some results of a numerical solution of the plasma\nPDAE are given\n', ['quasi-linear partial differential algebraic equations', 'spatial index', 'plasma physics', 'initial boundary value problems', 'finite difference splitting procedure', 'fractional step method', 'matrix factorization', 'convection', 'indices', 'numerical solution', 'initial value problems', 'matrix decomposition', 'partial differential equations']), ('The Internet, knowledge and the academy\nAs knowledge is released from the bounds of libraries, as research becomes no\nlonger confined to the academy, and education/certification is\navailable, any time/any place, the university and the faculty must\nredefine themselves. Liberal studies, once the core, and currently\neschewed in favor of science and technology, will be reborn in those\ninstitutions that can rise above the mundane and embrace an emerging\n"third culture"\n', ['Internet', 'knowledge', 'academy', 'education', 'certification', 'university', 'faculty', 'liberal studies', 'certification', 'humanities', 'Internet', 'teaching']), ('A round of cash, a pound of flesh [telecom]\nDespite the upheaval across telecom, venture capital firms are still investing\nin start-ups. But while a promising idea and a catchy name were enough\nto guarantee millions in funding at the peak of the dotcom frenzy, now\nstart-ups must prove-their long-term viability, and be willing to\nconcede control of their business to their VC suitors\n', ['telecom', 'venture capital firms', 'viability', 'telecommunication']), ("Work sequencing in a manufacturing cell with limited labour constraints\nThis study focuses on the analysis of group scheduling heuristics in a\ndual-constrained, automated manufacturing cell, where labour\nutilization is limited to setups, tear-downs and loads/unloads. This\nscenario is realistic in today's automated manufacturing cells. The\nresults indicate that policies for allocating labour to tasks have very\nlittle impact in such an environment. Furthermore, the performance of\nefficiency oriented, exhaustive, group scheduling heuristics\ndeteriorated while the performance of the more complex, non-exhaustive\nheuristics improved. Thus, it is recommended that production managers\nuse the simplest labour scheduling policy, and instead focus their\nefforts to activities such as job scheduling and production planning in\nsuch environments\n", ['work sequencing', 'manufacturing cell', 'limited labour constraints', 'group scheduling heuristics', 'dual-constrained automated manufacturing cell', 'automated manufacturing cells', 'labour allocation policies', 'efficiency oriented exhaustive group scheduling heuristics', 'nonexhaustive heuristics', 'production planning', 'job scheduling', 'assembly planning', 'heuristic programming', 'production control', 'production engineering computing', 'resource allocation']), ('Speaker adaptive modeling by vocal tract normalization\nThis paper presents methods for speaker adaptive modeling using vocal tract\nnormalization (VTN) along with experimental tests on three databases.\nWe propose a new training method for VTN: By using single-density\nacoustic models per HMM state for selecting the scale factor of the\nfrequency axis, we avoid the problem that a mixture-density tends to\nlearn the scale factors of the training speakers and thus cannot be\nused for selecting the scale factor. We show that using single Gaussian\ndensities for selecting the scale factor in training results in lower\nerror rates than using mixture densities. For the recognition phase, we\npropose an improvement of the well-known two-pass strategy: by using a\nnon-normalized acoustic model for the first recognition pass instead of\na normalized model, lower error rates are obtained. In recognition\ntests, this method is compared with a fast variant of VTN. The two-pass\nstrategy is an efficient method, but it is suboptimal because the scale\nfactor and the word sequence are determined sequentially. We found that\nfor telephone digit string recognition this suboptimality reduces the\nVTN gain in recognition performance by 30% relative. In summary, on the\nGerman spontaneous speech task Verbmobil, the WSJ task and the German\ntelephone digit string corpus SieTill, the proposed methods for VTN\nreduce the error rates significantly\n', ['speaker adaptive modeling', 'vocal tract normalization', 'databases', 'training method', 'single-density acoustic models', 'HMM state', 'frequency scale factor', 'training speakers', 'single Gaussian densities', 'training results', 'error rate reduction', 'two-pass strategy', 'nonnormalized acoustic model', 'word sequence', 'telephone digit string recognition', 'German spontaneous speech task', 'Verlimobil', 'WSJ task', 'German telephone digit string corpus', 'SieTill', 'acoustic signal processing', 'adaptive signal processing', 'Gaussian processes', 'hidden Markov models', 'speech recognition']), ('Orthogonal decompositions of complete digraphs\nA family G of isomorphic copies of a given digraph G is said to be an\northogonal decomposition of the complete digraph D/sub n/ by G, if\nevery arc of D/sub n/ belongs to exactly one member of G and the union\nof any two different elements from G contains precisely one pair of\nreverse arcs. Given a digraph h, an h family mh is the vertex-disjoint\nunion of m copies of h . In this paper, we consider orthogonal\ndecompositions by h-families. Our objective is to prove the existence\nof such an orthogonal decomposition whenever certain necessary\nconditions hold and m is sufficiently large\n', ['orthogonal decompositions', 'complete digraphs', 'isomorphic copies', 'vertex-disjoint union', 'necessary conditions', 'computational geometry', 'directed graphs']), ('Analysis of exclusively kinetic two-link underactuated mechanical systems\nAnalysis of exclusively kinetic two-link underactuated mechanical systems is\nundertaken. It is first shown that such systems are not full-state\nfeedback linearizable around any equilibrium point. Also, the\nequilibrium points for which the system is small-time locally\ncontrollable (STLC) is at most a one-dimensional submanifold. A concept\nless restrictive than STLC, termed the small-time local output\ncontrollability (STLOC) is introduced, the satisfaction of which\nguarantees that a chosen configuration output can be controlled at its\ndesired value. It is shown that the class of systems considered is\nSTLOC, if the inertial coupling between the input and output is\nnonzero. Also, in such a case, the system is nonminimum phase. An\nexample section illustrates all the results presented\n', ['exclusively kinetic two-link underactuated mechanical systems', 'equilibrium points', 'small-time locally controllable system', 'one-dimensional submanifold', 'small-time local output controllability', 'nonminimum phase', 'asymptotic stability', 'controllability', 'feedback', 'linearisation techniques', 'manipulators']), ('Optimal strategies for a semi-Markovian inventory system\nControl for a semi-Markovian inventory system is considered. Under general\nassumptions on system functioning, conditions for existence of an\noptimal nonrandomized Markovian strategy are found. It is shown that\nunder some additional assumptions on storing conditions for the\ninventory, the optimal strategy has a threshold (s, S)-frame\n', ['optimal strategies', 'semi-Markovian inventory system', 'system functioning', 'optimal nonrandomized Markovian strategy', 'optimal strategy', 'Markov processes', 'probability', 'stock control']), ("Caring for your new lawyers\nIn any given year, a striking number of lawyers are in a state of flux, from\nnewly minted law school graduates looking for their first job, to\nsenior litigators migrating to new challenges with new firms. The one\ncertainty is that lawyers new to any firm need care and feeding in\nmyriad ways. All of them need to know and understand three things: (1)\nthe firm's culture; (2) the resources available to help them develop\ntheir practices; and (3) where to get help and guidance for research\nand practice purposes. Obtaining a thorough understanding of a new\nfirm's workings may be the greatest research project lawyers face. How\ncan a firm help its new lawyers learn what they need to know? To offer\nan example, here are programs in place at my firm\n", ['new lawyers', "firm's culture", 'resources', 'law administration']), ('Evaluation of videoconferenced grand rounds\nWe evaluated various aspects of grand rounds videoconferenced from a tertiary\ncare hospital to a regional hospital in Nova Scotia. During a\nfive-month study period, 29 rounds were broadcast (19 in medicine and\n10 in cardiology). The total recorded attendance at the remote site was\n103, comprising 70 specialists, nine family physicians and 24 other\nhealth-care professionals. We received 55 evaluations, a response rate\nof 53%. On a five-point Likert scale (on which higher scores indicated\nbetter quality), mean ratings by remote-site participants of the\ntechnical quality of the videoconference were 3.0-3.5, with the lowest\nratings being for ability to hear the discussion (3.0) and to see\nvisual aids (3.1). Mean ratings for content, presentation, discussion\nand educational value were 3.8 or higher. Of the 49 physicians who\npresented the rounds, we received evaluations from 41, a response rate\nof 84%. The presenters rated all aspects of the videoconference and\ninteraction with remote sites at 3.8 or lower. The lowest ratings were\nfor ability to see the remote sites (3.0) and the usefulness of the\ndiscussion (3.4). We received 278 evaluations from participants at the\npresenting site, an estimated response rate of about 55%. The results\nindicated no adverse opinions of the effect of videoconferencing (mean\nscores 3.1-3.3). The estimated costs of videoconferencing one grand\nround to one site and four sites were C$723 and C$1515, respectively.\nThe study confirmed that videoconferenced rounds can provide\nsatisfactory continuing medical education to community specialists,\nwhich is an especially important consideration as maintenance of\ncertification becomes mandatory\n', ['videoconferenced grand rounds', 'tertiary care hospital', 'regional hospital', 'telemedicine', 'cardiology', 'health-care professionals', 'five-point Likert scale', 'remote sites', 'continuing medical education', 'certification', 'biomedical education', 'certification', 'continuing education', 'educational computing', 'medical computing', 'teleconferencing', 'telemedicine']), ('Tracking with sensor failures\nStudies the reliability with sensor failures of the asymptotic tracking problem\nfor linear time invariant systems using the factorization approach. The\nplant is two-output and the compensator is two-degree-of-freedom.\nNecessary and sufficient conditions are presented for the general\nproblem and a simple solution is given for problems with stable plants\n', ['sensor failures', 'reliability', 'asymptotic tracking problem', 'linear time invariant systems', 'factorization approach', 'two-output plant', 'two-degree-of-freedom compensator', 'necessary and sufficient conditions', 'closed loop systems', 'compensation', 'continuous time systems', 'control system synthesis', 'fault tolerance', 'linear systems', 'sensors', 'stability', 'tracking']), ('Adaptive optimizing compilers for the 21st century\nHistorically, compilers have operated by applying a fixed set of optimizations\nin a predetermined order. We call such an ordered list of optimizations\na compilation sequence. This paper describes a prototype system that\nuses biased random search to discover a program-specific compilation\nsequence that minimizes an explicit, external objective function. The\nresult is a compiler framework that adapts its behavior to the\napplication being compiled, to the pool of available transformations,\nto the objective function, and to the target machine. This paper\ndescribes experiments that attempt to characterize the space that the\nadaptive compiler must search. The preliminary results suggest that\noptimal solutions are rare and that local minima are frequent. If this\nholds true, biased random searches, such as a,genetic algorithm, should\nfind good solutions more quickly than simpler strategies, such as hill\nclimbing\n', ['compilers', 'optimizations', 'compilation sequence', 'adaptive compiler', 'optimizing compilers', 'biased random search', 'configurable compilers', 'optimising compilers']), ("Verizon leapfrogs Sprint PCS with Q2 subscriber numbers\nThe wireless carrier industry's second-quarter results showed a surprising\nshift in market share as Sprint PCS fell from grace after a nearly\nfour-year lead in subscriber additions and Verizon Wireless added\nconsiderably more customers than analysts expected\n", ['wireless carrier industry', 'Sprint PCS', 'Verizon Wireless', 'radio access networks', 'telecommunication']), ('Techniques for compiling and implementing all NAS parallel benchmarks in HPF\nThe NAS parallel benchmarks (NPB) are a well-known benchmark set for\nhigh-performance machines. Much effort has been made to implement them\nin High-Performance Fortran (HPF). In previous attempts, however, the\nHPF versions did not include the complete set of benchmarks, and the\nperformance was not always good. In this study, we implement all eight\nbenchmarks of the NPB in HPF, and parallelize them using an HPF\ncompiler that we have developed. This report describes the\nimplementation techniques and compiler features necessary to achieve\ngood performance. We evaluate the HPF version on the Hitachi SR2201, a\ndistributed-memory parallel machine. With 16 processors, the execution\ntime of the HPF version is within a factor of 1.5 of the\nhand-parallelized version of the NPB 2.3 beta\n', ['NAS parallel benchmarks', 'high-performance machines', 'compiler', 'distributed-memory parallel supercomputers', 'HPF compiler', 'FORTRAN', 'parallel programming', 'parallelising compilers', 'performance evaluation']), ('A framework for evaluating the data-hiding capacity of image sources\nAn information-theoretic model for image watermarking and data hiding is\npresented in this paper. Previous theoretical results are used to\ncharacterize the fundamental capacity limits of image watermarking and\ndata-hiding systems. Capacity is determined by the statistical model\nused for the host image, by the distortion constraints on the data\nhider and the attacker, and by the information available to the data\nhider, to the attacker, and to the decoder. We consider autoregressive,\nblock-DCT, and wavelet statistical models for images and compute\ndata-hiding capacity for compressed and uncompressed host-image\nsources. Closed-form expressions are obtained under sparse-model\napproximations. Models for geometric attacks and distortion measures\nthat are invariant to such attacks are considered\n', ['data-hiding capacity', 'image sources', 'information-theoretic model', 'watermarking', 'capacity limits', 'statistical model', 'distortion constraints', 'autoregressive statistical models', 'block-DCT statistical models', 'wavelet statistical models', 'compressed host-image sources', 'uncompressed host-image sources', 'closed-form expressions', 'sparse-model approximations', 'geometric attacks', 'distortion measures', 'autoregressive processes', 'copy protection', 'data compression', 'data encapsulation', 'discrete cosine transforms', 'image coding', 'statistical analysis', 'wavelet transforms']), ("Local activity criteria for discrete-map CNN\nDiscrete-time CNN systems are studied in this paper by the application of\nChua's local activity principle. These systems are locally active\neverywhere except for one isolated parameter value. As a result,\nnonhomogeneous spatiotemporal patterns may be induced by any initial\nsetting of the CNN system when the strength of the system diffusion\ncoupling exceeds a critical threshold. The critical coupling\ncoefficient can be derived from the loaded cell impedance of the CNN\nsystem. Three well-known 1D map CNN's (namely, the logistic map CNN,\nthe magnetic vortex pinning map CNN, and the spiral wave reproducing\nmap CNN) are introduced to illustrate the applications of the local\nactivity principle. In addition, we use the cell impedance to\ndemonstrate the period-doubling scenario in the logistic and the\nmagnetic vortex pinning maps\n", ['discrete-time CNN systems', 'local activity criteria', 'difference equation', 'discrete-map CNN', 'loaded cell impedance', "Chua's local activity principle", 'nonhomogeneous spatiotemporal patterns', 'critical coupling coefficient', 'logistic map CNN', 'magnetic vortex pinning map CNN', 'spiral wave reproducing map CNN', 'period-doubling', 'cellular neural nets', 'chaos', 'difference equations']), ('An approach to developing computational supports for reciprocal tutoring\nThis study presents a novel approach to developing computational supports for\nreciprocal tutoring. Reciprocal tutoring is a collaborative learning\nactivity, where two participants take turns to play the role of a tutor\nand a tutee. The computational supports include scaffolding tools for\nthe tutor and a computer-simulated virtual participant. The approach,\nincluding system architecture, implementations of scaffolding tools for\nthe tutor and of a virtual participant is presented herein.\nFurthermore, a system for reciprocal tutoring is implemented as an\nexample of the approach\n', ['reciprocal tutoring computational support', 'collaborative learning', 'scaffolding tools', 'computer-simulated virtual participant', 'system architecture', 'intelligent tutoring system', 'groupware', 'intelligent tutoring systems', 'teaching', 'user interfaces']), ('Parcel boundary identification with computer-assisted boundary overlay process\nfor Taiwan\nThe study investigates the design of a process for parcel boundary\nidentification with cadastral map overlay using the principle of least\nsquares. The objective of this research is to provide an objective tool\nfor boundary identification survey. The proposed process includes an\nadjustment model, a weighting scheme, and other related operations. A\nnumerical example is included\n', ['parcel boundary identification', 'computer assisted boundary overlay process', 'Taiwan', 'cadastral map overlay', 'objective tool', 'boundary identification survey', 'adjustment model', 'weighting scheme', 'Gauss-Marker model', 'geographic information system', 'weighted least squares adjustment', 'cartography', 'geographic information systems', 'least squares approximations', 'town and country planning']), ('Direct self control with minimum torque ripple and high dynamics for a double\nthree-level GTO inverter drive\nA highly dynamic control scheme with very low torque ripple-direct self control\n(DSC) with torque hysteresis control-for very high-power medium-voltage\ninduction motor drives fed by a double three-level inverter (D3LI) is\npresented. In this arrangement, two three-level inverters that are\nconnected in parallel at their DC sides are feeding the open motor\nwindings. The DSC, well known from two- and three-level inverters, is\nadapted to the D3LI and optimized for a minimum torque ripple. An\n18-corner trajectory is chosen for the stator flux of the induction\nmachine since it is approaching the ideal circle much better than the\nhexagon known from DSC for two-level inverters, without any detriment\nto the torque ripple. The machine and inverter control are explained\nand the proposed torque quality and dynamics are verified by\nmeasurements on a 180-kW laboratory drive\n', ['highly dynamic control scheme', 'very low torque ripple', 'direct self control', 'torque hysteresis control', 'medium-voltage induction motor drives', 'double three-level inverter', 'parallel connected inverters', 'open motor windings', 'stator flux', 'torque quality', 'variable-speed drives', 'multilevel converters', 'machine observer', '180 kW', 'induction motor drives', 'machine control', 'stators', 'thyristor convertors', 'torque control']), ('Developing a high-performance web server in Concurrent Haskell\nServer applications, and in particular network-based server applications, place\na unique combination of demands on a programming language: lightweight\nconcurrency, high I/O throughput, and fault tolerance are all\nimportant. This paper describes a prototype Web server written in\nConcurrent Haskell (with extensions), and presents two useful results:\nfirstly, a conforming server could be written with minimal effort,\nleading to an implementation in less than 1500 lines of code, and\nsecondly the naive implementation produced reasonable performance.\nFurthermore, making minor modifications to a few time-critical\ncomponents improved performance to a level acceptable for anything but\nthe most heavily loaded Web servers\n', ['high-performance Web server', 'Concurrent Haskell', 'network-based server applications', 'lightweight concurrency', 'high I/O throughput', 'fault tolerance', 'conforming server', 'time-critical components', 'concurrency control', 'fault tolerant computing', 'file servers', 'functional programming', 'Internet']), ("Elastically adaptive deformable models\nWe present a technique for the automatic adaptation of a deformable model's\nelastic parameters within a Kalman filter framework for shape\nestimation applications. The novelty of the technique is that the\nmodel's elastic parameters are not constant, but spatio-temporally\nvarying. The variation of the elastic parameters depends on the\ndistance of the model from the data and the rate of change of this\ndistance. Each pass of the algorithm uses physics-based modeling\ntechniques to iteratively adjust both the geometric and the elastic\ndegrees of freedom of the model in response to forces that are computed\nfrom the discrepancy between the model and the data. By augmenting the\nstate equations of an extended Kalman filter to incorporate these\nadditional variables, we are able to significantly improve the quality\nof the shape estimation. Therefore, the model's elastic parameters are\nalways initialized to the same value and they are subsequently modified\ndepending on the data and the noise distribution. We present results\ndemonstrating the effectiveness of our method for both two-dimensional\nand three-dimensional data\n", ['elastically adaptive deformable models', 'automatic adaptation', 'elastic parameters', 'Kalman filter framework', 'shape estimation', 'physics-based modeling techniques', 'geometric degrees of freedom', 'elastic degrees of freedom', 'state equations', 'extended Kalman filter', 'computer vision', 'filtering theory', 'geometry', 'Kalman filters', 'nonlinear filters', 'parameter estimation']), ('Quantum computing with spin qubits in semiconductor structures\nWe survey recent work on designing and evaluating quantum computing\nimplementations based on nuclear or bound-electron spins in\nsemiconductor heterostructures at low temperatures and in high magnetic\nfields. General overview is followed by a summary of results of our\ntheoretical calculations of decoherence time scales and spin-spin\ninteractions. The latter were carried out for systems for which the\ntwo-dimensional electron gas provides the dominant carrier for spin\ndynamics via exchange of spin-excitons in the integer quantum Hall\nregime\n', ['quantum computing', 'semiconductor heterostructures', 'low temperatures', 'high magnetic fields', 'spin-spin interactions', '2D electron gas', '2DEG', 'spin dynamics', 'dominant carrier', 'spin-excitons exchange', 'integer quantum Hall regime', 'integer QHE', 'spin qubits', 'semiconductor structures', 'excitons', 'interface states', 'quantum computing', 'quantum Hall effect', 'semiconductor heterojunctions', 'spin dynamics', 'two-dimensional electron gas']), ('Rapid microwell polymerase chain reaction with subsequent ultrathin-layer gel\nelectrophoresis of DNA\nLarge-scale genotyping, mapping and expression profiling require affordable,\nfully automated high-throughput devices enabling rapid,\nhigh-performance analysis using minute quantities of reagents. In this\npaper, we describe a new combination of microwell polymerase chain\nreaction (PCR) based DNA amplification technique with automated\nultrathin-layer gel electrophoresis analysis of the resulting products.\nThis technique decreases the reagent consumption (total reaction volume\n0.75-1 mu L), the time requirement of the PCR (15-20 min) and\nsubsequent ultrathin-layer gel electrophoresis based fragment analysis\n(5 min) by automating the current manual procedure and reducing the\nhuman intervention using sample loading robots and computerized real\ntime data analysis. Small aliquots (0.2 mu L) of the submicroliter size\nPCR reaction were transferred onto loading membranes and analyzed by\nultrathin-layer gel electrophoresis which is a novel, high-performance\nand automated microseparation technique. This system employs integrated\nscanning laser-induced fluorescence-avalanche photodiode detection and\ncombines the advantages of conventional slab and capillary gel\nelectrophoresis. Visualization of the DNA fragments was accomplished by\n"in migratio" complexation with ethidium bromide during the\nelectrophoresis process also enabling real time imaging and data\nanalysis\n', ['rapid microwell polymerase chain reaction', 'ultrathin-layer gel electrophoresis', 'DNA amplification', 'large-scale genotyping', 'expression profiling', 'rapid high-performance analysis', 'automated electrophoresis analysis', 'reagent consumption', 'sample loading robots', 'computerized real time data analysis', 'automated microseparation', 'integrated scanning LIF APD detection', 'complexation with ethidium bromide', 'real time imaging', 'biochemistry', 'biocontrol', 'biological techniques', 'DNA', 'electrochemical analysis', 'electrophoresis', 'physical instrumentation control', 'separation']), ('Education, training and development policies and practices in medium-sized\ncompanies in the UK: do they really influence firm performance?\nThis paper sets out to examine the relationship between training and firm\nperformance in middle-sized UK companies. It recognises that there is\nevidence that "high performance work practices" appear to be associated\nwith better performance in large US companies, but argues that this\nrelationship is less likely to be present in middle-sized companies.\nThe paper\'s key contribution is to justify the wider concept of\neducation, training and development (ETD) as applicable to such\ncompanies. It then finds that clusters of some ETD variables do appear\nto be associated with better middle-sized company performance\n', ['medium-sized UK companies', 'training', 'firm performance', 'education', 'development policies', 'high performance work practices', 'ETD variable clusters', 'human resources', 'education', 'human resource management', 'management science', 'training']), ('National learning systems: a new approach on technological change in late\nindustrializing economies and evidences from the cases of Brazil and\nSouth Korea\nThe paper has two intertwined parts. The first one is a proposal for a\nconceptual and theoretical framework to understand technical change in\nlate industrializing economies. The second part develops a kind of\nempirical test of the usefulness of that new framework by means of a\ncomparative study of the Brazilian and South Korean cases. All the four\ntypes of macroevidences of the technical change processes of Brazil and\nKorea corroborate, directly or indirectly, the hypothesis of the\nexistence of actual cases of national learning systems (NLSs) of\npassive and active nature, as it is shown to be the cases of Brazil and\nSouth Korea, respectively. The contrast between the two processes of\ntechnical change prove remarkable, despite both processes being\nessentially confined to learning. The concepts of passive and active\nNLSs show how useful they are to apprehend the diversity of those\nrealities, and, consequently, to avoid, for instance, interpretations\nthat misleadingly suppose (based on conventional economic theory) that\nthose countries have a similar lack of technological dynamism\n', ['national learning systems', 'technological change', 'late industrializing economies', 'Brazil', 'South Korea', 'national innovation system', 'economic cybernetics', 'technological forecasting']), ('Enhanced product support through intelligent product manuals\nThe scope of this paper is the provision of intelligent product support within\nthe distributed Intranet/Internet environment. From the point of view\nof user requirements, the limitations of conventional product manuals\nand methods of authoring them are first outlined. It is argued that\nenhanced product support requires new technology solutions both for\nproduct manuals and for their authoring and presentation. The concept\nand the architecture of intelligent product manuals are then discussed.\nA prototype system called ProARTWeb is presented to demonstrate\nadvanced features of intelligent product manuals. Next, the problem of\nproducing such manuals in a cost-effective way is addressed and a\nconcurrent engineering approach to their authoring is proposed. An\nintegrated environment for collaborative authoring called ProAuthor is\ndescribed to illustrate the approach suggested and to show how\nconsistent, up-to-date and user-oriented-product manuals can be\ndesigned. The solutions presented here enable product knowledge to be\ncaptured and delivered to users and developers of product manuals when,\nwhere and in the form they need it\n', ['intelligent product support', 'intelligent product manuals', 'product manuals', 'ProARTWeb', 'concurrent engineering', 'product knowledge', 'technical information', 'concurrent engineering', 'electronic publishing']), ('Reproducibility of mammary gland structure during repeat setups in a supine\nposition\nPurpose: In breast conserving therapy, complete excision of the tumor with an\nacceptable cosmetic outcome depends on accurate localization in terms\nof both the position of the lesion and its extent. We hypothesize that\npreoperative contrast-enhanced magnetic resonance (MR) imaging of the\npatient in a supine position may be used for accurate tumor\nlocalization and marking of its extent immediately prior to surgery.\nOur aims in this study are to assess the reproducibility of mammary\ngland structure during repeat setups in a supine position, to evaluate\nthe effect of a breast immobilization device, and to derive\nreproducibility margins that take internal tissue shifts into account\noccurring between repeat setups. Materials Methods: The reproducibility\nof mammary gland structure during repeat setups in a supine position is\nestimated by quantification of tissue shifts in the breasts of healthy\nvolunteers between repeat MR setups. For each volunteer fiducials are\nidentified and registered with their counter locations in corresponding\nMR volumes. The difference in position denotes the shift of breast\ntissue. The dependence on breast volume and the part of the breast, as\nwell as the effect of a breast immobilization cast are studied.\nResults: The tissue shifts are small with a mean standard deviation on\nthe order of 1.5 mm, being slightly larger in large breasts (V>1000\ncm/sup 3/), and in the posterior part (toward the pectoral muscle) of\nboth small and large breasts. The application of a breast\nimmobilization cast reduces the tissue shifts in large breasts. A\nreproducibility margin on the order of 5 mm will take the internal\ntissue shifts into account that occur between repeat setups.\nConclusion: The results demonstrate a high reproducibility of mammary\ngland structure during repeat setups in a supine position\n', ['mammary gland structure reproducibility', 'repeat setups', 'supine position', 'breast conserving therapy', 'contrast-enhanced magnetic resonance imaging', 'accurate tumor localization', 'breast immobilization device', 'reproducibility margins', 'internal tissue shifts', 'localization methods', 'biomedical MRI', 'image registration', 'image segmentation', 'mammography', 'medical image processing', 'tumours']), ('Efficient parallel programming on scalable shared memory systems with High\nPerformance Fortran\nOpenMP offers a high-level interface for parallel programming on scalable\nshared memory (SMP) architectures. It provides the user with simple\nwork-sharing directives while it relies on the compiler to generate\nparallel programs based on thread parallelism. However, the lack of\nlanguage features for exploiting data locality often results in poor\nperformance since the non-uniform memory access times on scalable SMP\nmachines cannot be neglected. High Performance Fortran (HPF), the\nde-facto standard for data parallel programming, offers a rich set of\ndata distribution directives in order to exploit data locality, but it\nhas been mainly targeted towards distributed memory machines. In this\npaper we describe an optimized execution model for HPF programs on SMP\nmachines that avails itself with mechanisms provided by OpenMP for work\nsharing and thread parallelism, while exploiting data locality based on\nuser-specified distribution directives. Data locality does not only\nensure that most memory accesses are close to the executing threads and\nare therefore faster, but it also minimizes synchronization overheads,\nespecially in the case of unstructured reductions. The proposed shared\nmemory execution model for HPF relies on a small set of language\nextensions, which resemble the OpenMP work-sharing features. These\nextensions, together with an optimized shared memory parallelization\nand execution model, have been implemented in the ADAPTOR HPF\ncompilation system and experimental results verify the efficiency of\nthe chosen approach\n', ['parallel programming', 'scalable shared memory', 'High Performance Fortran', 'multiprocessor architectures', 'scalable hardware', 'shared memory multiprocessor', 'FORTRAN', 'parallel programming', 'shared memory systems']), ('Automatic multilevel thresholding for image segmentation by the growing time\nadaptive self-organizing map\nIn this paper, a Growing TASOM (Time Adaptive Self-Organizing Map) network\ncalled "GTASOM" along with a peak finding process is proposed for\nautomatic multilevel thresholding. The proposed GTASOM is tested for\nimage segmentation. Experimental results demonstrate that the GTASOM is\na reliable and accurate tool for image segmentation and its results\noutperform other thresholding methods\n', ['automatic multilevel thresholding', 'image segmentation', 'growing time adaptive self-organizing map', 'Growing TASOM', 'GTASOM', 'peak finding process', 'image segmentation', 'self-organising feature maps']), ('On optimality in auditory information processing\nWe study limits for the detection and estimation of weak sinusoidal signals in\nthe primary part of the mammalian auditory system using a stochastic\nFitzhugh-Nagumo model and an action-recovery model for synaptic\ndepression. Our overall model covers the chain from a hair cell to a\npoint just after the synaptic connection with a cell in the cochlear\nnucleus. The information processing performance of the system is\nevaluated using so-called phi -divergences from statistics that\nquantify "dissimilarity" between probability measures and are\nintimately related to a number of fundamental limits in statistics and\ninformation theory (IT). We show that there exists a set of parameters\nthat can optimize several important phi -divergences simultaneously and\nthat this set corresponds to a constant quiescent firing rate (QFR) of\nthe spiral ganglion neuron. The optimal value of the QFR is frequency\ndependent but is essentially independent of the amplitude of the signal\n(for small amplitudes). Consequently, optimal processing according to\nseveral standard IT criteria can be accomplished for this model if and\nonly if the parameters are "tuned" to values that correspond to one and\nthe same QFR. This offers a new explanation for the QFR and can provide\nnew insight into the role played by several other parameters of the\nperipheral auditory system\n', ['weak sinusoidal signals', 'mammalian auditory system', 'stochastic Fitzhugh-Nagumo model', 'action-recovery model', 'peripheral auditory system', 'quiescent firing rate', 'spiral ganglion neuron', 'brain', 'brain models', 'dendrites', 'hearing', 'neural nets']), ("An attack-finding algorithm for security protocols\nThis paper proposes an automatic attack construction algorithm in order to find\npotential attacks on security protocols. It is based on a dynamic\nstrand space model, which enhances the original strand space model by\nintroducing active nodes on strands so as to characterize the dynamic\nprocedure of protocol execution. With exact causal dependency relations\nbetween messages considered in the model, this algorithm can avoid\nstate space explosion caused by asynchronous composition. In order to\nget a finite state space, a new method called strand-added on demand is\nexploited, which extends a bundle in an incremental manner without\nrequiring explicit configuration of protocol execution parameters. A\nfiner granularity model of term structure is also introduced, in which\nsubterms are divided into check subterms and data subterms. Moreover,\ndata subterms can be further classified based on the compatible data\nsubterm relation to obtain automatically the finite set of valid\nacceptable terms for an honest principal. In this algorithm, terms core\nis designed to represent the intruder's knowledge compactly, and\nforward search technology is used to simulate attack patterns easily.\nUsing this algorithm, a new attack on the Dolve-Yao protocol can be\nfound, which is even more harmful because the secret is revealed before\nthe session terminates\n", ['attack-finding algorithm', 'security protocols', 'dynamic strand space model', 'strand space model', 'state space explosion', 'asynchronous composition', 'strand-added on demand', 'check subterms', 'data subterms', 'Dolve-Yao protocol', 'data structures', 'protocols', 'security of data']), ('Adaptive state feedback control for a class of linear systems with unknown\nbounds of uncertainties\nThe problem of adaptive robust stabilization for a class of linear time-varying\nsystems with disturbance and nonlinear uncertainties is considered. The\nbounds of the disturbance and uncertainties are assumed to be unknown,\nbeing even arbitrary. For such uncertain dynamical systems, the\nadaptive robust state feedback controller is obtained. And the\nresulting closed-loop systems are asymptotically stable in theory.\nMoreover, an adaptive robust state feedback control scheme is given.\nThe scheme ensures the closed-loop systems exponentially practically\nstable and can be used in practical engineering. Finally, simulations\nshow that the control scheme is effective\n', ['robust stabilization', 'adaptive stabilization', 'linear time-varying systems', 'nonlinear uncertainties', 'closed-loop systems', 'uncertain dynamical systems', 'state feedback', 'adaptive controller', 'robust control', 'uncertain systems', 'adaptive systems', 'linear systems', 'robust control', 'state feedback', 'time-varying systems', 'uncertain systems']), ('Operations that do not disturb partially known quantum states\nConsider a situation in which a quantum system is secretly prepared in a state\nchosen from the known set of states. We present a principle that gives\na definite distinction between the operations that preserve the states\nof the system and those that disturb the states. The principle is\nderived by alternately applying a fundamental property of classical\nsignals and a fundamental property of quantum ones. The principle can\nbe cast into a simple form by using a decomposition of the relevant\nHilbert space, which is uniquely determined by the set of possible\nstates. The decomposition implies the classification of the degrees of\nfreedom of the system into three parts depending on how they store the\ninformation on the initially chosen state: one storing it classically,\none storing it nonclassically, and the other one storing no\ninformation. Then the principle states that the nonclassical part is\ninaccessible and the classical part is read-only if we are to preserve\nthe state of the system. From this principle, many types of no-cloning,\nno-broadcasting, and no-imprinting conditions can easily be derived in\ngeneral forms including mixed states. It also gives a unified view on\nhow various schemes of quantum cryptography work. The principle helps\none to derive optimum amount of resources (bits, qubits, and ebits)\nrequired in data compression or in quantum teleportation of mixed-state\nensembles\n', ['partially known quantum states', 'quantum system', 'secretly prepared quantum state', 'classical signals', 'Hilbert space', 'degrees of freedom', 'nonclassical part', 'quantum cryptography', 'bits', 'qubits', 'ebits', 'quantum teleportation', 'mixed-state ensembles', 'Hilbert spaces', 'quantum communication', 'quantum cryptography', 'quantum theory']), ('An intelligent information gathering method for dynamic information mediators\nThe Internet is spreading into our society rapidly and is becoming one of the\ninformation infrastructures that are indispensable for our daily life.\nIn particular, the WWW is widely used for various purposes such as\nsharing personal information, academic research, business work, and\nelectronic commerce, and the amount of available information is\nincreasing rapidly. We usually utilize information sources on the\nInternet as individual stand-alone sources, but if we can integrate\nthem, we can add more value to each of them. Hence, information\nmediators, which integrate information distributed on the Internet, are\ndrawing attention. In this paper, under the assumption that the\ninformation sources to be integrated are updated frequently and\nasynchronously, we propose an information gathering method that\nconstructs an answer to a query from a user, accessing information\nsources to be integrated properly within an allowable time period. The\nproposed method considers the reliability of data in the cache and the\nquality of answer in order to efficiently access information sources\nand to provide appropriate answers to the user. As evaluation, we show\nthe effectiveness of the proposed method by using an artificial\ninformation integration problem, in which some parameters can be\nmodified, and a real-world flight information service compared with a\nconventional FIFO information gathering method\n', ['intelligent information gathering method', 'dynamic information mediators', 'Internet', 'information infrastructures', 'WWW', 'academic research', 'business work', 'electronic commerce', 'artificial information integration problem', 'real-world flight information service', 'information resources', 'Internet', 'software agents']), ('The n-tier hub technology\nDuring 2001, the Enterprise Engineering Laboratory at George Mason University\nwas contracted by the Boeing Company to develop an eHub capability for\naerospace suppliers in Taiwan. In a laboratory environment, the core\ntechnology was designed, developed, and tested, and now a large\nfirst-tier aerospace supplier in Taiwan is commercializing the\ntechnology. The project objective was to provide layered network and\napplication services for transporting XML-based business transaction\nflows across multi-tier, heterogeneous data processing environments.\nThis paper documents the business scenario, the eHub application, and\nthe network transport mechanisms that were used to build the n-tier\nhub. In contrast to most eHubs, this solution takes the point of view\nof suppliers, pushing data in accordance with supplier requirements;\nhence, enhancing the probability of supplier adoption. The unique\ncontribution of this project is the development of an eHub that meets\nthe needs of small and medium enterprises (SMEs) and first-tier\nsuppliers\n', ['n-tier hub technology', 'aerospace suppliers', 'Boeing Company', 'Taiwan', 'XML-based business transaction flows', 'multi-tier heterogeneous data processing environments', 'business scenario', 'network transport mechanisms', 'supplier adoption', 'small and medium enterprises', 'first-tier suppliers', 'aerospace industry', 'electronic commerce', 'hypermedia markup languages', 'transaction processing']), ('Process specialization: defining specialization for state diagrams\nA precise definition of specialization and inheritance promises to be as useful\nin organizational process modeling as it is in object modeling. It\nwould help us better understand, maintain, reuse, and generate process\nmodels. However, even though object-oriented analysis and design\nmethodologies take full advantage of the object specialization\nhierarchy, the process specialization hierarchy is not supported in\nmajor process representations, such as the state diagram, data flow\ndiagram, and UML representations. Partly underlying this lack of\nsupport is an implicit assumption that we can always specialize a\nprocess by treating it as "just another object." We argue in this paper\nthat this is not so straightforward as it might seem; we argue that a\nprocess-specific approach must be developed. We propose such an\napproach in the form of a set of transformations which, when applied to\na process description, always result in specialization. We illustrate\nthis approach by applying it to the state diagram representation and\ndemonstrate that this approach to process specialization is not only\ntheoretically possible, but shows promise as a method for categorizing\nand analyzing processes. We point out apparent inconsistencies between\nour notion of process specialization and existing work on object\nspecialization but show that these inconsistencies are superficial and\nthat the definition we provide is compatible with the traditional\nnotion of specialization\n', ['process specialization', 'state diagrams', 'inheritance', 'organizational process modeling', 'object-oriented analysis', 'object specialization hierarchy', 'process representation', 'object-oriented design', 'business data processing', 'corporate modelling', 'diagrams', 'inheritance', 'object-oriented methods']), ('Optimal online algorithm for scheduling on two identical machines with machine\navailability constraints\nThis paper considers the online scheduling on two identical machines with\nmachine availability constraints for minimizing makespan. We assume\nthat machine M/sub j/ is unavailable during period from s/sub j/ to\nt/sub j/ (0 <or= s/sub j/ < t/sub j/), j = 1, 2, and the\nunavailable periods of two machines do not overlap. We show that the\ncompetitive ratio of list scheduling is 3. We further give an optimal\nalgorithm with a competitive ratio 5/2\n', ['optimal online algorithm', 'makespan minimisation', 'list scheduling', 'identical machines scheduling', 'machine availability constraints', 'processor scheduling']), ("Stability of W-methods with applications to operator splitting and to geometric\ntheory\nWe analyze the stability properties of W-methods applied to the parabolic\ninitial value problem u' + Au = Bu. We work in an abstract Banach space\nsetting, assuming that A is the generator of an analytic semigroup and\nthat B is relatively bounded with respect to A. Since W-methods treat\nthe term with A implicitly, whereas the term involving B is discretized\nin an explicit way, they can be regarded as splitting methods. As an\napplication of our stability results, convergence for nonsmooth initial\ndata is shown. Moreover, the layout of a geometric theory for\ndiscretizations of semilinear parabolic problems u' + Au = f (u) is\npresented\n", ['W-methods stability', 'operator splitting', 'geometric theory', 'parabolic initial value problem', 'abstract Banach space', 'analytic semigroup', 'nonsmooth initial data', 'linearly implicit Runge-Kutta methods', 'Banach spaces', 'initial value problems', 'Runge-Kutta methods', 'stability']), ('Turning telecommunications call details to churn prediction: a data mining\napproach\nAs deregulation, new technologies, and new competitors open up the mobile\ntelecommunications industry, churn prediction and management has become\nof great concern to mobile service providers. A mobile service provider\nwishing to retain its subscribers needs to be able to predict which of\nthem may be at-risk of changing services and will make those\nsubscribers the focus of customer retention efforts. In response to the\nlimitations of existing churn-prediction systems and the unavailability\nof customer demographics in the mobile telecommunications provider\ninvestigated, we propose, design, and experimentally evaluate a\nchurn-prediction technique that predicts churning from subscriber\ncontractual information and call pattern changes extracted from call\ndetails. This proposed technique is capable of identifying potential\nchurners at the contract level for a specific prediction time-period.\nIn addition, the proposed technique incorporates the multi-classifier\nclass-combiner approach to address the challenge of a highly skewed\nclass distribution between churners and non-churners. The empirical\nevaluation results suggest that the proposed call-behavior-based\nchurn-prediction technique exhibits satisfactory predictive\neffectiveness when more recent call details are employed for the churn\nprediction model construction. Furthermore, the proposed technique is\nable to demonstrate satisfactory or reasonable predictive power within\nthe one-month interval between model construction and churn prediction.\nUsing a previous demographics-based churn-prediction system as a\nreference, the lift factors attained by our proposed technique appear\nlargely satisfactory\n', ['telecommunications call details', 'mobile telecommunications industry', 'mobile service providers', 'deregulation', 'customer retention efforts', 'customer demographics', 'subscriber contractual information', 'call pattern changes', 'multi-classifier class-combiner approach', 'skewed class distribution', 'lift factors', 'decision tree induction', 'data mining', 'decision trees', 'demography', 'mobile communication', 'telecommunication computing', 'very large databases']), ('Secrets of the Glasgow Haskell compiler inliner\nHigher-order languages such as Haskell encourage the programmer to build\nabstractions by composing functions. A good compiler must inline many\nof these calls to recover an efficiently executable program. In\nprinciple, inlining is dead simple: just replace the call of a function\nby an instance of its body. But any compiler-writer will tell you that\ninlining is a black art, full of delicate compromises that work\ntogether to give good performance without unnecessary code bloat. The\npurpose of this paper is, therefore, to articulate the key lessons we\nlearned from a full-scale "production" inliner, the one used in the\nGlasgow Haskell compiler. We focus mainly on the algorithmic aspects,\nbut we also provide some indicative measurements to substantiate the\nimportance of various aspects of the inliner\n', ['Glasgow Haskell compiler inliner', 'higher-order languages', 'functional programming', 'abstractions', 'executable program', 'performance', 'algorithmic aspects', 'functional language', 'optimising compiler', 'functional languages', 'functional programming', 'optimising compilers']), ('Visual-word identification thresholds for the 260 fragmented words of the\nSnodgrass and Vanderwart pictures in Spanish\nWord difficulty varies from language to language; therefore, normative data of\nverbal stimuli cannot be imported directly from another language. We\npresent mean identification thresholds for the 260 screen-fragmented\nwords corresponding to the total set of Snodgrass and Vanderwart (1980)\npictures. Individual words were fragmented in eight levels using Turbo\nPascal, and the resulting program was implemented on a PC\nmicrocomputer. The words were presented individually to a group of 40\nSpanish observers, using a controlled time procedure. An unspecific\nlearning effect was found showing that performance improved due to\npractice with the task. Finally, of the 11 psycholinguistic variables\nthat previous researchers have shown to affect word identification,\nonly imagery accounted for a significant amount of variance in the\nthreshold values\n', ['visual-word identification thresholds', 'fragmented words', 'Snodgrass and Vanderwart pictures', 'Spanish', 'word difficulty', 'verbal stimuli', 'mean identification thresholds', 'screen-fragmented words', 'Turbo Pascal', 'PC microcomputer', 'controlled time procedure', 'unspecific learning effect', 'psycholinguistic variables', 'word identification', 'linguistics', 'psychology']), ('Implementation of DIMSIMs for stiff differential systems\nSome issues related to the implementation of diagonally implicit multistage\nintegration methods for stiff differential systems are discussed. They\ninclude reliable estimation of the local discretization error,\nconstruction of continuous interpolants, solution of nonlinear systems\nof equations by simplified Newton iterations, choice of initial\nstepsize and order, and step and order changing strategy. Numerical\nresults are presented which indicate that an experimental Matlab code\nbased on type 2 methods of order one, two and three outperforms ode15s\ncode from Matlab ODE suite on problems whose Jacobian has eigenvalues\nwhich are close to the imaginary axis\n', ['DIMSIMs', 'stiff differential systems', 'reliable estimation', 'local discretization error', 'interpolants', 'nonlinear systems of equations', 'simplified Newton iterations', 'experimental Matlab code', 'diagonally implicit multistage integration methods', 'difference equations', 'eigenvalues and eigenfunctions', 'matrix algebra', 'stability']), ('Robust L/sub 2/ disturbance attenuation for nonlinear systems with input\ndynamical uncertainty\nDeals with the problem of robust L/sub 2/ disturbance attenuation for nonlinear\nsystems with input dynamical uncertainty. The input dynamical\nuncertainty is restricted to be minimum-phase and relative degree zero.\nA sufficient condition. is given such that the nonlinear system\nsatisfies the L/sub 2/ gain performance and input-to-state stable\nproperty. Using this condition, a design approach is given for a smooth\nstate feedback control law that solves the robust L/sub 2/ disturbance\nattenuation problem, and the approach is extended to a more general\ncase where the nominal system has higher relative degree. Finally, a\nnumerical example is given to demonstrate the proposed approach\n', ['robust L/sub 2/ disturbance attenuation', 'nonlinear systems', 'input dynamical uncertainty', 'sufficient condition', 'L/sub 2/ gain performance', 'input-to-state stable property', 'design approach', 'smooth state feedback control law', 'nominal system', 'robust control', 'control system synthesis', 'nonlinear control systems', 'robust control', 'state feedback', 'uncertain systems']), ('Traffic engineering with traditional IP routing protocols\nTraffic engineering involves adapting the routing of traffic to network\nconditions, with the joint goals of good user performance and efficient\nuse of network resources. We describe an approach to intradomain\ntraffic engineering that works within the existing deployed base of\ninterior gateway protocols, such as Open Shortest Path First and\nIntermediate System-Intermediate System. We explain how to adapt the\nconfiguration of link weights, based on a networkwide view of the\ntraffic and topology within a domain. In addition, we summarize the\nresults of several studies of techniques for optimizing OSPF/IS-IS\nweights to the prevailing traffic. The article argues that traditional\nshortest path routing protocols are surprisingly effective for\nengineering the flow of traffic in large IP networks\n', ['IP routing protocols', 'interior gateway protocols', 'link weights configuration', 'traffic routing', 'network conditions', 'user performance', 'network resources', 'intradomain traffic engineering', 'network topology', 'OSPF/IS-IS weights', 'shortest path routing protocols', 'IP networks', 'TCP', 'transmission control protocol', 'Open Shortest Path First protocol', 'Intermediate System-Intermediate System protocol', 'Internet', 'internetworking', 'network topology', 'optimisation', 'telecommunication network routing', 'telecommunication traffic', 'transport protocols']), ("Transmission of real-time video over IP differentiated services\nMultimedia applications require high bandwidth and guaranteed quality of\nservice (QoS). The current Internet, which provides 'best effort'\nservices, cannot meet the stringent QoS requirements for delivering\nMPEG videos. It is proposed that MPEG frames are transported through\nvarious service models of DiffServ. Performance analysis and simulation\nresults show that the proposed approach can not only guarantee QoS but\ncan also achieve high bandwidth utilisation\n", ['IP differentiated services', 'real-time video transmission', 'multimedia applications', 'quality of service', 'QoS guarantees', 'Internet', 'MPEG video', 'DiffServ', 'high bandwidth utilisation', 'Internet', 'multimedia communication', 'quality of service', 'visual communication']), ('Resolving Web user on the fly\nIdentity authentication systems and procedures are rapidly becoming central\nissues in the practice and study of information systems development and\nsecurity. Requirements for Web transaction security (WTS) include\nstrong authentication of a user, non-repudiation and encryption of all\ntraffic. In this paper, we present an effective mechanism involving two\ndifferent channels, which addresses the prime concerns involved in the\nsecurity of electronic commerce transactions (ECT) viz. user\nauthentication and non-repudiation. Although the product is primarily\ntargeted to provide a fillip to transactions carried out over the Web,\nthis product can also be effectively used for non-Internet transactions\nthat are carried out where user authentication is required\n', ['identity authentication systems', 'information systems development', 'information systems security', 'Web transaction security', 'nonrepudiation', 'encryption', 'traffic', 'electronic commerce transactions', 'cryptography', 'electronic commerce', 'Internet', 'message authentication', 'transaction processing']), ('Dual nature of mass multi-agent systems\nDual nature of mass multi-agent systems (mMAS) emerging as an internal discord\nof two spheres - micro (virtual) consisting of agents and their\ninternal phenomena, and macro arising at the interface to the real\nworld $stems the necessity of a new approach to analysis, design and\nutilisation of such systems. Based on the concept of VR decomposition,\nthe problem of management of such systems is discussed. As a sub-type\nthat makes mMAS closer to the application sphere, an evolutionary\nmulti-agent system (EMAS) is proposed. EMAS combines features of mMAS\nwith advantages of an evolutionary model of computation. As an\nillustration of this consideration two particular EMAS are presented,\nwhich allow us to obtain promising results in the fields of\nmultiobjective optimisation and time-series prediction, and thus\njustify the approach\n', ['mass multiple agent systems', 'VR decomposition', 'virtual reality', 'multiobjective optimisation', 'time-series prediction', 'micro-macro link', 'formal model', 'evolutionary multiple agent system', 'forecasting theory', 'multi-agent systems', 'optimisation', 'time series', 'virtual reality']), ('Well behaved women rarely make history!\nThe author considers women in the history of computer science. Prior to the\nENIAC, women were extremely important to the computing business as\n"computers". Just as women had taken over the tasks as secretaries in\nthe late 1800s with the advent of the typewriter, and in the early\n1900s staffing telephone exchanges, so computing relied on women as the\n"workhorses" of the business\n', ['women', 'history', 'computer science', 'ENIAC', 'business', 'gender issues', 'gender issues', 'history', 'social aspects of automation']), ('Digital-domain self-calibration technique for video-rate pipeline A/D\nconverters using Gaussian white noise\nA digital-domain self-calibration technique for video-rate pipeline A/D\nconverters based on a Gaussian white noise input signal is presented.\nThe proposed algorithm is simple and efficient. A design example is\nshown to illustrate that the overall linearity of a pipeline ADC can be\nhighly improved using this technique\n', ['digital-domain self-calibration technique', 'video-rate pipeline A/D converters', 'Gaussian white noise input signal', 'pipeline ADC linearity', 'analogue-digital conversion', 'calibration', 'Gaussian noise', 'pipeline processing', 'white noise']), ('Automation of the recovery of efficiency of complex structure systems\nBasic features are set forth of the method for automation of the serviceability\nrecovery of systems of complex structures in real time without the\ninterruption of operation. Specific features of the method are revealed\nin an important example of the system of control of hardware components\nof ships\n', ['efficiency recovery', 'serviceability recovery', 'complex structure systems', 'ships', 'hardware components', 'boilers', 'fault tolerance', 'graph theory', 'large-scale systems', 'ships']), ('IT as a key enabler to law firm competitiveness\nProfessional services firms have traditionally been able to thrive in virtually\nany market conditions. They have been consistently successful for\nseveral decades without ever needing to reexamine or change their basic\noperating model. However, gradual but inexorable change in client\nexpectations and the business environment over recent years now means\nthat more of the same is no longer enough. In future, law firms will\nincreasingly need to exploit IT more effectively in order to remain\ncompetitive. To do this, they will need to ensure that all their\ninformation systems function as an integrated whole and are available\nto their staff, clients and business partners. The authors set out the\nlessons to be learned for law firms in the light of the recent PA\nConsulting survey\n', ['professional services firms', 'client expectations', 'business environment', 'information systems', 'law firms', 'information technology', 'legislation', 'professional aspects']), ('Business data management for business-to-business electronic commerce\nBusiness-to-business electronic commerce (B2B EC) opens up new possibilities\nfor trade. For example, new business partners from around the globe can\nbe found, their offers can be compared, even complex negotiations can\nbe conducted electronically, and a contract can be drawn up and\nfulfilled via an electronic marketplace. However, sophisticated data\nmanagement is required to provide such facilities. In this paper, the\nresults of a multi-national project on creating a business-to-business\nelectronic marketplace for small and medium-sized enterprises are\npresented. Tools for information discovery, protocol-based\nnegotiations, and monitored contract enactment are provided and based\non a business data repository. The repository integrates heterogeneous\nbusiness data with business communication. Specific problems such as\nmultilingual nature, data ownership, and traceability of contracts and\nrelated negotiations are addressed and it is shown that the present\napproach provides efficient business data management for B2B EC\n', ['business-to-business electronic commerce', 'business data management', 'electronic marketplace', 'small and medium-sized enterprises', 'multi-national project', 'information discovery', 'protocol-based negotiations', 'monitored contract enactment', 'business data repository', 'heterogeneous business data', 'business communication', 'data ownership', 'multilingual system', 'traceability', 'business communication', 'contracts', 'electronic commerce']), ('Girls, boys, and computers\nToday North American girls, boys, teachers, and parents frequently regard\ncomputer science and programming as something boys are better at. The\nauthor considers how many of the factors that contribute to the low\nparticipation of women in computing occur first, and perhaps most\nforcefully, in childhood. She presents four recommendations to address\nthe situation\n', ['girls', 'teachers', 'computer science', 'programming', 'women', 'childhood', 'gender issues', 'boys', 'computer science education', 'gender issues', 'social aspects of automation']), ('Max and min limiters\nIf A contained in omega , n>or=2, and the function max({x/sub 1/,...,x/sub\nn/} intersection A) is partial recursive, it is easily seen that A is\nrecursive. In this paper, we weaken this hypothesis in various ways\n(and similarly for "min" in place of "max") and investigate what effect\nthis has on the complexity of A. We discover a sharp contrast between\nretraceable and co-retraceable sets, and we characterize sets which are\nthe union of a recursive set and a co-r.e., retraceable set. Most of\nour proofs are noneffective. Several open questions are raised\n', ['min limiters', 'max limiters', 'complexity', 'retraceable sets', 'recursive set', 'computational complexity', 'formal logic']), ('Data mining efforts increase business productivity and efficiency\nThe use and acquisition of information is a key part of the way any business\nmakes money. Data mining technologies provide greater insight into how\nthis information can be better used and more effectively acquired.\nSteven Kudyba, an expert in the field of data mining technologies,\nshares his expertise in an interview\n', ['data mining', 'productivity', 'efficiency', 'data mining']), ('Information architecture without internal theory: an inductive design process\nThis article suggests that Information Architecture (IA) design is primarily an\ninductive process. Although top-level goals, user attributes and\navailable content are periodically considered, the process involves\nbottom-up design activities. IA is inductive partly because it lacks\ninternal theory, and partly because it is an activity that supports\nemergent phenomena (user experiences) from basic design components. The\nnature of IA design is well described by Constructive Induction (CI), a\ndesign process that involves locating the best representational\nframework for the design problem, identifying a solution within that\nframework and translating it back to the design problem at hand. The\nfuture of IA, if it remains inductive or develops a body of theory (or\nboth), is considered\n', ['information architecture design', 'inductive design process', 'bottom-up design activities', 'internal theory', 'emergent phenomena', 'user experiences', 'constructive induction', 'electronic publishing', 'hypermedia', 'information resources']), ('Greenberger-Horne-Zeilinger paradoxes for many qubits\nWe construct Greenberger-Horne-Zeilinger (GHZ) contradictions for three or more\nparties sharing an entangled state, the dimension of each subsystem\nbeing an even integer d. The simplest example that goes beyond the\nstandard GHZ paradox (three qubits) involves five ququats (d = 4). We\nthen examine the criteria that a GHZ paradox must satisfy in order to\nbe genuinely M partite and d dimensional\n', ['Greenberger-Horne-Zeilinger paradoxes', 'many qubits', 'GHZ contradictions', 'entangled state', 'GHZ paradox', 'quantum computing', 'quantum theory']), ('Lower bounds on the information rate of secret sharing schemes with homogeneous\naccess structure\nWe present some new lower bounds on the optimal information rate and on the\noptimal average information rate of secret sharing schemes with\nhomogeneous access structure. These bounds are found by using some\ncovering constructions and a new parameter, the k-degree of a\nparticipant, that is introduced in this paper. Our bounds improve the\nprevious ones in almost all cases\n', ['lower bounds', 'optimal information rate', 'optimal average information rate', 'k-degree', 'cryptography', 'information rate', 'secret sharing schemes', 'homogeneous access structure', 'cryptography']), ('Fuzzy system modeling in pharmacology: an improved algorithm\nIn this paper, we propose an improved fuzzy system modeling algorithm to\naddress some of the limitations of the existing approaches identified\nduring our modeling with pharmacological data. This algorithm differs\nfrom the existing ones in its approach to the cluster validity problem\n(i.e., number of clusters), the projection schema (i.e., input\nmembership assignment and rule determination), and significant input\ndetermination. The new algorithm is compared with the Bazoon-Turksen\nmodel, which is based on the well-known Sugeno-Yasukawa approach. The\ncomparison was made in terms of predictive performance using two\ndifferent data sets. The first comparison was with a two variable\nnonlinear function prediction problem and the second comparison was\nwith a clinical pharmacokinetic modeling problem. It is shown that the\nproposed algorithm provides more precise predictions. Determining the\ndegree of significance for each input variable, allows the user to\ndistinguish their relative importance\n', ['fuzzy system modeling', 'pharmacology', 'cluster validity problem', 'projection schema', 'significant input determination', 'predictive performance', 'fuzzy sets', 'fuzzy logic', 'pharmacokinetic modeling', 'fuzzy logic', 'medicine', 'modelling']), ('The impact of EAD adoption on archival programs: a pilot survey of early\nimplementers\nThe article reports the results of a survey conducted to assess the impact that\nthe implementation of Encoded Archival Description (EAD) has on\narchival programs. By gathering data related to the funding, staffing,\nand evaluation of EAD programs and about institutional goals for EAD\nimplementation, the study explored how EAD has affected the operations\nof the institutions which are utilizing it and the extent to which EAD\nhas become a part of regular repository functions\n', ['EAD adoption', 'archival programs', 'Encoded Archival Description', 'funding', 'staffing', 'EAD programs', 'institutional goals', 'EAD implementation', 'regular repository functions', 'archival descriptive standards', 'diffusion of innovation', 'information retrieval systems', 'page description languages', 'personnel', 'records management']), ("What you get is what you see [Web performance monitoring]\nTo get the best possible performance from your Web infrastructure, you'll need\na complete view. Don't neglect the big picture because you're too busy\nconcentrating on details. The increasing complexity of Web sites and\nthe content they provide has consequently increased the complexity of\nthe infrastructure that supports them. But with some knowledge of\nnetworking, a handful of useful tools, and the insight that those tools\nprovide, designing and operating for optimal performance and\nreliability is within your grasp\n", ['Web performance', 'Web sites', 'Web infrastructure', 'networking', 'reliability', 'file servers', 'information resources', 'Internet', 'monitoring', 'performance evaluation']), ('Storage functionals and Lyapunov functions for passive dynamical systems\nFor nonlinear time-invariant input-output dynamical systems the passivity\nconditions are obtained under some restrictions. The conditions imply\nstorage functions satisfying a dissipation inequality. A class of\nstorage functions allowing unique reconstruction of a passive dynamical\nsystem is defined. These results are illustrated by an example of a\nlinear system with fading memory. An important, for practical\napplication, class of the linear relaxation systems without direct\ninput-output interaction is considered. A necessary condition for\ndynamical systems to be of the relaxation type is obtained for this\nclass. The condition is connected with the existence of a unique\nquadratic Lyapunov function satisfying the complete monotonicity\ncondition. This unique Lyapunov function corresponds to a "standard"\nthermodynamic potential in a compact family of potentials in the\nnonequilibrium thermodynamics. The results obtained can be useful in\nautomatic control, mechanics of viscoelastic materials, and various\napplications in physics and the system theory\n', ['storage functionals', 'passive dynamical systems', 'nonlinear time-invariant input-output dynamical systems', 'passivity conditions', 'dissipation inequality', 'linear system', 'fading memory', 'necessary condition', 'unique quadratic Lyapunov function', 'complete monotonicity condition', 'thermodynamic potential', 'nonequilibrium thermodynamics', 'automatic control', 'mechanics', 'viscoelastic materials', 'functional equations', 'functions', 'linear systems', 'Lyapunov methods', 'nonlinear systems']), ('From the DOS dog days to e-filing [law firms]\nThe poster child for a successful e-filing venture is the Case Management and\nElectronic Case File system now rolling through the district and\nbankruptcy courts. A project of the Administrative Office of the United\nStates Courts, CM/ECF is a loud proponent of the benefits of the PDF\napproach and it has a full head of steam. Present plans are for all\nfederal courts to implement CM/ECF by 2005. That means a radical shift\nin methodology and tools for a lot of lawyers. It also means that you\nshould get cozy with Acrobat real soon\n', ['e-filing', 'Case Management and Electronic Case File system', 'United States Courts', 'Adobe Acrobat', 'PDF', 'document handling', 'law administration']), ("Abacus, EFI and anti-virus\nThe Extensible Firmware Interface (EFI) standard emerged as a logical step to\nprovide flexibility and extensibility to boot sequence processes,\nenabling the complete abstraction of a system's BIOS interface from the\nsystem's hardware. In doing so, this provided the means of\nstandardizing a boot-up sequence, extending device drivers and boot\ntime applications' portability to non PC-AT-based architectures,\nincluding embedded systems like Internet appliances, TV Internet\nset-top boxes and 64-bit Itanium platforms\n", ['Extensible Firmware Interface standard', 'anti-virus', 'embedded systems', 'computer viruses', 'security of data', 'software standards']), ('Simultaneous iterative reconstruction of emission and attenuation images in\npositron emission tomography from emission data only\nFor quantitative image reconstruction in positron emission tomography\nattenuation correction is mandatory. In case that no data are available\nfor the calculation of the attenuation correction factors one can try\nto determine them from the emission data alone. However, it is not\nclear if the information content is sufficient to yield an adequate\nattenuation correction together with a satisfactory activity\ndistribution. Therefore, we determined the log likelihood distribution\nfor a thorax phantom depending on the choice of attenuation and\nactivity pixel values to measure the crosstalk between both. In\naddition an iterative image reconstruction (one-dimensional Newton-type\nalgorithm with a maximum likelihood estimator), which simultaneously\nreconstructs the images of the activity distribution and the\nattenuation coefficients is used to demonstrate the problems and\npossibilities of such a reconstruction. As result we show that for a\nchange of the log likelihood in the range of statistical noise, the\nassociated change in the activity value of a structure is between 6%\nand 263%. In addition, we show that it is not possible to choose the\nbest maximum on the basis of the log likelihood when a regularization\nis used, because the coupling between different structures mediated by\nthe (smoothing) regularization prevents an adequate solution due to\ncrosstalk. We conclude that taking into account the attenuation\ninformation in the emission data improves the performance of image\nreconstruction with respect to the bias of the activities, however, the\nreconstruction still is not quantitative\n', ['image reconstruction', 'positron emission tomography attenuation correction', 'attenuation correction factors', 'log likelihood distribution', 'thorax phantom', 'activity pixel values', 'crosstalk', 'iterative image reconstruction', 'one-dimensional Newton-type algorithm', 'maximum likelihood estimator', 'activity distribution', 'attenuation coefficients', 'statistical noise', 'smoothing', 'attenuation information', 'eigenvalues and eigenfunctions', 'image reconstruction', 'iterative methods', 'maximum likelihood estimation', 'medical image processing', 'optimisation', 'positron emission tomography']), ('A fuzzy-soft learning vector quantization for control chart pattern recognition\nThis paper presents a supervised competitive learning network approach, called\na fuzzy-soft learning vector quantization, for control chart pattern\nrecognition. Unnatural patterns in control charts mean that there are\nsome unnatural causes for variations in statistical process control\n(SPC). Hence, control chart pattern recognition becomes more important\nin SPC. In order to detect effectively the patterns for the six main\ntypes of control charts, Pham and Oztemel (1994) described a class of\npattern recognizers for control charts based on the learning vector\nquantization (LVQ) such as LVQ, LVQ2 and LVQ-X etc. In this paper, we\npropose a new supervised LVQ for control charts based on a fuzzy-soft\ncompetitive learning network. The proposed fuzzy-soft LVQ (FS-LVQ) uses\na fuzzy relaxation technique and simultaneously updates all neurons. It\ncan increase correct recognition accuracy and also decrease the\nlearning time. Comparisons between LVQ, LVQ-X and FS-LVQ are made\n', ['control chart pattern recognition', 'fuzzy-soft learning vector quantization', 'supervised competitive learning network approach', 'unnatural patterns', 'statistical process control', 'SPC', 'supervised LVQ', 'fuzzy relaxation technique', 'simultaneous neuron update', 'correct recognition accuracy', 'learning time', 'numerical results', 'manufacturing process', 'fuzzy neural nets', 'learning (artificial intelligence)', 'manufacturing processes', 'pattern recognition', 'statistical process control', 'vector quantisation']), ('Hidden Markov model-based tool wear monitoring in turning\nThis paper presents a new modeling framework for tool wear monitoring in\nmachining processes using hidden Markov models (HMMs). Feature vectors\nare extracted from vibration signals measured during turning. A\ncodebook is designed and used for vector quantization to convert the\nfeature vectors into a symbol sequence for the hidden Markov model. A\nseries of experiments are conducted to evaluate the effectiveness of\nthe approach for different lengths of training data and observation\nsequence. Experimental results show that successful tool state\ndetection rates as high as 97% can be achieved by using this approach\n', ['tool wear monitoring', 'machining processes', 'hidden Markov models', 'vibration signals', 'codebook', 'vector quantization', 'feature extraction', 'tool state detection', 'turning process', 'HMM training', 'discrete wavelet transform', 'condition monitoring', 'discrete wavelet transforms', 'feature extraction', 'hidden Markov models', 'learning systems', 'machine tools', 'machining', 'vector quantisation', 'wear']), ("Prospects for quantitative computed tomography imaging in the presence of\nforeign metal bodies using statistical image reconstruction\nX-ray computed tomography (CT) images of patients bearing metal intracavitary\napplicators or other metal foreign objects exhibit severe artifacts\nincluding streaks and aliasing. We have systematically evaluated via\ncomputer simulations the impact of scattered radiation, the\npolyenergetic spectrum, and measurement noise on the performance of\nthree reconstruction algorithms: conventional filtered backprojection\n(FBP), deterministic iterative deblurring, and a new iterative\nalgorithm, alternating minimization (AM), based on a CT detector model\nthat includes noise, scatter, and polyenergetic spectra. Contrary to\nthe dominant view of the literature, FBP streaking artifacts are due\nmostly to mismatches between FBP's simplified model of CT detector\nresponse and the physical process of signal acquisition. Artifacts on\nAM images are significantly mitigated as this algorithm substantially\nreduces detector-model mismatches. However, metal artifacts are reduced\nto acceptable levels only when prior knowledge of the metal object in\nthe patient, including its pose, shape, and attenuation map, are used\nto constrain AM's iterations. AM image reconstruction, in combination\nwith object-constrained CT to estimate the pose of metal objects in the\npatient, is a promising approach for effectively mitigating metal\nartifacts and making quantitative estimation of tissue attenuation\ncoefficients a clinical possibility\n", ['quantitative computed tomography imaging', 'foreign metal bodies', 'statistical image reconstruction', 'metal artifact reduction', 'brachytherapy', 'medical diagnostic imaging', 'signal acquisition physical process', 'object-constrained CT', 'iterative algorithm', 'alternating minimization', 'CT detector model', 'noise', 'scatter', 'polyenergetic spectra', 'clinical possibility', 'deterministic iterative deblurring', 'filtered backprojection', 'computerised tomography', 'digital simulation', 'image reconstruction', 'iterative methods', 'medical image processing', 'X-ray absorption', 'X-ray scattering']), ('A nonlinear modulation strategy for hybrid AC/DC power systems\nA nonlinear control strategy to improve transient stability of a multi-machine\nAC power system with several DC links terminated in the presence of\nlarge disturbances is presented. The approach proposed in this paper is\nbased on differential geometric theory, and the HVDC systems are taken\nas a variable admittance connected at the inverter or rectifier AC bus.\nAfter deriving the analytical description of the relationship between\nthe variable admittance and active power flows of each generator, the\ntraditional generator dynamic equations can thus be expressed with the\nvariable admittance of HVDC systems as an additional state variable and\nchanged to an affine form, which is suitable for global linearization\nmethod being used to determine its control variable. An important\nfeature of the proposed method is that, the modulated DC power is an\nadaptive and non-linear function of AC system states, and it can be\nrealized by local feedback and less transmitted data from, adjacent\ngenerators. The design procedure is tested on a dual-infeed hybrid\nAC/DC system\n', ['nonlinear control strategy', 'transient stability', 'multi-machine AC power system', 'DC links', 'nonlinear modulation strategy', 'hybrid AC/DC power systems', 'differential geometric theory', 'HVDC systems', 'variable admittance', 'inverter', 'rectifier AC bus', 'active power flows', 'generator dynamic equations', 'affine form', 'global linearization method', 'local feedback', 'adjacent generators', 'dual-infeed hybrid AC/DC system', 'electric admittance', 'feedback', 'HVDC power transmission', 'invertors', 'linearisation techniques', 'load flow', 'nonlinear control systems', 'power system control', 'rectifying circuits']), ('The fully entangled fraction as an inclusive measure of entanglement\napplications\nCharacterizing entanglement in all but the simplest case of a two qubit pure\nstate is a hard problem, even understanding the relevant experimental\nquantities that are related to entanglement is difficult. It may not be\nnecessary, however, to quantify the entanglement of a state in order to\nquantify the quantum information processing significance of a state. It\nis known that the fully entangled fraction has a direct relationship to\nthe fidelity of teleportation maximized under the actions of local\nunitary operations. In the case of two qubits we point out that the\nfully entangled fraction can also be related to the fidelities,\nmaximized under the actions of local unitary operations, of other\nimportant quantum information tasks such as dense coding, entanglement\nswapping and quantum cryptography in such a way as to provide an\ninclusive measure of these entanglement applications. For two qubit\nsystems the fully entangled fraction has a simple known closed-form\nexpression and we establish lower and upper bounds of this quantity\nwith the concurrence. This approach is readily extendable to more\ncomplicated systems\n', ['entanglement', 'two qubit pure state', 'quantum information processing', 'fully entangled fraction', 'fidelity', 'teleportation', 'entanglement swapping', 'quantum cryptography', 'information theory', 'quantum communication', 'quantum cryptography', 'quantum interference phenomena']), ("Breaking the myths of rewards: an exploratory study of attitudes about\nknowledge sharing\nMany CEO and managers understand the importance of knowledge sharing among\ntheir employees and are eager to introduce the knowledge management\nparadigm in their organizations. However little is known about the\ndeterminants of the individual's knowledge sharing behavior. The\npurpose of this study is to develop an understanding of the factors\naffecting the individual's knowledge sharing behavior in the\norganizational context. The research model includes various constructs\nbased on social exchange theory, self-efficacy, and theory of reasoned\naction. Research results from the field survey of 467 employees of four\nlarge, public organizations show that expected associations and\ncontribution are the major determinants of the individual's attitude\ntoward knowledge sharing. Expected rewards, believed by many to be the\nmost important motivating factor for knowledge sharing, are not\nsignificantly related to the attitude toward knowledge sharing. As\nexpected, positive attitude toward knowledge sharing is found to lead\nto positive intention to share knowledge and, finally, to actual\nknowledge sharing behaviors\n", ['knowledge sharing', 'knowledge management', 'social exchange theory', 'self-efficacy', 'theory of reasoned action', 'public organizations', 'rewards', 'strategic management', 'information resources', 'information systems', 'social aspects of automation', 'strategic planning']), ('Industry insiders loading up on cheap company stock\nA surge of telecom executives and directors purchasing their own companies,\nstock in the last two months points toward a renewed optimism in the\nbeleaguered sector, say some observers, who view the rash of insider\nbuying as a vote of confidence from management. Airgate PCS, Charter\nCommunications, Cox Communications, Crown Castle International, Nextel\nCommunications and Nortel Networks all have seen infusions of insider\ninvestment this summer, echoing trends in both the telecom industry and\nthe national economy\n', ['telecom industry', 'insider investment', 'telecommunication']), ('Union outreach - a pilgrim\'s progress\nAs the American labor movement continues on its path toward reorganization and\nrejuvenation, archivists are challenged to ensure that the\norganizational, political, and cultural changes labor unions are\nexperiencing are fully documented. The article examines the need for\nlabor archivists to reach out actively to unions and the problems they\nface in getting their message across, not only to union leadership but\nalso to union members. Outreach by labor archivists is vital on three\ncritical fronts: the need to secure union funding in support of labor\narchival programs; obtaining union cooperation in reviewing and\namending obsolete deposit agreements; and coordinating efforts with\nunions to save the records of closing district and local union offices.\nAttempting to resolve these outstanding issues, labor archivists are\npulled between two distinct institutional cultures (one academic in\nnature, the other enmeshed in a union bureaucracy) and often have their\nown labor archival programs compromised by the internal dynamics and\npolitics inherent in administering large academic libraries and unions.\nIf labor archivists are to be successful, they must find their\ncollective voice within the labor movement and establish their\nrelevancy to unions during a period of momentous change and\nrestructuring. Moreover, archivists need to give greater thought to\ndesigning and implementing outreach programs that bridge the\nfundamental "disconnect" between union bureaucracies and the rank and\nfile, and unions and the public\n', ['American labor movement', 'archivists', 'political changes', 'cultural changes', 'labor unions', 'labor archivists', 'union leadership', 'union members', 'union funding', 'labor archival programs', 'union cooperation', 'obsolete deposit agreements', 'union offices', 'institutional cultures', 'union bureaucracy', 'internal dynamics', 'large academic libraries', 'collective voice', 'information retrieval systems', 'politics', 'records management', 'service industries']), ("Spam solution?\nThe author describes a solution to spam E-mails: disposable E-mail addresses\n(DEA). Mailshell's free trial Web-based E-mail service allows you, if\nyou start getting spammed on that DEA, just to delete the DEA in\nMailshell, and all E-mail thereafter sent to that address will\nautomatically be junked (though you can later restore that address if\nyou want). Mailshell allows any number of DEA\n", ['spam E-mails', 'disposable E-mail addresses', 'Mailshell', 'Web-based E-mail', 'data privacy', 'electronic mail', 'information resources']), ("Computational challenges in cell simulation: a software engineering approach\nMolecular biology's advent in the 20th century has exponentially increased our\nknowledge about the inner workings of life. We have dozens of completed\ngenomes and an array of high-throughput methods to characterize gene\nencodings and gene product operation. The question now is how we will\nassemble the various pieces. In other words, given sufficient\ninformation about a living cell's molecular components, can we predict\nits behavior? We introduce the major classes of cellular processes\nrelevant to modeling, discuss software engineering's role in cell\nsimulation, and identify cell simulation requirements. Our E-Cell\nproject aims to develop the theories, techniques, and software\nplatforms necessary for whole-cell-scale modeling, simulation, and\nanalysis. Since the project's launch in 1996, we have built a variety\nof cell models, and we are currently developing new models that vary\nwith respect to species, target subsystem, and overall scale\n", ['cell simulation', 'software engineering', 'object-oriented design', 'molecular biology', 'E-Cell project', 'whole-cell-scale modeling', 'biology computing', 'cellular biophysics', 'digital simulation', 'molecular biophysics', 'object-oriented methods', 'software engineering']), ("The impact of the Internet on public library use: an analysis of the current\nconsumer market for library and Internet services\nThe potential impact of the Internet on the public's demand for the services\nand resources of public libraries is an issue of critical importance.\nThe research reported in this article provides baseline data concerning\nthe evolving relationship between the public's use of the library and\nits use of the Internet. The authors developed a consumer model of the\nAmerican adult market for information services and resources, segmented\nby use (or nonuse) of the public library and by access (or lack of\naccess) to, and use (or nonuse) of, the Internet. A national Random\nDigit Dialing telephone survey collected data to estimate the size of\neach of six market segments, and to describe their usage choices\nbetween the public library and the Internet. The analyses presented in\nthis article provide estimates of the size and demographics of each of\nthe market segments; describe why people are currently using the public\nlibrary and the Internet; identify the decision criteria people use in\ntheir choices of which provider to use; identify areas in which\nlibraries and the Internet appear to be competing and areas in which\nthey appear to be complementary; and identify reasons why people choose\nnot to use the public library and/or the Internet. The data suggest\nthat some differentiation between the library and the Internet is\ntaking place, which may very well have an impact on consumer choices\nbetween the two. Longitudinal research is necessary to fully reveal\ntrends in these usage choices, which have implications for all types of\nlibraries in planning and policy development\n", ['Internet', 'public libraries', 'baseline data', 'consumer model', 'American adult market', 'national Random Digit Dialing telephone survey', 'decision criteria', 'public library', 'longitudinal research', 'Internet', 'public libraries']), ('An efficient retrieval selection algorithm for video servers with random\nduplicated assignment storage technique\nRandom duplicated assignment (RDA) is an approach in which video data is stored\nby assigning a number of copies of each data block to different,\nrandomly chosen disks. It has been shown that this approach results in\nsmaller response times and lower disk and RAM costs compared to the\nwell-known disk stripping techniques. Based on this storage approach,\none has to determine, for each given batch of data blocks, from which\ndisk each of the data blocks is to be retrieved. This is to be done in\nsuch a way that the maximum load of the disks is minimized. The problem\nis called the retrieval selection problem (RSP). In this paper, we\npropose a new efficient algorithm for RSP. This algorithm is based on\nthe breadth-first search approach and is able to guarantee optimal\nsolutions for RSP in O(n/sup 2/+mn), where m and n correspond to the\nnumber of data blocks and the number of disks, respectively. We show\nthat our proposed algorithm has a lower time complexity than an\nexisting algorithm, called the MFS algorithm\n', ['efficient retrieval selection algorithm', 'video servers', 'random duplicated assignment storage technique', 'copies', 'data block', 'randomly chosen disks', 'response times', 'RAM costs', 'disk costs', 'maximum load', 'breadth-first search', 'optimal solutions', 'time complexity', 'computational complexity', 'magnetic disc storage', 'random-access storage', 'resource allocation', 'storage allocation', 'tree searching', 'video on demand', 'video servers']), ('Self-calibration from image derivatives\nThis study investigates the problem of estimating camera calibration parameters\nfrom image motion fields induced by a rigidly moving camera with\nunknown parameters, where the image formation is modeled with a linear\npinhole-camera model. The equations obtained show the flow to be\nseparated into a component due to the translation and the calibration\nparameters and a component due to the rotation and the calibration\nparameters. A set of parameters encoding the latter component is\nlinearly related to the flow, and from these parameters the calibration\ncan be determined. However, as for discrete motion, in general it is\nnot possible to decouple image measurements obtained from only two\nframes into translational and rotational components. Geometrically, the\nambiguity takes the form of a part of the rotational component being\nparallel to the translational component, and thus the scene can be\nreconstructed only up to a projective transformation. In general, for\nfull calibration at least four successive image frames are necessary,\nwith the 3D rotation changing between the measurements. The geometric\nanalysis gives rise to a direct self-calibration method that avoids\ncomputation of optical flow or point correspondences and uses only\nnormal flow measurements. New constraints on the smoothness of the\nsurfaces in view are formulated to relate structure and motion directly\nto image derivatives, and on the basis of these constraints the\ntransformation of the viewing geometry between consecutive images is\nestimated. The calibration parameters are then estimated from the\nrotational components of several flow fields. As the proposed technique\nneither requires a special set up nor needs exact correspondence it is\npotentially useful for the calibration of active vision systems which\nhave to acquire knowledge about their intrinsic parameters while they\nperform other tasks, or as a tool for analyzing image sequences in\nlarge video databases\n', ['camera calibration parameters', 'image motion fields', 'rigidly moving camera', 'image formation', 'linear pinhole-camera model', 'calibration parameters', 'image measurements', 'translational components', 'rotational components', 'direct self-calibration method', 'optical flow', 'point correspondences', 'normal flow measurements', 'active vision systems', 'image sequences', 'large video databases', 'depth distortion', 'active vision', 'calibration', 'image motion analysis', 'image sequences', 'motion estimation']), ("Perspectives on scholarly online books: the Columbia University Online Books\nEvaluation Project\nThe Online Books Evaluation Project at Columbia University studied the\npotential for scholarly online books from 1995 to 1999. Issues included\nscholars' interest in using online books, the role they might play in\nscholarly life, features that scholars and librarians sought in online\nbooks, the costs of producing and owning print and online books, and\npotential marketplace arrangements. Scholars see potential for online\nbooks to make their research, learning, and teaching more efficient and\neffective. Librarians see potential to serve their scholars better.\nLibrarians may face lower costs if they can serve their scholars with\nonline books instead of print books. Publishers may be able to offer\nscholars greater opportunities to use their books while enhancing their\nown profitability\n", ['Columbia University Online Books Evaluation Project', 'scholarly online books', 'print books', 'costs', 'marketplace arrangements', 'research', 'learning', 'academic libraries', 'economics', 'electronic publishing', 'research libraries']), ('From continuous recovery to discrete filtering in numerical approximations of\nconservation laws\nModern numerical approximations of conservation laws rely on numerical\ndissipation as a means of stabilization. The older, alternative\napproach is the use of central differencing with a dose of artificial\ndissipation. In this paper we review the successful class of weighted\nessentially non-oscillatory finite volume schemes which comprise\nsophisticated methods of the first kind. New developments in image\nprocessing have made new devices possible which can serve as highly\nnonlinear artificial dissipation terms. We view artificial dissipation\nas discrete filter operation and introduce several new algorithms\ninspired by image processing\n', ['continuous recovery', 'discrete filtering', 'numerical approximations', 'conservation laws', 'numerical dissipation', 'central differencing', 'artificial dissipation', 'finite volume schemes', 'image processing', 'highly nonlinear artificial dissipation terms', 'discrete filter operation', 'conservation laws', 'finite volume methods', 'fluid mechanics', 'image processing', 'polynomials']), ("PacketVideo. One step ahead of the streaming wireless market\nGo beyond the hype, however, and it's clear that PacketVideo is making strides\nin delivering streaming multimedia content to wireless devices. For one\nthing, its technology, based on the industry-standard Motion Pictures\nExpert Group 4 (MPEG-4) video encoder/decoder, actually works as\npromised. Secondly, the company has forged a broad-based band of\nalliances that not only will eventually help it reach potential\ncustomers down the road, but provides it financial support until the\ncompany can ramp up sales. The list of PacketVideo's technology\npartners who are also investors-and who have pumped more than $121\nmillion into the company-includes not just wireless device\nmanufacturers, but content providers and semiconductor vendors, all of\nwhom stand to benefit by increased sales of handheld wireless terminals\n", ['PacketVideo', 'multimedia content streaming', 'wireless devices', 'MPEG-4', 'wireless device manufacturers', 'content providers', 'semiconductor vendors', 'handheld wireless terminals', 'multimedia communication', 'notebook computers', 'radio access networks']), ('What do you say? Open letters to women considering a computer science major\nIn the last decade we have both monitored with great interest the ratio of\nfemale to male computer science majors at our respective institutions.\nWith each entering class, we think: "Surely, now is the time when the\nnumbers will become more balanced." Logic tells us that this must\neventually happen, because the opportunities in computing are simply\ntoo attractive for an entire segment of our population to routinely\npass up. But each year we are again disappointed in the number of women\nstudents, as they continue to be woefully under-represented among\ncomputer science majors. So, what do you say to a young woman who is\nconsidering a college choice and a choice of major in order to make\ncomputer science a more attractive option? We have organized some\nthoughts on that subject into open letters\n', ['women', 'computer science education', 'female', 'male', 'computer science majors', 'gender issues', 'computer science education', 'gender issues', 'social aspects of automation']), ('Image reconstruction of simulated specimens using convolution back projection\nThis paper reports the reconstruction of cross-sections of composite\nstructures. The convolution back projection (CBP) algorithm has been\nused to capture the attenuation field over the specimen. Five different\ntest cases have been taken up for evaluation. These cases represent\nvarying degrees of complexity. In addition, the role of filters on the\nnature of the reconstruction errors has also been discussed. Numerical\nresults obtained in the study reveal that CBP algorithm is a useful\ntool for qualitative as well as quantitative assessment of composite\nregions encountered in engineering applications\n', ['image reconstruction', 'simulated specimens', 'convolution back projection', 'composite structures', 'attenuation field', 'filters', 'reconstruction errors', 'CBP algorithm', 'composite regions', 'engineering applications', 'computerised tomography', 'computerised tomography', 'convolution', 'image reconstruction', 'nondestructive testing']), ("WebCAD: A computer aided design tool constrained with explicit 'design for\nmanufacturability' rules for computer numerical control milling\nA key element in the overall efficiency of a manufacturing enterprise is the\ncompatibility between the features that have been created in a newly\ndesigned part, and the capabilities of the downstream manufacturing\nprocesses. With this in mind, a process-aware computer aided design\n(CAD) system called WebCAD has been developed. The system restricts the\nfreedom of the designer in such a way that the designed parts can be\nmanufactured on a three-axis computer numerical control milling\nmachine. This paper discusses the vision of WebCAD and explains the\nrationale for its development in comparison with commercial CAD/CAM\n(computer aided design/manufacture) systems. The paper then goes on to\ndescribe the implementation issues that enforce the manufacturability\nrules. Finally, certain design tools are described that aid a user\nduring the design process. Some examples are given of the parts\ndesigned and manufactured with WebCAD\n", ['WebCAD', 'computer aided design tool', 'design for manufacturability rules', 'computer numerical control milling', 'manufacturing enterprise efficiency', 'process-aware CAD system', 'three-axis CNC milling machine', 'CAD/CAM systems', 'manufacturability rules', 'design tools', 'Internet-based CAD/CAM', 'CAD/CAM', 'computerised numerical control', 'design for manufacture', 'Internet', 'machining']), ('Gender, software design, and occupational equity\nAfter reviewing the work on gender bias in software design, a model of\ngender-role influenced achievement choice taken from Eccles (1994) is\npresented. The paper concludes that (1) though laudable, reduction of\ngender bias in software design is not the most straightforward way to\nreduce gender inequity in the choice of computing as a career, (2) the\nmodel itself makes more clear some of the ethical issues involved in\nattempting to achieve gender equity on computing, and (3) efforts to\nreduce gender inequity in the choice of computing as a career need to\nbe evaluated in the light of this model\n', ['gender bias', 'software design', 'gender-role influenced achievement choice model', 'computing career', 'ethical issues', 'occupational equity', 'employment', 'gender issues', 'software engineering']), ('Copyright of electronic publishing\nWith the spreading of the Internet and the wide use of computers, electronic\npublishing is becoming an indispensable measure to gain knowledge and\nskills. Meanwhile, copyright is facing much more infringement than ever\nin this electronic environment. So, it is a key factor to effectively\nprotect copyright of electronic publishing to foster the new\npublication fields. The paper analyzes the importance of copyright, the\nmain causes for copyright infringement in electronic publishing, and\npresents viewpoints on the definition and application of fair use of a\ncopyrighted work and thinking of some means to combat breach of\ncopyright\n', ['electronic publishing copyright', 'Internet', 'copyright infringement', 'electronic environment', 'copyright protection', 'fair use', 'copyrighted work', 'copyright', 'electronic publishing', 'Internet']), ('MACLP: multi agent constraint logic programming\nMulti agent systems (MAS) have become the key technology for decomposing\ncomplex problems in order to solve them more efficiently, or for\nproblems distributed in nature. However, many industrial applications,\nbesides their distributed nature, also involve a large number of\nparameters and constraints, i.e. they are combinatorial. Solving such\nparticularly hard problems efficiently requires programming tools that\ncombine MAS technology with a programming schema that facilitates the\nmodeling and solution of constraints. This paper presents MACLP (multi\nagent constraint logic programming), a logic programming platform for\nbuilding, in a declarative way, multi agent systems with\nconstraint-solving capabilities. MACLP extends CSPCONS, a logic\nprogramming system that permits distributed program execution through\ncommunicating sequential Prolog processes with constraints, by\nproviding all the necessary facilities for communication between\nagents. These facilities abstract from the programmer all the low-level\ndetails of the communication and allow him to focus on the development\nof the agent itself\n', ['multi agent constraint logic programming', 'multi agent systems', 'parameters', 'combinatorial problems', 'hard problems', 'constraint solving', 'distributed program execution', 'communicating sequential Prolog processes', 'constraint handling', 'distributed programming', 'multi-agent systems', 'PROLOG']), ('Adaptive and efficient mutual exclusion\nThe paper presents adaptive algorithms for mutual exclusion using only read and\nwrite operations; the performance of the algorithms depends only on the\npoint contention, i.e., the number of processes that are concurrently\nactive during algorithm execution (and not on n, the total number of\nprocesses). Our algorithm has O(k) remote step complexity and O(log k)\nsystem response time, where k is the point contention. The remote step\ncomplexity is the maximal number of steps performed by a process where\na wait is counted as one step. The system response time is the time\ninterval between subsequent entries to the critical section, where one\ntime unit is the minimal interval in which every active process\nperforms at least one step. The space complexity of this algorithm is\nO(N log n), where N is the range of process names. We show how to make\nthe space complexity of our algorithm depend solely on n, while\npreserving the other performance measures of the algorithm\n', ['adaptive mutual exclusion', 'adaptive algorithms', 'read operations', 'write operations', 'point contention', 'algorithm execution', 'remote step complexity', 'system response time', 'critical section', 'minimal interval', 'active process', 'space complexity', 'performance measures', 'adaptive systems', 'computational complexity', 'distributed algorithms', 'shared memory systems']), ("Mapping CCF to MARC21: an experimental approach\nThe purpose of this article is to raise and address a number of issues\npertaining to the conversion of Common Communication Format (CCF) into\nMARC21. In this era of global resource sharing, exchange of\nbibliographic records from one system to another is imperative in\ntoday's library communities. Instead of using a single standard to\ncreate machine-readable catalogue records, more than 20 standards have\nemerged and are being used by different institutions. Because of these\nvariations in standards, sharing of resources and transfer of data from\none system to another among the institutions locally and globally has\nbecome a significant problem. Addressing this problem requires keeping\nin mind that countries such as India and others in southeast Asia are\nusing the CCF as a standard for creating bibliographic cataloguing\nrecords. This paper describes a way to map the bibliographic catalogue\nrecords from CCF to MARC21, although 100% mapping is not possible. In\naddition, the paper describes an experimental approach that enumerates\nproblems that may occur during the mapping of records/exchanging of\nrecords and how these problems can be overcome\n", ['Common Communication Format conversion', 'MARC21', 'global resource sharing', 'bibliographic records exchange', 'library communities', 'machine-readable catalogue records', 'standards', 'data transfer', 'India', 'southeast Asia', 'CCF to MARC21 mapping', 'cataloguing', 'electronic data interchange', 'library automation', 'records management', 'standards']), ('Questioning the RFP process [telecom]\nIn the current climate, the most serious concern about the purchasing habits of\ntelecom carriers is obviously the lack of spending. Even against a\nbackdrop of economic constraints and financial struggles, however,\ngenuine concerns about the purchasing process itself are being raised\nby some of those closest to it\n', ['telecom carriers', 'purchasing process', 'sales cycle', 'request for information', 'request for proposal', 'purchasing', 'telecommunication']), ('Completion to involution and semidiscretisations\nWe discuss the relation between the completion to involution of linear\nover-determined systems of partial differential equations with constant\ncoefficients and the properties of differential algebraic equations\nobtained by their semidiscretisation. For a certain class of "weakly\nover-determined" systems, we show that the differential algebraic\nequations do not contain hidden constraints, if and only if the\noriginal partial differential system is involutive. We also demonstrate\nhow the formal theory can be used to obtain an existence and uniqueness\ntheorem for smooth solutions of strongly hyperbolic systems and to\nestimate the drift off the constraints, if an underlying equation is\nnumerically solved. Finally, we show for general linear systems how the\nindex of differential algebraic equations obtained by\nsemidiscretisations can be predicted from the result of a completion of\nthe partial differential system\n', ['completion', 'involution', 'linear over-determined systems', 'partial differential equations', 'matrices', 'semidiscretisations', 'constant coefficients', 'index', 'differential algebraic equations', 'uniqueness theorem', 'strongly hyperbolic systems', 'hyperbolic equations', 'matrix algebra', 'partial differential equations']), ('Embedding the outer automorphism group Out(F/sub n/) of a free group of rank n\nin the group Out(F/sub m/) for m > n\nIt is proved that for every n >or= 1, the group Out(F/sub n/) is embedded in\nthe group Out(F/sub m/) with m = 1 + (n - 1)k/sup n/, where k is an\narbitrary natural number coprime to n - 1\n', ['outer automorphism group embedding', 'free group', 'arbitrary natural number coprime', 'formal logic', 'group theory']), ('Himalayan information system: a proposed model\nThe information explosion and the development in information technology force\nus to develop information systems in various fields. The research on\nHimalaya has achieved phenomenal growth in recent years in India. The\ninformation requirements of Himalayan researchers are divergent in\nnature. In order to meet these divergent needs, all information\ngenerated in various Himalayan research institutions has to be\ncollected and organized to facilitate free flow of information. This\npaper describes the need for a system for Himalayan information. It\nalso presents the objectives of Himalayan information system (HIMIS).\nIt discusses in brief the idea of setting up a HIMIS and explains its\nutility to the users. It appeals to the government for supporting the\ndevelopment of such system\n', ['Himalayan information system model', 'information explosion', 'information technology', 'India', 'information requirements', 'HIMIS', 'government', 'information network', 'geographic information systems', 'information needs', 'information networks', 'information resources']), ('Time-integration of multiphase chemistry in size-resolved cloud models\nThe existence of cloud drops leads to a transfer of chemical species between\nthe gas and aqueous phases. Species concentrations in both phases are\nmodified by chemical reactions and by this phase transfer. The model\nequations resulting from such multiphase chemical systems are\nnonlinear, highly coupled and extremely stiff. In the paper we\ninvestigate several numerical approaches for treating such processes.\nThe droplets are subdivided into several classes. This decomposition of\nthe droplet spectrum into classes is based on their droplet size and\nthe amount of scavenged material inside the drops, respectively. The\nvery fast dissociations in the aqueous phase chemistry are treated as\nforward and backward reactions. The aqueous phase and gas phase\nchemistry, the mass transfer between the different droplet classes\namong themselves and with the gas phase are integrated in an implicit\nand coupled manner by the second order BDF method. For this part we\napply a modification of the code LSODE with special linear system\nsolvers. These direct sparse techniques exploit the special block\nstructure of the corresponding Jacobian. Furthermore we investigate an\napproximate matrix factorization which is related to operator splitting\nat the linear algebra level. The sparse Jacobians are generated\nexplicitly and stored in a sparse form. The efficiency and accuracy of\nour time-integration schemes is discussed for four multiphase chemistry\nsystems of different complexity and for a different number of droplet\nclasses\n', ['multiphase chemistry', 'size-resolved cloud models', 'cloud drops', 'chemical species', 'chemical reactions', 'multiphase chemical systems', 'aqueous phase chemistry', 'gas phase chemistry', 'approximate matrix factorization', 'operator splitting', 'linear algebra', 'sparse Jacobians', 'time-integration schemes', 'air pollution modelling', 'atmospheric chemistry', 'clouds', 'environmental science computing', 'geophysics computing', 'Jacobian matrices', 'matrix decomposition', 'multiphase flow', 'sparse matrices']), ('Completeness of timed mu CRL\nPreviously a straightforward extension of the process algebra mu CRL was\nproposed to explicitly deal with time. The process algebra mu CRL has\nbeen especially designed to deal with data in a process algebraic\ncontext. Using the features for data, only a minor extension of the\nlanguage was needed to obtain a very expressive variant of time.\nPreviously it contained syntax, operational semantics and axioms\ncharacterising timed mu CRL. It did not contain an in depth analysis of\ntheory of timed mu CRL. This paper fills this gap, by providing\nsoundness and completeness results. The main tool to establish these is\na mapping of timed to untimed mu CRL and employing the completeness\nresults obtained for untimed mu CRL\n', ['timed mu CRL', 'completeness', 'process algebra', 'operational semantics', 'bisimulation equivalence', 'process algebra']), ('Reachability in contextual nets\nContextual nets, or Petri nets with read arcs, are models of concurrent systems\nwith context dependent actions. The problem of reachability in such\nnets consists in finding a sequence of transitions that leads from the\ninitial marking of a given contextual net to a given goal marking. The\nsolution to this problem that is presented in this paper consists in\nconstructing a finite complete prefix of the unfolding of the given\ncontextual net, that is a finite prefix in which all the markings that\nare reachable from the initial marking are present, and in searching in\neach branch of this prefix for the goal marking by solving an\nappropriate linear programming problem\n', ['contextual nets reachability', 'Petri nets', 'concurrent systems', 'context dependent actions', 'finite prefix', 'goal marking', 'linear programming', 'formal specification', 'linear programming', 'Petri nets', 'process algebra']), ('Mobile commerce: transforming the vision into reality\nThis editorial preface investigates current developments in mobile commerce\n(M-commerce) and proposes an integrated architecture that supports\nbusiness and consumer needs in an optimal way to successfully implement\nM-commerce business processes. The key line of thought is based on the\nheuristic observation that customers will not want to receive\nM-commerce offerings to their mobile telephones. As a result, a pull as\nopposed to a push approach becomes a necessary requirement to conduct\nM-commerce. In addition, M-commerce has to rely on local, regional,\ndemographic and many other variables to be truly effective. Both\nobservations necessitate an M-commerce architecture that allows the\ncoherent integration of enterprise-level systems as well as the\naggregation of product and service offerings from many different and\npartially competing parties into a collaborative M-commerce platform.\nThe key software component within this integrated architecture is an\nevent management engine to monitor, detect, store, process and measure\ninformation about outside events that are relevant to all participants\nin M-commerce\n', ['M-commerce', 'mobile commerce', 'integrated architecture', 'consumer needs', 'business needs', 'mobile telephones', 'pull approach', 'collaborative platform', 'event management engine', 'cellular radio', 'electronic commerce', 'integrated software', 'mobile computing']), ('Maple 8 keeps everyone happy\nThe author is impressed with the upgrade to the mathematics package Maple 8,\nfinding it genuinely useful to scientists and educators. The\ndevelopments Waterloo Maple class as revolutionary include a student\ncalculus package, and Maplets. The first provides a high-level command\nset for calculus exploration and plotting (removing the need to work\nwith, say, plot primitives). The second is a package for hand-coding\ncustom graphical user interfaces (GUIs) using elements such as check\nboxes, radio buttons, slider bars and pull-down menus. When called, a\nMaplet launches a runtime Java environment that pops up a\nwindow-analogous to a Java applet-to perform a programmed routine, if\nrequired passing the result back to the Maple worksheet\n', ['Maple 8 mathematics package', 'student calculus package', 'high-level command set', 'calculus exploration', 'calculus plotting', 'GUIs', 'Maplet', 'runtime Java environment', 'software reviews', 'symbol manipulation']), ('Quantum-information processing by nuclear magnetic resonance: Experimental\nimplementation of half-adder and subtractor operations using an\noriented spin-7/2 system\nThe advantages of using quantum systems for performing many computational tasks\nhave already been established. Several quantum algorithms have been\ndeveloped which exploit the inherent property of quantum systems such\nas superposition of states and entanglement for efficiently performing\ncertain tasks. The experimental implementation has been achieved on\nmany quantum systems, of which nuclear magnetic resonance has shown the\nlargest progress in terms of number of qubits. This paper describes the\nuse of a spin-7/2 as a three-qubit system and experimentally implements\nthe half-adder and subtractor operations. The required qubits are\nrealized by partially orienting /sup 133/Cs nuclei in a\nliquid-crystalline medium, yielding a quadrupolar split well-resolved\nseptet. Another feature of this paper is the proposal that labeling of\nquantum states of system can be suitably chosen to increase the\nefficiency of a computational task\n', ['quantum-information processing', 'nuclear magnetic resonance', 'half-adder operations', 'subtractor operations', 'oriented spin-7/2 system', 'quantum systems', 'computational tasks', 'quantum algorithms', 'state superposition', 'entanglement', 'qubits', 'three-qubit system', '/sup 133/Cs nuclei', 'liquid-crystalline medium', 'quadrupolar split well-resolved septet', 'quantum states', 'computational task', '/sup 133/Cs', 'adders', 'computation theory', 'nuclear magnetic resonance', 'quantum gates', 'spin systems']), ("Packet spacing: an enabling mechanism for delivering multimedia content in\ncomputational grids\nStreaming multimedia with UDP has become increasingly popular over distributed\nsystems like the Internet. Scientific applications that stream\nmultimedia include remote computational steering of visualization data\nand video-on-demand teleconferencing over the Access Grid. However, UDP\ndoes not possess a self-regulating, congestion-control mechanism; and\nmost best-effort traffic is served by congestion-controlled TCP.\nConsequently, UDP steals bandwidth from TCP such that TCP flows starve\nfor network resources. With the volume of Internet traffic continuing\nto increase, the perpetuation of UDP-based streaming will cause the\nInternet to collapse as it did in the mid-1980's due to the use of\nnon-congestion-controlled TCP. To address this problem, we introduce\nthe counter-intuitive notion of inter-packet spacing with control\nfeedback to enable UDP-based applications to perform well in the\nnext-generation Internet and computational grids. When compared with\ntraditional UDP-based streaming, we illustrate that our approach can\nreduce packet loss over 50% without adversely affecting delivered\nthroughput\n", ['streaming multimedia', 'UDP', 'distributed systems', 'Internet', 'remote computational steering', 'visualization data', 'inter-packet spacing', 'UDP-based streaming', 'network protocol', 'transport protocols', 'multimedia communication', 'transport protocols']), ("The UK's National Electronic Site Licensing Initiative (NESLI)\nIn 1998 the UK created the National Electronic Site Licensing Initiative\n(NESLI) to increase and improve access to electronic journals and to\nnegotiate license agreements on behalf of academic libraries. The use\nof a model license agreement and the success of site licensing is\ndiscussed. Highlights from an interim evaluation by the Joint\nInformation Systems Committee (JISC) are noted and key issues and\nquestions arising from the evaluation are identified\n", ['National Electronic Site Licensing Initiative', 'NESLI', 'electronic journals', 'license agreements', 'academic libraries', 'Joint Information Systems Committee', 'usage statistics', 'JISC', 'ICOLC', 'academic libraries', 'contracts', 'electronic publishing', 'information resources', 'library automation']), ('Efficient feasibility testing for dial-a-ride problems\nDial-a-ride systems involve dispatching a vehicle to satisfy demands from a set\nof customers who call a vehicle-operating agency requesting that an\nitem tie picked up from a specific location and delivered to a specific\ndestination. Dial-a-ride problems differ from other routing and\nscheduling problems, in that they typically involve service-related\nconstraints. It is common to have maximum wait time constraints and\nmaximum ride time constraints. In the presence of maximum wait time and\nmaximum ride time restrictions, it is not clear how to efficiently\ndetermine, given a sequence of pickups and deliveries, whether a\nfeasible schedule exists. We demonstrate that this, in fact, can be\ndone in linear time\n', ['feasibility testing', 'dial-a-ride problems', 'dispatching', 'vehicle-operating agency', 'routing', 'scheduling', 'service-related constraints', 'maximum wait time constraints', 'maximum ride time constraints', 'computational complexity', 'dispatching', 'scheduling', 'transportation']), ('Quantized-State Systems: A DEVS-approach for continuous system simulation\nA new class of dynamical systems, Quantized State Systems or QSS, is introduced\nin this paper. QSS are continuous time systems where the input\ntrajectories are piecewise constant functions and the state variable\ntrajectories - being themselves piecewise linear functions - are\nconverted into piecewise constant functions via a quantization function\nequipped with hysteresis. It is shown that QSS can be exactly\nrepresented and simulated by a discrete event model, within the\nframework of the DEVS-approach. Further, it is shown that QSS can be\nused to approximate continuous systems, thus allowing their\ndiscrete-event simulation in opposition to the classical discrete-time\nsimulation. It is also shown that in an approximating QSS, some\nstability properties of the original system are conserved and the\nsolutions of the QSS go to the solutions of the original system when\nthe quantization goes to zero\n', ['dynamical systems', 'Quantized State Systems', 'continuous time systems', 'piecewise constant functions', 'discrete event model', 'discrete-event simulation', 'continuous time systems', 'discrete event simulation', 'piecewise constant techniques']), ('Characterization of sheet buckling subjected to controlled boundary constraints\nA wedge strip test is designed to study the onset and post-buckling behavior of\na sheet under various boundary constraints. The device can be easily\nincorporated into a conventional tensile test machine, and material\nresistance to buckling is measured as the buckling height versus the\nin-plane strain state. The design yields different but consistent\nbuckling modes with easy changes of boundary conditions (either clamped\nor freed) and sample geometry. Experimental results are then used to\nverify a hybrid approach to buckling prediction, i.e., the combination\nof the FEM analysis and an energy-based analytical wrinkling criterion.\nThe FEM analysis is used to obtain the stress field and deformed\ngeometry in a complex forming condition, while the analytical solution\nis to provide the predictions less sensitive to artificial numerical\nparameters. A good agreement between experimental data and numerical\npredictions is obtained\n', ['wedge strip test', 'boundary constraints', 'sheet buckling', 'forming processes', 'tensile test machine', 'strain state', 'energy-based analytical wrinkling criterion', 'stress field', 'deformed geometry', 'finite element analysis', 'buckling', 'deformation', 'finite element analysis', 'forming processes', 'stress effects']), ("TCP explicit congestion notification over ATM-UBR: a simulation study\nThe enhancement of transmission control protocol's (TCP's) congestion control\nmechanisms using explicit congestion notification (ECN) over\nasynchronous transfer mode (ATM) networks is overviewed. TCP's\ncongestion control is enhanced so that congestion is indicated by not\nonly packet losses as is currently the case but an agent implemented at\nthe ATM network's edge as well. The novel idea uses EFCI (explicit\nforward congestion indication) bits (available in every ATM cell\nheader) to generalize the ECN response to the UBR (unspecified bit\nrate) service, notify congestion, and adjust the credit-based window\nsize of the TCR. The authors' simulation experiments show that TCP ECN\nachieves significantly lower cell loss, packet retransmissions, and\nbuffer utilization, and exhibits better throughput than (non-ECN) TCP\nReno\n", ['TCP explicit congestion notification', 'ATM-UBR', 'simulation', 'congestion control mechanisms', 'ATM networks', 'packet losses', 'agent', 'explicit forward congestion indication bits', 'unspecified bit rate service', 'credit-based window size', 'cell loss', 'packet retransmissions', 'buffer utilization', 'throughput', 'asynchronous transfer mode', 'digital simulation', 'local area networks', 'telecommunication congestion control', 'transport protocols', 'wide area networks']), ('Wired right [accounting]\nFrom business intelligence to wireless networking to service providers, here is\nwhat you need to know to keep up to speed with a changing landscape\n', ['accounting', 'business intelligence', 'wireless networking', 'service providers', 'accounting', 'computer based training', 'document handling', 'information resources', 'management information systems', 'marketing', 'outsourcing', 'teleconferencing', 'wireless LAN']), ('Design of 1-D and 2-D variable fractional delay allpass filters using weighted\nleast-squares method\nIn this paper, a weighted least-squares method is presented to design\none-dimensional and two-dimensional variable fractional delay allpass\nfilters. First, each coefficient of the variable allpass filter is\nexpressed as the polynomial of the fractional delay parameter. Then,\nthe nonlinear phase error is approximated by a weighted equation error\nsuch that the cost function can be converted into a quadratic form.\nNext, by minimizing the weighted equation error, the optimal polynomial\ncoefficients can be obtained iteratively by solving a set of linear\nsimultaneous equations at each iteration. Finally, the design examples\nare demonstrated to illustrate the effectiveness of the proposed\napproach\n', ['weighted least-squares method', 'variable fractional delay allpass filters', '1D allpass filters', '2D allpass filters', 'fractional delay parameter', 'nonlinear phase error approximation', 'weighted equation error', 'cost function', 'optimal polynomial coefficients', 'linear simultaneous equations', 'all-pass filters', 'delays', 'filtering theory', 'iterative methods', 'least squares approximations', 'polynomials']), ('Tracking control of the flexible slider-crank mechanism system under impact\nThe variable structure control (VSC) and the stabilizer design by using the\npole placement technique are applied to the tracking control of the\nflexible slider-crank mechanism under impact. The VSC strategy is\nemployed to track the crank angular position and speed, while the\nstabilizer design is involved to suppress the flexible vibrations\nsimultaneously. From the theoretical impact consideration, three\napproaches including the generalized momentum balance (GMB), the\ncontinuous force model (CFM), and the CFM associated with the effective\nmass compensation EMC are adopted, and are derived on the basis of the\nenergy and impulse-momentum conservations. Simulation results are\nprovided to demonstrate the performance of the motor-controller\nflexible slider-crank mechanism not only accomplishing good tracking\ntrajectory of the crank angle, but also eliminating vibrations of the\nflexible connecting rod\n', ['tracking control', 'flexible slider-crank mechanism system', 'impact', 'variable structure control', 'stabilizer design', 'crank angular position', 'flexible vibrations', 'generalized momentum balance', 'continuous force model', 'effective mass compensation', 'conservation laws', 'tracking trajectory', 'flexible connecting rod', 'multibody dynamics', 'pole placement technique', 'flexible structures', 'impact (mechanical)', 'position control', 'tracking', 'variable structure systems', 'vibration control']), ('Using DEVS formalism to operationalize ELP models for diagnosis in SACHEM\nThis paper describes an original approach to discrete event control of\ncontinuous processes by means of expert knowledge. We present an\napplication of this approach on the SACHEM diagnosis subsystem. The\nSACHEM system is a large-scale knowledge-based system that aims in\nhelping a set of operators to control the dynamics of complex\ncontinuous systems (e.g., blast furnaces). The proposed method is based\non: (i) The definition of a language facilitating the acquisition and\nrepresentation of expert knowledge, called ELP (Expert Language\nProcess); (ii) The use of the DEVS formalism to make ELP models\noperational; (iii) Algorithms for exploiting operational models\n', ['SACHEM', 'ELP models', 'discrete event control', 'continuous processes', 'SACHEM system', 'expert knowledge', 'complex continuous systems', 'knowledge representation', 'knowledge acquisition', 'DEVS', 'discrete event simulation', 'knowledge acquisition', 'knowledge representation', 'process control']), ("LR parsing for conjunctive grammars\nThe generalized LR parsing algorithm for context-free grammars, introduced by\nTomita in 1986, is a polynomial-time implementation of nondeterministic\nLR parsing that uses graph-structured stack to represent the contents\nof the nondeterministic parser's pushdown for all possible branches of\ncomputation at a single computation step. It has been specifically\ndeveloped as a solution for practical parsing tasks arising in\ncomputational linguistics, and indeed has proved itself to be very\nsuitable for natural language processing. Conjunctive grammars extend\ncontext-free grammars by allowing the use of an explicit intersection\noperation within grammar rules. This paper develops a new LR-style\nparsing algorithm for these grammars, which is based on the very same\nidea of a graph-structured pushdown, where the simultaneous existence\nof several paths in the graph is used to perform the mentioned\nintersection operation. The underlying finite automata are treated in\nthe most general way: instead of showing the algorithm's correctness\nfor some particular way of constructing automata, the paper defines a\nwide class of automata usable with a given grammar, which includes not\nonly the traditional LR(k) automata, but also, for instance, a trivial\nautomaton with a single reachable state. A modification of the SLR(k)\ntable construction method that makes use of specific properties of\nconjunctive grammars is provided as one possible way of making finite\nautomata to use with the algorithm\n", ['conjunctive grammars', 'generalized LR parsing algorithm', 'graph-structured stack', 'nondeterministic parser pushdown', 'computation', 'computational linguistics', 'natural language processing', 'context-free grammars', 'explicit intersection operation', 'grammar rules', 'finite automata', 'trivial automaton', 'single reachable state', 'Boolean closure', 'deterministic context-free languages', 'computational linguistics', 'context-free grammars', 'context-free languages', 'pushdown automata']), ('Hybrid simulation of space plasmas: models with massless fluid representation\nof electrons. IV. Kelvin-Helmholtz instability\nFor pt.III. see Prikl. Mat. Informatika, MAKS Press, no. 4, p. 5-56 (2000).\nThis is a survey of the literature on hybrid simulation of the\nKelvin-Helmholtz instability. We start with a brief review of the\ntheory: the simplest model of the instability - a transition layer in\nthe form of a tangential discontinuity; compressibility of the medium;\nfinite size of the velocity shear region; pressure anisotropy. We then\ndescribe the electromagnetic hybrid model (ions as particles and\nelectrons as a massless fluid) and the main numerical schemes. We\nreview the studies on two-dimensional and three-dimensional hybrid\nsimulation of the process of particle mixing across the magnetopause\nshear layer driven by the onset of a Kelvin-Helmholtz instability. The\narticle concludes with a survey of literature on hybrid simulation of\nthe Kelvin-Helmholtz instability in finite-size objects: jets moving\nacross the magnetic field in the middle of the field reversal layer;\ninteraction between a magnetized plasma flow and a cylindrical plasma\nsource with zero own magnetic field\n', ['hybrid simulation', 'space plasmas', 'massless fluid representation', 'Kelvin-Helmholtz instability', 'transition layer', 'tangential discontinuity', 'pressure anisotropy', 'electromagnetic hybrid model', 'three-dimensional hybrid simulation', 'magnetopause shear layer', 'field reversal layer', 'magnetized plasma flow', 'cylindrical plasma source', 'flow instability', 'flow simulation', 'hybrid simulation', 'magnetohydrodynamics', 'plasma instability', 'stability']), ('Diagnostic expert system using non-monotonic reasoning\nThe objective of this work is to develop an expert system for cucumber disorder\ndiagnosis using non-monotonic reasoning to handle the situation when\nthe system cannot reach a conclusion. One reason for this situation is\nwhen the information is incomplete. Another reason is when the domain\nknowledge itself is incomplete. Another reason is when the information\nis inconsistent. This method maintains the truth of the system in case\nof changing a piece of information. The proposed method uses two types\nof non-monotonic reasoning namely: default reasoning and reasoning in\nthe presence of inconsistent information to achieve its goal\n', ['diagnostic expert system', 'nonmonotonic reasoning', 'cucumber disorder diagnosis', 'incomplete information', 'inconsistent information', 'truth maintenance', 'default reasoning', 'agriculture', 'agriculture', 'diagnostic expert systems', 'nonmonotonic reasoning', 'truth maintenance', 'uncertainty handling']), ("The effect of voxel size on the accuracy of dose-volume histograms of prostate\n/sup 125/I seed implants\nCumulative dose-volume histograms (DVH) are crucial in evaluating the quality\nof radioactive seed prostate implants. When calculating DVHs, the\nchoice of voxel size is a compromise between computational speed\n(larger voxels) and accuracy (smaller voxels). We quantified the effect\nof voxel size on the accuracy of DVHs using an in-house computer\nprogram. The program was validated by comparison with a hand-calculated\nDVH for a single 0.4-U iodine-125 model 6711 seed. We used the program\nto find the voxel size required to obtain accurate DVHs of five\niodine-125 prostate implant patients at our institution. One-millimeter\ncubes were sufficient to obtain DVHs that are accurate within 5% up to\n200% of the prescription dose. For the five patient plans, we obtained\ngood agreement with the VariSeed (version 6.7, Varian, USA) treatment\nplanning software's DVH algorithm by using voxels with a sup-inf\ndimension equal to the spacing between successive transverse seed\nimplant planes (5 mm). The volume that receives at least 200% of the\ntarget dose, V/sub 200/, calculated by VariSeed was 30% to 43% larger\nthan that calculated by our program with small voxels. The single-seed\nDVH calculated by VariSeed fell below the hand calculation by up to 50%\nat low doses (30 Gy), and above it by over 50% at high doses (>250\nGy)\n", ['cumulative dose-volume histograms', 'prostate /sup 125/I seed implants', 'radioactive seed prostate implants', 'voxel size', 'computational speed', 'in-house computer program', 'hand-calculated dose-volume histograms', 'single-seed dose-volume histograms', '/sup 125/I model', '/sup 125/I prostate implant patients', "VariSeed treatment planning software's dose-volume histogram algorithm", 'I', 'dosimetry', 'iodine', 'medical computing', 'radiation therapy', 'radioactive sources', 'radioisotopes']), ('Rank tests of association for exchangeable paired data\nWe describe two rank tests of association for paired exchangeable data\nmotivated by the study of lifespans in twins. The pooled sample is\nranked. The nonparametric test of association is based on R/sup +/, the\nsum of the smaller within-pair ranks. A second measure L/sup +/ is the\nsum of within-pair rank products. Under the null hypothesis of\nwithin-pair independence, the two test statistics are approximately\nnormally distributed. Expressions for the exact means and variances of\nR/sup +/ and L/sup +/ are given. We describe the power of these two\nstatistics under a close alternative hypothesis to that of\nindependence. Both the R/sup +/ and L/sup +/ tests indicate\nnonparametric statistical evidence of positive association of longevity\nin identical twins and a negligible relationship between the lifespans\nof fraternal twins listed in the Danish twin registry. The statistics\nare also applied to the analysis of a clinical trial studying the time\nto failure of ventilation tubes in children with bilateral otitis media\n', ['rank tests', 'association', 'paired exchangeable data', 'twin lifespans', 'pooled sample', 'nonparametric test', 'within-pair ranks', 'within-pair rank products', 'null hypothesis', 'within-pair independence', 'test statistics', 'exact means', 'exact variances', 'nonparametric statistical evidence', 'longevity', 'identical twins', 'fraternal twins', 'Danish twin registry', 'clinical trial', 'ventilation tube failure time', 'bilateral otitis media', 'data analysis', 'medical computing', 'paediatrics', 'statistical analysis']), ('Designing human-centered distributed information systems\nMany computer systems are designed according to engineering and technology\nprinciples and are typically difficult to learn and use. The fields of\nhuman-computer interaction, interface design, and human factors have\nmade significant contributions to ease of use and are primarily\nconcerned with the interfaces between systems and users, not with the\nstructures that are often more fundamental for designing truly\nhuman-centered systems. The emerging paradigm of human-centered\ncomputing (HCC)-which has taken many forms-offers a new look at system\ndesign. HCC requires more than merely designing an artificial agent to\nsupplement a human agent. The dynamic interactions in a distributed\nsystem composed of human and artificial agents-and the context in which\nthe system is situated-are indispensable factors. While we have\nsuccessfully applied our methodology in designing a prototype of a\nhuman-centered intelligent flight-surgeon console at NASA Johnson Space\nCenter, this article presents a methodology for designing\nhuman-centered computing systems using electronic medical records (EMR)\nsystems\n', ['human-centered distributed information systems design', 'distributed cognition', 'artificial agents', 'human agents', 'multiple analysis levels', 'human-computer interaction', 'interface design', 'human factors', 'human-centered computing systems', 'human-centered intelligent flight surgeon console', 'NASA Johnson Space Center', 'electronic medical records systems', 'human factors', 'information systems', 'medical information systems', 'records management', 'software agents', 'task analysis', 'user centred design', 'user interfaces']), ('Novel line conditioner with voltage up/down capability\nIn this paper, a novel pulsewidth-modulated line conditioner with fast output\nvoltage control is proposed. The line conditioner is made up of an AC\nchopper with reversible voltage control and a transformer for series\nvoltage compensation. In the AC chopper, a proper switching operation\nis achieved without the commutation problem. To absorb energy stored in\nline stray inductance, a regenerative DC snubber can be utilized which\nhas only one capacitor without discharging resistors or complicated\nregenerative circuit for snubber energy. Therefore, the proposed AC\nchopper gives high efficiency and reliability. The output voltage of\nthe line conditioner is controlled using a fast sensing technique of\nthe output voltage. It is also shown via some experimental results that\nthe presented line conditioner gives good dynamic and steady-state\nperformance for high quality of the output voltage\n', ['pulsewidth-modulated line conditioner', 'output voltage control', 'AC chopper', 'reversible voltage control', 'series voltage compensation transformer', 'switching operation', 'commutation', 'line stray inductance', 'regenerative DC snubber', 'dynamic performance', 'steady-state performance', 'choppers (circuits)', 'commutation', 'power transformers', 'PWM power convertors', 'snubbers', 'voltage control']), ('Modeling and simulation of an ABR flow control algorithm using a virtual\nsource/virtual destination switch\nThe available bit rate (ABR) service class of asynchronous transfer mode\nnetworks uses a feedback control mechanism to adapt to varying link\ncapacities. The virtual source/virtual destination (VS/VD) technique\noffers the possibility of segmenting the otherwise end-to-end ABR\ncontrol loop into separate loops. The improved feedback delay and\ncontrol of ABR traffic inside closed segments provide a better\nperformance for ABR connections. This article presents the use of\nclassical linear control theory to model and develop an ABR VS/VD flow\ncontrol algorithm. Discrete event simulations are used to analyze the\nbehavior of the algorithm with respect to transient behavior and\ncorrectness of the control model. Linear control theory offers the\nmeans to derive correct choices of parameters and to assess performance\nissues, such as stability of the system, during the design phase. The\nperformance goals are high link utilization, fair bandwidth\ndistribution, and robust operation in various environments, which are\nverified by discrete event simulations. The major contribution of this\nwork is the use of analytic methods (linear control theory) to model\nand design an ABR flow control algorithm tailored for the special\nlayout of a VS/VD switch, and the use of simulation techniques to\nverify the result\n', ['modeling', 'ABR flow control algorithm', 'virtual source/virtual destination switch', 'ATM networks', 'available bit rate service class', 'feedback control mechanism', 'link capacities', 'control loop', 'feedback delay', 'traffic control', 'closed segments', 'classical linear control theory', 'discrete event simulations', 'transient behavior', 'control model', 'performance issues', 'stability', 'high link utilization', 'fair bandwidth distribution', 'robust operation', 'asynchronous transfer mode', 'discrete event simulation', 'feedback', 'telecommunication congestion control', 'telecommunication traffic']), ("Minimised geometric Buchberger algorithm for integer programming\nRecently, various algebraic integer programming (IP) solvers have been proposed\nbased on the theory of Grobner bases. The main difficulty of these\nsolvers is the size of the Grobner bases generated. In algorithms\nproposed so far, large Grobner bases are generated by either\nintroducing additional variables or by considering the generic IP\nproblem IP/sub A,C/. Some improvements have been proposed such as\nHosten and Sturmfels' method (GRIN) designed to avoid additional\nvariables and Thomas' truncated Grobner basis method which computes the\nreduced Grobner basis for a specific IP problem IP/sub A,C/(b) (rather\nthan its generalisation IPA,C). In this paper we propose a new\nalgebraic algorithm for solving IP problems. The new algorithm, called\nMinimised Geometric Buchberger Algorithm, combines Hosten and\nSturmfels' GRIN and Thomas' truncated Grobner basis method to compute\nthe fundamental segments of an IP problem IP/sub A,C/ directly in its\noriginal space and also the truncated Grobner basis for a specific IP\nproblem IP/sub A,C/ (b). We have carried out experiments to compare\nthis algorithm with others such as the geometric Buchberger algorithm,\nthe truncated geometric Buchberger algorithm and the algorithm in GRIN.\nThese experiments show that the new algorithm offers significant\nperformance improvement\n", ['algebraic integer programming', 'minimised geometric Buchberger algorithm', 'Grobner bases', 'GRIN algorithm', 'truncated Grobner basis method', 'reduced Grobner basis', 'fundamental segments', 'geometric Buchberger algorithm', 'truncated geometric Buchberger algorithm', 'performance improvement', 'integer programming', 'minimisation']), ('Post-haste. 100th robotic containerization system installed in US mail sorting\ncenter\nSpot welding, machine tending, material handling, picking, packing, painting,\npalletizing, assembly...the list of tasks being performed by ABB robots\nkeeps on growing. Adding to this portfolio is a new robot\ncontainerization system (RCS) that ABB developed specifically for the\nUnited States Postal Service (USPS). The RCS has brought new levels of\nspeed, accuracy, efficiency and productivity to the process of sorting\nand containerizing mail and packages. Recently, the 100th ABB RCS was\ninstalled at the USPS processing and distribution center in Columbus,\nOhio\n', ['mail sorting center', 'robotic containerization system', 'USA', 'ABB robots', 'United States Postal Service', 'mail sorting', 'packages sorting', 'industrial robots', 'materials handling', 'postal services', 'sorting']), ('A 120-mW 3-D rendering engine with 6-Mb embedded DRAM and 3.2-GB/s runtime\nreconfigurable bus for PDA chip\nA low-power three-dimensional (3-D) rendering engine is implemented as part of\na mobile personal digital assistant (PDA) chip. Six-megabit embedded\nDRAM macros attached to 8-pixel-parallel rendering logic are logically\nlocalized with a 3.2-GB/s runtime reconfigurable bus, reducing the area\nby 25% compared with conventional local frame-buffer architectures. The\nlow power consumption is achieved by polygon-dependent access to the\nembedded DRAM macros with line-block mapping providing\nread-modify-write data transaction. The 3-D rendering engine with\n2.22-Mpolygons/s drawing speed was fabricated using 0.18- mu m CMOS\nembedded memory logic technology. Its area is 24 mm/sup 2/ and its\npower consumption is 120 mW\n', ['low-power 3D rendering engine', 'three-dimensional rendering engine', 'mobile PDA chip', 'mobile personal digital assistant chip', 'embedded DRAM macros', '8-pixel-parallel rendering logic', 'reconfigurable bus', 'low power consumption', 'polygon-dependent access', 'line-block mapping', 'read-modify-write data transaction', 'CMOS embedded memory logic technology', '3D graphics rendering', '120 mW', '6 Mbit', '3.2 GB/s', '0.18 micron', 'CMOS digital integrated circuits', 'computer graphic equipment', 'low-power electronics', 'microprocessor chips', 'mobile computing', 'notebook computers', 'random-access storage', 'rendering (computer graphics)']), ('Global action rules in distributed knowledge systems\nPreviously Z. Ras and J.M. Zytkow (2000) introduced and investigated query\nanswering system based on distributed knowledge mining. The notion of\nan action rule was introduced by Z. Ras and A. Wieczorkowska (2000) and\nits application domain e-business was taken. In this paper, we\ngeneralize the notion of action rules in a similar way to handling\nglobal queries. Mainly, when values of attributes for a given customer,\nused in action rules, can not be easily changed by business user,\ndefinitions of these attributes are extracted from other sites of a\ndistributed knowledge system. To be more precise, attributes at every\nsite of a distributed knowledge system are divided into two sets:\nstable and flexible. Values of flexible attributes, for a given\nconsumer, sometime can be changed and this change can be influenced and\ncontrolled by a business user. However, some of these changes (for\ninstance to the attribute "profit\') can not be done directly to a\nchosen attribute. In this case, definitions of such an attribute in\nterms of other attributes have to be learned. These new definitions are\nused to construct action rules showing what changes in values of\nflexible attributes, for a given consumer, are needed in order to\nre-classify this consumer the way business user wants. But, business\nuser may be either unable or unwilling to proceed with actions leading\nto such changes. In all such cases we may search for definitions of\nthese flexible attributes looking at either local or remote sites for\nhelp\n', ['global action rules', 'query answering system', 'action rules', 'attributes', 'e-commerce', 'distributed knowledge mining', 'data mining', 'electronic commerce', 'knowledge based systems']), ('Relationship between strong monotonicity property, P/sub 2/-property, and the\nGUS-property in semidefinite linear complementarity problems\nIn a recent paper on semidefinite linear complementarity problems, Gowda and\nSong (2000) introduced and studied the P-property, P/sub 2/-property,\nGUS-property, and strong monotonicity property for linear\ntransformation L: S/sup n/ to S/sup n/, where S/sup n/ is the space of\nall symmetric and real n * n matrices. In an attempt to characterize\nthe P/sub 2/-property, they raised the following two questions: (i)\nDoes the strong monotonicity imply the P/sub 2/-property? (ii) Does the\nGUS-property imply the P/sub 2/-property? In this paper, we show that\nthe strong monotonicity property implies the P/sub 2/-property for any\nlinear transformation and describe an equivalence between these two\nproperties for Lyapunov and other transformations. We show by means of\nan example that the GUS-property need not imply the P/sub 2/-property,\neven for Lyapunov transformations\n', ['semidefinite linear complementarity problems', 'strong monotonicity property', 'P/sub 2/-property', 'GUS-property', 'linear transformation', 'symmetric real matrices', 'Lyapunov transformations', 'complementarity', 'Lyapunov matrix equations', 'matrix algebra']), ('The 3D visibility complex\nVisibility problems are central to many computer graphics applications. The\nmost common examples include hidden-part removal for view computation,\nshadow boundaries, mutual visibility of objects for lighting\nsimulation. In this paper, we present a theoretical study of 3D\nvisibility properties for scenes of smooth convex objects. We work in\nthe space of light rays, or more precisely, of maximal free segments.\nWe group segments that "see" the same object; this defines the 3D\nvisibility complex. The boundaries of these groups of segments\ncorrespond to the visual events of the scene (limits of shadows,\ndisappearance of an object when the viewpoint is moved, etc.). We\nprovide a worst case analysis of the complexity of the visibility\ncomplex of 3D scenes, as well as a probabilistic study under a simple\nassumption for "normal" scenes. We extend the visibility complex to\nhandle temporal visibility. We give an output-sensitive construction\nalgorithm and present applications of our approach\n', ['3D visibility complex', 'computer graphics', 'hidden-part removal', 'view computation', 'shadow boundaries', 'mutual object visibility', 'lighting simulation', 'smooth convex objects', 'light rays', 'maximal free segments', 'visual events', 'worst case complexity analysis', 'probabilistic study', 'normal scenes', 'temporal visibility', 'output-sensitive construction algorithm', 'computational complexity', 'computer graphics', 'visibility']), ('Uniform supersaturated design and its construction\nSupersaturated designs are factorial designs in which the number of main\neffects is greater than the number of experimental runs. In this paper,\na discrete discrepancy is proposed as a measure of uniformity for\nsupersaturated designs, and a lower bound of this discrepancy is\nobtained as,a benchmark of design uniformity. A construction method for\nuniform supersaturated designs via resolvable balanced incomplete block\ndesigns is also presented along with the investigation of properties of\nthe resulting designs. The construction method shows a strong link\nbetween these two different kinds of designs\n', ['uniform supersaturated design', 'factorial designs', 'experimental runs', 'discrete discrepancy', 'resolvable balanced incomplete block designs', 'CAD']), ('A friction compensator for pneumatic control valves\nA procedure that compensates for static friction (stiction) in pneumatic\ncontrol valves is presented. The compensation is obtained by adding\npulses to the control signal. The characteristics of the pulses are\ndetermined from the control action. The compensator is implemented in\nindustrial controllers and control systems, and the industrial\nexperiences show that the procedure reduces the control error during\nstick-slip motion significantly compared to standard control without\nstiction compensation\n', ['friction compensator', 'pneumatic control valves', 'static friction compensation', 'stiction compensation', 'industrial controllers', 'control error reduction', 'stick-slip motion', 'standard control', 'compensation', 'pneumatic control equipment', 'process control', 'stiction', 'valves']), ('Hours of operation and service in academic libraries: toward a national\nstandard\nIn an effort toward establishing a standard for academic library hours, the\narticle surveys and compares hours of operation and service for ARL\nlibraries and IPEDS survey respondents. The article ranks the ARL\n(Association for Research Libraries) libraries according to hours of\noperation and reference hours and then briefly discusses such issues as\nlibraries offering twenty-four access and factors affecting service\nhour decisions\n', ['academic library hours', 'operation/service hours', 'ARL libraries', 'IPEDS survey respondents', 'Integrated Post Secondary Education Data System', 'Association for Research Libraries', 'academic libraries', 'management', 'research libraries', 'standards']), ('Decentralized adaptive output feedback stabilization for a class of\ninterconnected systems with unknown bound of uncertainties\nThe problem of adaptive decentralized stabilization for a class of linear\ntime-invarying large-scale systems with nonlinear interconnectivity and\nuncertainties is discussed. The bounds of uncertainties are assumed to\nbe unknown. For such uncertain dynamic systems, an adaptive\ndecentralized controller is presented. The resulting closed-loop\nsystems are asymptotically stable in theory. Moreover, an adaptive\ndecentralized control scheme is given. The scheme ensures the\nclosed-loop systems exponentially practically stable and can be used in\npractical engineering. Finally, simulations show that the control\nscheme is effective\n', ['adaptive decentralized stabilization', 'closed-loop systems', 'uncertain dynamic systems', 'robust control', 'large scale systems', 'adaptive systems', 'closed loop systems', 'interconnected systems', 'large-scale systems', 'state feedback', 'uncertain systems']), ("Commerce Department plan eases 3G spectrum crunch\nThe federal government made its first move last week toward cleaning up a\nspectrum allocation system that was in shambles just a year ago and had\nsome, spectrum-starved wireless carriers fearing they wouldn't be able\nto compete in third-generation services. The move, however, is far from\ncomplete and leaves numerous details unsettled\n", ['3G spectrum', 'federal government', 'spectrum allocation system', 'wireless carriers', 'cellular radio', 'frequency allocation', 'mobile communication']), ('eMarketing: restaurant Web sites that click\nA number of global companies have adopted electronic commerce as a means of\nreducing transaction related expenditures, connecting with current and\npotential customers, and enhancing revenues and profitability. If a\nrestaurant is to have an Internet presence, what aspects of the\nbusiness should be highlighted? Food service companies that have\nsuccessfully ventured onto the web have employed assorted web-based\ntechnologies to create a powerful marketing tool of unparalleled\nstrength. Historically, it has been difficult to create a set of\ncriteria against which to evaluate website effectiveness. As\npractitioners consider additional resources for website development,\nthe effectiveness of e-marketing investment becomes increasingly\nimportant. Care must be exercised to ensure that the quality of the\nsite adheres to high standards and incorporates evolving technology, as\nappropriate. Developing a coherent website strategy, including an\neffective website design, are proving critical to an effective web\npresence\n', ['e-marketing', 'restaurant Web sites', 'electronic commerce', 'Internet presence', 'food service companies', 'revenues', 'profitability', 'catering industry', 'electronic commerce', 'information resources', 'Internet', 'marketing data processing']), ("What's best practice for open access?\nThe business of publishing journals is in transition. Nobody knows exactly how\nit will work in the future, but everybody knows that the electronic\npublishing revolution will ensure it won't work as it does now. This\nknowledge has provoked a growing sense of nervous anticipation among\nthose concerned, some edgy and threatened by potential changes to their\nbusiness, others excited by the prospect of change and opportunity. The\npaper discusses the open publishing model for dissemination of research\n", ['open access', 'journal publishing', 'electronic publishing', 'business', 'open publishing model', 'research dissemination', 'electronic publishing']), ('Two quantum analogues of Fisher information from a large deviation viewpoint of\nquantum estimation\nWe discuss two quantum analogues of the Fisher information, the symmetric\nlogarithmic derivative Fisher information and Kubo-Mori-Bogoljubov\nFisher information from a large deviation viewpoint of quantum\nestimation and prove that the former gives the true bound and the\nlatter gives the bound of consistent superefficient estimators. As\nanother comparison, it is shown that the difference between them is\ncharacterized by the change of the order of limits\n', ['quantum analogues', 'quantum estimation', 'Kubo-Mori-Bogoljubov Fisher information', 'consistent superefficient estimators', 'statistical inference', 'large deviation viewpoint', 'symmetric logarithmic derivative Fisher information', 'information theory', 'parameter estimation', 'probability', 'quantum theory']), ('Enlisting on-line residents: Expanding the boundaries of e-government in a\nJapanese rural township\nThe purpose of this article is to analyze and learn from an unusual way in\nwhich local bureaucrats in a Japanese rural township are using the\nInternet to serve their constituents by enlisting the support of\n"on-line residents." Successful e-government requires not only\nrethinking the potential uses of computer technology, but in adopting\nnew patterns of decision-making, power sharing, and office management\nthat many bureaucrats may not be predisposed to make. The main thesis\nof this article is that necessity and practicality can play a powerful\nmotivational role in facilitating the incorporation of information\ntechnology (IT) at the level of local government. This case study of\nhow bureaucrats in Towa-cho, a small, agricultural town in Northeastern\nJapan, have harnessed the Internet demonstrates clearly the\nfundamentals of building a successful e-government framework in this\nrural municipality, similar to many communities in Europe and North\nAmerica today\n', ['on-line residents', 'e-government', 'Japanese rural township', 'local bureaucrats', 'Internet', 'decision-making', 'power sharing', 'office management', 'Towa-cho', 'rural municipality', 'government data processing', 'Internet']), ('A unified view for vector rotational CORDIC algorithms and architectures based\non angle quantization approach\nVector rotation is the key operation employed extensively in many digital\nsignal processing applications. In this paper, we introduce a new\ndesign concept called Angle Quantization (AQ). It can be used as a\ndesign index for vector rotational operation, where the rotational\nangle is known in advance. Based on the AQ process, we establish a\nunified design framework for cost-effective low-latency rotational\nalgorithms and architectures. Several existing works, such as\nconventional COordinate Rotational Digital Computer (CORDIC),\nAR-CORDIC, MVR-CORDIC, and EEAS-based CORDIC, can be fitted into the\ndesign framework, forming a Vector Rotational CORDIC Family. Moreover,\nwe address four searching algorithms to solve the optimization problem\nencountered in the proposed vector rotational CORDIC family. The\ncorresponding scaling operations of the CORDIC family are also\ndiscussed. Based on the new design framework, we can realize\nhigh-speed/low-complexity rotational VLSI circuits, whereas without\ndegrading the precision performance in fixed-point implementations\n', ['vector rotational CORDIC algorithms', 'digital signal processing applications', 'DSP applications', 'angle quantization', 'design index', 'vector rotational operation', 'unified design framework', 'low-latency rotational algorithms', 'greedy searching algorithm', 'low-latency rotational architectures', 'searching algorithms', 'optimization problem', 'scaling operations', 'high-speed rotational VLSI circuits', 'low-complexity rotational VLSI circuits', 'fixed-point implementations', 'trellis-based searching algorithm', 'digital signal processing chips', 'fixed point arithmetic', 'optimisation', 'parallel architectures', 'pipeline arithmetic', 'search problems', 'signal processing', 'VLSI']), ('A hybrid-neural network and population learning algorithm approach to solving\nreliability optimization problem\nProposes a hybrid approach integrating a dedicated artificial neural network\nand population learning algorithm applied to maximising system\nreliability under cost and technical feasibility constraints. The paper\nincludes a formulation of the system reliability optimisation (SRO)\nproblem and a description of the dedicated neural network trained by\napplying the population learning algorithm. A solution to the example\nSRO problem is shown and results of the computational experiment are\npresented and discussed\n', ['hybrid approach', 'dedicated artificial neural network', 'population learning algorithm', 'system reliability', 'cost constraints', 'technical feasibility constraints', 'reliability optimization problem', 'learning (artificial intelligence)', 'neural nets', 'optimisation', 'reliability theory']), ('General solution of a density functionally gradient piezoelectric cantilever\nand its applications\nWe have used the plane strain theory of transversely isotropic bodies to study\na piezoelectric cantilever. In order to find the general solution of a\ndensity functionally gradient piezoelectric cantilever, we have used\nthe inverse method (i.e. the Airy stress function method). We have\nobtained the stress and induction functions in the form of polynomials\nas well as the general solution of the beam. Based on this general\nsolution, we have deduced the solutions of the cantilever under\ndifferent loading conditions. Furthermore, as applications of this\ngeneral solution in engineering, we have studied the tip deflection and\nblocking force of a piezoelectric cantilever actuator. Finally, we have\naddressed a method to determine the density distribution profile for a\ngiven piezoelectric material\n', ['plane strain theory', 'transversely isotropic bodies', 'inverse method', 'Airy stress function', 'polynomials', 'loading conditions', 'piezoelectric cantilever actuator', 'density distribution profile', 'piezoelectric material', 'inverse problems', 'piezoelectric actuators', 'piezoelectric materials', 'polynomials']), ('Simulation of ecological and economical structural-type functions\nAn optimization approach to the simulation of ecological and economical\nstructural-type functions is proposed. A methodology for construction\nof such functions is created in an explicit analytical form\n', ['economical structural-type functions', 'optimisation approach', 'simulation', 'explicit analytical form', 'natural resources', 'pollution control']), ('On a general constitutive description for the inelastic and failure behavior of\nfibrous laminates. II. Laminate theory and applications\nFor pt. I see ibid., pp. 1159-76. The two papers report systematically a\nconstitutive description for the inelastic and strength behavior of\nlaminated composites reinforced with various fiber preforms. The\nconstitutive relationship is established micromechanically, through\nlayer-by-layer analysis. Namely, only the properties of the constituent\nfiber and matrix materials of the composites are required as input\ndata. In the previous part lamina theory was presented. Three\nfundamental quantities of the laminae, i.e. the internal stresses\ngenerated in the constituent fiber and matrix materials and the\ninstantaneous compliance matrix, with different fiber preform\n(including woven, braided, and knitted fabric) reinforcements were\nexplicitly obtained by virtue of the bridging micromechanics model. In\nthis paper, the laminate stress analysis is shown. The purpose of this\nanalysis is to determine the load shared by each lamina in the\nlaminate, so that the lamina theory can be applied. Incorporation of\nthe constitutive equations into an FEM software package is illustrated.\nA number of application examples are given to demonstrate the\nefficiency of the constitutive theory. The predictions made include:\nfailure envelopes of multidirectional laminates subjected to biaxial\nin-plane loads, thermomechanical cycling stress-strain curves of a\ntitanium metal matrix composite laminate, S-N curves of multilayer\nknitted fabric reinforced laminates under tensile fatigue, and bending\nload-deflection plots and ultimate bending strengths of laminated\nbraided fabric reinforced beams subjected to lateral loads\n', ['general constitutive description', 'inelastic behavior', 'failure behavior', 'fibrous laminates', 'laminate theory', 'strength behavior', 'composites', 'fiber preforms', 'micromechanics', 'layer-by-layer analysis', 'internal stresses', 'matrix materials', 'instantaneous compliance matrix', 'stress analysis', 'load', 'FEM software package', 'failure envelopes', 'multidirectional laminates', 'biaxial in-plane loads', 'thermomechanical cycling stress-strain curves', 'titanium metal matrix composite laminate', 'S-N curves', 'multilayer knitted fabric reinforced laminates', 'tensile fatigue', 'bending load deflection plots', 'ultimate bending strengths', 'laminated braided fabric reinforced beams', 'lateral loads', 'bending', 'bending strength', 'fatigue', 'fibre reinforced composites', 'finite element analysis', 'internal stresses', 'laminates', 'mechanical engineering computing', 'stress analysis', 'stress-strain relations', 'thermomechanical treatment']), ('ECG-gated /sup 18/F-FDG positron emission tomography. Single test evaluation of\nsegmental metabolism, function and contractile reserve in patients with\ncoronary artery disease and regional dysfunction\n/sup 18/F-fluorodeoxyglucose (/sup 18/F-FDG)-positron emission tomography (PET)\nprovides information about myocardial glucose metabolism to diagnose\nmyocardial viability. Additional information about the functional\nstatus is necessary. Comparison of tomographic metabolic PET with data\nfrom other imaging techniques is always hampered by some transfer\nuncertainty and scatter. We wanted to evaluate a new Fourier-based\nECG-gated PET technique using a high resolution scanner providing both\nmetabolic and functional data with respect to feasibility in patients\nwith diseased left ventricles. Forty-five patients with coronary artery\ndisease and at least one left ventricular segment with severe\nhypokinesis or akinesis at biplane cineventriculography were included.\nA new Fourier-based ECG-gated metabolic /sup 18/F-FDG-PET was performed\nin these patients. Function at rest and /sup 18/F-FDG uptake were\nexamined in the PET study using a 36-segment model. Segmental\ncomparison with ventriculography revealed a high reliability in\nidentifying dysfunctional segments (>96%). /sup 18/F-FDG uptake of\nnormokinetic/hypokinetic/akinetic segments was 75.4+or-7.5,\n65.3+or-10.5, and 35.9+or-15.2% (p<0.001). In segments >or=70%\n/sup 18/F-FDG uptake no akinesia was observed. No residual function was\nfound below 40% /sup 18/F-FDG uptake. An additional dobutamine test was\nperformed and revealed inotropic reserve (viability) in 42 akinetic\nsegments and 45 hypokinetic segments. ECG-gated metabolic PET with\npixel-based Fourier smoothing provides reliable data on regional\nfunction. Assessment of metabolism and function makes complete\njudgement of segmental status feasible within a single study without\nany transfer artefacts or test-to-test variability. The results\nindicate the presence of considerable amounts of viable myocardium in\nregions with an uptake of 40-50% /sup 18/F-FDG\n', ['Fourier-based ECG-gated metabolic /sup 18/F-fluorodeoxyglucose-positron emission tomography', '/sup 18/F-fluorodeoxyglucose uptake', 'thirty six-segment model', 'ventriculography', 'dysfunctional segments', 'normokinetic/hypokinetic/akinetic segments', 'residual function', 'dobutamine test', 'inotropic reserve', 'akinetic segments', 'hypokinetic segments', 'pixel-based Fourier smoothing', 'regional function', 'segmental status', 'transfer artefacts', 'viable myocardium', 'regional dysfunction', 'myocardial glucose metabolism', 'myocardial viability', 'functional', 'transfer uncertainty', 'Fourier-based ECG-gated PET technique', 'high resolution scanner', 'patients', 'diseased left ventricles', 'coronary artery disease', 'left ventricular segment', 'severe hypokinesis', 'akinesis', 'biplane cineventriculography', 'cardiology', 'diseases', 'electrocardiography', 'Fourier analysis', 'medical image processing', 'physiological models', 'positron emission tomography']), ('Estimation of blocking probabilities in cellular networks with dynamic channel\nassignment\nBlocking probabilities in cellular mobile communication networks using dynamic\nchannel assignment are hard to compute for realistic sized systems.\nThis computational difficulty is due to the structure of the state\nspace, which imposes strong coupling constraints amongst components of\nthe occupancy vector. Approximate tractable models have been proposed,\nwhich have product form stationary state distributions. However, for\nreal channel assignment schemes, the product form is a poor\napproximation and it is necessary to simulate the actual occupancy\nprocess in order to estimate the blocking probabilities. Meaningful\nestimates of the blocking probability typically require an enormous\namount of CPU time for simulation, since blocking events are usually\nrare. Advanced simulation approaches use importance sampling (IS) to\novercome this problem. We study two regimes under which blocking is a\nrare event: low-load and high cell capacity. Our simulations use the\nstandard clock (SC) method. For low load, we propose a change of\nmeasure that we call static ISSC, which has bounded relative error. For\nhigh capacity, we use a change of measure that depends on the current\nstate of the network occupancy. This is the dynamic ISSC method. We\nprove that this method yields zero variance estimators for single\nclique models, and we empirically show the advantages of this method\nover naive simulation for networks of moderate size and traffic loads\n', ['blocking probability estimation', 'dynamic channel assignment', 'cellular mobile communication networks', 'strong coupling constraints', 'occupancy vector', 'approximate tractable models', 'product form stationary state distributions', 'CPU time', 'simulation', 'importance sampling', 'low-load', 'high cell capacity', 'standard clock method', 'static ISSC method', 'bounded relative error', 'quality of service', 'dynamic ISSC method', 'zero variance estimators', 'single clique models', 'network traffic load', 'cellular radio', 'channel allocation', 'digital simulation', 'importance sampling', 'probability', 'quality of service', 'telecommunication computing', 'telecommunication traffic']), ('Pulmonary perfusion patterns and pulmonary arterial pressure\nUses artificial intelligence methods to determine whether quantitative\nparameters describing the perfusion image can be synthesized to make a\nreasonable estimate of the pulmonary arterial (PA) pressure measured at\nangiography. Radionuclide perfusion images were obtained in 120\npatients with normal chest radiographs who also underwent angiographic\nPA pressure measurement within 3 days of the radionuclide study. An\nartificial neural network (ANN) was constructed from several image\nparameters describing statistical and boundary characteristics of the\nperfusion images. With use of a leave-one-out cross-validation\ntechnique, this method was used to predict the PA systolic pressure in\ncases on which the ANN had not been trained. A Pearson correlation\ncoefficient was determined between the predicted and measured PA\nsystolic pressures. ANN predictions correlated with measured pulmonary\nsystolic pressures (r=0.846, P<.001). The accuracy of the\npredictions was not influenced by the presence of pulmonary embolism.\nNone of the 51 patients with predicted PA pressures of less than 29 mm\nHg had pulmonary hypertension at angiography. All 13 patients with\npredicted PA pressures greater than 48 mm Hg had pulmonary hypertension\nat angiography. Meaningful information regarding PA pressure can be\nderived from noninvasive radionuclide perfusion scanning. The use of\nimage analysis in concert with artificial intelligence methods helps to\nreveal physiologic information not readily apparent at visual image\ninspection\n', ['pulmonary perfusion patterns', 'angiographic pulmonary arterial pressure measurement', 'artificial neural network predictions', 'accuracy', 'pulmonary embolism', 'pulmonary hypertension', 'noninvasive radionuclide perfusion scanning', 'image analysis', 'physiologic information', 'visual image inspection', 'image parameters', 'statistical characteristics', 'boundary characteristics', 'leave-one-out cross-validation technique', 'pulmonary arterial systolic pressure', 'Pearson correlation coefficient', 'artificial intelligence methods', 'quantitative parameters', 'perfusion image', 'angiography', 'radionuclide perfusion images', 'patients', 'normal chest radiographs', '29 Pa', '48 Pa', 'haemodynamics', 'lung', 'medical image processing', 'neural nets', 'radioisotope imaging', 'statistical analysis']), ('Mining the optimal class association rule set\nWe define an optimal class association rule set to be the minimum rule set with\nthe same predictive power of the complete class association rule set.\nUsing this rule set instead of the complete class association rule set\nwe can avoid redundant computation that would otherwise be required for\nmining predictive association rules and hence improve the efficiency of\nthe mining process significantly. We present an efficient algorithm for\nmining the optimal class association rule set using an upward closure\nproperty of pruning weak rules before they are actually generated. We\nhave implemented the algorithm and our experimental results show that\nour algorithm generates the optimal class association rule set, whose\nsize is smaller than 1/17 of the complete class association rule set on\naverage, in significantly less time than generating the complete class\nassociation rule set. Our proposed criterion has been shown very\neffective for pruning weak rules in dense databases\n', ['optimal class association rule set mining', 'minimum rule set', 'predictive power', 'redundant computation', 'predictive association rules', 'relational database', 'upward closure property', 'data mining', 'weak rule pruning', 'experimental results', 'dense databases', 'data mining', 'relational databases', 'very large databases']), ('Convolution-based global simulation technique for millimeter-wave photodetector\nand photomixer circuits\nA fast convolution-based time-domain approach to global photonic-circuit\nsimulation is presented that incorporates a physical device model in\nthe complete detector or mixer circuit. The device used in the\ndemonstration of this technique is a GaAs metal-semiconductor-metal\n(MSM) photodetector that offers a high response speed for the detection\nand generation of millimeter waves. Global simulation greatly increases\nthe accuracy in evaluating the complete circuit performance because it\naccounts for the effects of the millimeter-wave embedding circuit.\nDevice and circuit performance are assessed by calculating optical\nresponsivity and bandwidth. Device-only simulations using GaAs MSMs are\ncompared with global simulations that illustrate the strong\ninterdependence between device and external circuit\n', ['convolution-based time-domain approach', 'global photonic-circuit simulation', 'physical device model', 'GaAs MSM photodetector', 'millimeter-wave photodetector', 'photomixer', 'MM-wave embedding circuit', 'optical responsivity', 'bandwidth', 'convolution-based global simulation', 'GaAs', 'circuit simulation', 'convolution', 'equivalent circuits', 'gallium arsenide', 'metal-semiconductor-metal structures', 'microwave photonics', 'millimetre wave detectors', 'millimetre wave mixers', 'photodetectors', 'time-domain analysis']), ('The importance of continuity: a reply to Chris Eliasmith\nIn his reply to Eliasmith (see ibid., vol.11, p.417-26, 2001) Poznanski\nconsiders how the notion of continuity of dynamic representations\nserves as a beacon for an integrative neuroscience to emerge. He\nconsiders how the importance of continuity has come under attack from\nEliasmith (2001) who claims: (i) continuous nature of neurons is not\nrelevant to the information they process, and (ii) continuity is not\nimportant for understanding cognition because the various sources of\nnoise introduce uncertainty into spike arrival times, so encoding and\ndecoding spike trains must be discrete at some level\n', ['continuity', 'dynamic representations', 'integrative neuroscience', 'neurons', 'cognition', 'uncertainty', 'spike arrival times', 'spike trains', 'cognitive systems', 'neural nets', 'cognitive systems', 'neural nets', 'neurophysiology']), ('Blending parametric patches with subdivision surfaces\nIn this paper the problem of blending parametric surfaces using subdivision\npatches is discussed. A new approach, named removing-boundary, is\npresented to generate piecewise-smooth subdivision surfaces through\ndiscarding the outmost quadrilaterals of the open meshes derived by\neach subdivision step. Then the approach is employed both to blend\nparametric bicubic B-spline surfaces and to fill n-sided holes. It is\neasy to produce piecewise-smooth subdivision surfaces with both convex\nand concave corners on the boundary, and limit surfaces are guaranteed\nto be C/sup 2/ continuous on the boundaries except for a few singular\npoints by the removing-boundary approach. Thus the blending method is\nvery efficient and the blending surface generated is of good effect\n', ['subdivision surfaces', 'parametric surfaces blending', 'subdivision patches', 'piecewise-smooth subdivision surfaces', 'quadrilaterals', 'parametric bicubic B-spline surfaces', 'piecewise smooth subdivision surfaces', 'computer graphics', 'solid modelling', 'splines (mathematics)']), ('Sharpening the estimate of the stability constant in the maximum-norm of the\nCrank-Nicolson scheme for the one-dimensional heat equation\nThis paper is concerned with the stability constant C/sub infinity / in the\nmaximum-norm of the Crank-Nicolson scheme applied. to the\none-dimensional heat equation. A well known result due to S.J.\nSerdyukova is that C/sub infinity / < 23. In the present paper, by\nusing a sharp resolvent estimate for the discrete Laplacian together\nwith the Cauchy formula, it is shown that 3 <or= C/sub infinity /\n< 4.325. This bound also holds when the heat equation is considered\non a bounded interval along with Dirichlet or Neumann boundary\nconditions\n', ['stability constant', 'Crank-Nicolson scheme', 'one-dimensional heat equation', 'sharp resolvent estimate', 'discrete Laplacian', 'Cauchy formula', 'Neumann boundary conditions', 'Dirichlet boundary conditions', 'iterative methods', 'Laplace equations', 'stability']), ('Web content extraction. A WhizBang! approach\nThe extraction technology that Whizbang uses consists of a unique approach to\nscouring the Web for current, very specific forms of information.\nFlipDog, for example, checks company Web sites for hyperlinks to pages\nthat list job opportunities. It then crawls to the deeper page and,\nusing the WhizBang! Extraction Framework, extracts the key elements of\nthe postings, such as job title, name of employer, job category, and\njob function. Click on a job and you are transferred to the company Web\nsite to view the job description as it appears there\n', ['Web content extraction', 'FlipDog', 'job description', 'job-hunting site', 'company Web sites', 'WhizBang! Extraction Framework', 'human resource management', 'information resources', 'information retrieval']), ("Shaping the future. BendWizard: a tool for off-line programming of robotic\ntending systems\nSetting up a robot to make metal cabinets or cases for desktop computers can be\na complex operation. For instance, one expert might be required to\ncarry out a feasibility study, and then another to actually program the\nrobot. Understandably, the need for so much expertise, and the time\nthat's required, generally limits the usefulness of automation to\nhigh-volume production. Workshops producing parts in batches smaller\nthan 50 or so, or which rely heavily on semiskilled operators, are\ntherefore often discouraged from investing in automation, and so miss\nout on its many advantages. What is needed is a software tool that\noperators without special knowledge of robotics, or with no more than\nrudimentary CAD skills, can use. One which allows easy offline\nprogramming and simulation of the work cell on a PC\n", ['robotic tending systems', 'BendWizard offline programming tool', 'metal cabinets', 'desktop computer cases', 'feasibility study', 'high-volume production', 'workshops', 'CAD skills', 'work cell simulation', 'bending', 'industrial robots', 'machining', 'robot programming']), ('The pedagogy of on-line learning: a report from the University of the Highlands\nand Islands Millennium Institute\nAuthoritative sources concerned with computer-aided learning, resource-based\nlearning and on-line learning and teaching are generally agreed that,\nin addition to subject matter expertise and technical support, the\nquality of the learning materials and the learning experiences of\nstudents are critically dependent on the application of pedagogically\nsound theories of learning and teaching and principles of course\ndesign. The University of the Highlands and Islands Project (UHIMI) is\ndeveloping "on-line learning" on a large scale. These developments have\nbeen accompanied by a comprehensive programme of staff development. A\nmajor emphasis of the programme is concerned with ensuring that course\ndevelopers and tutors are pedagogically aware. This paper reviews (i)\nwhat is meant by "on-line learning" in the UHIMI context (ii) the\ntheories of learning and teaching and principles of course design that\ninform the staff development programme and (iii) a review of progress\nto date\n', ['online learning', 'pedagogy', 'computer-aided learning', 'resource-based learning', 'teaching', 'technical support', 'educational course design', 'distance education', 'Internet', 'University of the Highlands and Islands Project', 'staff development', 'distance learning', 'educational computing', 'educational courses', 'Internet', 'teaching']), ('On the distribution of Lachlan nonsplitting bases\nWe say that a computably enumerable (c.e.) degree b is a Lachlan nonsplitting\nbase (LNB), if there is a computably enumerable degree a such that\na>b, and for any c.e. degrees w, v<or=a, if a<or=wVvV b then\neither a<or=wV b or a<or=vV b. In this paper we investigate the\nrelationship between bounding and nonbounding of Lachlan nonsplitting\nbases and the high/low hierarchy. We prove that there is a non-Low/sub\n2/ c.e. degree which bounds no Lachlan nonsplitting base\n', ['Lachlan nonsplitting bases distribution', 'computably enumerable degree', 'Turing degrees', 'formal logic', 'Turing machines']), ("The variance of firm growth rates: the 'scaling' puzzle\nRecent evidence suggests that a power-law relationship exists between a firm's\nsize and the variance of its growth rate. The flatness of the relation\nis regarded as puzzling, in that it suggests that large firms are not\nmuch more stable than small firms. It has been suggested that the\npowerlaw nature of the relationship reflects the presence of some form\nof correlation of growth rates across the firm's constituent\nbusinesses. Here, it is shown that a model of independent businesses\nwhich allows for the fact that these businesses vary in size, as\nmodelled by a simple 'partitions of integers' model, provides a good\nrepresentation of what is observed empirically\n", ['firm growth rates', 'scaling puzzle', 'power-law', 'flatness', 'correlation', 'constituent businesses', 'partitions of integers model', 'size distribution', 'corporate growth', 'corporate modelling', 'economics', 'fluctuations', 'nonlinear dynamical systems', 'probability', 'statistical mechanics']), ('Online longitudinal survey research: viability and participation\nThis article explores the viability of conducting longitudinal survey research\nusing the Internet in samples exposed to trauma. A questionnaire\nbattery assessing psychological adjustment following adverse life\nexperiences was posted online. Participants who signed up to take part\nin the longitudinal aspect of the study were contacted 3 and 6 months\nafter initial participation to complete the second and third waves of\nthe research. Issues of data screening and sample attrition rates are\nconsidered and the demographic profiles and questionnaire scores of\nthose who did and did not take part in the study during successive time\npoints are compared. The results demonstrate that it is possible to\nconduct repeated measures survey research online and that the\nsimilarity in characteristics between those who do and do not take part\nduring successive time points mirrors that found in traditional\npencil-and-paper trauma surveys\n', ['online longitudinal survey research', 'Internet', 'trauma', 'questionnaire', 'psychological adjustment', 'data screening', 'sample attrition rates', 'demographic profiles', 'World Wide Web', 'psychology research', 'demography', 'information resources', 'Internet', 'psychology']), ('AGC for autonomous power system using combined intelligent techniques\nIn the present work two intelligent load frequency controllers have been\ndeveloped to regulate the power output and system frequency by\ncontrolling the speed of the generator with the help of fuel rack\nposition control. The first controller is obtained using fuzzy logic\n(FL) only, whereas the second one by using a combination of FL, genetic\nalgorithms and neural networks. The aim of the proposed controller(s)\nis to restore in a very smooth way the frequency to its nominal value\nin the shortest time possible whenever there is any change in the load\ndemand etc. The action of these controller(s) provides a satisfactory\nbalance between frequency overshoot and transient oscillations with\nzero steady-state error. The design and performance evaluation of the\nproposed controller(s) structure are illustrated with the help of case\nstudies applied (without loss of generality) to a typical single-area\npower system. It is found that the proposed controllers exhibit\nsatisfactory overall dynamic performance and overcome the possible\ndrawbacks associated with other competing techniques\n', ['autonomous power system', 'combined intelligent techniques', 'power output regulation', 'generator speed control', 'fuel rack position control', 'fuzzy logic', 'genetic algorithms', 'neural networks', 'load demand', 'frequency overshoot', 'transient oscillations', 'zero steady-state error', 'performance evaluation', 'single-area power system', 'overall dynamic performance', 'competing techniques', 'frequency control', 'controller design', 'frequency control', 'fuzzy control', 'genetic algorithms', 'load regulation', 'neurocontrollers', 'oscillations', 'power generation control']), ('Wavelet-based image segment representation\nAn efficient representation method for arbitrarily shaped image segments is\nproposed. This method includes a smart way to select a wavelet basis to\napproximate the given image segment, with improved image quality and\nreduced computational load\n', ['image segment representation', 'arbitrarily shaped image segments', 'wavelet basis', 'improved image quality', 'reduced computational load', 'discrete wavelet transform', 'DWT', 'discrete wavelet transforms', 'image representation', 'image segmentation']), ('An optimization approach to plan for reusable software components\nIt is well acknowledged in software engineering that there is a great potential\nfor accomplishing significant productivity improvements through the\nimplementation of a successful software reuse program. On the other\nhand, such gains are attainable only by instituting detailed action\nplans at both the organizational and program level. Given this need,\nthe paucity of research papers related to planning, and in particular,\noptimized planning is surprising. This research, which is aimed at this\ngap, brings out an application of optimization for the planning of\nreusable software components (SCs). We present a model that selects a\nset of SCs that must be built, in order to lower development and\nadaptation costs. We also provide implications to project management\nbased on simulation, an approach that has been adopted by other cost\nmodels in the software engineering literature. Such a prescriptive\nmodel does not exist in the literature\n', ['software engineering', 'productivity improvements', 'software reuse program', 'optimization', 'action plans', 'optimized planning', 'reusable software components', 'adaptation costs', 'development costs', 'project management', 'simulation', 'object-oriented programming', 'optimisation', 'planning', 'project management', 'software development management', 'software reusability']), ('Bistability of harmonically forced relaxation oscillations\nRelaxation oscillations appear in processes which involve transitions between\ntwo states characterized by fast and slow time scales. When a\nrelaxation oscillator is coupled to an external periodic force its\nentrainment by the force results in a response which can include\nmultiple periodicities and bistability. The prototype of these\nbehaviors is the harmonically driven van der Pol equation which\ndisplays regions in the parameter space of the driving force amplitude\nwhere stable orbits of periods 2n+or-1 coexist, flanked by regions of\nperiods 2n+1 and 2n-1. The parameter regions of such bistable orbits\nare derived analytically for the closely related harmonically driven\nStoker-Haag piecewise discontinuous equation. The results are valid\nover most of the control parameter space of the system. Also considered\nare the reasons for the more complicated dynamics featuring regions of\nhigh multiple periodicity which appear like noise between ordered\nperiodic regions. Since this system mimics in detail the less\nanalytically tractable forced van der Pol equation, the results suggest\nextensions to situations where forced relaxation oscillations are a\ncomponent of the operating mechanisms\n', ['bistability', 'harmonically forced relaxation oscillations', 'external periodic force', 'entrainment', 'van der Pol equation', 'harmonically driven Stoker-Haag piecewise discontinuous equation', 'control parameter space', 'nonlinear dynamics', 'bifurcation', 'nonlinear control systems', 'nonlinear dynamical systems', 'oscillations', 'piecewise constant techniques', 'relaxation', 'relaxation oscillators']), ('Building an effective computer science student organization: the Carnegie\nMellon Women@SCS action plan\nThis paper aims to provide a practical guide for building a student\norganization and designing activities and events that can encourage and\nsupport a community of women in computer science. This guide is based\non our experience in building Women@SCS, a community of women in the\nSchool of Computer Science (SCS) at Carnegie Mellon University. Rather\nthan provide an abstract "to-do" or "must-do" list, we present a\nsampling of concrete activities and events in the hope that these might\nsuggest possibilities for a likeminded student organization. However,\nsince we have found it essential to have a core group of activist\nstudents at the helm, we provide a "to-do" list of features that we\nfeel are essential for forming, supporting and sustaining creative and\neffective student leadership\n', ['computer science student organization', 'Women@SCS action plan', 'gender issues', 'women', 'computer science education', 'Carnegie Mellon University', 'student leadership', 'computer science education', 'gender issues', 'social aspects of automation']), ('A new subspace identification approach based on principal component analysis\nPrincipal component analysis (PCA) has been widely used for monitoring complex\nindustrial processes with multiple variables and diagnosing process and\nsensor faults. The objective of this paper is to develop a new subspace\nidentification algorithm that gives consistent model estimates under\nthe errors-in-variables (EIV) situation. In this paper, we propose a\nnew subspace identification approach using principal component\nanalysis. PCA naturally falls into the category of EIV formulation,\nwhich resembles total least squares and allows for errors in both\nprocess input and output. We propose to use PCA to determine the system\nobservability subspace, the matrices and the system order for an EIV\nformulation. Standard PCA is modified with instrumental variables in\norder to achieve consistent estimates of the system matrices. The\nproposed subspace identification method is demonstrated using a\nsimulated process and a real industrial process for model\nidentification and order determination. For comparison the MOESP\nalgorithm and N4SID algorithm are used as benchmarks to demonstrate the\nadvantages of the proposed PCA based subspace model identification\n(SMI) algorithm\n', ['subspace identification approach', 'principal component analysis', 'PCA', 'complex industrial process monitoring', 'process fault diagnosis', 'sensor fault diagnosis', 'errors-in-variables situation', 'EIV situation', 'total least-squares approximation', 'system observability subspace', 'consistent system matrix estimates', 'MOESP algorithm', 'N4SID algorithm', 'subspace model identification', 'SMI', 'fault diagnosis', 'identification', 'least squares approximations', 'principal component analysis', 'process monitoring']), ('Discrete output feedback sliding mode control of second order systems - a\nmoving switching line approach\nThe sliding mode control systems (SMCS) for which the switching variable is\ndesigned independent of the initial conditions are known to be\nsensitive to parameter variations and extraneous disturbances during\nthe reaching phase. For second order systems this drawback is\neliminated by using the moving switching line technique where the\nswitching line is initially designed to pass the initial conditions and\nis subsequently moved towards a predetermined switching line. In this\npaper, we make use of the above idea of moving switching line together\nwith the reaching law approach to design a discrete output feedback\nsliding mode control. The main contributions of this work are such that\nwe do not require to use system states as it makes use of only the\noutput samples for designing the controller. and by using the moving\nswitching line a low sensitivity system is obtained through shortening\nthe reaching phase. Simulation results show that the fast output\nsampling feedback guarantees sliding motion similar to that obtained\nusing state feedback\n', ['sliding mode control', 'switching variable', 'parameter variations', 'moving switching line', 'discrete output feedback', 'fast output sampling feedback', 'state feedback', 'state feedback', 'variable structure systems']), ('Design and prototype of a performance tool interface for OpenMP\nThis paper proposes a performance tools interface for OpenMP, similar in spirit\nto the MPI profiling interface in its intent to define a clear and\nportable API that makes OpenMP execution events visible to runtime\nperformance tools. We present our design using a source-level\ninstrumentation approach based on OpenMP directive rewriting. Rules to\ninstrument each directive and their combination are applied to generate\ncalls to the interface consistent with directive semantics and to pass\ncontext information (e.g., source code locations) in a portable and\nefficient way. Our proposed OpenMP performance API further allows user\nfunctions and arbitrary code regions to be marked and performance\nmeasurement to be controlled using new OpenMP directives. To prototype\nthe proposed OpenMP performance interface, we have developed compatible\nperformance libraries for the EXPERT automatic event trace analyzer\n[17, 18] and the TAU performance analysis framework [13]. The directive\ninstrumentation transformations we define are implemented in a\nsource-to-source translation tool called OPARI. Application examples\nare presented for both EXPERT and TAU to show the OpenMP performance\ninterface and OPARI instrumentation tool in operation. When used\ntogether with the MPI profiling interface (as the examples also\ndemonstrate), our proposed approach provides a portable and robust\nsolution to performance analysis of OpenMP and mixed-mode (OpenMP +\nMPI) applications\n', ['performance tool interface', 'MPI profiling interface', 'API', 'source-level instrumentation approach', 'OpenMP directive rewriting', 'directive semantics', 'arbitrary code regions', 'performance libraries', 'EXPERT automatic event trace analyzer', 'TAU performance analysis framework', 'source-to-source translation tool', 'OPARI', 'parallel programming', 'application program interfaces', 'message passing', 'parallel programming', 'program compilers', 'software performance evaluation']), ('All-optical logic NOR gate using two-cascaded semiconductor optical amplifiers\nThe authors present a novel all-optical logic NOR gate using two-cascaded\nsemiconductor optical. amplifiers (SOAs) in a counterpropagating\nfeedback configuration. This configuration accentuates the gain\nnonlinearity due to the mutual gain modulation of the two SOAs. The\nall-optical NOR gate feasibility has been demonstrated delivering an\nextinction ratio higher than 12 dB over a wide range of wavelength\n', ['all-optical logic NOR gate', 'two-cascaded semiconductor optical amplifiers', 'SOA', 'counterpropagating feedback configuration', 'gain nonlinearity', 'mutual gain modulation', 'extinction ratio', 'wide wavelength range', 'laser feedback', 'optical logic', 'optical modulation', 'semiconductor optical amplifiers']), ('Cooperative three- and four-player quantum games\nA cooperative multi-player quantum game played by 3 and 4 players has been\nstudied. A quantum superposed operator is introduced in this work which\nsolves the non-zero sum difficulty in previous treatments. The role of\nquantum entanglement of the initial state is discussed in detail\n', ['cooperative three-player quantum games', 'quantum entanglement', 'initial state', 'cooperative four-player quantum games', 'quantum superposed operator', 'nonzero sum difficulty', 'bound states', 'cooperative systems', 'game theory', 'probability', 'quantum computing', 'quantum theory']), ('Optimization of the memory weighting function in stochastic functional\nself-organized sorting performed by a team of autonomous mobile agents\nThe activity of a team of autonomous mobile agents formed by identical\n"robot-like-ant" individuals capable of performing a random walk\nthrough an environment that are able to recognize and move different\n"objects" is modeled. The emergent desired behavior is a distributed\nsorting and clustering based only on local information and a memory\nregister that records the past objects encountered. An optimum\nweighting function for the memory registers is theoretically derived.\nThe optimum time-dependent weighting function allows sorting and\nclustering of the randomly distributed objects in the shortest time. By\nmaximizing the average speed of a texture feature (the contrast) we\ncheck the central assumption, the intermediate steady-states\nhypothesis, of our theoretical result. It is proved that the algorithm\noptimization based on maximum speed variation of the contrast feature\ngives relationships similar to the theoretically derived annealing law\n', ['autonomous mobile agents', 'random walk', 'memory weighting function', 'sorting', 'clustering', 'algorithm optimization', 'distributed algorithms', 'mobile computing', 'robots', 'self-adjusting systems', 'software agents', 'sorting']), ('Recruitment and retention of women graduate students in computer science and\nengineering: results of a workshop organized by the Computing Research\nAssociation\nThis document is the report of a workshop that convened a group of experts to\ndiscuss the recruitment and retention of women in computer science and\nengineering (CSE) graduate programs. Participants included long-time\nmembers of the CSE academic and research communities, social scientists\nengaged in relevant research, and directors of successful retention\nefforts. The report is a compendium of the experience and expertise of\nworkshop participants, rather than the result of a full-scale,\nscholarly study into the range of issues. Its goal is to provide\ndepartments with practical advice on recruitment and retention in the\nform of a set of specific recommendations\n', ['recruitment', 'retention', 'women graduate students', 'computer science', 'engineering', 'Computing Research Association', 'social scientists', 'academic communities', 'research communities', 'directors', 'workshop participants', 'computer science', 'employment', 'gender issues', 'human resource management', 'social aspects of automation']), ('Is diversity in computing a moral matter?\nWe have presented an ethical argument that takes into consideration the\nsubtleties of the issue surrounding under-representation in computing.\nWe should emphasize that there is nothing subtle about overt, unfair\ndiscrimination. Where such injustice occurs, we condemn it. Our concern\nis that discrimination need not be explicit or overt. It need not be\nindividual-to-individual. Rather, it can be subtly built into social\npractices and social institutions. Our analysis raises ethical\nquestions about aspects of computing that drive women away, aspects\nthat can be changed in ways that improve the profession and access to\nthe profession. We hope that computing will move towards these\nimprovements\n', ['ethical argument', 'computing under-representation', 'unfair discrimination', 'social practices', 'social institutions', 'women', 'computer science', 'gender issues', 'professional aspects', 'social aspects of automation']), ('Nuts and bolts: implementing descriptive standards to enable virtual\ncollections\nTo date, online archival information systems have relied heavily on legacy\nfinding aids for data to encode and provide to end users, despite\nfairly strong indications in the archival literature that such legacy\ndata is problematic even as a mediated access tool. Archivists have\nonly just begun to study the utility of archival descriptive data for\nend users in unmediated settings such as via the Web. The ability of\nfuture archival information systems to respond to the expectations and\nneeds of end users is inextricably linked to archivists getting their\ncollective data house in order. The General International Standard\nArchival Description (ISAD(G)) offers the profession a place from which\nto start extricating ourselves from the idiosyncracies of our legacy\ndata and description practices\n', ['descriptive standards', 'virtual collections', 'online archival information systems', 'end users', 'archival literature', 'legacy data', 'mediated access tool', 'archivists', 'archival descriptive data', 'archival information systems', 'collective data house', 'General International Standard Archival Description', 'ISAD', 'Online Archive of California', 'OAC', 'information resources', 'information retrieval systems', 'professional aspects', 'records management', 'standards']), ('Using molecular equivalence numbers to visually explore structural features\nthat distinguish chemical libraries\nA molecular equivalence number (meqnum) classifies a molecule with respect to a\nclass of structural features or topological shapes such as its cyclic\nsystem or its set of functional groups. Meqnums can be used to organize\nmolecular structures into nonoverlapping, yet highly relatable classes.\nWe illustrate the construction of some different types of meqnums and\npresent via examples some methods of comparing diverse chemical\nlibraries based on meqnums. In the examples we compare a library which\nis a random sample from the MDL Drug Data Report (MDDR) with a library\nwhich is a random sample from the Available Chemical Directory (ACD).\nIn our analyses, we discover some interesting features of the\ntopological shape of a molecule and its set of functional groups that\nare strongly linked with compounds occurring in the MDDR but not in the\nACD. We also illustrate the utility of molecular equivalence indices in\ndelineating the structural domain over which an SAR conclusion is valid\n', ['molecular equivalence number', 'molecule classification', 'structural features', 'topological shapes', 'cyclic system', 'functional groups', 'nonoverlapping relatable classes', 'chemical libraries', 'MDL Drug Data Report', 'Available Chemical Directory', 'molecular equivalence indices', 'biology computing', 'chemical structure', 'chemistry computing', 'equivalence classes', 'medical computing', 'medical information systems', 'molecular configurations', 'organic compounds', 'pharmaceutical industry', 'scientific information systems', 'special libraries']), ("Market watch - air conditioning\nAfter a boom period in the late nineties, the air conditioning market finds\nitself in something of a lull at present, but manufacturers aren't\npanicking\n", ['air conditioning', 'market', 'air conditioning']), ("A modified Fieller interval for the interval estimation of effective doses for\na logistic dose-response curve\nInterval estimation of the gamma % effective dose ( mu /sub gamma / say) is\noften based on the asymptotic variance of the maximum likelihood\nestimator (delta interval) or Fieller's theorem (Fieller interval).\nSitter and Wu (1993) compared the delta and Fieller intervals for the\nmedian effective dose ( mu /sub 50/) assuming a logistic dose-response\ncurve. Their results indicated that although Fieller intervals are\ngenerally superior to delta intervals, they appear to be conservative.\nHere an adjusted form of the Fieller interval for mu /sub gamma /\ntermed an adjusted Fieller (AF) interval is introduced. A comparison of\nthe AF interval with the delta and Fieller intervals is provided and\nthe properties of these three interval estimation methods are\ninvestigated\n", ['modified Fieller interval', 'interval estimation', 'effective doses', 'logistic dose-response curve', 'asymptotic variance', 'maximum likelihood estimator', 'delta interval', "Fieller's theorem", 'median effective dose', 'biology computing', 'data analysis', 'maximum likelihood estimation', 'medical computing', 'statistical analysis']), ("HEW selects network management software\nFor more than 100 years, Hamburgische Electricitats-Werke AG (HEW) has provided\na reliable electricity service to the city of Hamburg, Germany. Today,\nthe company supplies electricity to some 1.7 million inhabitants via\n285000 connections. During 1999, the year the energy market was started\nin Germany, HEW needed to operate and maintain a safe and reliable\nnetwork cheaply. The development and implementation of a distribution\nmanagement system (DMS) is key to the success of HEW. HEW's strategy\nwas to obtain efficient new software for network management that also\noffered a good platform for future applications. Following a pilot and\nprequalification phase, HEW invited several companies to process the\nrequirements catalog and to submit a detailed tender. The network\ninformation management system, Xpower, developed by Tekla Oyj,\nsuccessfully passed HEW's test program and satisfied all the\nperformance and system capacity requirements. The system met all HEW's\nconditions by presenting the reality of a network with the attributes\nof the operating resources. Xpower platform provides the ability to\nintegrate future applications\n", ['Hamburgische Electricitats-Werke', 'Hamburg', 'Germany', 'distribution management system', 'network management software', 'Xpower', 'Tekla Oyj', 'management', 'power distribution control', 'software packages']), ("Advancements during the past quarter century in on-line monitoring of motor and\ngenerator winding insulation\nElectrical insulation plays a critical role in the operation of motor and\ngenerator rotor and stator windings. Premature failure of the\ninsulation can cost millions of dollars per day. With advancements in\nelectronics, sensors, computers and software, tremendous progress has\nbeen made in the past 25 yr which has transformed on-line insulation\nmonitoring from a rarely used and expensive tool, to the point where\n50% of large utility generators in North America are now equipped for\nsuch monitoring. This review paper outlines the motivation for online\nmonitoring, discusses the transition to today's technology, and\ndescribes the variety of methods now in use for rotor winding and\nstator winding monitoring\n", ['generator winding insulation', 'motor generator winding insulation', 'winding insulation on-line monitoring', 'premature insulation failure', 'electrical insulation', 'electronics', 'sensors', 'computers', 'software', 'rotor windings', 'stator windings', 'temperature monitoring', 'condition monitors', 'tagging compounds', 'ozone monitoring', 'PD monitoring', 'magnetic flux monitoring', 'partial discharge monitoring', 'endwinding vibration monitoring', 'chemical variables measurement', 'computerised monitoring', 'condition monitoring', 'electric generators', 'electric motors', 'insulation testing', 'machine insulation', 'magnetic flux', 'magnetic variables measurement', 'partial discharge measurement', 'rotors', 'stators', 'temperature measurement', 'vibration measurement']), ("Towards an ontology of approximate reason\nThis article introduces structural aspects in an ontology of approximate\nreason. The basic assumption in this ontology is that approximate\nreason is a capability of an agent. Agents are designed to classify\ninformation granules derived from sensors that respond to stimuli in\nthe environment of an agent or received from other agents.\nClassification of information granules is carried out in the context of\nparameterized approximation spaces and a calculus of granules. Judgment\nin agents is a faculty of thinking about (classifying) the particular\nrelative to decision rules derived from data. Judgment in agents is\nreflective, but not in the classical philosophical sense (e.g., the\nnotion of judgment in Kant). In an agent, a reflective judgment itself\nis an assertion that a particular decision rule derived from data is\napplicable to an object (input). That is, a reflective judgment by an\nagent is an assertion that a particular vector of attribute (sensor)\nvalues matches to some degree the conditions for a particular rule. In\neffect, this form of judgment is an assertion that a vector of sensor\nvalues reflects a known property of data expressed by a decision rule.\nSince the reasoning underlying a reflective judgment is inductive and\nsurjective (not based on a priori conditions or universals), this form\nof judgment is reflective, but not in the sense of Kant. Unlike Kant, a\nreflective judgment is surjective in the sense that it maps\nexperimental attribute values onto the most closely matching\ndescriptors (conditions) in a derived rule. Again, unlike Kant's notion\nof judgment, a reflective judgment is not the result of searching for a\nuniversal that pertains to a particular set of values of descriptors.\nRather, a reflective judgment by an agent is a form of recognition that\na particular vector of sensor values pertains to a particular rule in\nsome degree. This recognition takes the form of an assertion that a\nparticular descriptor vector is associated with a particular decision\nrule. These considerations can be repeated for other forms of\nclassifiers besides those defined by decision rules\n", ['ontology', 'approximate reason', 'information granules', 'parameterized approximation spaces', 'granules', 'decision rules', 'reflective judgment', 'pattern recognition', 'rough sets', 'inference mechanisms', 'neural nets', 'software agents', 'uncertainty handling']), ('Linear tense logics of increasing sets\nWe provide an extension of the language of linear tense logic with future and\npast connectives F and P, respectively, by a modality that quantifies\nover the points of some set which is assumed to increase in the course\nof time. In this way we obtain a general framework for modelling growth\nqualitatively. We develop an appropriate logical system, prove a\ncorresponding completeness and decidability result and discuss the\nvarious kinds of flow of time in the new context. We also consider\ndecreasing sets briefly\n', ['linear tense logic', 'future and past connectives', 'logical system', 'completeness', 'decidability', 'decreasing sets', 'temporal reasoning', 'decidability', 'equivalence classes', 'temporal logic']), ('Evaluation of existing and new feature recognition algorithms. 1. Theory and\nimplementation\nThis is the first of two papers evaluating the performance of general-purpose\nfeature detection techniques for geometric models. In this paper, six\ndifferent methods are described to identify sets of faces that bound\ndepression and protrusion faces. Each algorithm has been implemented\nand tested on eight components from the National Design Repository. The\nalgorithms studied include previously published general-purpose feature\ndetection algorithms such as the single-face inner-loop and concavity\ntechniques. Others are improvements to existing algorithms such as\nextensions of the two-dimensional convex hull method to handle curved\nfaces as well as protrusions. Lastly, new algorithms based on the\nthree-dimensional convex hull, minimum concave, visible and\nmultiple-face inner-loop face sets are described\n', ['feature recognition algorithms', 'geometric models', 'general-purpose feature detection techniques', 'sets of faces', 'depression faces', 'protrusion faces', 'National Design Repository', 'single-face inner-loop technique', 'concavity technique', 'two-dimensional convex hull method', 'curved faces', 'three-dimensional convex hull', 'CAD/CAM software', 'geometric reasoning algorithms', 'minimum concave', 'visible inner-loop face sets', 'multiple-face inner-loop face sets', 'CAD/CAM', 'computational geometry', 'feature extraction', 'spatial reasoning']), ('Sensing and control of double-sided arc welding process\nThe welding industry is driven to improve productivity without sacrificing\nquality. For thick material welding, the current practice is to use\nbacking or multiple passes. The laser welding process, capable of\nachieving deep narrow penetration, can significantly improve welding\nproductivity for such applications by reducing the number of passes.\nHowever, its competitiveness in comparison with traditional arc welding\nis weakened by its high cost, strict fit-up requirement, and difficulty\nin welding large structures. In this work, a different method, referred\nto as double-sided arc welding (DSAW) is developed to improve the arc\nconcentration for arc welding. A sensing and control system is\ndeveloped to achieve deep narrow penetration under variations in\nwelding conditions. Experiments verified that the pulsed keyhole DSAW\nsystem developed is capable of achieving deep narrow penetration on a\n1/2 inch thick square butt joint in a single pass\n', ['double-sided arc welding', 'laser welding process', 'control system', 'process control', 'thick material welding', 'energy density', 'controlled pulse keyhole', 'arc welding', 'control systems', 'process control']), ("Duality revisited: construction of fractional frequency distributions based on\ntwo dual Lotka laws\nFractional frequency distributions of, for example, authors with a certain\n(fractional) number of papers are very irregular, and therefore not\neasy to model or to explain. The article gives a first attempt to this\nby as suming two simple Lotka laws (with exponent 2): one for the\nnumber of authors with n papers (total count here) and one for the\nnumber of papers with n authors, n in N. Based on an earlier made\nconvolution model of Egghe, interpreted and reworked now for discrete\nscores, we are able to produce theoretical fractional frequency\ndistributions with only one parameter, which are in very close\nagreement with the practical ones as found in a large dataset produced\nearlier by Rao (1995). The article also shows that (irregular)\nfractional frequency distributions are a consequence of Lotka's law,\nand are not examples of breakdowns of this famous historical law\n", ['dual Lotka laws', 'convolution model', 'discrete scores', 'irregular fractional frequency distributions', 'citation analysis']), ('New water management system begins operation at US projects\nThe US Army Corps of Engineers has developed a new automated information system\nto support its water control management mission. The new system\nprovides a variety of decision support tools, enabling water control\nmanagers to acquire, transform, verify, store, display, analyse, and\ndisseminate data and information efficiently and around the clock\n', ['watershed modelling', 'water management system', 'US projects', 'US Army Corps of Engineers', 'automated information system', 'water control management mission', 'decision support tools', 'water control managers', 'data dissemination', 'data acquisition', 'data storage', 'data verification', 'data display', 'data analysis', 'data visualization', 'decision support system', 'Corps Water Management System', 'data acquisition', 'data visualisation', 'decision support systems', 'engineering information systems', 'geophysics computing', 'management', 'natural resources']), ('Layer-based machining: recent development and support structure design\nThere is growing interest in additive and subtractive shaping theories that are\nsynthesized to integrate the layered manufacturing process and material\nremoval process. Layer-based machining has emerged as a promising\nmethod for integrated additive and subtractive shaping theory. In the\npaper, major layer-based machining systems are reviewed and compared\naccording to characteristics of stock layers, numerical control\nmachining configurations, stacking operations, input format and raw\nmaterials. Support structure, a major issue in machining-based systems\nwhich has seldom been addressed in previous research, is investigated\nin the paper with considerations of four situations: floating overhang,\ncantilever, vaulted overhang and ceiling. Except for the floating\noverhang where a support structure should not be overlooked, the\nnecessity for support structures for the other three situations is\ndetermined by stress and deflection analysis. This is demonstrated by\nthe machining of a large castle model\n', ['layer-based machining', 'support structure design', 'additive shaping theories', 'subtractive shaping theories', 'layered manufacturing process', 'material removal process', 'stock layers', 'numerical control machining configurations', 'stacking operations', 'input format', 'raw materials', 'floating overhang', 'cantilever', 'vaulted overhang', 'ceiling', 'stress', 'deflection analysis', 'machining', 'numerical control', 'rapid prototyping (industrial)']), ('Application of heuristic methods for conformance test selection\nIn this paper we focus on the test selection problem. It is modeled after a\nreal-life problem that arises in telecommunication when one has to\ncheck the reliability of an application. We apply different\nmetaheuristics, namely Reactive Tabu Search (RTS), Genetic Algorithms\n(GA) and Simulated Annealing (SA) to solve the problem. We propose some\nmodifications to the conventional schemes including an adaptive\nneighbourhood sampling in RTS, an adaptive variable mutation rate in GA\nand an adaptive variable neighbourhood structure in SA. The performance\nof the algorithms is evaluated in different models for existing\nprotocols. Computational results show that GA and SA can provide\nhigh-quality solutions in acceptable time compared to the results of a\ncommercial software, which makes them applicable in practical test\nselection\n', ['telecommunication conformance test selection', 'heuristic methods', 'test selection problem', 'reliability', 'metaheuristics', 'reactive Tabu search', 'genetic algorithms', 'simulated annealing', 'adaptive neighbourhood sampling', 'adaptive variable mutation rate', 'adaptive variable neighbourhood structure', 'ISDN protocol', 'GSM protocol', 'adaptive systems', 'cellular radio', 'conformance testing', 'genetic algorithms', 'heuristic programming', 'ISDN', 'protocols', 'search problems', 'simulated annealing', 'telecommunication computing', 'telecommunication equipment testing', 'telecommunication network reliability']), ('The results of experimental studies of the reflooding of fuel-rod assemblies\nfrom above and problems for future investigations\nProblems in studying the reflooding of assemblies from above conducted at\nforeign and Russian experimental installations are considered. The\nefficiency of cooling and flow reversal under countercurrent flow of\nsteam and water, as well as the scale effect are analyzed. The tasks\nfor future experiments that are necessary for the development of modern\ncorrelations for the loss-of-coolant accident (LOCA) computer codes are\nstated\n', ['fuel-rod assemblies reflooding', 'Russian experimental installations', 'cooling efficiency', 'flow reversal', 'countercurrent flow', 'steam', 'water', 'loss-of-coolant accident computer codes', 'LOCA computer codes', 'fission reactor cooling', 'fission reactor fuel', 'fuel element failure', 'nuclear engineering computing']), ('Five-axis NC milling of ruled surfaces: optimal geometry of a conical tool\nThe side milling of ruled surfaces using a conical milling cutter was studied.\nThis is a field that has largely been ignored by research scientists,\nbut it is much used in industry, especially to machine turbine blades.\nWe first suggest an improved positioning with respect to the\ndirectrices of the ruled surface. As compared with the methods already\ndeveloped for the cylindrical cutter, this positioning enables the\nerror between the cutter and the work-piece to be reduced. An algorithm\nis then introduced to calculate error so one can determine the cutter\ndimensions (cone radius and angle) in order to respect the tolerance\ninterval imposed by the design office. This study provides an\nopportunity to determine cutters with greater dimensions, thus\nalleviating bending problems during milling\n', ['five-axis NC milling', 'ruled surfaces', 'optimal geometry', 'conical', 'side milling', 'conical milling cutter', 'positioning', 'cutter dimensions', 'tolerance interval', 'CAD/CAM', 'machine tools', 'machining', 'numerical control', 'position control']), ("Explicit solutions for transcendental equations\nA simple method to formulate an explicit expression for the roots of any\nanalytic transcendental function is presented. The method is based on\nCauchy's integral theorem and uses only basic concepts of complex\nintegration. A convenient method for numerically evaluating the exact\nexpression is presented. The application of both the formulation and\nevaluation of the exact expression is illustrated for several classical\nroot finding problems\n", ['analytic functions', 'transcendental equations', 'Cauchy integral theorem', 'complex integration', 'root finding', 'singularity', 'polynomial', 'Fourier transform', 'approximation theory', 'Fourier transforms', 'integral equations', 'mathematics computing', 'polynomials']), ('The curious ways of professional cultures and the "two-body opportunity"\nWhen two professionals are a couple, we sometimes refer to them as having a\n"two-body problem." However, when each partner of a couple exists in\nthe same cultures, they also have an opportunity for deeply shared\nunderstanding and empathy, simply because each understands at a deep\nlevel the culture in which the other works. I explore this notion. A\ncouple has what we call the "two-body problem" when both are\nprofessionals who are qualified for a kind of position that is\nrelatively rare and who are very selective about the positions that\nthey accept. For example, there are relatively scant numbers of jobs as\na computer science professor-at any level. An individual considering an\nacademic job may only be interested in research universities, or in\nteaching universities, restricting the choice of open positions\nsubstantially. The classic two-body "problem" arises when one partner\nwants to accept a new position that requires geographical relocation.\nThen, the other partner also needs to find a new position. Moreover, it\ncan be very difficult to find a suitable position when they are\nnaturally scarce\n', ['professional cultures', 'computer science', 'two-body opportunity', 'academics', 'professional aspects']), ('Adaptive stabilization of undamped flexible structures\nIn the paper non-identifier-based adaptive stabilization of undamped flexible\nstructures is considered in the case of collocated input and output\noperators. The systems have poles and zeros on the imaginary axis. In\nthe case where velocity feedback is available, the adaptive stabilizer\nis constructed by an adaptive PD-controller (proportional plus\nderivative controller). In the case where only position feedback is\navailable, the adaptive stabilizer is constructed by an adaptive\nP-controller for the augmented system which consists of the controlled\nsystem and a parallel compensator. Numerical examples are given to\nillustrate the effectiveness of the proposed controllers\n', ['adaptive stabilization', 'undamped flexible structures', 'poles and zeros', 'imaginary axis', 'velocity feedback', 'adaptive PD-controller', 'proportional plus derivative controller', 'position feedback', 'adaptive P-controller', 'augmented system', 'parallel compensator', 'adaptive control', 'closed loop systems', 'compensation', 'flexible structures', 'poles and zeros', 'position control', 'proportional control', 'stability', 'two-term control', 'velocity control']), ('Incorporating multi-leaf collimator leaf sequencing into iterative IMRT\noptimization\nIntensity modulated radiation therapy (IMRT) treatment planning typically\nconsiders beam optimization and beam delivery as separate tasks.\nFollowing optimization, a multi-leaf collimator (MLC) or other beam\ndelivery device is used to generate fluence patterns for patient\ntreatment delivery. Due to limitations and characteristics of the MLC,\nthe deliverable intensity distributions often differ from those\nproduced by the optimizer, leading to differences between the delivered\nand the optimized doses. Objective function parameters are then\nadjusted empirically, and the plan is reoptimized to achieve a desired\ndeliverable dose distribution. The resulting plan, though usually\nacceptable, may not be the best achievable. A method has been developed\nto incorporate the MLC restrictions into the optimization process. Our\nin-house IMRT system has been modified to include the calculation of\nthe deliverable intensity into the optimizer. In this process, prior to\ndose calculation, the MLC leaf sequencer is used to convert intensities\nto dynamic MLC sequences, from which the deliverable intensities are\nthen determined. All other optimization steps remain the same. To\nevaluate the effectiveness of deliverable-based optimization, 17\npatient cases have been studied. Compared with standard optimization\nplus conversion to deliverable beams, deliverable-based optimization\nresults show improved isodose coverage and a reduced dose to critical\nstructures. Deliverable-based optimization results are close to the\noriginal nondeliverable optimization results, suggesting that IMRT can\novercome the MLC limitations by adjusting individual beamlets. The use\nof deliverable-based optimization may reduce the need for empirical\nadjustment of objective function parameters and reoptimization of a\nplan to achieve desired results\n', ['intensity modulated radiation therapy', 'treatment planning', 'iterative optimization', 'multileaf collimator leaf sequencing', 'beam delivery', 'beam optimization', 'fluence patterns', 'objective function parameters', 'deliverable dose distribution', 'empirical adjustment', 'tumor dose', 'optimized intensity', 'gradient-based search algorithm', 'beamlet ray intensities', 'Newton method', 'dose-volume objective values', 'dosimetry', 'gradient methods', 'medical computing', 'Newton method', 'optimisation', 'radiation therapy']), ('Central hub for design assets: Adobe GoLive 6.0\nAdobe GoLive is a strong contender for Web authoring and publishing. Version\n6.0 features a flexible GUI environment combined with a comprehensive\nworkgroup and collaboration server, plus tight integration with leading\ndesign tools\n', ['Adobe GoLive 6.0', 'Flash', 'Real', 'Java', 'application servers', 'Web authoring', 'GUI', 'workgroup server', 'collaboration server', 'LiveMotion 2.0', 'animation and scripting tool', 'Macromedia SWF format', 'workgroup environment', 'Web publishing environment', 'design-centric dynamic content', 'authoring systems', 'graphical user interfaces', 'groupware', 'information resources', 'Internet', 'software reviews']), ('Support communities for women in computing\nThis article highlights the many activities provided by the support communities\navailable for women in computing. Thousands of women actively\nparticipate in these programs and they receive many benefits including\nnetworking and professional support. In addition, the organizations and\nassociations help promote the accomplishments of women computer\nscientists and disseminate valuable information. This article surveys\nsome of these organizations and concludes with a list of suggestions\nfor how faculty members can incorporate the benefits of these\norganizations in their own institutions\n', ['support communities', 'women', 'computing', 'networking', 'professional support', 'information dissemination', 'faculty members', 'computer science education', 'gender issues', 'professional communication']), ('Designing a new urban Internet\nThe parallel between designing a Web site and the construction of a building is\na familiar one, but how often do we think of the Internet as having\nparks and streets? It would be absurd to say that the Internet could\never take the place of real, livable communities; however, it is safe\nto say that the context for using the Internet is on a path of change.\nAs the Internet evolves beyond a simple linkage of disparate Web sites\nand applications, the challenge for Information Architects is\nestablishing a process by which to structure, organize, and design\nnetworked environments. The principles that guide New Urbanism can\noffer much insight into networked electronic environment design. At the\ncore of every New Urbanism principle is the idea of "wholeness"-of\nmaking sure that neighborhoods and communities are knit together in a\nway that supports civic activities, economic development, efficient\necosystems, aesthetic beauty, and human interaction\n', ['Web site', 'Internet', 'information architects', 'private-public sector cooperation', 'global information networks', 'networked environments', 'networked electronic environment design', 'communities', 'electronic publishing', 'hypermedia', 'information resources', 'Internet']), ('Fresh tracks [food processing]\nBar code labels and wireless terminals linked to a centralized database\naccurately track meat products from receiving to customers for Farmland\nFoods\n', ['food processing', 'bar code labels', 'wireless terminals', 'Farmland Foods', 'automatic data capture', 'Intermec Technologies', 'bar codes', 'data acquisition', 'food processing industry', 'mobile computing', 'production control']), ('An identity-based society oriented signature scheme with anonymous signers\nIn this paper, we propose a new society oriented scheme, based on the\nGuillou-Quisquater (1989) signature scheme. The scheme is\nidentity-based and the signatures are verified with respect to only one\nidentity. That is, the verifier does not have to know the identity of\nthe co-signers, but just that of the organization they represent\n', ['identity-based society oriented signature scheme', 'anonymous signers', 'signature verification', 'cryptography']), ("Ten years of strategies to increase participation of women in computing\nprograms. The Central Queensland University experience: 1999-2001\nIn the late eighties, the participation rate of women in information technology\ncourses in most Australian Universities was around 25%. This low level\nof women's participation in computing courses occurs not only in\nAustralia but also overseas. More studies indicate that the\nparticipation rates have not improved and in fact may be even further\nin decline. Participation rates in the workforce also appear to be in\ndecline. Concerned at the imbalance within Australia, the Federal\ngovernment directed all Australian Universities to increase the number\nof women in courses leading to a professional computing qualification\n(i.e., information technology courses) to 40% of students by 1995. This\npaper details one Australian university's approach, over a 10 year\nperiod (1991-2001), to redress this imbalance. We provide examples of\nintervention strategies developed and the outcomes for these\nstrategies. We present the outcomes against a background frame of the\nAustralian Higher Education scene of that decade which was influenced\nby funding levels to universities in general and to equity programs in\nparticular. We present data related to the participation of women in\ncomputing programs along with snapshots of the overall changing student\ndemographics over this period\n", ['Central Queensland University', 'women', 'computing programs', 'demographics', 'Australian Higher Education', 'computer science education', 'demography', 'gender issues', 'management of change']), ('An ACL for a dynamic system of agents\nIn this article we present the design of an ACL for a dynamic system of agents.\nThe ACL includes a set of conversation performatives extended with\noperations to register, create, and terminate agents. The main design\ngoal at the agent-level is to provide only knowledge-level primitives\nthat are well integrated with the dynamic nature of the system. This\ngoal has been achieved by defining an anonymous interaction protocol\nwhich enables agents to request and supply knowledge without\nconsidering symbol-level issues concerning management of agent names,\nrouting, and agent reachability. This anonymous interaction protocol\nexploits a distributed facilitator schema which is hidden at the\nagent-level and provides mechanisms for registering capabilities of\nagents and delivering requests according to the competence of agents.\nWe present a formal specification of the ACL and of the underlying\narchitecture, exploiting an algebra of actors, and illustrate it with\nthe help of a graphical notation. This approach provides the basis for\ndiscussing dynamic primitives in ACL and for studying properties of\ndynamic multi agent systems, for example concerning the behavior of\nagents and the correctness of their conversation policies\n', ['ACL', 'dynamic system of agents', 'system of agents', 'agents', 'Agent Communication Languages', 'dynamic system', 'distributed facilitator', 'actors', 'anonymous interaction protocol', 'high level languages', 'multi-agent systems', 'software agents']), ("Fully automatic algorithm for region of interest location in camera calibration\nWe present an automatic method for region of interest (ROI) location in camera\ncalibration used in computer vision inspection. An intelligent ROI\nlocation algorithm based on the Radon transform is developed to\nautomate the calibration process. The algorithm remains robust even if\nthe anchor target has a notable rotation angle in the target plane.\nThis method functions well although the anchor target is not carefully\npositioned. Several improvement methods are studied to avoid the\nalgorithm's huge time/space consumption problem. The algorithm runs\nabout 100 times faster if these improvement methods are applied. Using\nthis method fully automatic camera calibration is achieved without\nhuman interactive ROI specification. Experiments show that this\nalgorithm can help to calibrate the intrinsic parameters of the zoom\nlens and the camera parameters quickly and automatically\n", ['Fully automatic algorithm', 'interest location', 'camera calibration', 'region of interest location', 'computer vision inspection', 'ROI location algorithm', 'Radon transform', 'calibration process', 'rotation angle', 'time/space consumption problem', 'fully automatic camera calibration', 'human interactive specification', 'intrinsic parameters', 'zoom lens', 'camera parameters', 'automatic optical inspection', 'calibration', 'cameras', 'computer vision', 'image sensors', 'lenses', 'Radon transforms']), ('Integrating building management system and facilities management on the\nInternet\nRecently, it is of great interest to adopt the Internet/intranet to develop\nbuilding management systems (BMS) and facilities management systems\n(FMS). This paper addresses two technical issues: the Web-based access\n(including database integration) and the integration of BMS and FMS.\nThese should be addressed for accessing BMS remotely via the Internet,\nintegrating control networks using the Internet protocols and\ninfrastructures, and using Internet/intranet for building facilities\nmanagement. An experimental Internet-enabled system that integrates\nbuilding and facilities management systems has been developed and\ntested. This system integrated open control networks with the Internet\nand is developed utilizing the embedded Web server, the PC Web server\nand the Distributed Component Object Model (DCOM) software development\ntechnology on the platform of an open control network. Three strategies\nfor interconnecting BMS local networks via Internet/intranet are\npresented and analyzed\n', ['intranet', 'building management systems', 'BMS', 'facilities management systems', 'FMS', 'Web-based access', 'database integration', 'Internet protocols', 'embedded Web server', 'PC Web server', 'Distributed Component Object Model', 'DCOM', 'software development technology', 'open control network', 'local network interconnection', 'building management systems', 'distributed object management', 'integrated software', 'Internet', 'intranets', 'LAN interconnection', 'open systems', 'protocols']), ('Algebraic conditions for high-order convergent deferred correction schemes\nbased on Runge-Kutta-Nystrom methods for second order boundary value\nproblems\nIn [T. Van Hecke, M. Van Daele, J. Comp. Appl. Math., vol. 132, p. 107-125,\n(2001)] the investigation of high-order convergence of deferred\ncorrection schemes for the numerical solution of second order nonlinear\ntwo-point boundary value problems not containing the first derivative,\nis made. The derivation of the algebraic conditions to raise the\nincrease of order by the deferred correction scheme was based on Taylor\nseries expansions. In this paper we describe a more elegant way by\nmeans of P-series to obtain this necessary conditions and generalize\nthis idea to equations of the form y" = f (t, y, y\')\n', ['high-order convergent deferred correction schemes', 'Runge-Kutta-Nystrom methods', 'second order boundary value problems', 'deferred correction schemes', 'second order nonlinear two-point boundary value problems', 'algebraic conditions', 'Taylor series expansions', 'boundary-value problems', 'convergence of numerical methods', 'Runge-Kutta methods', 'series (mathematics)']), ('Syndicators turn to the enterprise\nSyndicators have started reshaping offerings, products, and services towards\nthe marketplace that was looking for enterprise-wide content\nsyndication technology and service. Syndication companies are turning\nthemselves into infrastructure companies. Many syndication companies\nare now focusing their efforts on enterprise clients instead of the\nrisky dot coms\n', ['enterprise-wide content syndication technology', 'business model', 'enterprise clients', 'aggregator', 'business Web sites', 'customer base', 'infrastructure companies', 'electronic commerce', 'information resources']), ('Unlocking the potential of videoconferencing\nI propose in this paper to show, through a number of case studies, that\nvideoconferencing is user-friendly, cost-effective, time-effective and\nlife-enhancing for people of all ages and abilities and that it\nrequires only a creative and imaginative approach to unlock its\npotential. I believe that these benefits need not, and should not, be\nrestricted to the education sector. My examples will range from simple\nstorytelling, through accessing international experts, professional\ndevelopment and distance learning in a variety of forms, to the use of\nvideoconferencing for virtual meetings and planning sessions. In some\ncases, extracts from the reactions and responses of the participants\nwill be included to illustrate the impact of the medium\n', ['videoconferencing', 'benefits', 'case studies', 'education', 'education', 'teleconferencing']), ('Switching controller design via convex polyhedral Lyapunov functions\nWe propose a systematic switching control design method for a class of\nnonlinear discrete time hybrid systems. The novelty of the adopted\napproach is in the fact that unlike conventional control the control\nburden is shifted to a logical level thus creating the need for the\ndevelopment of new analysis/design methods\n', ['switching controller design', 'convex polyhedral Lyapunov functions', 'nonlinear discrete time hybrid systems', 'systematic design method', 'asymptotic stability', 'control system analysis', 'control system synthesis', 'discrete time systems', 'Lyapunov methods', 'nonlinear control systems', 'optimal control']), ("Are we there yet?: facing the never-ending speed and change of technology in\nmidlife\nThis essay is a personal reflection on entering librarianship in middle age at\na time when the profession, like society in general, is experiencing\nrapidly accelerating change. Much of this change is due to the\nincreased use of computers and information technologies in the library\nsetting. These aids in the production, collection, storage, retrieval,\nand dissemination of the collective information, knowledge, and\nsometimes wisdom of the past and the contemporary world can exhilarate\nor burden depending on one's worldview, the organization, and the\nflexibility of the workplace. This writer finds herself working in a\nlibrary where everyone is expected continually to explore and use new\nways of working and providing library service to a campus and a wider\ncommunity. No time is spent in reflecting on what was, but all efforts\nare to anticipate and prepare for what will be\n", ['librarianship', 'middle age', 'changing technology', 'computers', 'information technologies', 'dissemination', 'retrieval', 'storage', 'collection', 'library automation', 'technology transfer']), ('Design of high-performance wavelets for image coding using a perceptual time\ndomain criterion\nThis paper presents a new biorthogonal linear-phase wavelet design for image\ncompression. Instead of calculating the prototype filters as spectral\nfactors of a half-band filter, the design is based on the direct\noptimization of the low pass analysis filter using an objective\nfunction directly related to a perceptual criterion for image\ncompression. This function is defined as the product of the theoretical\ncoding gain and an index called the peak-to-peak ratio, which was shown\nto have high correlation with perceptual quality. A distinctive feature\nof the proposed technique is a procedure by which, given a "good"\nstarting filter, "good" filters of longer lengths are generated. The\nresults are excellent, showing a clear improvement in perceptual image\nquality. Also, we devised a criterion for constraining the coefficients\nof the filters in order to design wavelets with minimum ringing\n', ['high-performance wavelets', 'image coding', 'perceptual time domain criterion', 'biorthogonal linear-phase wavelet design', 'image compression', 'prototype filters', 'half-band filter', 'low pass filter', 'analysis filter', 'objective function', 'coding gain', 'peak-to-peak ratio', 'perceptual image quality', 'filter banks', 'channel bank filters', 'data compression', 'image coding', 'low-pass filters', 'optimisation', 'time-domain analysis', 'transform coding', 'visual perception', 'wavelet transforms']), ('Adaptive filtering for noise reduction in hue saturation intensity color space\nEven though the hue saturation intensity (HSI) color model has been widely used\nin color image processing and analysis, the conversion formulas from\nthe RGB color model to HSI are nonlinear and complicated in comparison\nwith the conversion formulas of other color models. When an RGB image\nis degraded by random Gaussian noise, this nonlinearity leads to a\nnonuniform noise distribution in HSI, making accurate image analysis\nmore difficult. We have analyzed the noise characteristics of the HSI\ncolor model and developed an adaptive spatial filtering method to\nreduce the magnitude of noise and the nonuniformity of noise variance\nin the HSI color space. With this adaptive filtering method, the filter\nkernel for each pixel is dynamically adjusted, depending on the values\nof intensity and saturation. In our experiments we have filtered the\nsaturation and hue components and generated edge maps from color\ngradients. We have found that by using the adaptive filtering method,\nthe minimum error rate in edge detection improves by approximately 15%\n', ['adaptive filtering', 'noise reduction', 'hue saturation intensity color space', 'color image processing', 'color image analysis', 'RGB color model', 'random Gaussian noise', 'nonuniform noise distribution', 'accurate image analysis', 'adaptive spatial filtering method', 'nonuniformity', 'noise variance', 'HSI color space', 'filter kernel', 'pixel', 'saturation', 'intensity', 'generated edge maps', 'color gradients', 'edge detection', 'minimum error rate', 'adaptive optics', 'colorimetry', 'edge detection', 'image colour analysis', 'optical noise', 'optical saturation', 'spatial filters']), ('Supporting unified interface to wrapper generator in integrated information\nretrieval\nGiven the ever-increasing scale and diversity of information and applications\non the Internet, improving the technology of information retrieval is\nan urgent research objective. Retrieved information is either\nsemi-structured or unstructured in format and its sources are extremely\nheterogeneous. In consequence, the task of efficiently gathering and\nextracting information from documents can be both difficult and\ntedious. Given this variety of sources and formats, many choose to use\nmediator/wrapper architecture, but its use demands a fast means of\ngenerating efficient wrappers. In this paper, we present a design for\nan automatic eXtensible Markup Language (XML)-based framework with\nwhich to generate wrappers rapidly. Wrappers created with this\nframework support a unified interface for a meta-search information\nretrieval system based on the Internet Search Service using the Common\nObject Request Broker Architecture (CORBA) standard. Greatly advantaged\nby the compatibility of CORBA and XML, a user can quickly and easily\ndevelop information-gathering applications, such as a meta-search\nengine or any other information source retrieval method. The two main\nthings our design provides are a method of wrapper generation that is\nfast, simple, and efficient, and a wrapper generator that is CORBA and\nXML-compliant and that supports a unified interface\n', ['unified interface', 'wrapper generator', 'integrated information retrieval', 'Internet', 'automatic eXtensible Markup Language', 'CORBA', 'meta-search engine', 'distributed object management', 'hypermedia markup languages', 'information retrieval', 'pattern matching']), ('Analysis of the surface roughness and dimensional accuracy capability of fused\ndeposition modelling processes\nBuilding up materials in layers poses significant challenges from the viewpoint\nof material science, heat transfer and applied mechanics. However,\nnumerous aspects of the use of these technologies have yet to be\nstudied. One of these aspects is the characterization of the surface\nroughness and dimensional precision obtainable in layered manufacturing\nprocesses. In this paper, a study of roughness parameters obtained\nthrough the use of these manufacturing processes was made. Prototype\nparts were manufactured using FDM techniques and an experimental\nanalysis of the resulting roughness average (R/sub a/) and rms\nroughness (R/sub q/) obtained through the use of these manufacturing\nprocesses was carried out. Dimensional parameters were also studied in\norder to determine the capability of the Fused Deposition Modelling\nprocess for manufacturing parts\n', ['fused deposition modelling processes', 'surface roughness', 'dimensional accuracy capability', 'dimensional precision', 'layered manufacturing processes', 'prototype parts', 'roughness average', 'rms roughness', 'rapid prototyping', 'three-dimensional solid objects', 'CAD model', 'CNC-controlled robot', 'extrusion head', 'computerised numerical control', 'extrusion', 'industrial robots', 'rapid prototyping (industrial)', 'surface topography measurement']), ("A synergic analysis for Web-based enterprise resources planning systems\nAs the central nervous system for managing an organization's mission and\ncritical business data, Enterprise Resource Planning (ERP) system has\nevolved to become the backbone of e-business implementation. Since an\nERP system is multimodule application software that helps a company\nmanage its important business functions, it should be versatile enough\nto automate every aspect of business processes, including e-business\n", ['Enterprise Resource Planning', 'e-business', 'ERP', 'customer relationship management', 'synergic analysis', 'Web-based enterprise resources planning', 'business data processing']), ('Spectral characteristics of the linear systems over a bounded time interval\nConsideration was given to the spectral characteristics of the linear dynamic\nsystems over a bounded time interval. Singular characteristics of\nstandard dynamic blocks, transcendental characteristic equations, and\npartial spectra of the singular functions were studied. Relationship\nbetween the spectra under study and the classical frequency\ncharacteristic was demonstrated\n', ['spectral characteristics', 'bounded time interval', 'linear dynamic systems', 'singular characteristics', 'standard dynamic blocks', 'transcendental characteristic equations', 'partial spectra', 'singular functions', 'frequency characteristic', 'linear systems', 'spectral analysis']), ('Optimization of planning an advertising campaign of goods and services\nA generalization of the mathematical model and operations research problems\nformulated on its basis, which were presented by Belenky (2001) in the\nframework of an approach to planning an advertising campaign of goods\nand services, is considered, and corresponding nonlinear programming\nproblems with linear constraints are formulated\n', ['optimization', 'advertising campaign planning', 'operations research', 'OR', 'nonlinear programming', 'advertising', 'nonlinear programming']), ('The contiguity in R/M\nAn r.e. degree c is contiguous if deg/sub wtt/(A)=deg/sub wtt/(B) for any r.e.\nsets A,B in c. In this paper, we generalize the notation of contiguity\nto the structure R/M, the upper semilattice of the r.e. degree set R\nmodulo the cappable r.e. degree set M. An element [c] in R/M is\ncontiguous if [deg/sub wtt/(A)]=[deg/sub wtt/(B)] for any r.e. sets A,\nB such that deg/sub T/(A),deg/sub T/(B) in [c]. It is proved in this\npaper that every nonzero element in R/M is not contiguous, i.e., for\nevery element [c] in R/M, if [c] not=[o] then there exist at least two\nr.e. sets A, B such that deg/sub T/(A), deg/sub T/(B) in [c] and\n[deg/sub wtt/(A)] not=[deg/sub wtt/(B)]\n', ['contiguity', 'Turing degree', 'recursively enumerable set', 'upper semilattice', 'nonzero element', 'recursion theory', 'recursive functions', 'Turing machines']), ('On the design of gain-scheduled trajectory tracking controllers [AUV\napplication]\nA new methodology is proposed for the design of trajectory tracking controllers\nfor autonomous vehicles. The design technique builds on gain scheduling\ncontrol theory. An application is made to the design of a trajectory\ntracking controller for a prototype autonomous underwater vehicle\n(AUV). The effectiveness and advantages of the new control laws derived\nare illustrated in simulation using a full set of non-linear equations\nof motion of the vehicle\n', ['gain-scheduled trajectory tracking controller design', 'autonomous vehicles', 'gain scheduling control theory', 'autonomous underwater vehicle', 'control laws', 'nonlinear equations of motion', 'control system synthesis', 'nonlinear control systems', 'position control', 'remotely operated vehicles', 'tracking', 'underwater vehicles']), ('Wavelet collocation methods for a first kind boundary integral equation in\nacoustic scattering\nIn this paper we consider a wavelet algorithm for the piecewise constant\ncollocation method applied to the boundary element solution of a first\nkind integral equation arising in acoustic scattering. The conventional\nstiffness matrix is transformed into the corresponding matrix with\nrespect to wavelet bases, and it is approximated by a compressed\nmatrix. Finally, the stiffness matrix is multiplied by diagonal\npreconditioners such that the resulting matrix of the system of linear\nequations is well conditioned and sparse. Using this matrix, the\nboundary integral equation can be solved effectively\n', ['first kind integral operators', 'piecewise constant collocation', 'wavelet algorithm', 'boundary element solution', 'boundary integral equation', 'wavelet transform', 'computational complexity', 'acoustic scattering', 'stiffness matrix', 'linear equations', 'acoustics', 'boundary-value problems', 'communication complexity', 'integral equations', 'wavelet transforms']), ("Optimization of element-by-element FEM in HPF 1.1\nIn this study, Poisson's equation is numerically evaluated by the\nelement-by-element (EBE) finite-element method in a parallel\nenvironment using HPF 1.1 (High-Performance Fortran). In order to\nachieve high parallel efficiency, the data structures have been altered\nto node-based data instead of mixtures of node- and element-based data,\nrepresenting a node-based EBE finite-element scheme (nEBE). The\nparallel machine used in this study was the NEC SX-4, and experiments\nwere performed on a single node having 32 processors sharing common\nmemory. The HPF compiler used in the experiments is HPF/SX Rev 2.0\nreleased in 1997 (unofficial), which supports HPF 1.1. Models\ncontaining approximately 200 000 and 1,500,000 degrees of freedom were\nanalyzed in order to evaluate the method. The calculation time,\nparallel efficiency, and memory used were compared. The performance of\nHPF in the conjugate gradient solver for the large model, using the NEC\nSX-4 compiler option-noshrunk, was about 85% that of the message\npassing interface\n", ['finite element method', 'parallel programs', 'Poisson equation', 'HPF compiler', 'conjugate gradient solver', 'message passing', 'element-by-element', 'HPF', 'finite element analysis', 'FORTRAN', 'parallel programming', 'program compilers']), ('Estimation of trifocal tensor using GMM\nA novel estimation of a trifocal tensor based on the Gaussian mixture model\n(GMM) is presented. The mixture model is built assuming that the\nresiduals of inliers and outliers belong to different Gaussian\ndistributions. The Bayesian rule is then employed to detect the inliers\nfor re-estimation. Experiments show that the presented method is more\nprecise and relatively unaffected by outliers\n', ['trifocal tensor estimation', 'GMM', 'Gaussian mixture model', 'Gaussian distributions', 'Bayesian rule', 'inliers', 'outliers', 'motion analysis', 'image data', 'image analysis', 'Bayes methods', 'covariance matrices', 'Gaussian distribution', 'image motion analysis', 'tensors']), ('Labscape: a smart environment for the cell biology laboratory\nLabscape is a smart environment that we designed to improve the experience of\npeople who work in a cell biology laboratory. Our goal in creating it\nwas to simplify, laboratory work by making information available where\nit is needed and by collecting and organizing data where and when it is\ncreated into a formal representation that others can understand and\nprocess. By helping biologists produce a more complete record of their\nwork with less effort, Labscape is designed to foster improved\ncollaboration in conjunction with increased individual efficiency and\nsatisfaction. A user-driven system, although technologically\nconservative, embraces a central goal of ubiquitous computing: to\nenhance the ability to perform domain tasks through fluid interaction\nwith computational resources. Smart environments could soon replace the\npen and paper commonly used in the laboratory setting\n', ['cell biology', 'Labscape', 'laboratory work', 'ubiquitous computing', 'smart environment', 'experimental technologies', 'biochemical procedure', 'biology computing', 'laboratory techniques']), ('A review of methodologies used in research on cadastral development\nWorld-wide, much attention has been given to cadastral development. As a\nconsequence of experiences made during recent decades, several authors\nhave stated the need for research in the domain of cadastre and\nproposed methodologies to be used. The paper contributes to the\nacceptance of research methodologies needed for cadastral development,\nand thereby enhances theory in the cadastral domain. The paper reviews\nnine publications on cadastre and identifies the methodologies used.\nThe review focuses on the institutional, social, political and economic\naspects of cadastral development, rather than on the technical aspects.\nThe main conclusion is that the methodologies used are largely those of\nthe social sciences. That agrees with the notion that cadastre relates\nas much to people and institutions, as it relates to land, and that\ncadastral systems are shaped by social, political and economic\nconditions, as well as technology. Since the geodetic survey profession\nhas been the keeper of the cadastre, geodetic surveyors will have to\ndeal ever more with social science matters, a fact that universities\nwill have to consider\n', ['cadastral development methodologies', 'cadastre', 'research methodologies', 'political aspects', 'economic aspects', 'social sciences', 'economic conditions', 'geodetic survey profession', 'geodetic surveyors', 'land registration', 'case study', 'cartography', 'geodesy', 'government data processing', 'politics', 'socio-economic effects', 'town and country planning']), ('An inverse problem for a model of a hierarchical structure\nWe consider the inverse problem for the identification of the coefficient in a\nparabolic equation. The model is applied to describe the functioning of\na hierarchical structure; it is also relevant for heat-conduction\ntheory. Unique solvability of the inverse problem is proved\n', ['inverse problem', 'hierarchical structure', 'parabolic equation', 'heat-conduction theory', 'unique solvability', 'heat conduction', 'inverse problems', 'partial differential equations']), ("Fast accurate MEG source localization using a multilayer perceptron trained\nwith real brain noise\nIterative gradient methods such as Levenberg-Marquardt (LM) are in widespread\nuse for source localization from electroencephalographic (EEG) and\nmagnetoencephalographic (MEG) signals. Unfortunately, LM depends\nsensitively on the initial guess, necessitating repeated runs. This,\ncombined with LM's high per-step cost, makes its computational burden\nquite high. To reduce this burden, we trained a multilayer perceptron\n(MLP) as a realtime localizer. We used an analytical model of\nquasistatic electromagnetic propagation through a spherical head to map\nrandomly chosen dipoles to sensor activities according to the sensor\ngeometry of a 4D Neuroimaging Neuromag-122 MEG system, and trained a\nMLP to invert this mapping in the absence of noise or in the presence\nof various sorts of noise such as white Gaussian noise, correlated\nnoise, or real brain noise. A MLP structure was chosen to trade off\ncomputation and accuracy. This MLP was trained four times, with each\ntype of noise. We measured the effects of initial guesses on LM\nperformance, which motivated a hybrid MLP-start-LM method, in which the\ntrained MLP initializes LM. We also compared the localization\nperformance of LM, MLPs, and hybrid MLP-start-LMs for realistic brain\nsignals. Trained MLPs are much faster than other methods, while the\nhybrid MLP-start-LMs are faster and more accurate than\nfixed-4-start-LM. In particular, the hybrid MLP-start-LM initialized by\na MLP trained with the real brain noise dataset is 60 times faster and\nis comparable in accuracy to random-20-start-LM, and this hybrid system\n(localization error: 0.28 cm, computation time: 36 ms) shows almost as\ngood performance as optimal-1-start-LM (localization error: 0.23 cm,\ncomputation time: 22 ms), which initializes LM with the correct dipole\nlocation. MLPs trained with noise perform better than the MLP trained\nwithout noise, and the MLP trained with real brain noise is almost as\ngood an initial guesser for LM as the correct dipole location\n", ['MEG source localization', 'fast accurate localization', 'multilayer perceptron', 'real brain noise', 'real-time localizer', 'analytical model', 'quasistatic electromagnetic propagation', 'spherical head', 'white Gaussian noise', 'correlated noise', 'computation accuracy', 'Levenberg-Marquardt method', 'iterative gradient methods', 'forward model', 'Gaussian noise', 'gradient methods', 'inverse problems', 'learning (artificial intelligence)', 'magnetoencephalography', 'medical signal processing', 'multilayer perceptrons', 'white noise']), ('The chemical brotherhood\nIt has always been more difficult for chemistry to keep up in the Internet age\nbut a new language could herald a new era for the discipline. The paper\ndiscusses CML, or chemical mark-up language. The eXtensible Mark-up\nLanguage provides a universal format for structured documents and data\non the Web and so offers a way for scientists and others to carry a\nwide range of information types across the net in a transparent way.\nAll that is needed is an XML browser\n', ['chemistry', 'Internet', 'CML', 'chemical mark-up language', 'eXtensible Mark-up Language', 'structured document format', 'World Wide Web', 'XML browser', 'chemistry computing', 'hypermedia markup languages', 'information resources', 'Internet']), ('On the relationship between parametric variation and state feedback in chaos\ncontrol\nIn this Letter, we study the popular parametric variation chaos control and\nstate-feedback methodologies in chaos control, and point out for the\nfirst time that they are actually equivalent in the sense that there\nexist diffeomorphisms that can convert one to the other for most smooth\nchaotic systems. Detailed conversions are worked out for typical\ndiscrete chaotic maps (logistic, Henon) and continuous flows (Rossler,\nLorenz) for illustration. This unifies the two seemingly different\napproaches from the physics and the engineering communities on chaos\ncontrol. This new perspective reveals some new potential applications\nsuch as chaos synchronization and normal form analysis from a unified\nmathematical point of view\n', ['parametric variation', 'chaos control', 'state-feedback', 'logistic', 'Henon map', 'continuous flows', 'Rossler system', 'Lorenz system', 'diffeomorphisms', 'chaos', 'Henon mapping', 'nonlinear control systems', 'state feedback']), ('New kit on the block [IT upgrades]\nAs time passes, new hardware and software replace the old. The hows are\nstraightforward: IT resellers and consultants can help with upgrade\npracticalities. Will Dalrymple examines the business issues and costs\ninvolved in IT upgrades\n', ['IT upgrades', 'business issues', 'costs', 'IT resellers', 'consultants', 'Microsoft', 'contracts', 'DP management']), ('Flexibility analysis of complex technical systems under uncertainty\nAn important problem in designing technical systems under partial uncertainty\nof the initial physical, chemical, and technological data is the\ndetermination of a design in which the technical system is flexible,\ni.e., its control system is capable of guaranteeing that the\nconstraints hold even under changes in external and internal factors\nand application of fuzzy mathematical models in its design. Three\nflexibility problems, viz., the flexibility of a technical system of\ngiven structure, structural flexibility of a technical system, and the\noptimal design guaranteeing the flexibility of a technical system, are\nstudied. Two approaches to these problems are elaborated. Results of a\ncomputation experiment are given\n', ['flexibility analysis', 'complex technical systems', 'partial uncertainty', 'control system', 'fuzzy mathematical models', 'structural flexibility', 'optimal design', 'chemical technology', 'fuzzy set theory', 'large-scale systems', 'mathematical programming', 'uncertain systems']), ('The best circulant preconditioners for Hermitian Toeplitz systems.II. The\nmultiple-zero case\nFor pt.I. see SIAM J. Numer. Anal., vol. 38, p. 876-896. Circulant-type\npreconditioners have been proposed previously for ill-conditioned\nHermitian Toeplitz systems that are generated by nonnegative continuous\nfunctions with a zero of even order. The proposed circulant\npreconditioners can be constructed without requiring explicit knowledge\nof the generating functions. It was shown that the spectra of the\npreconditioned matrices are uniformly bounded except for a fixed number\nof outliers and that all eigenvalues are uniformly bounded away from\nzero. Therefore the conjugate gradient method converges linearly when\napplied to solving the circulant preconditioned systems. Previously it\nwas claimed that this result can be extended to the case where the\ngenerating functions have multiple zeros. The main aim of this paper is\nto give a complete convergence proof of the method for this class of\ngenerating functions\n', ['circulant preconditioners', 'Hermitian Toeplitz systems', 'multiple-zero case', 'nonnegative continuous functions', 'generating functions', 'preconditioned matrices', 'eigenvalues', 'conjugate gradient method', 'computational complexity', 'conjugate gradient methods', 'eigenvalues and eigenfunctions', 'Hermitian matrices', 'Toeplitz matrices']), ('On the relationship between omega -automata and temporal logic normal forms\nWe consider the relationship between omega -automata and a specific logical\nformulation based on a normal form for temporal logic formulae. While\nthis normal form was developed for use with execution and clausal\nresolution in temporal logics, we show how it can represent,\nsyntactically, omega -automata in a high-level way. Technical proofs of\nthe correctness of this representation are given\n', ['omega -automata', 'temporal logic normal forms', 'logical formulation', 'clausal resolution', 'program correctness', 'automata theory', 'temporal logic', 'theorem proving']), ('Closed-loop model set validation under a stochastic framework\nDeals with probabilistic model set validation. It is assumed that the dynamics\nof a multi-input multi-output (MIMO) plant is described by a model set\nwith unstructured uncertainties, and identification experiments are\nperformed in closed loop. A necessary and sufficient condition has been\nderived for the consistency of the model set with both the stabilizing\ncontroller and closed-loop frequency domain experimental data (FDED).\nIn this condition, only the Euclidean norm of a complex vector is\ninvolved, and this complex vector depends linearly on both the\ndisturbances and the measurement errors. Based on this condition, an\nanalytic formula has been derived for the sample unfalsified\nprobability (SUP) of the model set. Some of the asymptotic statistical\nproperties of the SUP have also been briefly discussed. A numerical\nexample is included to illustrate the efficiency of the suggested\nmethod in model set quality evaluation\n', ['closed-loop model set validation', 'stochastic framework', 'probabilistic model set validation', 'multi-input multi-output plant', 'MIMO plant', 'unstructured uncertainties', 'necessary and sufficient condition', 'stabilizing controller', 'closed-loop frequency domain experimental data', 'Euclidean norm', 'complex vector', 'asymptotic statistical properties', 'robust control', 'unstructured uncertainty', 'closed loop systems', 'identification', 'MIMO systems', 'probability', 'robust control', 'stochastic processes', 'uncertain systems', 'vectors']), ('Copyright management in the digital age\nListening to and buying music online is becoming increasingly popular with\nconsumers. So much so that Merrill Lynch forecasts the value of the\nonline music market will explode from $8 million in 2001 to $1,409\nmillion in 2005. But online delivery is not without problems; the issue\nof copyright management in particular has become a serious thorn in the\nside for digital content creators. Martin Brass, ex- music producer and\nsenior industry consultant at Syntegra, explains\n', ['digital age', 'online music delivery', 'music industry', 'Internet', 'Napster', 'digital content creators', 'copyright', 'Internet', 'music']), ("The culture of usability\nNow that most of us agree that usability testing is an integral investment in\nsite development, it's time to recognize that the standard approach\nfalls short. It is possible to do less work and get better results\nwhile spending less money. By bringing usability testing in-house and\nbreaking tests into more manageable sessions, you can vastly improve\nyour online offering without affecting your profit margin\n", ['usability testing program', 'Web site', 'Internet', 'program testing', 'user interfaces']), ('Direct aperture optimization: A turnkey solution for step-and-shoot IMRT\nIMRT treatment plans for step-and-shoot delivery have traditionally been\nproduced through the optimization of intensity distributions (or maps)\nfor each beam angle. The optimization step is followed by the\napplication of a leaf-sequencing algorithm that translates each\nintensity map into a set of deliverable aperture shapes. In this\narticle, we introduce an automated planning system in which we bypass\nthe traditional intensity optimization, and instead directly optimize\nthe shapes and the weights of the apertures. We call this approach\n"direct aperture optimization." This technique allows the user to\nspecify the maximum number of apertures per beam direction, and hence\nprovides significant control over the complexity of the treatment\ndelivery. This is possible because the machine dependent delivery\nconstraints imposed by the MLC are enforced within the aperture\noptimization algorithm rather than in a separate leaf-sequencing step.\nThe leaf settings and the aperture intensities are optimized\nsimultaneously using a simulated annealing algorithm. We have tested\ndirect aperture optimization on a variety of patient cases using the\nEGS4/BEAM Monte Carlo package for our dose calculation engine. The\nresults demonstrate that direct aperture optimization can produce\nhighly conformal step-and-shoot treatment plans using only three to\nfive apertures per beam direction. As compared with traditional\noptimization strategies, our studies demonstrate that direct aperture\noptimization can result in a significant reduction in both the number\nof beam segments and the number of monitor units. Direct aperture\noptimization therefore produces highly efficient treatment deliveries\nthat maintain the full dosimetric benefits of IMRT\n', ['direct aperture optimization', 'turnkey solution', 'step-and-shoot IMRT', 'IMRT treatment plans', 'intensity distributions', 'maps', 'beam angle', 'optimization step', 'leaf-sequencing algorithm', 'intensity map', 'deliverable aperture shapes', 'automated planning system', 'aperture weights', 'aperture shapes', 'treatment delivery complexity', 'machine dependent delivery constraints', 'MLC', 'aperture optimization algorithm', 'leaf settings', 'aperture intensities', 'simulated annealing algorithm', 'patient cases', 'EGS4/BEAM Monte Carlo package', 'dose calculation engine', 'highly conformal step-and-shoot treatment plans', 'beam segments', 'monitor units', 'highly efficient treatment deliveries', 'full dosimetric benefits', 'dosimetry', 'intensity modulation', 'inverse problems', 'medical computing', 'Monte Carlo methods', 'optimisation', 'planning', 'radiation therapy', 'simulated annealing']), ("NARX-based technique for the modelling of magneto-rheological damping devices\nThis paper presents a methodology for identifying variable-structure nonlinear\nmodels of magneto-rheological dampers (MRD) and similar devices. Its\npeculiarity with respect to the mainstream literature is to be\nespecially conceived for obtaining models that are structurally simple,\neasy to estimate and well suited for model-based control. This goal is\npursued by adopting linear-in-the-parameters NARX models, for which an\nidentification method is developed based on the minimization of the\nsimulation error. This method is capable of selecting the model\nstructure together with the parameters, thus it does not require a\npriori structural information. A set of validation tests is reported,\nwith the aim of demonstrating the technique's efficiency by comparing\nit to a widely accepted MRD modelling approach\n", ['modelling', 'magnetorheological damping', 'model-based control', 'NARX models', 'identification', 'minimization', 'simulation error', 'validation', 'MRD modelling', 'control system analysis', 'damping', 'identification', 'intelligent actuators', 'magnetorheology', 'minimisation', 'nonlinear control systems']), ('A server-side program for delivering experiments with animations\nA server-side program for animation experiments is presented. The program is\ncapable of delivering an experiment composed of discrete animation\nsequences in various file formats, collecting a discrete or continuous\nresponse from the observer, evaluating the appropriateness of the\nresponse, and ensuring that the user is not proceeding at an\nunreasonable rate. Most parameters of the program are controllable by\nexperimenter-edited text files or simple switches in the program code,\nthereby minimizing the need for programming to create new experiments.\nA simple demonstration experiment is discussed and is freely available\n', ['server-side program', 'animation experiment delivery', 'discrete animation sequences', 'file formats', 'Web based psychological experiments', 'Internet', 'experimenter-edited text files', 'computer animation', 'information resources', 'Internet', 'psychology']), ('Optical two-step modified signed-digit addition based on binary logic gates\nA new modified signed-digit (MSD) addition algorithm based on binary logic\ngates is proposed for parallel computing. It is shown that by encoding\neach of the input MSD digits and flag digits into a pair of binary\nbits, the number of addition steps can be reduced to two. The flag\ndigit is introduced to characterize the next low order pair (NLOP) of\nthe input digits in order to suppress carry propagation. The rules for\ntwo-step addition of binary coded MSD (BCMSD) numbers are formulated\nthat can be implemented using optical shadow-casting logic system\n', ['optical two-step modified signed-digit addition', 'binary logic gates', 'modified signed-digit addition algorithm', 'parallel computing', 'input MSD digits', 'flag digits', 'binary bits', 'addition steps', 'low order pair', 'carry propagation suppression', 'two-step addition', 'binary coded MSD', 'optical shadow-casting logic system', 'adders', 'digital arithmetic', 'encoding', 'optical logic', 'parallel architectures']), ('Information access for all: meeting the needs of deaf and hard of hearing\npeople\nDiscusses the nature of deafness and hearing impairments, with particular\nreference to the impact which the onset of hearing loss presents at\nvarious ages. The author goes on to present practical tips for\ninteracting with deaf and hard of hearing clients in various\ncommunication contexts, including sightreading, TTY communications, and\nASL interpreters. An annotated list of suggested readings is appended\n', ['information access', 'deaf clients', 'hard of hearing clients', 'deafness', 'hearing impairments', 'communication contexts', 'sightreading', 'TTY communications', 'ASL interpreters', 'handicapped aids', 'information retrieval']), ('Numerical validation of solutions of complementarity problems: the nonlinear\ncase\nThis paper proposes a validation method for solutions of nonlinear\ncomplementarity problems. The validation procedure performs a\ncomputational test. If the result of the test is positive, then it is\nguaranteed that a given multi-dimensional interval either includes a\nsolution or excludes all solutions of the nonlinear complementarity\nproblem\n', ['numerical validation', 'computational test', 'nonlinear complementarity problem', 'optimization', 'matrix algebra', 'optimisation']), ('Mathematical models of functioning of an insurance company with allowance for\nthe rate of return\nModels of the functioning of insurance companies are suggested, when the free\ncapital increases from interest at a certain rate. The basic\ncharacteristics of the capital of a company are studied in the\nstationary regime\n', ['mathematical models', 'insurance company functioning', 'return rate allowance', 'free capital increase', 'interest', 'stationary regime', 'insurance']), ("Enterprise content integration III: Agari Mediaware's Media Star\nSince we introduced the term Enterprise Content Integration (ECI) in January,\nthe concept has gained momentum in the market. In addition to Context\nMedia's Interchange Platform and Savantech's Photon Commerce, Agari\nMediaware's Media Star is in the fray. It is a middleware platform that\nallows large media companies to integrate their digital systems with\ngreat flexibility\n", ['Agari Mediaware Media Star', 'enterprise content integration', 'middleware', 'database management systems', 'document handling', 'electronic publishing', 'workflow management software']), ('An active functionality service for e-business applications\nService based architectures are a powerful approach to meet the fast evolution\nof business rules and the corresponding software. An active\nfunctionality service that detects events and involves the appropriate\nbusiness rules is a critical component of such a service-based\nmiddleware architecture. In this paper we present an active\nfunctionality service that is capable of detecting events in\nheterogeneous environments, it uses an integral ontology-based approach\nfor the semantic interpretation of heterogeneous events and data, and\nprovides notifications through a publish/subscribe notification\nmechanism. The power of this approach is illustrated with the help of\nan auction application and through the personalization of car and\ndriver portals in Internet-enabled vehicles\n', ['active functionality service', 'e-business applications', 'business rules', 'software', 'event detection', 'service-based middleware architecture', 'heterogeneous environments', 'ontology based approach', 'semantic interpretation', 'publish/subscribe notification mechanism', 'auction application', 'personalized car portals', 'personalized driver portals', 'Internet-enabled vehicles', 'client-server systems', 'driver information systems', 'electronic commerce', 'information dissemination', 'Internet', 'knowledge engineering', 'online front-ends']), ('Spatial solutions [office furniture]\nTake the stress out of the office by considering the design of furniture and\nstaff needs, before major buying decisions\n', ['office furniture', 'staff needs', 'buying decisions', 'furniture']), ('Presentation media, information complexity, and learning outcomes\nMultimedia computing provides a variety of information presentation modality\ncombinations. Educators have observed that visuals enhance learning\nwhich suggests that multimedia presentations should be superior to\ntext-only and text with static pictures in facilitating optimal human\ninformation processing and, therefore, comprehension. The article\nreports the findings from a 3 (text-only, overhead slides, and\nmultimedia presentation)*2 (high and low information complexity)\nfactorial experiment. Subjects read a text script, viewed an acetate\noverhead slide presentation, or viewed a multimedia presentation\ndepicting the greenhouse effect (low complexity) or photocopier\noperation (high complexity). Multimedia was superior to text-only and\noverhead slides for comprehension. Information complexity diminished\ncomprehension and perceived presentation quality. Multimedia was able\nto reduce the negative impact of information complexity on\ncomprehension and increase the extent of sustained attention to the\npresentation. These findings suggest that multimedia presentations\ninvoke the use of both the verbal and visual working memory channels\nresulting in a reduction of the cognitive load imposed by increased\ninformation complexity. Moreover, multimedia superiority in\nfacilitating comprehension goes beyond its ability to increase\nsustained attention; the quality and effectiveness of information\nprocessing attained (i.e., use of verbal and visual working memory) is\nalso significant\n', ['presentation media', 'information complexity', 'learning outcomes', 'cognitive processing limitations', 'human working memory', 'verbal working memory channel', 'visual working memory channel', 'multimedia computing', 'information presentation modality combinations', 'educators', 'multimedia presentations', 'static pictures', 'optimal human information processing', 'overhead slides', 'text script', 'acetate overhead slide presentation', 'multimedia presentation', 'greenhouse effect', 'photocopier operation', 'cognitive load', 'multimedia superiority', 'sustained attention', 'educational computing', 'ergonomics', 'human factors', 'multimedia computing', 'user interfaces']), ('Strong completeness of lattice-valued logic\nThis paper shows strong completeness of the system L for lattice valued logic\ngiven by S. Titani (1999), in which she formulates a lattice-valued set\ntheory by introducing the logical implication which represents the\norder relation on the lattice. Syntax and semantics concerned are\ndescribed and strong completeness is proved\n', ['strong completeness', 'lattice-valued set theory', 'order relation', 'semantics', 'syntax', 'lattice-valued logic', 'formal logic']), ("State-of-the-art in orthopaedic surgical navigation with a focus on medical\nimage modalities\nThis paper presents a review of surgical navigation systems in orthopaedics and\ncategorizes these systems according to the image modalities that are\nused for the visualization of surgical action. Medical images used to\nbe an essential part of surgical education and documentation as well as\ndiagnosis and operation planning over many years. With the recent\nintroduction of navigation techniques in orthopaedic surgery, a new\nfield of application has been opened. Today surgical navigation systems\n- also known as image-guided surgery systems - are available for\nvarious applications in orthopaedic surgery. They visualize the\nposition and orientation of surgical instruments as graphical overlays\nonto a medical image of the operated anatomy on a computer monitor.\nPreoperative image data such as computed tomography scans or intra\noperatively generated images (for example, ultrasonic, endoscopic or\nfluoroscopic images) are suitable for this purpose. A new category of\nmedical images termed 'surgeon-defined anatomy' has been developed that\nexclusively relies upon the usage of navigation technology. Points on\nthe anatomy are digitized interactively by the surgeon and are used to\nbuild up an abstract geometrical model of the bony structures to be\noperated on. This technique may be used when no other image data is\navailable or appropriate for a given application\n", ['orthopaedic surgical navigation', 'medical image modalities', 'surgical action visualization', 'medical image processing', 'surgical education', 'image-guided surgery systems', 'surgical instruments', 'graphical overlays', 'computer monitor', 'computed tomography scans', 'intra operatively generated images', 'surgeon-defined anatomy', 'abstract geometrical model', 'bony structures', 'image registration', 'computerised tomography', 'data visualisation', 'image registration', 'medical image processing', 'orthopaedics', 'surgery']), ('Blind identification of non-stationary MA systems\nA new adaptive algorithm for blind identification of time-varying MA channels\nis derived. This algorithm proposes the use of a novel system of\nequations derived by combining the third- and fourth-order statistics\nof the output signals of MA models. This overdetermined system of\nequations has the important property that it can be solved adaptively\nbecause of their symmetries via an overdetermined recursive\ninstrumental variable-type algorithm. This algorithm shows good\nbehaviour in arbitrary noisy environments and good performance in\ntracking time-varying systems\n', ['blind identification', 'time-varying channels', 'nonstationary systems', 'adaptive algorithm', 'fourth-order statistics', 'third-order statistics', 'MA models', 'overdetermined recursive algorithm', 'recursive instrumental variable algorithm', 'arbitrary noisy environments', 'tracking', 'iterative algorithms', 'additive Gaussian noise', 'higher-order statistics', 'adaptive estimation', 'adaptive signal processing', 'blind equalisers', 'Gaussian noise', 'higher order statistics', 'moving average processes', 'parameter estimation', 'time-varying channels']), ('A comparison of the discounted utility model and hyperbolic discounting models\nin the case of social and private intertemporal preferences for health\nWhilst there is substantial evidence that hyperbolic discounting models\ndescribe intertemporal preferences for monetary outcomes better than\nthe discounted utility (DU) model, there is only very limited evidence\nin the context of health outcomes. This study elicits private and\nsocial intertemporal preferences for non-fatal changes in health.\nSpecific functional forms of the DU model and three hyperbolic models\nare fitted. The results show that the stationarity axiom is violated,\nand that the hyperbolic models fit the data better than the DU model.\nIntertemporal preferences for private and social decisions are found to\nbe very similar\n', ['discounted utility model', 'hyperbolic discounting models', 'intertemporal preferences', 'health outcomes', 'private decisions', 'social decisions', 'decision theory', 'health care', 'social sciences']), ('Frontier between separability and quantum entanglement in a many spin system\nWe discuss the critical point x/sub c/ separating the quantum entangled and\nseparable states in two series of N spins S in the simple mixed state\ncharacterized by the matrix operator rho = x| phi >< phi\n|+1-x/D/sup N/I/sub D/N, where x in [0, 1], D = 2S + 1, I/sub D/N is\nthe D/sup N/ * D/sup N/ unity matrix and | phi > is a special\nentangled state. The cases x = 0 and x = 1 correspond respectively to\nfully random spins and to a fully entangled state. In the first of\nthese series we consider special states | phi > invariant under\ncharge conjugation, that generalizes the N = 2 spin S = 1/2\nEinstein-Podolsky-Rosen state, and in the second one we consider\ngeneralizations of the Werner (1989) density matrices. The evaluation\nof the critical point x/sub c/ was done through bounds coming from the\npartial transposition method of Peres (1996) and the conditional\nnonextensive entropy criterion. Our results suggest the conjecture that\nwhenever the bounds coming from both methods coincide the result of\nx/sub c/ is the exact one. The results we present are relevant for the\ndiscussion of quantum computing, teleportation and cryptography\n', ['separability', 'quantum entanglement', 'many spin system', 'separable states', 'matrix operator', 'unity matrix', 'entangled state', 'random spin', 'charge conjugation', 'Einstein-Podolsky-Rosen state', 'Werner density matrices', 'critical point', 'partial transposition method', 'nonextensive entropy criterion', 'quantum computing', 'teleportation', 'cryptography', 'bound states', 'eigenvalues and eigenfunctions', 'entropy', 'EPR paradox', 'many-body problems', 'probability', 'quantum communication', 'random processes']), ('Verification of timed automata based on similarity\nThe paper presents a modification of the standard partitioning technique to\ngenerate abstract state spaces preserving similarity for Timed\nAutomata. Since this relation is weaker than bisimilarity, most of the\nobtained models (state spaces) are smaller than bisimilar ones, but\nstill preserve the universal fragments of branching time temporal\nlogics. The theoretical results are exemplified for strong, delay, and\nobservational simulation relations\n', ['timed automata verification', 'partitioning technique', 'abstract state spaces', 'bisimilarity', 'universal fragments', 'branching time temporal logics', 'observational simulation relations', 'automata theory', 'bisimulation equivalence', 'minimisation', 'temporal logic']), ("Design and implementation of a flexible manufacturing control system using\nneural network\nDesign and implementation of a sequential controller based on the concept of\nartificial neural networks for a flexible manufacturing system are\npresented. The recurrent neural network (RNN) type is used for such a\npurpose. Contrary to the programmable controller, an RNN-based\nsequential controller is based on a definite mathematical model rather\nthan depending on the experience and trial and error techniques. The\nproposed controller is also more flexible because it is not limited by\nthe restrictions of the finite state automata theory. Adequate\nguidelines of how to construct an RNN-based sequential controller are\npresented. These guidelines are applied to different case studies. The\nproposed controller is tested by simulations and real-time experiments.\nThese tests prove the successfulness of the proposed controller\nperformances. Theoretical as well as experimental results are presented\nand discussed indicating that the proposed design procedure using\nElman's RNN can be effective in designing a sequential controller for\nevent-based type manufacturing systems. In addition, the simulation\nresults assure the effectiveness of the proposed controller to overcome\nthe effect of noisy inputs\n", ['Elman network', 'finite state automata', 'flexible manufacturing systems', 'ladder language', 'noisy inputs', 'pneumatic system', 'programmable controller', 'recurrent neural network', 'sequential control', 'learning', 'FMS', 'finite automata', 'flexible manufacturing systems', 'learning (artificial intelligence)', 'neurocontrollers', 'programmable controllers', 'recurrent neural nets']), ('Multiresolution Markov models for signal and image processing\nReviews a significant component of the rich field of statistical\nmultiresolution (MR) modeling and processing. These MR methods have\nfound application and permeated the literature of a widely scattered\nset of disciplines, and one of our principal objectives is to present a\nsingle, coherent picture of this framework. A second goal is to\ndescribe how this topic fits into the even larger field of MR methods\nand concepts-in particular, making ties to topics such as wavelets and\nmultigrid methods. A third goal is to provide several alternate\nviewpoints for this body of work, as the methods and concepts we\ndescribe intersect with a number of other fields. The principle focus\nof our presentation is the class of MR Markov processes defined on\npyramidally organized trees. The attractiveness of these models stems\nfrom both the very efficient algorithms they admit and their expressive\npower and broad applicability. We show how a variety of methods and\nmodels relate to this framework including models for self-similar and\n1/f processes. We also illustrate how these methods have been used in\npractice\n', ['multiresolution Markov models', 'statistical multiresolution modeling', 'wavelets', 'multigrid methods', 'pyramidally organized trees', 'self-similar processes', '1/f processes', 'image processing', 'Markov processes', 'signal processing', 'statistical analysis', 'trees (mathematics)', 'wavelet transforms']), ('Minimizing the number of successor states in the stubborn set method\nCombinatorial explosion which occurs in parallel compositions of LTSs can be\nalleviated by letting the stubborn set method construct on-the-fly a\nreduced LTS that is CFFD- or CSP-equivalent to the actual parallel\ncomposition. This article considers the problem of minimizing the\nnumber of successor states of a given state in the reduced LTS. The\nproblem can be solved by constructing an and/or-graph with weighted\nvertices and by finding a set of vertices that satisfies a certain\nconstraint such that no set of vertices satisfying the constraint has a\nsmaller sum of weights. Without weights, the and/or-graph can be\nconstructed in low-degree polynomial time w.r.t. the length of the\ninput of the problem. However, since actions can be nondeterministic\nand transitions can share target states, it is not known whether the\nweights are generally computable in polynomial time. Consequently, it\nis an open problem whether minimizing the number of successor states is\nas "easy" as minimizing the number of successor transitions\n', ['stubborn set method', 'combinatorial explosion', 'weighted vertices', 'low-degree polynomial time', 'CSP-equivalence', 'combinatorial mathematics', 'communicating sequential processes']), ('Data storage: re-format. Closely tracking a fast-moving sector\nIn the past few years the data center market has changed dramatically, forcing\nmany companies into consolidation or bankruptcy. Gone are the days when\ncompanies raised millions of dollars to acquire large industrial\nbuildings and transform them into glittering, high-tech palaces filled\nwith the latest telecommunication and data technology. Whereas\nmanufacturers of communication technology deliver the racked equipment\nin these, often mission-critical, facilities, ABB focuses mainly on the\nbuilding infrastructure. Besides the very important redundant power\nsupply, ABB also provides the redundant air conditioning and the\nsecurity system\n', ['building management', 'data centers', 'building infrastructure', 'mission-critical facilities', 'ABB', 'engineering management', 'project management', 'installation', 'commissioning', 'redundant power supply', 'redundant air conditioning', 'security system', 'air conditioning', 'building management systems', 'installation', 'power supplies to apparatus', 'project management', 'security']), ('Adaptive digital watermarking using fuzzy logic techniques\nDigital watermarking has been proposed for copyright protection in our digital\nsociety. We propose an adaptive digital watermarking scheme based on\nthe human visual system model and a fuzzy logic technique. The fuzzy\nlogic approach is employed to obtain the different strengths and\nlengths of a watermark by the local characteristics of the image in our\nproposed scheme. In our experiments, this scheme provides a more robust\nand imperceptible watermark\n', ['adaptive digital watermarking', 'fuzzy logic techniques', 'copyright protection', 'digital society', 'human visual system model', 'local characteristics', 'imperceptible watermark', 'robust watermark', 'image processing', 'copy protection', 'data compression', 'discrete cosine transforms', 'fuzzy logic', 'image coding', 'inference mechanisms']), ('A generalized PERT/CPM implementation in a spreadsheet\nThis paper describes the implementation of the traditional PERT/CPM algorithm\nfor finding the critical path in a project network in a spreadsheet.\nThe problem is of importance due to the recent shift of attention to\nusing the spreadsheet environment as a vehicle for delivering\nmanagement science/operations research (MS/OR) techniques to end-users\n', ['generalized PERT/CPM implementation', 'spreadsheet', 'critical path', 'MS/OR techniques', 'PERT', 'spreadsheet programs']), ('Mathematical model of functioning of an insurance company with allowance for\nadvertising expenses\nA mathematical model of the functioning of an insurance company with allowance\nfor advertising expenses is suggested. The basic characteristics of the\ncapital of the company and the advertising efficiency are examined in\nthe case in which the advertising expenses are proportional to the\ncapital\n', ['insurance company functioning', 'advertising expenses allowance', 'capital', 'mathematical model', 'advertising', 'insurance']), ('When reference works are not books-the new edition of the Guide to Reference\nBooks\nThe author considers the history of the Guide to Reference Books (GRB) and its\nimportance in librarianship. He discusses the ways in which the new\nedition is taking advantage of changing times. GRB has become a\ncornerstone of the literature of U.S. librarianship. The biggest change\nGRB will undergo to become GRS (Guide to Reference Sources) will be\ndesigning it primarily as a Web product\n', ['reference works', 'Guide to Reference Books', 'history', 'librarianship', 'GRB', 'GRS', 'Guide to Reference Sources', 'Web product', 'Internet', 'information resources', 'information science', 'Internet', 'library automation']), ('A transactional asynchronous replication scheme for mobile database systems\nIn mobile database systems, mobility of users has a significant impact on data\nreplication. As a result, the various replica control protocols that\nexist today in traditional distributed and multidatabase environments\nare no longer suitable. To solve this problem, a new mobile database\nreplication scheme, the Transaction-Level Result-Set Propagation\n(TLRSP) model, is put forward in this paper. The conflict detection and\nresolution strategy based on TLRSP is discussed in detail, and the\nimplementation algorithm is proposed. In order to compare the\nperformance of the TLRSP model with that of other mobile replication\nschemes, we have developed a detailed simulation model. Experimental\nresults show that the TLRSP model provides an efficient support for\nreplicated mobile database systems by reducing reprocessing overhead\nand maintaining database consistency\n', ['mobile database', 'data replication', 'distributed database', 'multidatabase', 'mobile database replication', 'Transaction-Level Result-Set Propagation', 'mobile computing', 'conflict reconciliation', 'transaction', 'concurrency control', 'mobile computing', 'replicated databases', 'transaction processing']), ('Rational systems exhibit moderate risk aversion with respect to "gambles" on\nvariable-resolution compression\nIn an embedded wavelet scheme for progressive transmission, a tree structure\nnaturally defines the spatial relationship on the hierarchical pyramid.\nTransform coefficients over each tree correspond to a unique local\nspatial region of the original image, and they can be coded bit-plane\nby bit-plane through successive-approximation quantization. After\nreceiving the approximate value of some coefficients, the decoder can\nobtain a reconstructed image. We show a rational system for progressive\ntransmission that, in absence of a priori knowledge about regions of\ninterest, chooses at any truncation time among alternative trees for\nfurther transmission in such a way as to avoid certain forms of\nbehavioral inconsistency. We prove that some rational transmission\nsystems might exhibit aversion to risk involving "gambles" on\ntree-dependent quality of encoding while others favor taking such\nrisks. Based on an acceptable predictor for visual distinctness from\ndigital imagery, we demonstrate that, without any outside knowledge,\nrisk-prone systems as well as those with strong risk aversion appear in\ncapable of attaining the quality of reconstructions that can be\nachieved with moderate risk-averse behavior\n', ['variable-resolution compression', 'progressive transmission', 'rational system', 'moderate risk aversion', 'embedded wavelet scheme', 'tree structure', 'hierarchical pyramid spatial relationship', 'transform coefficients', 'local spatial region', 'successive-approximation quantization', 'reconstructed image', 'truncation time', 'behavioral inconsistency avoidance', 'gambles', 'image encoding', 'acceptable predictor', 'visual distinctness', 'digital imagery', 'embedded coding', 'rate control optimization', 'decision problem', 'progressive transmission utility functions', 'information theoretic measure', 'decision theory', 'decision trees', 'decoding', 'image coding', 'image reconstruction', 'quantisation (signal)', 'wavelet transforms']), ('Adaptive neural/fuzzy control for interpolated nonlinear systems\nAdaptive control for nonlinear time-varying systems is of both theoretical and\npractical importance. We propose an adaptive control methodology for a\nclass of nonlinear systems with a time-varying structure. This class of\nsystems is composed of interpolations of nonlinear subsystems which are\ninput-output feedback linearizable. Both indirect and direct adaptive\ncontrol methods are developed, where the spatially localized models (in\nthe form of Takagi-Sugeno fuzzy systems or radial basis function neural\nnetworks) are used as online approximators to learn the unknown\ndynamics of the system. Without assumptions on rate of change of system\ndynamics, the proposed adaptive control methods guarantee that all\ninternal signals of the system are bounded and the tracking error is\nasymptotically stable. The performance of the adaptive controller is\ndemonstrated using a jet engine control problem\n', ['adaptive neural/fuzzy control', 'interpolated nonlinear systems', 'time-varying systems', 'input-output feedback linearizable systems', 'indirect control', 'direct control', 'spatially localized models', 'Takagi-Sugeno fuzzy systems', 'radial basis function neural networks', 'online approximators', 'unknown dynamics', 'tracking error', 'jet engine control', 'stability analysis', 'adaptive control', 'aerospace engines', 'asymptotic stability', 'fuzzy control', 'neurocontrollers', 'nonlinear control systems', 'radial basis function networks', 'time-varying systems']), ('Approximation and complexity. II. Iterated integration\nFor pt. I. see ibid., no. 1, p. 289-95 (2001). We introduce two classes of real\nanalytic functions W contained in/implied by U on an interval. Starting\nwith rational functions to construct functions in W we allow the\napplication of three types of operations: addition, integration, and\nmultiplication by a polynomial with rational coefficients. In a similar\nway, to construct functions in U we allow integration, addition, and\nmultiplication of functions already constructed in U and multiplication\nby rational numbers. Thus, U is a subring of the ring of Pfaffian\nfunctions. Two lower bounds on the L/sub infinity /-norm are proved on\na function f from W (or from U, respectively) in terms of the\ncomplexity of constructing f\n', ['real analytic functions', 'rational functions', 'addition', 'integration', 'multiplication', 'polynomial', 'Pfaffian functions', 'lower bounds', 'L/sub infinity /-norm', 'computational complexity', 'polynomials', 'rational functions']), ('The influence of tollbooths on highway traffic\nWe study the effects of tollbooths on the traffic flow. The highway traffic is\nsimulated by the Nagel-Schreckenberg model. Various types of toll\ncollection are examined, which can be characterized either by a waiting\ntime or a reduced speed. A first-order phase transition is observed.\nThe phase separation results a saturated flow, which is observed as a\nplateau region in the fundamental diagram. The effects of lane\nexpansion near the tollbooth are examined. The full capacity of a\nhighway can be restored. The emergence of vehicle queuing is studied.\nBesides the numerical results, we also obtain analytical expressions\nfor various quantities. The numerical simulations can be well described\nby the analytical formulas. We also discuss the influence on the travel\ntime and its variance. The tollbooth increases the travel time but\ndecreases its variance. The differences between long- and\nshort-distance travelers are also discussed\n', ['highway traffic', 'tollbooths', 'Nagel-Schreckenberg model', 'toll collection', 'waiting time', 'reduced speed', 'first-order phase transition', 'saturated flow', 'lane expansion', 'vehicle queuing', 'numerical simulations', 'road traffic']), ('On the discretization of double-bracket flows\nThis paper extends the method of Magnus series to Lie-algebraic equations\noriginating in double-bracket flows. We show that the solution of the\nisospectral flow Y\' = [[Y,N],Y], Y(O) = Y/sub 0/ in Sym(n), can be\nrepresented in the form Y(t) = e/sup Omega (t)/Y/sub 0/e/sup - Omega\n(1)/, where the Taylor expansion of Omega can be constructed\nexplicitly, term-by-term, identifying individual expansion terms with\ncertain rooted trees with bicolor leaves. This approach is extended to\nother Lie-algebraic equations that can be appropriately expressed in\nterms of a finite "alphabet"\n', ['double-bracket flows discretization', 'Magnus series', 'Lie-algebraic equations', 'isospectral flow', 'Taylor expansion', 'bicolor leaves', 'fluid dynamics', 'Lie algebras', 'matrix algebra', 'trees (mathematics)']), ("Pervasive computing goes to work: interfacing to the enterprise\nThe paperless office is an idea whose time has come, and come, and come again.\nTo see how pervasive computing applications might bring some substance\nto this dream, the author spoke recently with key managers and\ntechnologists at McKesson Corporation (San Francisco), a healthcare\nsupplier, service, and technology company with US$50 billion in sales\nlast year, and also at AvantGo (Hayward, Calif.), a provider of mobile\ninfrastructure software and services. For the past several years,\nMcKesson has used mobility middleware developed by AvantGo to deploy\nmajor supply chain applications with thousands of pervasive clients and\nmultiple servers that replace existing paper-based tracking systems.\nAccording to McKesson's managers, their system greatly reduced errors\nand associated costs caused by redelivery or loss of valuable products,\ngiving McKesson a solid return on its investment\n", ['paperless office', 'pervasive clients', 'multiple servers', 'mobile workers', 'enterprise resource planning', 'data warehousing', 'business data processing', 'client-server systems', 'mobile computing', 'user interfaces']), ('Life after bankruptcy [telecom carriers]\nHow comeback telecom carriers are changing industry economics, and why others\nmay have no choice but to follow their lead\n', ['telecom carriers', 'industry economics', 'restructured companies', 'debt levels', 'bankruptcy', 'telecommunication']), ('Anatomy of the coupling query in a Web warehouse\nTo populate a data warehouse specifically designed for Web data, i.e. Web\nwarehouse, it is imperative to harness relevant documents from the Web.\nIn this paper, we describe a query mechanism called coupling query to\nglean relevant Web data in the context of our Web warehousing system\ncalled Warehouse Of Web Data (WHOWEDA). A coupling query may be used\nfor querying both HTML and XML documents. Important features of our\nquery mechanism are the ability to query metadata, content, internal\nand external (hyperlink) structure of Web documents based on partial\nknowledge, ability to express constraints on tag attributes and tagless\nsegment of data, ability to express conjunctive as well as disjunctive\nquery conditions compactly, ability to control execution of a Web query\nand preservation of the topological structure of hyperlinked documents\nin the query results. We also discuss how to formulate a query\ngraphically and in textual form using a coupling graph and coupling\ntext, respectively\n', ['coupling query', 'Web warehouse', 'data warehouse', 'Warehouse Of Web Data', 'HTML documents', 'XML documents', 'metadata', 'content', 'internal structure', 'external structure', 'Web documents', 'partial knowledge', 'tag attributes', 'tagless segment', 'disjunctive query conditions', 'conjunctive query conditions', 'execution control', 'topological structure', 'hyperlinked documents', 'graphical query formulation', 'textual query formulation', 'coupling text', 'data warehouses', 'information resources', 'query formulation']), ('Controller performance analysis with LQG benchmark obtained under closed loop\nconditions\nThis paper proposes a new method for obtaining a linear quadratic Gaussian\n(LQG) benchmark in terms of the variances of process input and output\nfrom closed-loop data, for assessing the controller performance. LQG\nbenchmark has been proposed in the literature to assess controller\nperformance since the LQG tradeoff curve represents the limit of\nperformance in terms of input and output variances. However, an\nexplicit parametric model is required to calculate the LQG benchmark.\nIn this work, we propose a data driven subspace approach to calculate\nthe LQG benchmark under closed-loop conditions with certain external\nexcitations. The optimal LQG-benchmark variances are obtained directly\nfrom the subspace matrices corresponding to the deterministic inputs\nand the stochastic inputs, which are identified using closed-loop data\nwith setpoint excitation. These variances are used for assessing the\ncontroller performance. The method proposed in this paper is applicable\nto both univariate and multivariate systems. Profit analysis for the\nimplementation of feedforward control to the existing feedback-only\ncontrol system is also analyzed under the optimal LQG performance\nframework\n', ['controller performance analysis', 'LQG benchmark', 'linear quadratic Gaussian benchmark', 'closed-loop data', 'subspace matrices', 'deterministic inputs', 'stochastic inputs', 'univariate systems', 'multivariate systems', 'profit analysis', 'feedforward control', 'state space model', 'closed loop systems', 'control system analysis', 'feedforward', 'linear quadratic Gaussian control', 'matrix algebra', 'nonparametric statistics', 'state-space methods']), ('Descriptological foundations of programming\nDescriptological foundations of programming are constructed. An explication of\nthe concept of a descriptive process is given. The operations of\nintroduction and elimination of abstraction at the level of processes\nare refined. An intensional concept of a bipolar function is\nintroduced. An explication of the concept of introduction and\nextraction of abstraction at the bipole level is given. On this basis,\na complete set of descriptological operations is constructed\n', ['descriptological foundations', 'programming', 'descriptive process', 'intensional concept', 'bipolar function', 'bipole level', 'programming']), ('The p-p rearrangement and failure-tolerance of double p-ary multirings and\ngeneralized hypercubes\nIt is shown that an arbitrary grouped p-element permutation can be implemented\nin a conflict-free way through the commutation of channels on the\ndouble p-ary multiring or the double p-ary hypercube. It is revealed\nthat in arbitrary single-element permutations, these commutators\ndisplay the property of the (p-1)-nodal failure-tolerance and the\ngeneralized hypercube displays in addition the property of the\n(p-1)-channel failure-tolerance\n', ['p-p rearrangement', 'failure-tolerance', 'double p-ary multirings', 'generalized hypercubes', 'p-element permutation', 'conflict-free implementation', 'single-element permutations', 'commutators', 'computational complexity', 'fault tolerant computing', 'hypercube networks']), ('Identification of evolving fuzzy rule-based models\nAn approach to identification of evolving fuzzy rule-based (eR) models is\nproposed. eR models implement a method for the noniterative update of\nboth the rule-base structure and parameters by incremental unsupervised\nlearning. The rule-base evolves by adding more informative rules than\nthose that previously formed the model. In addition, existing rules can\nbe replaced with new rules based on ranking using the informative\npotential of the data. In this way, the rule-base structure is\ninherited and updated when new informative data become available,\nrather than being completely retrained. The adaptive nature of these\nevolving rule-based models, in combination with the highly transparent\nand compact form of fuzzy rules, makes them a promising candidate for\nmodeling and control of complex processes, competitive to neural\nnetworks. The approach has been tested on a benchmark problem and on an\nair-conditioning component modeling application using data from an\ninstallation serving a real building. The results illustrate the\nviability and efficiency of the approach\n', ['evolving fuzzy rule-based models', 'identification', 'noniterative update', 'rule-base structure', 'incremental unsupervised learning', 'ranking', 'informative potential', 'fuzzy rules', 'complex processes', 'air-conditioning component modeling', 'adaptive nonlinear control', 'fault detection', 'fault diagnostics', 'performance analysis', 'forecasting', 'knowledge extraction', 'robotics', 'behavior modeling', 'fuzzy logic', 'fuzzy set theory', 'identification', 'modelling', 'unsupervised learning']), ('Antipersistent Markov behavior in foreign exchange markets\nA quantitative check of efficiency in US dollar/Deutsche mark exchange rates is\ndeveloped using high-frequency (tick by tick) data. The antipersistent\nMarkov behavior of log-price fluctuations of given size implies, in\nprinciple, the possibility of a statistical forecast. We introduce and\nmeasure the available information of the quote sequence, and we show\nhow it can be profitable following a particular trading rule\n', ['antipersistent Markov behavior', 'foreign exchange markets', 'efficiency', 'US dollar', 'Deutsche mark', 'exchange rates', 'high-frequency data', 'log-price fluctuations', 'statistical forecast', 'quote sequence', 'trading rule', 'Shannon entropy', 'forecasting', 'entropy', 'fluctuations', 'forecasting theory', 'foreign exchange trading', 'Markov processes', 'nonlinear dynamical systems', 'probability', 'time series']), ('Phase control of higher-order squeezing of a quantum field\nIn a recent experiment [Phys. Rev. Lett. 88 (2002) 023601], phase-dependent\nphoton statistics in a c.w. system has been observed in the mixing of a\ncoherent field with a two-photon source. Their system has the advantage\nover other atomic transition-based fluorescent systems. In this paper,\nwe examine further the squeezing properties of higher-order quantum\nfluctuations in one of the quadrature components of the combined field\nin this system. We demonstrate that efficient and lasting higher-order\nsqueezing effects could be observed with proper choice of the relative\nphase between the pump and coherent fields. This nonclassical feature\nis attributed to a constructive two-photon interference. Relationship\nbetween the second- and higher-order squeezing of the field is\ndiscussed\n', ['phase control', 'higher-order squeezing', 'quantum field', 'phase-dependent photon statistics', 'coherent field mixing', 'atomic transition-based fluorescent systems', 'quantum fluctuations', 'two-photon interference', 'bound states', 'fluctuations', 'optical squeezing', 'phase control', 'quantum theory', 'two-photon processes']), ('The existence condition of gamma -acyclic database schemes with MVDs\nconstraints\nIt is very important to use database technology for a large-scale system such\nas ERP and MIS. A good database design may improve the performance of\nthe system. Some research shows that a gamma -acyclic database scheme\nhas many good properties, e.g., each connected join expression is\nmonotonous, which helps to improve query performance of the database\nsystem. Thus what conditions are needed to generate a gamma -acyclic\ndatabase scheme for a given relational scheme? In this paper, the\nsufficient and necessary condition of the existence of gamma -acyclic,\njoin-lossless and dependencies-preserved database schemes meeting 4NF\nis given\n', ['existence condition', 'database technology', 'large-scale system', 'connected join expression', 'query performance', 'sufficient and necessary condition', 'gamma -acyclic database schemes', 'MVDs constraints', 'database theory', 'multivalued logic']), ("Cutting the cord [wireless health care]\nMore and more healthcare executives are electing to cut the cord to their\nexisting computer systems by implementing mobile technology. The allure\nof information anywhere, anytime is intoxicating, demonstrated by the\ncell phones and personal digital assistants (PDAs) that adorn today's\nprofessionals. The utility and convenience of these devices is\nundeniable. But what is the best strategy for implementing a mobile\nsolution within a healthcare enterprise, be it large or small-and under\nwhat circumstances? What types of healthcare workers benefit most from\nmobile technology? And how state-of-the-art is security for wireless\napplications and devices? These are the questions that healthcare\nexecutives are asking-and should be asking-as they evaluate mobile\nsolutions\n", ['healthcare', 'mobile computing', 'wireless computing', 'security', 'health care', 'mobile computing', 'security of data']), ('Assignment of periods and priorities of messages and tasks in distributed\ncontrol systems\nPresents a task and message-based scheduling method to guarantee the given\nend-to-end constraints including precedence constraints, time\nconstraints, and period and priority of task and message. The method is\nan integrated one considering both tasks executed in each node and\nmessages transmitted via the network and is designed to apply to a\ngeneral distributed control system that has multiple loops and a single\nloop has sensor nodes with multiple sensors, actuator nodes with\nmultiple actuators, controller nodes with multiple tasks, and several\ntypes of constraints. The assigning method of the optimal period and\npriority of task and message is proposed, using the presented task and\nmessage-based scheduling method\n', ['periods assignment', 'priorities assignment', 'message-based scheduling method', 'distributed control systems', 'task-based scheduling method', 'end-to-end constraints', 'precedence constraints', 'time constraints', 'controller area networks', 'distributed control', 'field buses', 'message passing', 'scheduling']), ("NuVox shows staying power with new cash, new market\nWho says you can't raise cash in today's telecom market? NuVox Communications\npositions itself for the long run with $78.5 million in funding and a\nnew credit facility\n", ['telecom', 'competitive carrier market', 'NuVox Communications', 'investors', 'telecommunication']), ('Methods for outlier detection in prediction\nIf a prediction sample is different from the calibration samples, it can be\nconsidered as an outlier in prediction. In this work, two techniques,\nthe use of uncertainty estimation and the convex hull method are\nstudied to detect such prediction outliers. Classical techniques\n(Mahalanobis distance and X-residuals), potential functions and robust\ntechniques are used for comparison. It is concluded that the\ncombination of the convex hull method and uncertainty estimation offers\na practical way for detecting outliers in prediction. By adding the\npotential function method, inliers can also be detected\n', ['outlier detection', 'prediction sample', 'calibration samples', 'uncertainty estimation', 'convex hull method', 'Mahalanobis distance', 'X-residuals', 'potential functions', 'robust techniques', 'inliers', 'calibration', 'chemistry computing', 'uncertainty handling']), ("Fractional differentiation in passive vibration control\nFrom a single-degree-of-freedom model used to illustrate the concept of\nvibration isolation, a method to transform the design for a suspension\ninto a design for a robust controller is presented. Fractional\ndifferentiation is used to model the viscoelastic behaviour of the\nsuspension. The use of fractional differentiation not only permits\noptimisation of just four suspension parameters, showing the\n'compactness' of the fractional derivative operator, but also leads to\nrobustness of the suspension's performance to uncertainty of the sprung\nmass. As an example, an engine suspension is studied\n", ['fractional differentiation', 'passive vibration control', 'vibration isolation', 'suspension', 'robust controller', 'viscoelastic behaviour', 'sprung mass', 'engine suspension', 'damping', 'differentiation', 'engines', 'vibration isolation', 'viscoelasticity']), ('How much should publishers spend on technology?\nA study confirms that spending on publishing-specific information technology\n(IT) resources is growing much faster than IT spending for general\nbusiness activities, at least among leading publishers in the\nscientific, technical and medical (STM) market. The survey asked about\ninformation technology funding and staffing levels-past, present and\nfuture-and also inquired about activities in content management, Web\ndelivery, computer support and customer relationship management. The\nresults provide a starting point for measuring information technology\ngrowth and budget allocations in this publishing segment\n', ['IT spending', 'content management', 'Web delivery', 'publishing', 'budget', 'computer support', 'customer relationship management', 'DP management', 'publishing']), ('Comments on some recent methods for the simultaneous determination of\npolynomial zeros\nIn this note we give some comments on the recent results concerning a\nsimultaneous method of the fourth-order for finding complex zeros in\ncircular interval arithmetic. The main discussion is directed to a\nrediscovered iterative formula and its modification, presented recently\nin Sun and Kosmol, (2001). The presented comments include some critical\nparts of the papers Petkovic, Trickovic, Herceg, (1998) and Sun and\nKosmol, (2001) which treat the same subject\n', ['polynomial', 'zeros', 'complex zeros', 'circular interval arithmetic', 'iterative formula', 'iterative methods', 'poles and zeros', 'polynomials']), ('Integrated support based on task models for the design, evaluation, and\ndocumentation of interactive safety-critical systems: a case study in\nthe air-traffic control domain\nThis paper presents an approach to using task models in both the design and the\nevaluation phases of interactive safety-critical applications. We\nexplain how it is possible to use information contained in task models\nto support the design and development of effective user interfaces.\nMoreover, we show how task models can also support a systematic\ninspection-based usability assessment by examining possible deviations\nthat can occur while users interact with the system, an important issue\nespecially when coping with the peculiar requirements of\nsafety-critical applications. Such evaluation provides useful technical\ndocumentation to help users achieve an in-depth understanding of the\nsystem and its design rationale. Lastly, a description of the\napplication of our approach to a real case study in the air-traffic\ncontrol domain will illustrate the main features of the proposed\nmethod. In particular, we discuss examples taken from an application\nfor air-traffic controllers in an aerodrome supported by graphical user\ninterfaces for data-link communications with pilots\n', ['integrated support', 'user interfaces', 'inspection-based usability assessment', 'technical documentation', 'graphical user interfaces', 'data-link communications', 'task models', 'interactive safety-critical systems', 'air-traffic control domain', 'air traffic control', 'graphical user interfaces', 'safety-critical software', 'system documentation']), ('Numerical behaviour of stable and unstable solitary waves\nIn this paper we analyse the behaviour in time of the numerical approximations\nto solitary wave solutions of the generalized Benjamin-Bona-Mahony\nequation. This equation possesses an important property: the stability\nof these solutions depends on their velocity. We identify the error\npropagation mechanisms in both the stable and unstable case. In\nparticular, we show that in the stable case, numerical methods that\npreserve some conserved quantities of the problem are more appropriate\nfor the simulation of this kind of solutions\n', ['numerical behaviour', 'unstable solitary waves', 'numerical approximations', 'generalized Benjamin-Bona-Mahony equation', 'error propagation mechanisms', 'numerical methods', 'stable solitary waves', 'error analysis', 'numerical analysis', 'solitons']), ('The theory of information reversal\nThe end of the industrial age coincides with the advent of the information\nsociety as the next model of social and economic organization, which\nbrings about significant changes in the way modern man conceives work\nand the social environment. The functional basis of the new model is\npivoted upon the effort to formulate the theory on the violent reversal\nof the basic relationship between man and information, and isolate it\nas one of the components for the creation of the new electronic\nreality. The objective of the theory of reversal is to effectively\ncontribute to the formulation of a new definition consideration in\nregards to the concept of the emerging information society. In order to\nempirically apply the theory of reversal, we examine a case study based\non the example of the digital library\n', ['information reversal theory', 'information society', 'industrial age', 'social organization', 'economic organization', 'case study', 'digital library', 'information systems', 'digital libraries', 'information systems', 'social aspects of automation']), ("Leveraging an alternative source of computer scientists: reentry programs\nMuch has been written about the leaky pipeline of women in computer science\n(CS), with the percentage of women decreasing as one moves from lower\nlevels, such as college, to higher levels, culminating in full\nprofessorship. While significant attention focused on keeping women\nfrom leaving the pipeline, there is also an opportunity to bring women\ninto the pipeline through non-traditional programs, instead of\nrequiring that everyone enter at the undergraduate level. Both Mills\nCollege, a small liberal arts institution for women, and UC Berkeley, a\nlarge research university, established programs in the 80's to increase\nthe number of women in computer science by tapping non-traditional\nstudents. Both programs share the core value of accommodating older\nstudents lacking technical backgrounds. The two programs have produced\nsimilar results: graduate degrees earned in computer science by\nstudents who would not have qualified without these programs,\nprofessional employment in the computer science field by women and\nminorities, and a recognition that this population represents a rich\nsource of talent for our nation\n", ['reentry programs', 'computer science', 'women', 'Mills College', 'UC Berkeley', 'graduate degrees', 'students', 'professional employment', 'minorities', 'computer science education', 'educational courses', 'gender issues']), ('Distributed servers approach for large-scale secure multicast\nIn order to offer backward and forward secrecy for multicast applications\n(i.e., a new member cannot decrypt the multicast data sent before its\njoining and a former member cannot decrypt the data sent after its\nleaving), the data encryption key has to be changed whenever a user\njoins or leaves the system. Such a change has to be made known to all\nthe current users. The bandwidth used for such re-key messaging can be\nhigh when the user pool is large. We propose a distributed servers\napproach to minimize the overall system bandwidth (and complexity) by\nsplitting the user pool into multiple groups each served by a (logical)\nserver. After presenting an analytic model for the system based on a\nhierarchical key tree, we show that there is an optimal number of\nservers to achieve minimum system bandwidth. As the underlying user\ntraffic fluctuates, we propose a simple dynamic scheme with low\noverhead where a physical server adaptively splits and merges its\ntraffic into multiple groups each served by a logical server so as to\nminimize its total bandwidth. Our results show that a distributed\nservers approach is able to substantially reduce the total bandwidth\nrequired as compared with the traditional single-server approach,\nespecially for those applications with a large user pool, short holding\ntime, and relatively low bandwidth of a data stream, as in the Internet\nstock quote applications\n', ['distributed servers', 'large-scale secure multicast', 'backward secrecy', 'forward secrecy', 'multicast applications', 'data encryption key', 're-key messaging', 'system bandwidth', 'system complexity', 'hierarchical key tree', 'user traffic', 'traffic merging', 'short holding time', 'Internet stock quote applications', 'dynamic split-and-merge scheme', 'key management', 'cryptography', 'distributed processing', 'Internet', 'large-scale systems', 'multicast communication', 'stock markets', 'telecommunication traffic']), ('IT security issues: the need for end user oriented research\nConsiderable attention has been given to the technical and policy issues\ninvolved with IT security issues in recent years. The growth of\ne-commerce and the Internet, as well as widely publicized hacker\nattacks, have brought IT security into prominent focus and routine\ncorporate attention. Yet, much more research is needed from the end\nuser (EU) perspective. This position paper is a call for such research\nand outlines some possible directions of interest\n', ['IT security', 'end user oriented research', 'e-commerce', 'Internet', 'hacker attacks', 'information technology research', 'end user computing', 'information technology', 'personal computing', 'security of data']), ('Second term [International Telecommunication Union]\nLater this month Yoshio Utsumi is expected to be re-elected for a second four\nyear term as secretary general of the International Telecommunication\nUnion. Here he talks to Matthew May about getting involved in internet\naddressing, the prospects for 3g, the need for further reform of his\norganisation... and the translating telephone\n', ['International Telecommunication Union', 'internet addressing', 'translating telephone', '3G', 'telecommunication']), ('Full-screen ultrafast video modes over-clocked by simple VESA routines and\nregisters reprogramming under MS-DOS\nFast full-screen presentation of stimuli is necessary in psychological\nresearch. Although Spitczok von Brisinski (1994) introduced a method\nthat achieved ultrafast display by reprogramming the registers, he\ncould not produce an acceptable full-screen display. In this report,\nthe author introduces a new method combining VESA routine calling with\nregister reprogramming that can yield a display at 640 * 480\nresolution, with a refresh rate of about 150 Hz\n', ['full-screen ultrafast video modes', 'fast full-screen stimuli presentation', 'psychological research', 'VESA routine calling', 'MS-DOS', 'register reprogramming', 'computer displays', 'psychology', 'video signal processing']), ('In search of strategic operations research/management science\nWe define strategic OR/MS as "OR/MS work that leads to a sustainable\ncompetitive advantage." We found evidence of strategic OR/MS in the\nliterature of strategic information systems (SIS) and OR/MS. We\nexamined 30 early examples of SIS, many of which contained OR/MS work.\nMany of the most successful had high OR/MS content, while the least\nsuccessful contained none. The inclusion of OR/MS work may be a key to\nsustaining an advantage from information technology. We also examined\nthe Edelman Prize finalist articles published between 1990 and 1999. We\nfound that 13 of the 42 private sector applications meet our definition\nof strategic OR/MS\n', ['operations research', 'management science', 'strategic OR/MS', 'strategic information systems', 'SIS', 'information systems', 'management science', 'operations research']), ('Solution of a Euclidean combinatorial optimization problem by the\ndynamic-programming method\nA class of Euclidean combinatorial optimization problems is selected that can\nbe solved by the dynamic programming method. The problem of allocation\nof servicing enterprises is solved as an example\n', ['Euclidean combinatorial optimization problem', 'dynamic programming method', 'computational geometry', 'dynamic programming', 'optimisation']), ("Efficient simplicial reconstructions of manifolds from their samples\nAn algorithm for manifold learning is presented. Given only samples of a\nfinite-dimensional differentiable manifold and no a priori knowledge of\nthe manifold's geometry or topology except for its dimension, the goal\nis to find a description of the manifold. The learned manifold must\napproximate the true manifold well, both geometrically and\ntopologically, when the sampling density is sufficiently high. The\nproposed algorithm constructs a simplicial complex based on\napproximations to the tangent bundle of the manifold. An important\nproperty of the algorithm is that its complexity depends on the\ndimension of the manifold, rather than that of the embedding space.\nSuccessful examples are presented in the cases of learning curves in\nthe plane, curves in space, and surfaces in space; in addition, a case\nwhen the algorithm fails is analyzed\n", ['simplicial reconstructions', 'manifold learning', 'finite-dimensional differentiable manifold', 'learned manifold', 'true manifold', 'simplicial complex', 'sampling density', 'computational complexity', 'computational geometry', 'computer vision', 'Hilbert spaces', 'learning (artificial intelligence)', 'topology']), ('A spatial rainfall simulator for crop production modeling in Southern Africa\nThis paper describes a methodology for simulating rainfall in dekads across a\nset of spatial units in areas where long-term meteorological records\nare available for a small number of sites only. The work forms part of\na larger simulation model of the food system in a district of Zimbabwe,\nwhich includes a crop production component for yields of maize, small\ngrains and groundnuts. Only a limited number of meteorological stations\nare available within or surrounding the district that have long time\nseries of rainfall records. Preliminary analysis of rainfall data for\nthese stations suggested that intra-seasonal temporal correlation was\nnegligible, but that rainfall at any given station was correlated with\nrainfall at neighbouring stations. This spatial correlation structure\ncan be modeled using a multivariate normal distribution consisting of\n30 related variables, representing dekadly rainfall in each of the 30\nwards. For each ward, log-transformed rainfall for each of the 36\ndekads in the year was characterized by a mean and standard deviation,\nwhich were interpolated from surrounding meteorological stations. A\ncovariance matrix derived from a distance measure was then used to\nrepresent the spatial correlation between wards. Sets of random numbers\nwere then drawn from this distribution to simulate rainfall across the\nwards in any given dekad. Cross-validation of estimated rainfall\nparameters against observed parameters for the one meteorological\nstation within the district suggests that the interpolation process\nworks well. The methodology developed is useful in situations where\nlong-term climatic records are scarce and where rainfall shows\npronounced spatial correlation, but negligible temporal correlation\n', ['simulating rainfall', 'crop production modeling', 'Zimbabwe', 'covariance matrix', 'rainfall records', 'rainfall data', 'spatial correlation', 'multivariate normal distribution', 'parameter estimation', 'Southern Africa', 'agriculture', 'modelling', 'parameter estimation', 'rain', 'weather forecasting']), ('Reconstructing surfaces by volumetric regularization using radial basis\nfunctions\nWe present a new method of surface reconstruction that generates smooth and\nseamless models from sparse, noisy, nonuniform, and low resolution\nrange data. Data acquisition techniques from computer vision, such as\nstereo range images and space carving, produce 3D point sets that are\nimprecise and nonuniform when compared to laser or optical range\nscanners. Traditional reconstruction algorithms designed for dense and\nprecise data do not produce smooth reconstructions when applied to\nvision-based data sets. Our method constructs a 3D implicit surface,\nformulated as a sum of weighted radial basis functions. We achieve\nthree primary advantages over existing algorithms: (1) the implicit\nfunctions we construct estimate the surface well in regions where there\nis little data, (2) the reconstructed surface is insensitive to noise\nin data acquisition because we can allow the surface to approximate,\nrather than exactly interpolate, the data, and (3) the reconstructed\nsurface is locally detailed, yet globally smooth, because we use radial\nbasis functions that achieve multiple orders of smoothness\n', ['surfaces reconstruction', 'volumetric regularization', 'radial basis functions', 'sparse range data', 'noisy data', 'nonuniform data', 'low resolution range data', 'data acquisition techniques', 'computer vision', 'stereo range images', 'space carving', '3D point sets', 'vision-based data sets', '3D implicit surface', 'weighted radial basis functions', 'computer vision', 'image reconstruction', 'image texture', 'interpolation', 'surface fitting']), ('Exact frequency-domain reconstruction for thermoacoustic tomography. I. Planar\ngeometry\nWe report an exact and fast Fourier-domain reconstruction algorithm for\nthermoacoustic tomography in a planar configuration assuming thermal\nconfinement and constant acoustic speed. The effects of the finite size\nof the detector and the finite length of the excitation pulse are\nexplicitly included in the reconstruction algorithm. The algorithm is\nnumerically and experimentally verified. We also demonstrate that the\nblurring caused by the finite size of the detector surface is the\nprimary limiting factor on the resolution and that it can be\ncompensated for by deconvolution\n', ['medical diagnostic imaging', 'exact frequency-domain reconstruction', 'planar configuration', 'thermal confinement', 'constant acoustic speed', 'blurring', 'finite detector surface size', 'primary limiting factor', 'deconvolution', 'resolution limitation', 'excitation pulse', 'reconstruction algorithm', 'thermoacoustic tomography', 'planar geometry', 'acoustic tomography', 'biomedical ultrasonics', 'image reconstruction', 'medical image processing', 'thermoacoustics']), ('Understanding Internet traffic streams: dragonflies and tortoises\nWe present the concept of network traffic streams and the ways they aggregate\ninto flows through Internet links. We describe a method of measuring\nthe size and lifetime of Internet streams, and use this method to\ncharacterize traffic distributions at two different sites. We find that\nalthough most streams (about 45 percent of them) are dragonflies,\nlasting less than 2 seconds, a significant number of streams have\nlifetimes of hours to days, and can carry a high proportion (50-60\npercent) of the total bytes on a given link. We define tortoises as\nstreams that last longer than 15 minutes. We point out that streams can\nbe classified not only by lifetime (dragonflies and tortoises) but also\nby size (mice and elephants), and note that stream size and lifetime\nare independent dimensions. We submit that ISPs need to be aware of the\ndistribution of Internet stream sizes, and the impact of the difference\nin behavior between short and long streams. In particular, any\nforwarding cache mechanisms in Internet routers must be able to cope\nwith a high volume of short streams. In addition ISPs should realize\nthat long-running streams can contribute a significant fraction of\ntheir packet and byte volumes-something they may not have allowed for\nwhen using traditional "flat rate user bandwidth consumption"\napproaches to provisioning and engineering\n', ['Internet traffic streams', 'dragonflies', 'tortoises', 'network traffic streams', 'Internet stream size measurement', 'Internet stream lifetime measurement', 'traffic distributions', 'mice', 'elephants', 'ISP', 'forwarding cache mechanisms', 'Internet routers', 'long-running streams', 'packet volume', 'byte volume', 'traffic provisioning', 'traffic engineering', 'Internet', 'packet switching', 'performance evaluation', 'telecommunication network routing', 'telecommunication traffic']), ('Where have all the PC makers gone?\nPC makers are dwindling. If you are planning to make a PC purchase soon, here\nare a few things to look out for before you buy\n', ['PC purchase', 'PC makers', 'DP industry', 'microcomputers']), ('Creating the right mail model\nIf you know your post room is not as efficiently organised as it might be, but\nyou are not sure how best to go about making improvements, then\nconsider this advice from John Edgar of consultant MCS\n', ['mail', 'post room', 'MCS', 'consultant', 'mailing systems']), ("Computing 2002: democracy, education, and the future\nComputer scientists, computer engineers, information technologists, and their\ncollective products have grown and changed in quantity, quality, and\nnature. In the first decade of this new century, it should become\napparent to everyone that the computing and information fields, broadly\ndefined, will have a profound impact on every element of every person's\nlife. The author considers how women and girls of the world have been\nneither educated for computing nor served by computing. Globally,\nwomen's participation in computer science grew for a while, then\ndropped precipitously. Computing, science, engineering, and society\nwill suffer if this decline continues, because women have different\nperspectives on technology, what it is important for, how it should be\nbuilt, which projects should be funded, and so on. To create a positive\nfuture, to assure that women equally influence the future, computing\neducation must change\n", ['computer science education', 'gender issues', 'future', 'women', 'girls', 'society', 'democracy', 'computer science education', 'gender issues', 'social aspects of automation']), ('Survey says! [online world of polls and surveys]\nMany content managers miss the fundamental interactivity of the Web by not\nusing polls and surveys. Using interactive features-like a poll or\nquiz-offers your readers an opportunity to become more engaged in your\ncontent. Using a survey to gather feedback about your content provides\ncost-effective data to help make modifications or plot the appropriate\ncourse of action. The Web has allowed us to take traditional market\nresearch and turn it on its ear. Surveys and polls can be conducted\nfaster and cheaper than with telephone and mail. But if you are running\na Web site, should you care about polls and surveys? Do you know the\ndifference between the two in Web-speak?\n', ['site owners', 'polls', 'surveys', 'content managers', 'World Wide Web', 'site feedback', 'information resources']), ('All-optical XOR gate using semiconductor optical amplifiers without additional\ninput beam\nThe novel design of an all-optical XOR gate by using cross-gain modulation of\nsemiconductor optical amplifiers has been suggested and demonstrated\nsuccessfully at 10 Gb/s. Boolean AB and AB of the two input signals A\nand B have been obtained and combined to achieve the all-optical XOR\ngate. No additional input beam such as a clock signal or continuous\nwave light is used in this new design, which is required in other\nall-optical XOR gates\n', ['semiconductor optical amplifiers', 'all-optical-XOR gate', 'design', 'cross-gain modulation', 'Boolean logic', '10 Gbit/s', 'optical design techniques', 'optical logic', 'optical modulation', 'semiconductor optical amplifiers']), ('A parareal in time procedure for the control of partial differential equations\nWe have proposed in a previous note a time discretization for partial\ndifferential evolution equation that allows for parallel\nimplementations. This scheme is here reinterpreted as a preconditioning\nprocedure on an algebraic setting of the time discretization. This\nallows for extending the parallel methodology to the problem of optimal\ncontrol for partial differential equations. We report a first numerical\nimplementation that reveals a large interest\n', ['time procedure', 'partial differential equation control', 'evolution equation', 'preconditioning procedure', 'Hilbert space', 'algebraic setting', 'time discretization', 'optimal control', 'Hilbert spaces', 'optimal control', 'partial differential equations']), ("Female computer science doctorates: what does the survey of earned doctorates\nreveal?\nBased on the National Center for Education Statistics (2000), in the 1997-1998\nacademic year 26.7% of earned bachelors' degrees, 29.0% of earned\nmasters' degrees and 16.3% of earned doctorates' degrees in computer\nscience were awarded to women. As these percentages suggest, women are\nunderrepresented at all academic levels in computer science (Camp,\n1997). The most severe shortage occurs at the top level-the doctorate\nin computer science. We know very little about the women who persist to\nthe top level of academic achievement in computer science. This paper\nexamines a subset of data collected through the Survey of Earned\nDoctorates (SED). The specific focus of this paper is to identify\ntrends that have emerged from the SED with respect to females\ncompleting doctorates in computer science between the academic years\n1990-1991 and 1999-2000. Although computer science doctorates include\ndoctorates in information science, prior research (Camp, 1997) suggests\nthat the percentage of women completing doctorates in information\nscience as compared to computer science is low. The specific research\nquestions are: 1. How does the percentage of women who complete\ndoctorates in computer science compare to those that complete\ndoctorates in other fields? 2. How does the length of time in school\nand the sources of funding differ for females as compared to males who\ncomplete doctorates in computer science? 3. Where do women go after\ncompleting doctorates in computer science and what positions do they\nacquire? How do these experiences differ from their male peers?\n", ['female computer science doctorates', 'Survey of Earned Doctorates', 'information science', 'computer science education', 'gender issues', 'information science']), ('Information architecture: looking ahead\nIt may be a bit strange to consider where the field of information architecture\n(IA) is headed. After all, many would argue that it\'s too new to be\nconsidered as a field at all, or that it is mislabeled, and by no means\nis there a widely accepted definition of what information architecture\nactually is. Practicing information architects probably number in the\nthousands, and this vibrant group is already building various forms of\ncommunal infrastructure, ranging from an IA journal and a\nself-organizing "library" of resources to a passel of local\nprofessional groups and degree-granting academic programs. So the\nprofession has achieved a beachhead that will enable it to stabilize\nand perhaps even grow during these difficult times\n', ['information architecture', 'information architects', 'communal infrastructure', 'local professional groups', 'degree-granting academic programs', 'electronic publishing', 'hypermedia', 'information resources']), ('Input-output based pole-placement controller for a class of time-delay systems\nA controller structure valid for SISO plants involving both internal and\nexternal point delays is presented. The control signal is based only on\nthe input and output plant signals. The controller allows finite or\ninfinite spectrum assignment. The most important feature of the\nproposed controller is that it only involves the use of a class of\npoint-delayed signals. Thus the controller synthesis involves less\ncomputational cost than former methods. Since the plant control input\nis generated by filtering the input and output plant signals, this\ncontroller structure is potentially applicable to the adaptive case of\nunknown plant parameters\n', ['I/O-based pole-placement controller', 'input-output based pole-placement controller', 'time-delay systems', 'SISO plants', 'internal point delays', 'and external point delays', 'finite spectrum assignment', 'infinite spectrum assignment', 'point-delayed signals', 'controller synthesis', 'computational cost', 'filtering', 'adaptive control', 'computational complexity', 'control system synthesis', 'delay systems', 'filtering theory', 'pole assignment', 'uncertain systems']), ('Shortchanging the future of information technology: the untapped resource\nBuilding on ideas from a virtual workshop and additional input from the\nscientific community, the CISE Directorate at the National Science\nFoundation established the Information Technology Workforce Program\n(ITWF) in March 2000 to support a broad set of scientific research\nstudies focused on the under-representation of women and minorities in\nthe information technology workforce. In this paper, we explore various\napproaches that the funded researchers are taking to address the\nproblem of women in information technology. We begin with a brief\nhistory of the ITWF, and then focus on some of the research projects in\nterms of their goals, approaches, and expected outcomes\n', ['information technology future', 'untapped resources', 'virtual workshop', 'CISE Directorate', 'National Science Foundation', 'Information Technology Workforce Program', 'ITWF', 'scientific research studies', 'women under-representation', 'history', 'computer science', 'gender issues', 'history', 'information technology', 'personnel']), ('A pretopological approach for structural analysis\nThe aim of this paper is to present a methodological approach for problems\nencountered in structural analysis. This approach is based upon the\npretopological concepts of pseudoclosure and minimal closed subsets.\nThe advantage of this approach is that it provides a framework which is\ngeneral enough to model and formulate different types of connections\nthat exist between the elements of a population. In addition, it has\nenabled us to develop a new structural analysis algorithm. An\nexplanation of the definitions and properties of the pretopological\nconcepts applied in this work is first shown and illustrated in sample\nsettings. The structural analysis algorithm is then described and the\nresults obtained in an economic study of the impact of geographic\nproximity on scientific collaborations are presented\n', ['pretopological approach', 'structural analysis', 'minimal closed subsets', 'pseudoclosure', 'connections', 'economic study', 'geographic proximity', 'scientific collaborations', 'set theory', 'topology']), ('A characterization of generalized Pareto distributions by progressive censoring\nschemes and goodness-of-fit tests\nIn this paper we generalize a characterization property of generalized Pareto\ndistributions, which is known for ordinary order statistics, to\narbitrary schemes of progressive type-II censored order statistics.\nVarious goodness-of-fit tests for generalized Pareto distributions\nbased on progressively censored data statistics are discussed\n', ['generalized Pareto distributions', 'progressive censoring schemes', 'goodness-of-fit tests', 'progressive type-II censored order statistics', 'ordinary order statistics', 'Pareto distribution']), ('Knowledge flow management for distributed team software development\nCognitive cooperation is often neglected in current team software development\nprocesses. This issue becomes more important than ever when team\nmembers are globally distributed. This paper presents a notion of\nknowledge flow and the related management mechanism for realizing an\nordered knowledge sharing and cognitive cooperation in a geographically\ndistributed team software development process. The knowledge flow can\ncarry and accumulate knowledge when it goes through from one team\nmember to another. The coordination between the knowledge flow process\nand the workflow process of a development team provides a new way to\nimprove traditional team software development processes. A knowledge\ngrid platform has been implemented to support the knowledge flow\nmanagement across the Internet\n', ['knowledge flow management', 'distributed team software development', 'cognitive cooperation', 'knowledge flow representation', 'ordered knowledge sharing', 'workflow process', 'knowledge grid platform', 'Internet', 'software development management', 'cooperative work', 'groupware', 'Internet', 'knowledge representation', 'software development management', 'workflow management software']), ('Designing and delivering a university course - a process (or operations)\nmanagement perspective\nWith over 30 years of academic experience in both engineering and management\nfaculties, involving trial and error experimentation in teaching as\nwell as reading relevant literature and observing other instructors in\naction, the author has accumulated a number of ideas, regarding the\npreparation and delivery of a university course, that should be of\ninterest to other instructors. This should be particularly the case for\nthose individuals who have had little or no teaching experience (e.g.\nthose whose graduate education was recently completed at\nresearch-oriented institutions providing little guidance with respect\nto teaching). A particular perspective is used to convey the ideas,\nnamely one of viewing the preparation and delivery of a course as two\nmajor processes that should provide outputs or outcomes that are of\nvalue to a number of customers, in particular, students\n', ['university course delivery', 'management perspective', 'academic experience', 'management faculties', 'engineering faculties', 'search-oriented institutions', 'educational courses']), ('On M/D/1 queue with deterministic server vacations\nWe study a single server vacation queue with Poisson arrivals, deterministic\nservice of constant duration b (> 0) and deterministic vacations of\nconstant duration d (> 0) and designate this model as M/D/D/1. After\ncompletion of each service, the server may take a vacation with\nprobability p or may continue working in the system with probability 1\n- p. We obtain time-dependent as well as steady state probability\ngeneration functions for the number in the system. For the steady state\nwe obtain explicitly the mean number and the mean waiting time for the\nsystem and for the queue. All known results of the M/D/1 queue are\nderived as a special case. Finally, a numerical illustration is\ndiscussed\n', ['M/D/1 queue', 'deterministic server vacations', 'Poisson arrivals', 'deterministic service', 'deterministic vacations', 'M/D/D/1 model', 'time-dependent probability generation functions', 'steady state probability generation functions', 'mean number', 'mean waiting time', 'probability', 'queueing theory', 'stochastic processes']), ('Noninvasive myocardial activation time imaging: a novel inverse algorithm\napplied to clinical ECG mapping data\nLinear approaches like the minimum-norm least-square algorithm show\ninsufficient performance when it comes to estimating the activation\ntime map on the surface of the heart from electrocardiographic (ECG)\nmapping data. Additional regularization has to be considered leading to\na nonlinear problem formulation. The Gauss-Newton approach is one of\nthe standard mathematical tools capable of solving this kind of\nproblem. To our experience, this algorithm has specific drawbacks which\nare caused by the applied regularization procedure. In particular,\nunder clinical conditions the amount of regularization cannot be\ndetermined clearly. For this reason, we have developed an iterative\nalgorithm solving this nonlinear problem by a sequence of regularized\nlinear problems. At each step of iteration, an individual L-curve is\ncomputed. Subsequent iteration steps are performed with the individual\noptimal regularization parameter. This novel approach is compared with\nthe standard Gauss-Newton approach. Both methods are applied to\nsimulated ECG mapping data as well as to single beat sinus rhythm data\nfrom two patients recorded in the catheter laboratory. The proposed\napproach shows excellent numerical and computational performance, even\nunder clinical conditions at which the Gauss-Newton approach begins to\nbreak down\n', ['noninvasive myocardial activation time imaging', 'electrodiagnostics', 'activation time imaging', 'L-curve method', 'noninvasive electrocardiography', 'tikhonov regularization', 'Gauss-Newton approach', 'individual optimal regularization parameter', 'catheter laboratory', 'clinical conditions', 'iteration steps', 'heart surface', 'regularization procedure', 'inverse algorithm', 'clinical ECG mapping data', 'electrocardiography', 'inverse problems', 'iterative methods', 'medical image processing', 'muscle']), ('Single and multi-interval Legendre tau -methods in time for parabolic equations\nIn this paper, we take the parabolic equation with periodic boundary conditions\nas a model to present a spectral method with the Fourier approximation\nin spatial and single/multi-interval Legendre Petrov-Galerkin methods\nin time. For the single interval spectral method in time, we obtain the\noptimal error estimate in L/sup 2/-norm. For the multi-interval\nspectral method in time, the L/sup 2/-optimal error estimate is valid\nin spatial. Numerical results show the efficiency of the methods\n', ['interval decomposition', 'parabolic equation', 'interval spectral method', 'error analysis', 'optimal error estimation', 'periodic boundary conditions', 'Fourier approximation', 'Legendre Petrov-Galerkin method', 'partial differential equations', 'approximation theory', 'boundary-value problems', 'convergence of numerical methods', 'error analysis', 'optimisation', 'parabolic equations', 'partial differential equations']), ("SubSeven's Honey Pot program\nA serious security threat today are malicious executables, especially new,\nunseen malicious executables often arriving as email attachments. These\nnew malicious executables are created at the rate of thousands every\nyear and pose a serious threat. Current anti-virus systems attempt to\ndetect these new malicious programs with heuristics generated by hand.\nThis approach is costly and often ineffective. We introduce the Trojan\nHorse SubSeven, its capabilities and influence over intrusion detection\nsystems. A Honey Pot program is implemented, simulating the SubSeven\nServer. The Honey Pot Program provides feedback and stores data to and\nfrom the SubSeven's client\n", ['Honey Pot program', 'security threat', 'malicious executables', 'email attachments', 'anti-virus systems', 'Trojan Horse', 'SubSeven', 'intrusion detection systems', 'computer viruses', 'electronic mail']), ("Is open source more or less secure?\nNetworks dominate today's computing landscape and commercial technical\nprotection is lagging behind attack technology. As a result, protection\nprogramme success depends more on prudent management decisions than on\nthe selection of technical safeguards. The paper takes a management\nview of protection and seeks to reconcile the need for security with\nthe limitations of technology\n", ['open source software security', 'computer networks', 'commercial technical protection', 'attack technology', 'management', 'data security', 'computer networks', 'public domain software', 'security of data']), ('Option pricing from path integral for non-Gaussian fluctuations. Natural\nmartingale and application to truncated Levy distributions\nWithin a path integral formalism for non-Gaussian price fluctuations, we set up\na simple stochastic calculus and derive a natural martingale for option\npricing from the wealth balance of options, stocks, and bonds. The\nresulting formula is evaluated for truncated Levy distributions\n', ['option pricing', 'path integrals', 'stochastic calculus', 'stocks', 'bonds', 'nonGaussian fluctuations', 'natural martingale', 'truncated Levy distributions', 'fluctuations', 'integration', 'statistical mechanics', 'stochastic processes', 'stock markets']), ("A decision support model for selecting product/service benefit positionings\nThe art (and science) of successful product/service positioning generally\nhinges on the firm's ability to select a set of attractively priced\nconsumer benefits that are: valued by the buyer, distinctive in one or\nmore respects, believable, deliverable, and sustainable (under actual\nor potential competitive abilities to imitate, neutralize, or overcome)\nin the target markets that the firm selects. For many years, the\nubiquitous quadrant chart has been used to provide a simple graph of\nproduct/service benefits (usually called product/service attributes)\ndescribed in terms of consumers' perceptions of the importance of\nattributes (to brand/supplier choice) and the performance of competing\nfirms on these attributes. This paper describes a model that extends\nthe quadrant chart concept to a decision support system that optimizes\na firm's market share for a specified product/service. In particular,\nwe describe a decision support model that utilizes relatively simple\nmarketing research data on consumers' judged benefit importances, and\nsupplier performances on these benefits to develop message components\nfor specified target buyers. A case study is used to illustrate the\nmodel. The study deals with developing advertising message components\nfor a relatively new entrant in the US air shipping market. We also\ndiscuss, more briefly, management reactions to application of the model\nto date, and areas for further research and model extension\n", ['product/service benefit positionings', 'decision support model', 'attractively priced consumer benefits', 'quadrant chart', 'simple graph', 'product/service attributes', 'brand/supplier choice', 'market share optimization', 'marketing research data', 'consumer judged benefit importances', 'message components', 'advertising message components', 'US air shipping market', 'management reactions', 'advertising', 'greedy heuristic', 'optimal message design', 'advertising', 'decision support systems', 'marketing', 'sensitivity analysis']), ("Simple...But complex\nFlexPro 5.0, from Weisang and Co., is one of those products which aim to serve\nan often ignored range of data users: those who, in FlexPro's words,\nare interested in documenting, analysing and archiving data in the\nsimplest way possible. The online help system is clearly designed to\npromote the product in this market segment, with a very clear\nintroduction from first principles and a hands-on tutorial, and the\nlive project to which it was applied was selected with this in mind\n", ['FlexPro 5.0', 'data archiving', 'data analysis', 'data documentation', 'online help system', 'hands-on tutorial', 'data analysis', 'software reviews', 'statistical databases']), ('Optimization of the characteristics of computational processes in scalable\nresources\nThe scalableness of resources is taken to mean the possibility of the prior\nchange in the obtained dynamic characteristics of computational\nprocesses for a certain basic set of processors and the communication\nmedium in an effort to optimize the dynamics of software applications.\nA method is put forward for the generation of optimal strategies-a set\nof the versions of the fulfillment of programs on the basis of a vector\ncriterion. The method is urgent for the effective use of resources of\ncomputational clusters and metacomputational media and also for dynamic\ncontrol of processes in real time on the basis of the static scaling\n', ['computational processes', 'scalable resources', 'dynamic characteristics', 'communication medium', 'software applications', 'optimal strategies', 'vector criterion', 'computational clusters', 'metacomputational media', 'dynamic control', 'static scaling', 'distributed processing', 'optimisation', 'resource allocation']), ('Application of hybrid models for prediction and optimization of enzyme\nfermentation process. A comparative study\nThe paper presents a comparison of the biotechnological process prediction and\noptimization results obtained by using different structure hybrid\nmathematical models for modeling of the same bioprocess. The hybrid\nmodels under investigation consist of the product mass balance equation\nin which different means - an artificial neural network, fuzzy-neural\nnetwork and cell age distribution based calculation scheme - are\nincorporated for modeling the specific biosynthesis rate of a desired\nproduct. Experimental data from alpha -amylase laboratory and\nindustrial fermentation processes are used for model parameter\nidentification and the process prediction tests\n', ['mathematical models', 'bioprocess', 'hybrid models', 'product mass balance equation', 'fuzzy-neural network', 'cell age distribution', 'biosynthesis rate', 'enzyme fermentation', 'industrial processes', 'optimization', 'identification', 'biocontrol', 'fermentation', 'fuzzy neural nets', 'identification', 'optimisation', 'process control', 'proteins']), ('Weighted energy linear quadratic regulator vibration control of piezoelectric\ncomposite plates\nIn this paper on finite element linear quadratic regulator (LQR) vibration\ncontrol of smart piezoelectric composite plates, we propose the use of\nthe total weighted energy method to select the weighting matrices. By\nconstructing the optimal performance function as a relative measure of\nthe total kinetic energy, strain energy and input energy of the system,\nonly three design variables need to be considered to achieve a balance\nbetween the desired higher damping effect and lower input cost. Modal\ncontrol analysis is used to interpret the effects of three energy\nweight factors on the damping ratios and modal voltages and it is shown\nthat the modal damping effect will increase with the kinetic energy\nweight factor, approaching square root (2/2) as the strain energy\nweight factor increases and decrease with the input energy weight\nfactor. Numerical results agree well with those from the modal control\nanalysis. Since the control problem is simplified to three design\nvariables only, the computational cost will be greatly reduced and a\nmore accurate structural control analysis becomes more attractive for\nlarge systems\n', ['finite element linear quadratic regulator', 'vibration control', 'smart piezoelectric composite plates', 'total weighted energy', 'weighting matrices', 'optimal performance function', 'total kinetic energy', 'strain energy', 'damping effect', 'modal control analysis', 'damping ratios', 'strain energy weight factor', 'numerical results', 'computational cost', 'structural control analysis', 'composite materials', 'control system analysis computing', 'feedback', 'finite element analysis', 'intelligent actuators', 'intelligent control', 'intelligent structures', 'optimal control', 'piezoelectric actuators', 'structural engineering computing', 'vibration control']), ('An eight-year study of Internet-based remote medical counselling\nWe carried out a prospective study of an Internet-based remote counselling\nservice. A total of 15,456 Internet users visited the Web site over\neight years. From these, 1500 users were randomly selected for\nanalysis. Medical counselling had been granted to 901 of the people\nrequesting it (60%). One hundred and sixty-four physicians formed\nproject groups to process the requests and responded using email. The\ndistribution of patients using the service was similar to the\navailability of the Internet: 78% were from the European Union, North\nAmerica and Australia. Sixty-seven per cent of the patients lived in\nurban areas and the remainder were residents of remote rural areas with\nlimited local medical coverage. Sixty-five per cent of the requests\nwere about problems of internal medicine and 30% of the requests\nconcerned surgical issues. The remaining 5% of the patients sought\ninformation about recent developments, such as molecular medicine or\naviation medicine. During the project, our portal became inaccessible\nfive times, and counselling was not possible on 44 days. There was no\nhacking of the Web site. Internet-based medical counselling is a\nhelpful addition to conventional practice\n', ['Internet-based remote medical counselling', 'Internet users', 'Web site', 'email', 'urban areas', 'remote rural areas', 'surgical issues', 'telemedicine', 'medical education', 'portal', 'biomedical education', 'educational computing', 'electronic mail', 'information resources', 'Internet', 'telemedicine']), ('Modeling discourse in collaborative work support systems: a knowledge\nrepresentation and configuration perspective\nCollaborative work processes usually raise a lot of intricate debates and\nnegotiations among participants, whereas conflicts of interest are\ninevitable and support for achieving consensus and compromise is\nrequired. Individual contributions, brought up by parties with\ndifferent backgrounds and interests, need to be appropriately\nstructured and maintained. This paper presents a model of discourse\nacts that participants use to communicate their attitudes to each\nother, or affect the attitudes of others, in such environments. The\nfirst part deals with the knowledge representation and communication\naspects of the problem, while the second one, in the context of an\nalready implemented system, namely HERMES, with issues related to the\nconfiguration of the contributions asserted at each discourse instance.\nThe overall work focuses on the machinery needed in a computer-assisted\ncollaborative work environment, the aim being to further enhance the\nhuman-computer interaction\n', ['discourse modeling', 'collaborative work support systems', 'knowledge representation', 'conflicts of interest', 'consensus', 'compromise', 'knowledge communication', 'HERMES', 'human-computer interaction', 'business data processing', 'groupware', 'knowledge representation', 'user modelling']), ('Priming the pipeline [women in computer science careers]\nIn 1997 The Backyard Project, a pilot program of the Garnett Foundation, was\ninstituted to encourage high school girls to explore careers in the\ncomputer industry. At that time, the Garnett Foundation commissioned\nthe Global Strategy Group to execute a survey of 652 college-bound high\nschool students (grades 9 through 12), to help discover directions that\nThe Backyard Project might take to try to move toward the mission of\nthe pilot program. It conducted the study by telephone between March 25\nand April 8, 1997 in the Silicon Valley, Boston, and Austin\nmetropolitan areas. It conducted all interviews using a random digit\ndialing methodology, derived from a file of American households with\nhigh incidences of adolescent children. The top six answers from girls\nto the survey question "why are girls less likely to pursue computer\nscience careers?" in order of perceived importance by the girls were:\nnot enough role models; women have other interests; didn\'t know about\nthe industry; limited opportunity; negative media; and too nerdy. These\nresponses are discussed\n', ['The Backyard Project', 'high school girls', 'computer industry careers', 'college-bound high school students', 'computer science education', 'DP industry', 'employment', 'gender issues']), ("Applying BGL to computational geometry\nThe author applies Boost Graph Library to the domain of computational geometry.\nFirst, he formulates a concrete problem in graph terms. Second, he\ndevelops a way to transform the output of an existing algorithm into an\nappropriate Boost Graph Library data structure. Finally, he implements\ntwo new algorithms for my Boost Graph Library graph. The first\nalgorithm gets the job done, but could have been written in any\nprogramming language. The second algorithm, however, shows the power of\nBoost Graph Library's generic programming approach.Graphs, graphics,\nand generic programming combine in this novel use of the Boost Graph\nLibrary\n", ['Boost libraries', 'C++', 'threads', 'smart pointers', 'Boost Graph Library', 'graph-theoretic concepts', 'directed graph', 'file dependencies', 'computational geometry', 'BGL graph', 'generic programming approach', 'C++ language', 'computational geometry', 'directed graphs', 'software libraries']), ('A modal logic for indiscernibility and complementarity in information systems\nIn this paper, we study indiscernibility relations and complementarity\nrelations in information systems, The first-order characterization of\nindiscernibility and complementarity is obtained through a duality\nresult between information systems and certain structures of relational\ntype characterized by first-order conditions. The modal analysis of\nindiscernibility and complementarity is performed through a modal logic\nwhich modalities correspond to indiscernibility relations and\ncomplementarity relations in information systems\n', ['modal logic', 'indiscernibility', 'complementarity', 'information systems', 'first-order characterization', 'duality result', 'relational type', 'first-order conditions', 'formal logic', 'information systems', 'knowledge representation']), ('A comparison of high-power converter topologies for the implementation of FACTS\ncontrollers\nThis paper compares four power converter topologies for the implementation of\nflexible AC transmission system (FACTS) controllers: three multilevel\ntopologies (multipoint clamped (MPC), chain, and nested cell) and the\nwell-established multipulse topology. In keeping with the need to\nimplement very-high-power inverters, switching frequency is restricted\nto line frequency. The study addresses device count, DC filter ratings,\nrestrictions on voltage control, active power transfer through the DC\nlink, and balancing of DC-link voltages. Emphasis is placed on\ncapacitor sizing because of its impact on the cost and size of the\nFACTS controller. A method for the dimensioning the DC capacitor filter\nis presented. It is found that the chain converter is attractive for\nthe implementation of a static compensator or a static synchronous\nseries compensator. The MPC converter is attractive for the\nimplementation of a unified power flow controller or an interline power\nflow controller, but a special arrangement is required to overcome the\nlimitations on voltage control\n', ['FACTS controllers', 'high-power converter topologies comparison', 'multilevel topologies', 'multipulse topology', 'inverters', 'switching frequency', 'device count', 'DC filter ratings', 'multipoint clamped topology', 'unified power flow controller', 'static compensator', 'static synchronous series compensator', 'STATCOM', 'UPFC', 'DC-AC power convertors', 'flexible AC transmission systems', 'invertors', 'load flow control', 'power capacitors', 'power convertors', 'power filters', 'power transmission control', 'static VAr compensators', 'switching circuits']), ('Using duality to implicitize and find cusps and inflection points of Bezier\ncurves\nA planar algebraic curve C has an implicit equation and a tangential equation.\nThe tangential equation defines a dual curve to C. Starting with a\nparametrization of C, we find a parametrization of the dual curve, and\nthe tangential equation and implicit equation of C in a novel way. We\nalso find equations whose roots are the parameter values of the cusps\nand inflection points of C. Methods include polar reciprocation and the\ntheory of envelopes\n', ['planar algebraic curve', 'implicit equation', 'tangential equation', 'dual curve', 'parametrization', 'cusps', 'inflection points', 'polar reciprocation', 'envelope theory', 'Bezier curves', 'computational geometry']), ('An operations research approach to the problem of the sugar cane selection\nSelection for superior clones is the most important aspect of sugar cane\nimprovement programs, and is a long and expensive process. While\nstudies have investigated different components of selection\nindependently, there has not been a whole system approach to improve\nthe process. This study observes the problem as an integrated system,\nwhere if one parameter changes the state of the whole system changes. A\ncomputer based stochastic simulation model that accurately represents\nthe selection was developed. The paper describes the simulation model,\nshowing its accuracy as well as how a combination of dynamic\nprogramming and branch and bound can be applied to the model to\noptimise the selection system, giving a new application of these\ntechniques. The model can be directly applied to any region targeted by\nsugar cane breeding programs or to other clonally propagated crops\n', ['operations research approach', 'sugar cane selection', 'superior clones', 'improvement programs', 'computer based stochastic simulation model', 'dynamic programming', 'branch and bound', 'breeding programs', 'clonally propagated crops', 'agriculture', 'agriculture', 'covariance matrices', 'dynamic programming', 'genetics', 'operations research', 'simulation', 'tree searching']), ('Harmless delays in Cohen-Grossberg neural networks\nWithout assuming monotonicity and differentiability of the activation functions\nand any symmetry of interconnections, we establish some sufficient\nconditions for the globally asymptotic stability of a unique\nequilibrium for the Cohen-Grossberg (1983) neural network with multiple\ndelays. Lyapunov functionals and functions combined with the Razumikhin\ntechnique are employed. The criteria are all independent of the\nmagnitudes of the delays, and thus the delays under these conditions\nare harmless\n', ['harmless delays', 'Cohen-Grossberg neural networks', 'monotonicity', 'differentiability', 'activation functions', 'interconnections', 'globally asymptotic stability', 'multiple delays', 'Lyapunov functionals', 'Razumikhin technique', 'asymptotic stability', 'delay-differential systems', 'differential equations', 'interconnections', 'Lyapunov methods', 'neural nets', 'nonlinear dynamical systems']), ('A PID standard: What, why, how?\nThe paper is written for all who develop and use P&IDs. It will aid in solving\nthe long existing and continuing problem of confusing information on\nP&IDs. The acronym P&ID is widely understood to mean the principal\ndocument used to define the details of how a process works and how it\nis controlled. The ISA Dictionary definition for P&ID tells what they\ndo, "show the interconnection of process equipment and the\ninstrumentation used to control the process. In the process industry a\nstandard set of symbols is used to prepare drawings of processes. The\ninstrument symbols used in these drawings are generally based on\nISA-S5.1." In the paper the ISA standard is referred to as ISA-5.1. The\narticle develops the concept of the "standard" and poses some of the\nquestions that the "standard" can answer\n', ['P&ID standard', 'principal document', 'process controlled', 'ISA-5.1', 'ISA standard', 'diagrams', 'instrumentation', 'process control', 'safety', 'standardisation', 'standards']), ('H-matrix approximation for the operator exponential with applications\nWe previously developed a data-sparse and accurate approximation to parabolic\nsolution operators in the case of a rather general elliptic part given\nby a strongly P-positive operator . Also a class of matrices\n(H-matrices) has been analysed which are data-sparse and allow an\napproximate matrix arithmetic with almost linear complexity. In\nparticular, the matrix-vector/matrix-matrix product with such matrices\nas well as the computation of the inverse have linear-logarithmic cost.\nIn this paper, we apply the H-matrix techniques to approximate the\nexponent of an elliptic operator. Starting with the Dunford-Cauchy\nrepresentation for the operator exponent, we then discretise the\nintegral by the exponentially convergent quadrature rule involving a\nshort sum of resolvents. The latter are approximated by the H-matrices.\nOur algorithm inherits a two-level parallelism with respect to both the\ncomputation of resolvents and the treatment of different time values.\nIn the case of smooth data (coefficients, boundaries), we prove the\nlinear-logarithmic complexity of the method\n', ['H-matrix approximation', 'operator exponential', 'data-sparse approximation', 'almost linear complexity', 'parabolic solution operators', 'Dunford-Cauchy representation', 'exponentially convergent quadrature rule', 'strongly P-positive operator', 'approximation theory', 'computational complexity', 'differential equations', 'matrix algebra']), ('Application of traditional system design techniques to Web site design\nAfter several decades of computer program construction there emerged a set of\nprinciples that provided guidance to produce more manageable programs.\nWith the emergence of the plethora of Internet web sites one wonders if\nsimilar guidelines are followed in their construction. Since this is a\nnew technology no apparent universally accepted methods have emerged to\nguide the designer in Web site construction. This paper reviews the\ntraditional principles of structured programming and the preferred\ncharacteristics of Web sites. Finally a mapping of how the traditional\nguidelines may be applied to Web site construction is presented. The\napplication of the traditional principles of structured programming to\nthe design of a Web site can provide a more usable site for the\nvisitors to the site. The additional benefit of using these\ntime-honored techniques is the creation of a Web site that will be\neasier to maintain by the development staff\n', ['Internet Web site design', 'system design techniques', 'structured programming', 'distributed programming', 'information resources', 'Internet', 'structured programming', 'systems analysis']), ('Four-point wavelets and their applications\nMultiresolution analysis (MRA) and wavelets provide useful and efficient tools\nfor representing functions at multiple levels of details. Wavelet\nrepresentations have been used in a broad range of applications,\nincluding image compression, physical simulation and numerical\nanalysis. In this paper, the authors construct a new class of wavelets,\ncalled four-point wavelets, based on an interpolatory four-point\nsubdivision scheme. They are of local support, symmetric and stable.\nThe analysis and synthesis algorithms have linear time complexity.\nDepending on different weight parameters w, the scaling functions and\nwavelets generated by the four-point subdivision scheme are of\ndifferent degrees of smoothness. Therefore the user can select better\nwavelets relevant to the practice among the classes of wavelets. The\nauthors apply the four-point wavelets in signal compression. The\nresults show that the four-point wavelets behave much better than\nB-spline wavelets in many situations\n', ['four-point wavelets', 'multiresolution analysis', 'wavelet representations', 'image compression', 'physical simulation', 'numerical analysis', 'interpolatory four-point subdivision scheme', 'linear time complexity', 'weight parameters', 'scaling functions', 'B-spline wavelets', 'computational complexity', 'data compression', 'image coding', 'interpolation', 'splines (mathematics)', 'wavelet transforms']), ('Favorable noise uniformity properties of Fourier-based interpolation and\nreconstruction approaches in single-slice helical computed tomography\nVolumes reconstructed by standard methods from single-slice helical computed\ntomography (CT) data have been shown to have noise levels that are\nhighly nonuniform relative to those in conventional CT. These noise\nnonuniformities can affect low-contrast object detectability and have\nalso been identified as the cause of the zebra artifacts that plague\nmaximum intensity projection (MIP) images of such volumes. While these\nspatially variant noise levels have their root in the peculiarities of\nthe helical scan geometry, there is also a strong dependence on the\ninterpolation and reconstruction algorithms employed. In this paper, we\nseek to develop image reconstruction strategies that eliminate or\nreduce, at its source, the nonuniformity of noise levels in helical CT\nrelative to that in conventional CT. We pursue two approaches,\nindependently and in concert. We argue, and verify, that Fourier-based\nlongitudinal interpolation approaches lead to more uniform noise ratios\nthan do the standard 360LI and 180LI approaches. We also demonstrate\nthat a Fourier-based fan-to-parallel rebinning algorithm, used as an\nalternative to fanbeam filtered backprojection for slice\nreconstruction, also leads to more uniform noise ratios, even when\nmaking use of the 180LI and 360LI interpolation approaches\n', ['Fourier-based interpolation', 'single-slice helical computed tomography', 'reconstruction approaches', 'noise uniformity properties', 'medical diagnostic imaging', 'conventional CT', 'Fourier-based fan-to-parallel rebinning algorithm', 'more uniform noise ratios', 'low-contrast object detectability', 'zebra artifacts', 'maximum intensity projection images', 'helical span geometry', 'computerised tomography', 'image reconstruction', 'interpolation', 'medical image processing', 'noise']), ('Modelling of complete robot dynamics based on a multi-dimensional, RBF-like\nneural architecture\nA neural network based identification approach of manipulator dynamics is\npresented. For a structured modelling, RBF-like static neural networks\nare used in order to represent and adapt all model parameters with\ntheir non-linear dependences on the joint positions. The neural\narchitecture is hierarchically organised to reach optimal adjustment to\nstructural a priori-knowledge about the identification problem. The\nmodel structure is substantially simplified by general system analysis\nindependent of robot type. But also a lot of specific features of the\nutilised experimental robot are taken into account. A fixed, grid based\nneuron placement together with application of B-spline polynomial basis\nfunctions is utilised favourably for a very effective recursive\nimplementation of the neural architecture. Thus, an online\nidentification of a dynamic model is submitted for a complete 6 joint\nindustrial robot\n', ['complete robot dynamics', 'multi-dimensional RBF-like neural architecture', 'manipulator dynamics', 'static neural networks', 'neural architecture', 'general system analysis', 'fixed grid based neuron placement', 'B-spline polynomial basis functions', 'recursive implementation', 'online identification', 'dynamic model', 'complete 6 joint industrial robot', 'online learning', 'identification', 'learning (artificial intelligence)', 'manipulator dynamics', 'neural net architecture', 'radial basis function networks', 'splines (mathematics)']), ('On the Beth properties of some intuitionistic modal logics\nLet L be one of the intuitionistic modal logics. As in the classical modal\ncase, we define two different forms of the Beth property for L, which\nare denoted by B1 and B2; in this paper we study the relation among B1,\nB2 and the interpolation properties C1 and C2. It turns out that C1\nimplies B1, but contrary to the boolean case, is not equivalent to B1.\nIt is shown that B2 and C2 are independent, and moreover it comes out\nthat, in contrast to classical case, there exists an extension of the\nintuitionistic modal logic of S/sub 4/-type, that has not the property\nB2. Finally we give two algebraic properties, that characterize\nrespectively B1 and B2\n', ['Beth properties', 'interpolation properties', 'intuitionistic modal logics', 'formal logic']), ('Fresh voices, big ideas [IBM internship program]\nIBM is matching up computer-science and MBA students with its business managers\nin an 11-week summer internship program and challenging them to develop\ninnovative technology ideas\n', ['internship program', 'IBM business managers', 'MBA college students', 'computer-science students', 'patents', 'DP industry', 'IBM computers', 'research initiatives']), ('A scanline-based algorithm for the 2D free-form bin packing problem\nThis paper describes a heuristic algorithm for the 2D free-form bin packing\n(2D-FBP) problem. Given a set of 2D free-form bins and a set of 2D\nfree-form items, the 2D-FBP problem is to lay out items inside one or\nmore bins in such a way that the number of bins used is minimized, and\nfor each bin, the yield is maximized. The proposed algorithm handles\nthe problem as a variant of the 1D problem; i.e., items and bins are\napproximated as sets of scanlines, and scanlines are packed. The\ndetails of the algorithm are given, and its application to a nesting\nproblem in a shipbuilding company is reported. The proposed algorithm\nconsists of the basic and the group placement algorithms. The basic\nplacement algorithm is a variant of the first-fit decreasing algorithm\nwhich is simply extended from the 1D case to the 2D case by a novel\nscanline approximation. A numerical study with real instances shows\nthat the basic placement algorithm has sufficient performance for most\nof the instances, however, the group placement algorithm is required\nwhen items must be aligned in columns. The qualities of the resulting\nlayouts are good enough for practical use, and the processing times are\ngood\n', ['scanline-based algorithm', '2D free-form bin packing problem', 'heuristic algorithm', '2D-FBP problem', 'irregular cutting', 'irregular packing', 'nesting problem', 'minimization', 'yield maximization', 'shipbuilding company', 'group placement algorithm', 'first-fit decreasing algorithm', 'bin packing', 'heuristic programming', 'minimisation']), ('Choice from a three-element set: some lessons of the 2000 presidential campaign\nin the United States\nWe consider the behavior of four choice rules - plurality voting, approval\nvoting, Borda count, and self-consistent choice - when applied to\nchoose the best option from a three-element set. It is assumed that the\ntwo main options are preferred by a large majority of the voters, while\nthe third option gets a very small number of votes and influences the\nelection outcome only when the two main options receive a close number\nof votes. When used to rate the main options, Borda count and\nself-consistent choice contain terms that allow both for the "strength\nof preferences" of the voters and the rating of the main candidates by\nvoters who vote for the third option. In this way, it becomes possible\nto determine more reliably the winner when plurality voting or approval\nvoting produce close results\n', ['three-element set', '2000 presidential campaign', 'plurality voting', 'approval voting', 'Borda count', 'self-consistent choice', 'mathematical programming']), ('Cooperative mutation based evolutionary programming for continuous function\noptimization\nAn evolutionary programming (EP) algorithm adapting a new mutation operator is\npresented. Unlike most previous EPs, in which each individual is\nmutated on its own, each individual in the proposed algorithm is\nmutated in cooperation with the other individuals. This not only\nenhances convergence speed but also gives more chance to escape from\nlocal minima\n', ['cooperative mutation based evolutionary programming', 'continuous function optimization', 'convergence speed', 'local minima', 'convergence', 'evolutionary computation', 'functions', 'optimisation']), ('Quasi stage order conditions for SDIRK methods\nThe stage order condition is a simplifying assumption that reduces the number\nof order conditions to be fulfilled when designing a Runge-Kutta (RK)\nmethod. Because a DIRK (diagonally implicit RK) method cannot have\nstage order greater than 1, we introduce quasi stage order conditions\nand derive some of their properties for DIRKs. We use these conditions\nto derive a low-order DIRK method with embedded error estimator.\nNumerical tests with stiff ODEs and DAEs of index 1 and 2 indicate that\nthe method is competitive with other RK methods for low accuracy\ntolerances\n', ['quasi stage order conditions', 'diagonally implicit Runge-Kutta method', 'embedded error estimator', 'numerical tests', 'differential-algebraic systems', 'SDIRK methods', 'differential equations', 'Runge-Kutta methods']), ("Effectiveness of user testing and heuristic evaluation as a function of\nperformance classification\nFor different levels of user performance, different types of information are\nprocessed and users will make different types of errors. Based on the\nerror's immediate cause and the information being processed, usability\nproblems can be classified into three categories. They are usability\nproblems associated with skill-based, rule-based and knowledge-based\nlevels of performance. In this paper, a user interface for a Web-based\nsoftware program was evaluated with two usability evaluation methods,\nuser testing and heuristic evaluation. The experiment discovered that\nthe heuristic evaluation with human factor experts is more effective\nthan user testing in identifying usability problems associated with\nskill-based and rule-based levels of performance. User testing is more\neffective than heuristic evaluation in finding usability problems\nassociated with the knowledge-based level of performance. The practical\napplication of this research is also discussed in the paper\n", ['user testing', 'heuristic evaluation', 'performance classification', 'user performance', 'usability', 'knowledge-based performance levels', 'skill-based performance levels', 'user interface', 'Web-based software', 'experiment', 'human factors', 'rule-based performance levels', 'human factors', 'information resources', 'user interfaces']), ("Senate to Powell: regulate more [FCC]\nFCC Chairman Michael Powell pitched a six-step market-based recovery plan to\nthe Senate last week, but two members of the Commerce Committee told\nhim telecom's revival requires more reliance on regulation\n", ['FCC', 'recovery plan', 'US Senate Commerce Committee', 'telecom industry', 'telecommunication']), ('A novel approach for the detection of pathlines in X-ray angiograms: the\nwavefront propagation algorithm\nPresents a new pathline approach, based on the wavefront propagation principle,\nand developed in order to reduce the variability in the outcomes of the\nquantitative coronary artery analysis. This novel approach, called\nwavepath, reduces the influence of the user-defined start- and\nendpoints of the vessel segment and is therefore more robust and\nimproves the reproducibility of the lesion quantification\nsubstantially. The validation study shows that the wavepath method is\ntotally constant in the middle part of the pathline, even when using\nthe method for constructing a bifurcation or sidebranch pathline.\nFurthermore, the number of corrections needed to guide the wavepath\nthrough the correct vessel is decreased from an average of 0.44\ncorrections per pathline to an average of 0.12 per pathline. Therefore,\nit can be concluded that the wavepath algorithm improves the overall\nanalysis substantially\n', ['wavefront propagation principle', 'quantitative coronary artery analysis', 'user-defined startpoints', 'user-defined endpoints', 'vessel segment', 'lesion quantification', 'wavepath method', 'bifurcation', 'sidebranch pathline', 'corrections', 'correct vessel', 'wavefront propagation algorithm', 'X-ray angiograms', 'algorithm theory', 'bifurcation', 'blood vessels', 'diagnostic radiography', 'medical image processing']), ('Comparison of push and pull systems with transporters: a metamodelling approach\nAnalyses push and pull systems with transportation consideration. A\nmultiproduct, multiline, multistage production system was used to\ncompare the two systems. The effects of four factors (processing time\nvariation, demand variation, transporters, batch size) on throughput\nrate, average waiting time in the system and machine utilization were\nstudied. The study uses metamodels to compare the two systems. They\nserve a dual purpose of expressing system performance measures in the\nform of a simple equation and reducing computational time when\ncomparing the two systems. Research shows that the number of\ntransporters used and the batch size have a significant effect on the\nperformance measures of both systems\n', ['transporters', 'push systems', 'pull systems', 'metamodelling approach', 'multiproduct multiline multistage production system', 'processing time variation', 'demand variation', 'batch size', 'throughput rate', 'average waiting time', 'machine utilization', 'performance measures', 'batch processing (industrial)', 'materials handling', 'production control', 'statistical analysis']), ('Restoration of archival documents using a wavelet technique\nThis paper addresses a problem of restoring handwritten archival documents by\nrecovering their contents from the interfering handwriting on the\nreverse side caused by the seeping of ink. We present a novel method\nthat works by first matching both sides of a document such that the\ninterfering strokes are mapped with the corresponding strokes\noriginating from the reverse side. This facilitates the identification\nof the foreground and interfering strokes. A wavelet reconstruction\nprocess then iteratively enhances the foreground strokes and smears the\ninterfering strokes so as to strengthen the discriminating capability\nof an improved Canny edge detector against the interfering strokes. The\nmethod has been shown to restore the documents effectively with average\nprecision and recall rates for foreground text extraction at 84 percent\nand 96 percent, respectively\n', ['archival documents restoration', 'wavelet technique', 'handwritten archival documents', 'ink seepage', 'wavelet reconstruction process', 'iterative stroke enhancement', 'Canny edge detector', 'document image processing', 'edge detection', 'image restoration', 'wavelet transforms']), ('Mobile computing "Killer app" competition\nDesign competitions offer students an excellent way to gain hands-on experience\nin engineering and computer science courses. The University of Florida,\nin partnership with Motorola, has held two mobile computing design\ncompetitions. In Spring and Fall 2001, students in Abdelsalam Helal\'s\nMobile Computing class designed killer apps for a Motorola smart phone\n', ['mobile computing', 'smart phone', 'Motorola', 'design competitions', 'computer science education', 'mobile computing']), ('Development of an integrated and open-architecture precision motion control\nsystem\nIn this paper, the development of an integrated and open-architecture precision\nmotion control system is presented. The control system is generally\napplicable, but it is developed with a particular focus on direct drive\nservo systems based on linear motors. The overall control system is\ncomprehensive, comprising of various selected control and\ninstrumentation components, integrated within a configuration of\nhardware architecture centred around a dSPACE DS1004 DSP processor\nboard. These components include a precision composite controller\n(comprising of feedforward and feedback control), a disturbance\nobserver, an adaptive notch filter, and a geometrical error\ncompensator. The hardware architecture, software development platform,\nuser interface, and all constituent control components are described\n', ['motion control', 'direct drive servo systems', 'linear motors', 'dSPACE DS1004 processor', 'composite controller', 'feedforward', 'feedback', 'adaptive notch filter', 'open-architecture', 'precision', 'geometrical error compensation', 'electric drives', 'error compensation', 'feedback', 'feedforward', 'linear motors', 'motion control', 'open systems']), ('Pontryagin maximum principle of optimal control governed by fluid dynamic\nsystems with two point boundary state constraint\nWe study the optimal control problem subject to the semilinear equation with a\nstate constraint. We prove certain theorems and give examples of state\nconstraints so that the maximum principle holds. The main difficulty of\nthe problem is to make the sensitivity analysis of the state with\nrespect to the control caused by the unboundedness and nonlinearity of\nan operator\n', ['Pontryagin maximum principle', 'optimal control', 'fluid dynamics', 'semilinear equation', 'state constraints', 'fluid dynamics', 'maximum principle', 'optimal control']), ('Bounded model checking for the universal fragment of CTL\nBounded Model Checking (BMC) has been recently introduced as an efficient\nverification method for reactive systems. BMC based on SAT methods\nconsists in searching for a counterexample of a particular length and\ngenerating a propositional formula that is satisfiable iff such a\ncounterexample-exists. This new technique has been introduced by E.\nClarke et al. for model checking of linear time temporal logic (LTL).\nOur paper shows how the concept of bounded model checking can be\nextended to ACTL (the universal fragment of CTL). The implementation of\nthe algorithm for Elementary Net Systems is described together with the\nexperimental results\n', ['bounded model checking', 'universal fragment', 'verification method', 'reactive systems', 'SAT methods', 'propositional formula', 'model checking', 'linear time temporal logic', 'elementary net systems', 'bounded semantics', 'computability', 'semantic networks', 'temporal logic']), ("Computing grid unlocks research\nUnder the UK government's spending review in 2000 the Office of Science and\nTechnology was allocated Pounds 98m to establish a three year e-science\nresearch and development programme. The programme has a bold vision: to\nchange the dynamic of the way science is undertaken. The term\n'e-science' was introduced by John Taylor, director general of research\ncouncils in the Office of Science and Technology. He saw many areas of\nscience becoming increasingly reliant on new ways of collaborative,\nmultidisciplinary, interorganisation working. E-science is intended to\ncapture these new modes of working. There are two major components to\nthe programme: the science, and the infrastructure to support that\nscience. The infrastructure is generally referred to as the Grid. The\nchoice of name resonates with the idea of a future in which computing\nresources and storage, as well as expensive scientific facilities and\nsoftware, can be accessed on demand, like electricity. Open source\nprototypes of the middleware are available and under development as\npart of the e-science programme and other international efforts\n", ['UK programme', 'grid computing', 'collaboration', 'computing resources', 'e-science', 'middleware', 'software', 'open source prototypes', 'scientific research', 'government policies', 'groupware', 'natural sciences computing', 'wide area networks']), ('The mutual effects of grid and wind turbine voltage stability control\nThis note considers the results of wind turbine modelling and power system\nstability investigations. Voltage stability of the power grid with\ngrid-connected wind turbines will be improved by using blade angle\ncontrol for a temporary reduction of the wind turbine power during and\nshortly after a short circuit fault in the grid\n', ['grid voltage stability control', 'wind turbine voltage stability control', 'wind turbine modelling', 'power system stability', 'power grid', 'grid-connected wind turbines', 'blade angle control', 'wind turbine power reduction', 'short circuit fault', 'offshore wind turbines', 'power system dynamic stability', 'short-circuit currents', 'wind turbines']), ('Student consulting projects benefit faculty and industry\nStudent consulting projects require students to apply OR/MS tools to obtain\ninsight into the activities of firms in the community. These projects\nbenefit faculty by providing clear feedback on the real capabilities of\nstudents, a broad connection to local industry, and material for case\nstudies and research. They benefit companies by stimulating new\nthinking regarding their activities and delivering results they can\nuse. Projects provide insights into the end-user modeling mode of OR/MS\npractice. Projects support continuous improvement as the lessons gained\nfrom a crop of projects enable better teaching during the next course\noffering, which in turn leads to better projects and further insights\ninto teaching\n', ['student consulting projects', 'OR/MS tools', 'student placements', 'student capability feedback', 'case study material', 'feedback', 'management education', 'management science', 'operations research']), ('3D reconstruction from uncalibrated-camera optical flow and its reliability\nevaluation\nWe present a scheme for reconstructing a 3D structure from optical flow\nobserved by a camera with an unknown focal length in a statistically\noptimal way as well as evaluating the reliability of the computed\nshape. First, the flow fundamental matrices are optimally computed from\nthe observed flow. They are then decomposed into the focal length, its\nrate of change, and the motion parameters. Next, the flow is optimally\ncorrected so that it satisfies the epipolar equation exactly. Finally,\nthe 3D positions are computed, and their covariance matrices are\nevaluated. By simulations and real-image experiments, we test the\nperformance of our system and observe how the normalization (gauge) for\nremoving indeterminacy affects the description of uncertainty\n', ['3D reconstruction', 'uncalibrated-camera optical flow', 'reliability evaluation', 'flow fundamental matrices', 'motion parameters', 'epipolar equation', 'covariance matrices', 'real-image experiments', 'normalization', 'covariance matrices', 'image reconstruction', 'image sequences', 'renormalisation']), ('Synchronizing experiments with linear interval systems\nConcerns generalized control problems without exact information. <P>A\nmethod of constructing a minimal synchronizing sequence for a linear\ninterval system over the field of real numbers is developed. This\nproblem is reduced to a system of linear inequalities\n', ['synchronizing experiments', 'linear interval systems', 'minimal synchronizing sequence construction', 'real numbers', 'linear inequalities', 'generalized control problems', 'controllability', 'control system analysis', 'controllability', 'discrete systems', 'linear systems', 'minimisation', 'synchronisation', 'uncertain systems']), ('Control of a thrust-vectored flying wing: a receding horizon - LPV approach\nThis paper deals with the application of receding horizon methods to hover and\nforward flight models of an experimental tethered flying wing developed\nat Caltech. The dynamics of the system are representative of a vertical\nlanding and take off aircraft, such as a Harrier around hover, or a\nthrust-vectored aircraft such as F18-HARV or X-31 in forward flight.\nThe adopted control methodology is a hybrid of receding horizon\ntechniques and control Lyapunov function (CLF)-based ideas. First, a\nCLF is generated using quasi-LPV methods and then, by using the CLF as\nthe terminal cost in the receding horizon optimization, stability is\nguaranteed. The main advantage of this approach is that stability can\nbe guaranteed without imposing constraints in the on-line optimization,\nallowing the problem to be solved in a more efficient manner. Models of\nthe experimental set-up are obtained for the hover and forward flight\nmodes. Numerical simulations for different time horizons are presented\nto illustrate the effectiveness of the discussed methods. Specifically,\nit is shown that a mere upper bound on the cost-to-go is not an\nappropriate choice for a terminal cost, when the horizon length is\nshort. Simulation results are presented using experimentally verified\nmodel parameters\n', ['thrust-vectored flying wing control', 'receding horizon-LPV approach', 'hover flight models', 'forward flight models', 'tethered flying wing', 'Caltech', 'vertical landing take off aircraft', 'Harrier around hover', 'thrust-vectored aircraft', 'F18-HARV', 'X-31', 'receding horizon techniques', 'control Lyapunov function-based ideas', 'quasi-LPV methods', 'receding horizon optimization', 'stability guarantee', 'numerical simulations', 'terminal cost', 'nonlinear system', 'aircraft control', 'control system synthesis', 'Lyapunov methods', 'nonlinear control systems', 'optimal control']), ('Application-layer multicasting with Delaunay triangulation overlays\nApplication-layer multicast supports group applications without the need for a\nnetwork-layer multicast protocol. Here, applications arrange themselves\nin a logical overlay network and transfer data within the overlay. We\npresent an application-layer multicast solution that uses a Delaunay\ntriangulation as an overlay network topology. An advantage of using a\nDelaunay triangulation is that it allows each application to locally\nderive next-hop routing information without requiring a routing\nprotocol in the overlay. A disadvantage of using a Delaunay\ntriangulation is that the mapping of the overlay to the network\ntopology at the network and data link layer may be suboptimal. We\npresent a protocol, called Delaunay triangulation (DT protocol), which\nconstructs Delaunay triangulation overlay networks. We present\nmeasurement experiments of the DT protocol for overlay networks with up\nto 10 000 members, that are running on a local PC cluster with 100\nLinux PCs. The results show that the protocol stabilizes quickly, e.g.,\nan overlay network with 10 000 nodes can be built in just over 30 s.\nThe traffic measurements indicate that the average overhead of a node\nis only a few kilobits per second if the overlay network is in a steady\nstate. Results of throughput experiments of multicast transmissions\n(using TCP unicast connections between neighbors in the overlay\nnetwork) show an achievable throughput of approximately 15 Mb/s in an\noverlay with 100 nodes and 2 Mb/s in an overlay with 1000 nodes\n', ['application-layer multicasting', 'Delaunay triangulation overlays', 'group applications', 'network-layer multicast protocol', 'logical overlay network', 'data transfer', 'Delaunay triangulation protocol', 'measurement experiments', 'overlay networks', 'local PC cluster', 'Linux PC', 'overlay network topology', 'next-hop routing information', 'data link layer', 'DT protocol', 'network nodes', 'traffic measurements', 'average overhead', 'throughput experiments', 'multicast transmissions', 'TCP unicast connections', '15 Mbit/s', '2 Mbit/s', 'mesh generation', 'microcomputer applications', 'multicast communication', 'network topology', 'performance evaluation', 'telecommunication network routing', 'telecommunication traffic', 'transport protocols', 'workstation clusters']), ("Do you see what I see? [visual technology in law firms]\nThink of how well-done computer presentations can aid in the learning\nexperience. They are, however, less common in client meetings,\nsettlement conferences and the courtroom. And you have to wonder why,\nwhen the same benefits of attention focus and visual learning apply in\nthose legal communication settings. The software and hardware\ncomponents are easy to use, and they're increasingly affordable to\nboot. The next time you need to convey a point to an audience (be it\none person or many), think of how you might benefit from the visual\nimpact available through presentation software like PowerPoint. Anyone\nwill understand you more easily when assisted by visual input, and it\nmay make all the difference in reaching visual-focused learners\n", ['computer presentations', 'visual technology', 'law firms', 'PowerPoint', 'business graphics', 'law administration']), ('The quadratic 0-1 knapsack problem with series-parallel support\nWe consider various special cases of the quadratic 0-1 knapsack problem (QKP)\nfor which the underlying graph structure is fairly simple. For the\nvariant with edge series-parallel graphs, we give a dynamic programming\nalgorithm with pseudo-polynomial time complexity, and a fully\npolynomial time approximation scheme. In strong contrast to this, the\nvariant with vertex series-parallel graphs is shown to be strongly\nNP-complete\n', ['quadratic 0-1 knapsack problem', 'series-parallel support', 'underlying graph structure', 'dynamic programming algorithm', 'pseudo-polynomial time complexity', 'fully polynomial time approximation scheme', 'NP-complete problem', 'computational complexity', 'dynamic programming', 'graph theory', 'knapsack problems', 'polynomial approximation']), ('Use of neural networks in the analysis of particle size distribution by laser\ndiffraction: tests with different particle systems\nThe application of forward light scattering methods for estimating the particle\nsize distribution (PSD) is usually limited by the occurrence of\nmultiple scattering, which affects the angular distribution of light in\nhighly concentrated suspensions, thus resulting in false calculations\nby the conventionally adopted algorithms. In this paper, a previously\nproposed neural network-based method is tested with different particle\nsystems, in order to evaluate its applicability. In the first step of\nthe study, experiments were carried out with solid-liquid suspensions\nhaving different characteristics of particle shape and size\ndistribution, under varying solid concentrations. The experimental\nresults, consisting of the angular distribution of light intensity,\nparticle shape and suspension concentration, were used as input data in\nthe fitting of neural network models (NN) that replaced the optical\nmodel to provide the PSD. The reference values of particle shape and\nPSD for the NN fitting were based on image analysis. Comparisons\nbetween the PSD values computed by the NN model and the reference\nvalues indicate that the method can be used in monitoring the PSD of\nparticles with different shapes in highly concentrated suspensions,\nthus extending the range of application of forward laser diffraction to\na number of systems with industrial interest\n', ['particle size distribution', 'laser diffraction', 'neural network modeling', 'forward light scattering', 'multiple scattering', 'angular distribution of light', 'solid-liquid suspensions', 'particle shape distribution', 'image analysis', 'pattern recognition', 'powdered materials', 'backpropagation algorithm', 'Fraunhofer optical model', 'fluidized catalytic cracking', 'backpropagation', 'Fraunhofer diffraction', 'light scattering', 'measurement by laser beam', 'neural nets', 'particle size measurement', 'pattern recognition', 'physics computing', 'suspensions']), ('Record makers [UK health records]\nPlans for a massive cradle-to-grave electronic records project have been\nrevealed by the government. Is the scheme really viable?\n', ['UK health records', 'electronic records project', 'integrated care records services', 'health care', 'social care', 'health care']), ('On lag windows connected with Jacobi polynomials\nLag windows whose corresponding spectral windows are Jacobi polynomials or sums\nof Jacobi polynomials are introduced. The bias and variance of their\nspectral density estimators are investigated and their window bandwidth\nand characteristic exponent are determined\n', ['lag windows', 'Jacobi polynomials', 'spectral windows', 'spectral density estimators', 'window bandwidth', 'characteristic exponent', 'Jacobian matrices', 'polynomials']), ('Effects of the transition to a client-centred team organization in\nadministrative surveying work\nA new work organization was introduced in administrative surveying work in\nSweden during 1998. The new work organization implied a transition to a\nclient-centred team-based organization and required a change in\ncompetence from specialist to generalist knowledge as well as a\ntransition to a new information technology, implying a greater\nintegration within the company. The aim of this study was to follow the\nsurveyors for two years from the start of the transition and\ninvestigate how perceived consequences of the transition, job,\norganizational factors, well-being and effectiveness measures changed\nbetween 1998 and 2000. The Teamwork Profile and QPS Nordic\nquestionnaire were used. The 205 surveyors who participated in all\nthree study phases constituted the study group. The result showed that\nsurveyors who perceived that they were working as generalists rated the\nimprovements in job and organizational factors significantly higher\nthan those who perceived that they were not yet generalists.\nImprovements were noted in 2000 in quality of service to clients, time\navailable to handle a case and effectiveness of teamwork in a transfer\nto a team-based work organization group, cohesion and continuous\nimprovement practices-for example, learning by doing, mentoring and\nguided delegation-were important to improve the social effectiveness of\ngroup work\n', ['client-centred team organization', 'administrative surveying work', 'information technology', 'company', 'job', 'organizational factors', 'effectiveness measures', 'Teamwork Profile', 'QPS Nordic questionnaire', 'social effectiveness', 'public administrative sector', 'human factors', 'human resource management', 'information technology', 'personnel', 'public administration', 'social aspects of automation']), ('A 3-stage pipelined architecture for multi-view images decoder\nIn this paper, we proposed the architecture of the decoder which implements the\nmulti-view images decoding algorithm. The study of the hardware\nstructure of the multi-view image processing has not been accomplished.\nThe proposed multi-view images decoder operates in a three stage\npipelined manner and extracts the depth of the pixels of the decoded\nimage every clock. The multi-view images decoder consists of three\nmodules, Node selector which transfers the value of the nodes\nrepeatedly and Depth Extractor which extracts the depth of each pixel\nfrom the four values of the nodes and Affine Transformer which\ngenerates the projecting position on the image plane from the values of\nthe pixels and the specified viewpoint. The proposed architecture is\ndesigned and simulated by the Max+PlusII design tool and the operating\nfrequency is 30 MHz. The image can be constructed in a real time by the\ndecoder with the proposed architecture\n', ['three-stage pipelined architecture', 'multi-view images decoder', 'hardware structure', 'pixel depth', 'node selector', 'depth extractor', 'affine transformer', 'viewpoint', 'Max+PlusII design tool', 'operating frequency', '30 MHz', 'decoding', 'image coding', 'parallel architectures', 'pipeline processing']), ('Broadcasts keep staff in picture [intranets]\nMark Hawkins, chief operating officer at UK-based streaming media specialist\nTwofourtv, explains how firms can benefit by linking their corporate\nintranets to broadcasting technology\n', ['corporate intranets', 'Twofourtv', 'streaming media', 'broadcasting technology', 'broadcasting', 'intranets']), ('Improved detection of lung nodules by using a temporal subtraction technique\nThe authors evaluated the effect of a temporal subtraction technique for\ndigital chest radiography with regard to the accuracy of detection of\nlung nodules. Twenty solitary lung nodules smaller than 30 mm in\ndiameter, including 10 lung cancers and 10 benign nodules, were used.\nThe nodules were grouped subjectively according to their subtlety. For\nnonnodular cases, 20 nodules without perceptible interval changes were\nselected. All chest radiographs were obtained by using a computed\nradiographic system, and temporal subtraction images were produced by\nusing a program developed at the University of Chicago. The effect of\nthe temporal subtraction image was evaluated by using an observer\nperformance study, with use of receiver operating characteristic\nanalysis. Observer performance with temporal subtraction images was\nsubstantially improved (A/sub z/ = 0.980 and 0.958), as compared with\nthat without temporal subtraction images (A/sub z/ = 0.920 and 0.825)\nfor the certified radiologists and radiology residents, respectively.\nThe temporal subtraction technique clearly improved diagnostic accuracy\nfor detecting lung nodules, especially subtle cases. In conclusion, the\ntemporal subtraction technique is useful for improving detection\naccuracy for peripheral lung nodules on digital chest radiographs\n', ['improved lung nodules detection', 'temporal subtraction technique', 'digital chest radiography', 'perceptible interval changes', 'observer performance', 'peripheral lung nodules', 'radiology residents', 'certified radiologists', 'subtle cases', 'University of Chicago', 'computed radiographic system', 'medical diagnostic imaging', '30 mm', 'cancer', 'diagnostic radiography', 'lung', 'medical image processing']), ('An interlingua-based Chinese-English MT system\nChinese-English machine translation is a significant and challenging problem in\ninformation processing. The paper presents an interlingua-based\nChinese-English natural language translation system (ICENT). It\nintroduces the realization mechanism of Chinese language analysis,\nwhich contains syntactic parsing and semantic analyzing and gives the\ndesign of interlingua in details. Experimental results and system\nevaluation are given. The result is satisfying\n', ['interlingua-based Chinese-English machine translation system', 'information processing', 'natural language translation system', 'syntactic parsing', 'semantic analyzing', 'grammars', 'language translation', 'natural language interfaces']), ('Induced-shear piezoelectric actuators for rotor blade trailing edge flaps\nMuch of the current rotorcraft research is focused on improving performance by\nreducing unwanted helicopter noise and vibration. One of the most\npromising active rotorcraft vibration control systems is an active\ntrailing edge flap. In this paper, an induced-shear piezoelectric tube\nactuator is used in conjunction with a simple lever-cusp hinge\namplification device to generate a useful combination of trailing edge\nflap deflections and hinge moments. A finite-element model of the\nactuator tube and trailing edge flap (including aerodynamic and\ninertial loading) was used to guide the design of the actuator-flap\nsystem. A full-scale induced shear tube actuator flap system was\nfabricated and bench top testing was conducted to validate the\nanalysis. Hinge moments corresponding to various rotor speeds were\napplied to the actuator using mechanical springs. The testing\ndemonstrated that for an applied electric field of 3 kV cm/sup -1/ the\ntube actuator deflected a representative full-scale 12 inch flap\n+or-2.8 degrees at 0 rpm and +or-1.4 degrees for a hinge moment\nsimulating a 400 rpm condition. The per cent error between the\npredicted and experimental full-scale flap deflections ranged from 4%\n(low rpm) to 12.5% (large rpm). Increasing the electric field to 4 kV\ncm/sup -1/ results in +or-2.5 degrees flap deflection at a rotation\nspeed of 400 rpm, according to the design analysis. A trade study was\nconducted to compare the performance of the piezoelectric tube actuator\nto the state of the art in trailing edge flap actuators and indicated\nthat the induced-shear tube actuator shows promise as a trailing edge\nflap actuator\n', ['rotorcraft', 'helicopter noise', 'helicopter vibration', 'vibration control', 'active trailing edge flap', 'lever-cusp hinge amplification device', 'finite-element model', 'aerodynamic loading', '12 inch flap', 'inertial loading', 'design', 'shear tube actuator flap', 'bench top testing', 'piezoelectric tube actuator', 'induced-shear tube actuator', '12 inch', 'aircraft control', 'finite element analysis', 'helicopters', 'noise abatement', 'piezoelectric actuators', 'rotors', 'vibration control']), ('Outlier resistant adaptive matched filtering\nRobust adaptive matched filtering (AMF) whereby outlier data vectors are\ncensored from the covariance matrix estimate is considered in a maximum\nlikelihood estimation (MLE) setting. It is known that outlier data\nvectors whose steering vector is highly correlated with the desired\nsteering vector, can significantly degrade the performance of AMF\nalgorithms such as sample matrix inversion (SMI) or fast maximum\nlikelihood (FML). Four new algorithms that censor outliers are\npresented which are derived via approximation to the MLE solution. Two\nalgorithms each are related to using the SMI or the FML to estimate the\nunknown underlying covariance matrix. Results are presented using\ncomputer simulations which demonstrate the relative effectiveness of\nthe four algorithms versus each other and also versus the SMI and FML\nalgorithms in the presence of outliers and no outliers. It is shown\nthat one of the censoring algorithms, called the reiterative censored\nfast maximum likelihood (CFML) technique is significantly superior to\nthe other three censoring methods in stressful outlier scenarios\n', ['outlier resistant adaptive matched filtering', 'covariance matrix estimate', 'maximum likelihood estimation setting', 'steering vector', 'sample matrix inversion', 'fast maximum likelihood', 'censoring algorithms', 'reiterative censored fast maximum likelihood', 'adaptive filters', 'covariance matrices', 'filtering theory', 'matched filters', 'maximum likelihood estimation']), ('Research into telehealth applications in speech-language pathology\nA literature review was conducted to investigate the extent to which telehealth\nhas been researched within the domain of speech-language pathology and\nthe outcomes of this research. A total of 13 studies were identified.\nThree early studies demonstrated that telehealth was feasible, although\nthere was no discussion of the cost-effectiveness of this process in\nterms of patient outcomes. The majority of the subsequent studies\nindicated positive or encouraging outcomes resulting from telehealth.\nHowever, there were a number of shortcomings in the research, including\na lack of cost-benefit information, failure to evaluate the technology\nitself, an absence of studies of the educational and informational\naspects of telehealth in relation to speech-language pathology, and the\nuse of telehealth in a limited range of communication disorders. Future\nresearch into the application of telehealth to speech-language\npathology services must adopt a scientific approach, and have a well\ndefined development and evaluation framework that addresses the\neffectiveness of the technique, patient outcomes and satisfaction, and\nthe cost-benefit relationship\n', ['telehealth applications', 'speech-language pathology', 'literature review', 'telemedicine', 'cost-effectiveness', 'patient outcomes', 'cost-benefit analysis', 'communication disorders', 'patient satisfaction', 'cost-benefit analysis', 'medical computing', 'reviews', 'speech', 'telemedicine']), ('Numerical studies of 2D free surface waves with fixed bottom\nThe motion of surface waves under the effect of bottom is a very interesting\nand challenging phenomenon in the nature. we use boundary integral\nmethod to compute and analyze this problem. In the linear analysis, the\nlinearized equations have bounded error increase under some compatible\nconditions. This contributes to the cancellation of instable\nKelvin-Helmholtz terms. Under the effect of bottom, the existence of\nequations is hard to determine, but given some limitations it proves\ntrue. These limitations are that the swing of interfaces should be\nsmall enough, and the distance between surface and bottom should be\nlarge enough. In order to maintain the stability of computation, some\ncompatible relationship must be satisfied. In the numerical examples,\nthe simulation of standing waves and breaking waves are calculated. And\nin the case of shallow bottom, we found that the behavior of waves are\nrather singular\n', ['numerical studies', '2D free surface waves', 'boundary integral method', 'linear analysis', 'linearized equations', 'instable Kelvin-Helmholtz terms', 'boundary integral equations', 'extrapolation']), ('On generalized Gaussian quadratures for exponentials and their applications\nWe introduce new families of Gaussian-type quadratures for weighted integrals\nof exponential functions and consider their applications to integration\nand interpolation of bandlimited functions. We use a generalization of\na representation theorem due to Caratheodory to derive these\nquadratures. For each positive measure, the quadratures are\nparameterized by eigenvalues of the Toeplitz matrix constructed from\nthe trigonometric moments of the measure. For a given accuracy epsilon\n, selecting an eigenvalue close to epsilon yields an approximate\nquadrature with that accuracy. To compute its weights and nodes, we\npresent a new fast algorithm. These new quadratures can be used to\napproximate and integrate bandlimited functions, such as prolate\nspheroidal wave functions, and essentially bandlimited functions, such\nas Bessel functions. We also develop, for a given precision, an\ninterpolating basis for bandlimited functions on an interval\n', ['generalized Gaussian quadratures', 'weighted integrals', 'exponential functions', 'integration', 'interpolation', 'bandlimited functions', 'Caratheodory representation theorem', 'eigenvalues', 'Toeplitz matrix', 'trigonometric moments', 'approximation', 'prolate spheroidal wave functions', 'Bessel functions', 'approximation theory', 'Bessel functions', 'eigenvalues and eigenfunctions', 'integration', 'interpolation', 'Toeplitz matrices', 'wave functions']), ("When a better interface and easy navigation aren't enough: examining the\ninformation architecture in a law enforcement agency\nAn information architecture that allows users to easily navigate through a\nsystem and quickly recover from mistakes is often defined as a highly\nusable system. But usability in systems design goes beyond a good\ninterface and efficient navigation. In this article we describe two\ndatabase systems in a law enforcement agency. One system is a legacy,\ntext-based system with cumbersome navigation (RMS); the newer system is\na graphical user interface with simplified navigation (CopNet). It is\nhypothesized that law enforcement users will evaluate CopNet higher\nthan RMS, but experts of the older system will evaluate it higher than\nothers will. We conducted two user studies. One study examined what\nusers thought of RMS and CopNet, and compared RMS experts' evaluations\nwith nonexperts. We found that all users evaluated CopNet as more\neffective, easier to use, and easier to navigate than RMS, and this was\nespecially noticeable for users who were not experts with the older\nsystem. The second, follow-up study examined use behavior after CopNet\nwas deployed some time later. The findings revealed that evaluations of\nCopNet were not associated with its use. If the newer system had a\nbetter interface and was easier to navigate than the older, legacy\nsystem, why were law enforcement personnel reluctant to switch? We\ndiscuss reasons why switching to a new system is difficult, especially\nfor those who are most adept at using the older system. Implications\nfor system design and usability are also discussed\n", ['information architecture', 'law enforcement agency', 'legacy text-based system', 'RMS', 'graphical user interface', 'simplified navigation', 'CopNet', 'law enforcement users', 'graphical user interfaces', 'information systems', 'police data processing']), ('On the convergence of the Bermudez-Moreno algorithm with constant parameters\nA. Bermudez and C. Moreno (1981) presented a duality numerical algorithm for\nsolving variational inequalities of the second kind. The performance of\nthis algorithm strongly depends on the choice of two constant\nparameters. Assuming a further hypothesis of the inf-sup type, we\npresent here a convergence theorem that improves on the one presented\nby A. Bermudez and C. Moreno. We prove that the convergence is linear,\nand we give the expression of the asymptotic error constant and the\nexplicit form of the optimal parameters, as a function of some\nconstants related to the variational inequality. Finally, we present\nsome numerical examples that confirm the theoretical results\n', ['Bermudez-Moreno algorithm', 'duality numerical algorithm', 'variational inequalities', 'convergence theorem', 'asymptotic error constant', 'optimal parameters', 'constant parameters', 'convergence of numerical methods', 'variational techniques']), ('Approximating martingales for variance reduction in Markov process simulation\n"Knowledge of either analytical or numerical approximations should enable more\nefficient simulation estimators to be constructed." This principle\nseems intuitively plausible and certainly attractive, yet no completely\nsatisfactory general methodology has been developed to exploit it. The\nauthors present a new approach for obtaining variance reduction in\nMarkov process simulation that is applicable to a vast array of\ndifferent performance measures. The approach relies on the construction\nof a martingale that is then used as an internal control variate\n', ['Markov process simulation', 'variance reduction', 'approximating martingale-process method', 'martingales', 'performance measures', 'internal control variate', 'complex stochastic processes', 'single-server queue', 'approximation theory', 'Markov processes', 'queueing theory', 'state-space methods']), ('Estimation of thermal coefficients of magneto-optical media\nPreviously we described a method for estimating the thermal conductivity of\nmagneto-optic recording media. The method relies on identifying the\nlaser power that brings the maximum temperature of the TbFeCo layer to\nas high as the Curie temperature. We extensively use a similar method\nto estimate the heat capacity of a dielectric layer, a TbFeCo layer,\nand an aluminum alloy layer of magneto-optic recording media.\nMeasurements are conducted on static disks with a beam of light focused\non a TbFeCo layer. The method has the advantage of thermal diffusion\ndepending on a multilayer structure and irradiation time\n', ['thermal coefficients', 'magneto-optical media', 'thermal conductivity', 'laser power', 'maximum temperature', 'TbFeCo layer', 'Curie temperature', 'dielectric layer', 'heat capacity', 'aluminum alloy layer', 'magneto-optic recording media', 'static disks', 'light focusing', 'thermal diffusion', 'multilayer structure', 'irradiation time', 'TbFeCo', 'cobalt alloys', 'iron alloys', 'magnetic thin films', 'magneto-optical recording', 'optical disc storage', 'optical multilayers', 'terbium alloys', 'thermal conductivity measurement']), ('Women of color in computing\nIt is well known that there is a need to increase the number of women in the\narea of computing, that is in computer science and computer\nengineering. If we consider women of color, that is women of\nunder-represented ethnicities, we find the numbers are very dismal. The\ngoal of this article is to bring to light the unique issues of women of\ncolor based upon the personal experience of one African-American woman\nwho has been in the field of computing for over 20 years (including the\nyears of higher education)\n', ['women of color', 'computer science', 'computer engineering', 'ethnic minority', 'higher education', 'society', 'gender issues', 'computer science', 'computer science education', 'gender issues', 'prejudicial factors']), ("The dynamics of a railway freight wagon wheelset with dry friction damping\nWe investigate the dynamics of a simple model of a wheelset that supports one\nend of a railway freight wagon by springs with linear characteristics\nand dry friction dampers. The wagon runs on an ideal, straight and\nlevel track with constant speed. The lateral dynamics in dependence on\nthe speed is examined. We have included stick-slip and hysteresis in\nour model of the dry friction and assume that Coulomb's law holds\nduring the slip phase. It is found that the action of dry friction\ncompletely changes the bifurcation diagram, and that the longitudinal\ncomponent of the dry friction damping forces destabilizes the wagon\n", ['dynamics', 'railway freight wagon wheelset', 'dry friction damping', 'linear characteristics', 'lateral dynamics', 'stick-slip', 'hysteresis', 'Coulomb law', 'bifurcation diagram', 'longitudinal component', 'bifurcation', 'damping', 'dynamics', 'friction', 'railways']), ("Accurate modeling of lossy nonuniform transmission lines by using differential\nquadrature methods\nThis paper discusses an efficient numerical approximation technique, called the\ndifferential quadrature method (DQM), which has been adapted to model\nlossy uniform and nonuniform transmission lines. The DQM can quickly\ncompute the derivative of a function at any point within its bounded\ndomain by estimating a weighted linear sum of values of the function at\na small set of points belonging to the domain. Using the DQM, the\nfrequency-domain Telegrapher's partial differential equations for\ntransmission lines can be discretized into a set of easily solvable\nalgebraic equations. DQM reduces interconnects into multiport models\nwhose port voltages and currents are related by rational formulas in\nthe frequency domain. Although the rationalization process in DQM is\ncomparable with the Pade approximation of asymptotic waveform\nevaluation (AWE) applied to transmission lines, the derivation\nmechanisms in these two disparate methods are significantly different.\nUnlike AWE, which employs a complex moment-matching process to obtain\nrational approximation, the DQM requires no approximation of\ntranscendental functions, thereby avoiding the process of moment\ngeneration and moment matching. Due to global sampling of points in the\nDQM approximation, it requires far fewer grid points in order to build\naccurate discrete models than other numerical methods do. The DQM-based\ntime-domain model can be readily integrated in a circuit simulator like\nSPICE\n", ['lossy nonuniform transmission lines', 'differential quadrature method', 'numerical approximation technique', 'frequency-domain Telegrapher PDE', 'partial differential equations', 'algebraic equations', 'interconnects', 'multiport models', 'multiconductor transmission lines', 'rationalization process', 'time-domain model', 'circuit simulation', 'integrated circuit interconnections', 'matrix algebra', 'time-domain analysis', 'transient analysis', 'transmission line theory']), ("New age computing [autonomic computing]\nAutonomic computing (AC), sometimes called self-managed computing, is the name\nchosen by IBM to describe the company's new initiative aimed at making\ncomputing more reliable and problem-free. It is a response to a growing\nrealization that the problem today with computers is not that they need\nmore speed or have too little memory, but that they crash all too\noften. This article reviews current initiatives being carried out in\nthe AC field by the IT industry, followed by key challenges which\nrequire to be addressed in its development and implementation\n", ['autonomic computing', 'new age computing', 'AC', 'self-managed computing', 'IBM initiative', 'computing reliability', 'problem-free computing', 'computer speed', 'computer memory', 'computer crash', 'IT industry initiatives', 'AC requirements', 'AC development', 'AC implementation', 'open standards', 'self-healing computing', 'adaptive algorithms', 'computer network management', 'computer network reliability', 'error correction', 'fault tolerant computing', 'open systems', 'reviews', 'self-adjusting systems', 'software reliability', 'software standards', 'technological forecasting']), ('A novel genetic algorithm for the design of a signed power-of-two coefficient\nquadrature mirror filter lattice filter bank\nA novel genetic algorithm (GA) for the design of a canonical signed\npower-of-two (SPT) coefficient lattice structure quadrature mirror\nfilter bank is presented. Genetic operations may render the SPT\nrepresentation of a value noncanonical. A new encoding scheme is\nintroduced to encode the SPT values. In this new scheme, the canonical\nproperty of the SPT values is preserved under genetic operations.\nAdditionally, two new features that drastically improve the performance\nof our GA are introduced. (1) An additional level of natural selection\nis introduced to simulate the effect of natural selection when sperm\ncells compete to fertilize an ovule; this dramatically improves the\noffspring survival rate. A conventional GA is analogous to\nintracytoplasmic sperm injection and has an extremely low offspring\nsurvival rate, resulting in very slow convergence. (2) The probability\nof mutation for each codon of a chromosome is weighted by the\nreciprocal of its effect. Because of these new features, the\nperformance of our new GA outperforms conventional GAs\n', ['genetic algorithm', 'signed power-of-two coefficient lattice structure', 'quadrature mirror filter', 'QMF', 'lattice filter bank', 'encoding scheme', 'natural selection', 'offspring survival rate', 'chromosome codon', 'signal processing', 'perfect reconstruction', 'channel bank filters', 'encoding', 'filtering theory', 'genetic algorithms', 'lattice filters', 'quadrature mirror filters', 'signal reconstruction']), ('Autonomous detection of crack initiation using surface-mounted piezotransducers\nIn this paper we report on the application of an in situ health monitoring\nsystem, comprising an array of piezoceramic wafer elements, to the\ndetection of fatigue degradation in metallic specimens exposed to\ncyclic loading. Lamb waves, transmitted through a beam test coupon, are\nsensed using small surface-mounted piezotransducer elements, and the\nsignals are then autonomously analysed for indications relating to the\nonset of structural degradation. The experimental results confirm the\nefficacy of the approach and provide a demonstration of good robustness\nunder realistic loading conditions, emphasizing the great potential for\ndeveloping an automated in situ structural health monitoring system for\napplication to fatigue-prone operational structures, such as aircraft\n', ['in situ health monitoring', 'piezoceramic wafer elements', 'fatigue degradation', 'metallic specimens', 'cyclic loading', 'Lamb waves', 'surface-mounted piezotransducer elements', 'structural degradation', 'robustness', 'loading conditions', 'automated in situ structural health monitoring', 'fatigue operational structures', 'aircraft', 'aircraft testing', 'automatic testing', 'computerised monitoring', 'condition monitoring', 'crack detection', 'fatigue testing', 'intelligent actuators', 'piezoceramics', 'piezoelectric transducers', 'surface acoustic wave transducers', 'surface acoustic waves']), ('Quantum learning and universal quantum matching machine\nSuppose that three kinds of quantum systems are given in some unknown states\n|f>/sup (X)N/, |g/sub 1/>/sup (X)K/, and |g/sub 2/>/sup (X)K/,\nand we want to decide which template state |g/sub 1/> or |g/sub\n2/>, each representing the feature of the pattern class C/sub 1/ or\nC/sub 2/, respectively, is closest to the input feature state |f>.\nThis is an extension of the pattern matching problem into the quantum\ndomain. Assuming that these states are known a priori to belong to a\ncertain parametric family of pure qubit systems, we derive two kinds of\nmatching strategies. The first one is a semiclassical strategy that is\nobtained by the natural extension of conventional matching strategies\nand consists of a two-stage procedure: identification (estimation) of\nthe unknown template states to design the classifier (learning process\nto train the classifier) and classification of the input system into\nthe appropriate pattern class based on the estimated results. The other\nis a fully quantum strategy without any intermediate measurement, which\nwe might call as the universal quantum matching machine. We present the\nBayes optimal solutions for both strategies in the case of K=1, showing\nthat there certainly exists a fully quantum matching procedure that is\nstrictly superior to the straightforward semiclassical extension of the\nconventional matching strategy based on the learning process\n', ['quantum learning', 'universal quantum matching machine', 'pattern class', 'pattern matching problem', 'quantum domain', 'qubit systems', 'matching strategies', 'semiclassical strategy', 'two-stage procedure', 'quantum strategy', 'Bayes optimal solutions', 'quantum matching procedure', 'semiclassical extension', 'matching strategy', 'learning process', 'learning systems', 'quantum theory']), ("The role of speech input in wearable computing\nSpeech recognition seems like an attractive input mechanism for wearable\ncomputers, and as we saw in this magazine's first issue, several\ncompanies are promoting products that use limited speech interfaces for\nspecific tasks. However, we must overcome several challenges to using\nspeech recognition in more general contexts, and interface designers\nmust be wary of applying the technology to situations where speech is\ninappropriate\n", ['wearable computing', 'speech input', 'speech recognition', 'wearable computer', 'speech recognizers', 'mobile speech recognition', 'background noise', 'speech interfaces', 'mobile computing', 'portable computers', 'speech recognition', 'speech-based user interfaces']), ('A gendered view of computer professionals: preliminary results of a survey\nThe under-representation of women in the computing profession in many parts the\nwestern world has received our attention through numerous publications,\nthe noticeable low representation of women at computer science\nconferences and in the lecture halls. Over the past two decades, the\nsituation had become worse. This paper seeks to add to the dialogue by\npresenting preliminary findings from a research project conducted in\nfour countries. The aim of this research was to gain an insight into\nthe perceptions future computer professionals hold on the category of\nemployment loosely defined under the term of "a computer professional."\nOne goal was to get insight into whether or not there is a difference\nbetween female and mate students regarding their view of computer\nprofessionals. Other goals were to determine if there was any\ndifference between female and male students in different parts of the\nworld, as well as who or what most influences the students to undertake\ntheir courses in computing\n', ['women under-representation', 'computing profession', 'future computer professional perceptions', 'employment', 'mate students', 'female students', 'computing courses', 'computer science education', 'employment', 'gender issues', 'professional aspects']), ('Mothball mania [3G licences]\nTelefonica Moviles has frozen its 3G operations in Germany, Austria, Italy and\nSwitzerland. With other 3G licence holders questioning the logic of\nentering already saturated markets with unproven technology, Emma\nMcClune asks if the mothball effect is set to snowball any further\n', ['3G licence holders', 'saturated markets', 'mothball', 'mobile telephony', 'cellular radio', 'mobile computing']), ('X-Rite: more than a graphic arts company\nAlthough it is well known as a maker of densitometers and spectrophotometers,\nX-Rite is active in measuring light and shape in many industries. Among\nthem are automobile finishes, paint and home improvements, scientific\ninstruments, optical semiconductors and even cosmetic dentistry\n', ['X-Rite', 'graphic arts', 'colour measurement', 'colour graphics', 'publishing']), ('Non-optimal universal quantum deleting machine\nWe verify the non-existence of some standard universal quantum deleting\nmachine. Then a non-optimal universal quantum deleting machine is\nconstructed and we emphasize the difficulty for improving its fidelity.\nIn a way, our results complement the universal quantum cloning machine\nestablished by Buzek and Hillery (1996), and manifest some of their\ndistinctions\n', ['nonoptimal universal quantum deleting machine', 'fidelity', 'NUQDM', 'universal quantum cloning machine', 'bound states', 'information theory', 'quantum computing', 'quantum cryptography', 'quantum theory']), ('Debugging Web applications\nThe author considers how one can save time tracking down bugs in Web-based\napplications by arming yourself with the right tools and programming\npractices. A wide variety of debugging tools have been written with Web\ndevelopers in mind\n', ['Web application debugging tools', 'programming', 'Internet', 'program debugging', 'software tools']), ('From FREE to FEE [online advertising market]\nAs the online advertising market continues to struggle, many online content\nmarketers are wrestling with the issue of how to add at least some\nlevel of paid subscription income to their revenue mix in order to\nreach or improve profitability. Since the business of selling content\nonline is still in its infancy, and many consumers clearly still think\nof Web content as simply and rightfully free, few roadmaps are\navailable to show the way to effective marketing strategies, but some\nguiding principles have emerged\n', ['online advertising market', 'paid subscription income', 'selling content online', 'marketing strategies', 'advertising', 'Internet', 'marketing']), ('Quantum-state information retrieval in a Rydberg-atom data register\nWe analyze a quantum search protocol to retrieve phase information from a\nRydberg-atom data register using a subpicosecond half-cycle electric\nfield pulse. Calculations show that the half-cycle pulse can perform\nthe phase retrieval only within a range of peak field values. By\nvarying the phases of the constituent orbitals of the Rydberg wave\npacket register, we demonstrate coherent control of the phase retrieval\nprocess. By specially programming the phases of the orbitals comprising\nthe initial wave packet, we show that it is possible to use the search\nmethod as a way to synthesize single energy eigenstates\n', ['quantum-state information retrieval', 'Rydberg-atom data register', 'quantum search protocol', 'phase information', 'subpicosecond half-cycle electric field pulse', 'phase retrieval', 'peak field values', 'constituent orbitals', 'Rydberg wave packet register', 'coherent control', 'initial wave packet', 'search method', 'single energy eigenstates', 'half-cycle pulse', 'information theory', 'quantum computing', 'quantum theory', 'Rydberg states', 'storage media']), ('Recognizing groups G/sub 2/(3/sup n/) by their element orders\nIt is proved that a finite group that is isomorphic to a simple non-Abelian\ngroup G = G/sub 2/(3/sup n/) is, up to isomorphism, recognized by a set\nomega (G) of its element orders, that is, H approximately= G if omega\n(H) = omega (G) for some finite group H\n', ['element orders', 'finite group', 'isomorphism', 'formal logic', 'group theory']), ('An efficient parallel algorithm for the calculation of canonical MP2 energies\nWe present the parallel version of a previous serial algorithm for the\nefficient calculation of canonical MP2 energies. It is based on the\nSaebo-Almlof direct-integral transformation, coupled with an efficient\nprescreening of the AO integrals. The parallel algorithm avoids\nsynchronization delays by spawning a second set of slaves during the\nbin-sort prior to the second half-transformation. Results are presented\nfor systems with up to 2000 basis functions. MP2 energies for molecules\nwith 400-500 basis functions can be routinely calculated to\nmicrohartree accuracy on a small number of processors (6-8) in a matter\nof minutes with modern PC-based parallel computers\n', ['parallel algorithm', 'canonical MP2 energies', 'Saebo-Almlof direct-integral transformation', 'AO integrals', 'synchronization delays', 'second half-transformation', 'basis functions', 'MP2 energies', 'microhartree accuracy', 'PC-based parallel computers', 'chemistry computing', 'orbital calculations', 'parallel algorithms', 'physics computing', 'transforms']), ('Reconstruction of time-varying 3-D left-ventricular shape from multiview X-ray\ncineangiocardiograms\nThis paper reports on the clinical application of a system for recovering the\ntime-varying three-dimensional (3-D) left-ventricular (LV) shape from\nmultiview X-ray cineangiocardiograms. Considering that X-ray\ncineangiocardiography is still commonly employed in clinical cardiology\nand computational costs for 3-D recovery and visualization are rapidly\ndecreasing, it is meaningful to develop a clinically applicable system\nfor 3-D LV shape recovery from X-ray cineangiocardiograms. The system\nis based on a previously reported closed-surface method of shape\nrecovery from two-dimensional occluding contours with multiple views.\nTo apply the method to "real" LV cineangiocardiograms, user-interactive\nsystems were implemented for preprocessing, including detection of LV\ncontours, calibration of the imaging geometry, and setting of the LV\nmodel coordinate system. The results for three real LV angiographic\nimage sequences are presented, two with fixed multiple views (using\nsupplementary angiography) and one with rotating views. 3-D\nreconstructions utilizing different numbers of views were compared and\nevaluated in terms of contours manually traced by an experienced\nradiologist. The performance of the preprocesses was also evaluated,\nand the effects of variations in user-specified parameters on the final\n3-D reconstruction results were shown to be sufficiently small. These\nexperimental results demonstrate the potential usefulness of combining\nmultiple views for 3-D recovery from "real" LV cineangiocardiograms\n', ['medical diagnostic imaging', 'time-varying 3-D left-ventricular shape reconstruction', 'multiview X-ray cineangiocardiograms', 'clinical cardiology', 'two-dimensional occluding contours', 'arterial septal defect', 'B-spline', 'computational costs', 'user-interactive systems', 'angiographic image sequences', 'fixed multiple views', 'experienced radiologist', 'user-specified parameters variations', 'angiocardiography', 'calibration', 'image reconstruction', 'image sequences', 'medical image processing', 'shape measurement']), ('Diagnosis of the technical state of heat systems\nA step-by-step approach to the diagnosis of the technical state of heat systems\nis stated. The class of physical defects is supplemented by the\nbehavioral defects of objects, which are related to the disturbance of\nthe modes of their operation. The implementation of the approach is\nillustrated by an example of the solution of a specific problem of the\ndiagnosis of a closed heat consumption system\n', ['heat system technical state diagnosis', 'step-by-step diagnosis', 'operational mode disturbance', 'closed heat consumption system diagnosis', 'district heating', 'fault diagnosis']), ('Accuracy and stability of splitting with Stabilizing Corrections\nThis paper contains a convergence analysis for the method of stabilizing\ncorrections, which is an internally consistent splitting scheme for\ninitial-boundary value problems. To obtain more accuracy and a better\ntreatment of explicit terms several extensions are regarded and\nanalyzed. The relevance of the theoretical results is tested for\nconvection-diffusion-reaction equations\n', ['stability', 'convergence analysis', 'stabilizing corrections', 'splitting scheme', 'initial-boundary value problems', 'convection-diffusion-reaction equations', 'eigenvalues and eigenfunctions', 'initial value problems', 'matrix algebra', 'stability']), ('Well-posed anisotropic diffusion for image denoising\nA nonlinear iterative smoothing filter based on a second-order partial\ndifferential equation is introduced. It smooths out the image according\nto an anisotropic diffusion process. The approach is based on a smooth\napproximation of the total variation (TV) functional which overcomes\nthe non-differentiability of the TV functional at the origin. In\nparticular, the authors perform linear smoothing over smooth areas but\nselective smoothing over candidate edges. By relating the smoothing\nparameter to the time step, they arrive at a CFL condition which\nguarantees the causality of the discrete scheme. This allows the\nadoption of higher time discretisation steps, while ensuring the\nabsence of artefacts deriving from the non-smooth behaviour of the TV\nfunctional at the origin. In particular, it is shown that the proposed\napproach avoids the typical staircase effects in smooth areas which\noccur in the standard time-marching TV scheme\n', ['image denoising', 'well-posed anisotropic diffusion', 'nonlinear iterative smoothing filter', 'second-order partial differential equation', 'total variation functional', 'linear smoothing', 'selective smoothing', 'CFL condition', 'discrete scheme', 'causality', 'higher time discretisation steps', 'image restoration problem', 'random Gaussian noise', 'causality', 'filtering theory', 'functional analysis', 'Gaussian noise', 'image restoration', 'iterative methods', 'nonlinear filters', 'partial differential equations', 'smoothing methods']), ('Component support in PLT scheme\nPLT Scheme (DrScheme and MzScheme) supports the Component Object Model (COM)\nstandard with two pieces of software. The first piece is MzCOM, a COM\nclass that makes a Scheme evaluator available to COM clients. With\nMzCOM, programmers can embed Scheme code in programs written in\nmainstream languages such as C++ or Visual BASIC. Some applications can\nalso be used as MzCOM clients. The other piece of component-support\nsoftware is MysterX, which makes COM classes available to PLT Scheme\nprograms. When needed, MysterX uses a programmable Web browser to\ndisplay COM objects. We describe the technical issues encountered in\nbuilding these two systems and sketch some applications\n', ['PLT Scheme', 'Component Object Model', 'MzCOM', 'reuse', 'Web browser', 'distributed object management', 'online front-ends', 'software reusability']), ('Relation between glare and driving performance\nThe present study investigated the effects of discomfort glare on driving\nbehavior. Participants (old and young; US and Europeans) were exposed\nto a simulated low- beam light source mounted on the hood of an\ninstrumented vehicle. Participants drove at night in actual traffic\nalong a track consisting of urban, rural, and highway stretches. The\nresults show that the relatively low glare source caused a significant\ndrop in detecting simulated pedestrians along the roadside and made\nparticipants drive significantly slower on dark and winding roads.\nOlder participants showed the largest drop in pedestrian detection\nperformance and reduced their driving speed the most. The results\nindicate that the de Boer rating scale, the most commonly used rating\nscale for discomfort glare, is practically useless as a predictor of\ndriving performance. Furthermore, the maximum US headlamp intensity\n(1380 cd per headlamp) appears to be an acceptable upper limit\n', ['glare', 'driving performance', 'discomfort glare', 'simulated low-beam light source', 'road traffic', 'urban road', 'rural road', 'highway', 'deBoer rating scale', 'human factors', 'road vehicles', 'visual perception']), ("Explanations for the perpetration of and reactions to deception in a virtual\ncommunity\nCases of identity deception on the Internet are not uncommon. Several cases of\na revealed identity deception have been reported in the media. The\nauthors examine a case of deception in an online community composed\nprimarily of information technology professionals. In this case, an\nestablished community member (DF) invented a character (Nowheremom)\nwhom he fell in love with and who was eventually killed in a tragic\naccident. When other members of the community eventually began to\nquestion Nowheremom's actual identity, DF admitted that he invented\nher. The discussion board was flooded with reactions to DF's\nrevelation. The authors propose several explanations for the\nperpetration of identity deception, including psychiatric illness,\nidentity play, and expressions of true self. They also analyze the\nreactions of community members and propose three related explanations\n(social identity, deviance, and norm violation) to account for their\nreactions. It is argued that virtual communities' reactions to such\nthreatening events provide invaluable clues for the study of group\nprocesses on the Internet\n", ['virtual community', 'identity deception', 'Internet', 'online community', 'information technology professionals', 'psychiatric illness', 'group processes', 'social processes', 'Web sites', 'psychology', 'bulletin boards', 'electronic mail', 'human factors', 'information resources', 'Internet', 'psychology', 'social aspects of automation']), ('A new architecture for implementing pipelined FIR ADF based on classification\nof coefficients\nIn this paper, we propose a new method for implementing pipelined\nfinite-impulse response (FIR) adaptive digital filter (ADF), with an\naim of reducing the maximum delay of the filtering portion of\nconventional delayed least mean square (DLMS) pipelined ADF. We achieve\na filtering section with a maximum delay of one by simplifying a\npre-upsampled and a post-downsampled FIR filter using the concept of\nclassification of coefficients. This reduction is independent of the\norder of the filter, which is an advantage when the order of the filter\nis very large, and as a result the method can also be applied to\ninfinite impulse response (IIR) filters. Furthermore, when the proposed\nmethod is compared with the transpose ADF, which has a filtering\nsection with zero delay, it is realized that it significantly reduces\nthe maximum delay associated with updating the coefficients of FIR ADF.\nThe effect of this is that, the proposed method exhibits a higher\nconvergence speed in comparison to the transpose FIR ADF\n', ['pipelined FIR ADF', 'coefficient classification', 'adaptive digital filter', 'maximum delay', 'delayed least mean square filter', 'pre-upsampled filter', 'post-downsampled filter', 'convergence speed', 'adaptive filters', 'delays', 'digital filters', 'FIR filters', 'least mean squares methods', 'pipeline processing']), ('In search of a general enterprise model\nMany organisations, particularly SMEs, are reluctant to invest time and money\nin models to support decision making. Such reluctance could be overcome\nif a model could be used for several purposes rather than using a\ntraditional "single perspective" model. This requires the development\nof a "general enterprise model" (GEM), which can be applied to a wide\nrange of problem domains with unlimited scope. Current enterprise\nmodelling frameworks only deal effectively with nondynamic modelling\nissues whilst dynamic modelling issues have traditionally only been\naddressed at the operational level. Although the majority of research\nin this area relates to manufacturing companies, the framework for a\nGEM must be equally applicable to service and public sector\norganisations. The paper identifies five key design issues that need to\nbe considered when constructing a GEM. A framework for such a GEM is\npresented based on a "plug and play" methodology and demonstrated by a\nsimple case study\n', ['general enterprise model', 'business process re-engineering', 'SMEs', 'decision making', 'single perspective model', 'GEM', 'problem domains', 'enterprise modelling frameworks', 'operational level', 'dynamic modelling issues', 'public sector organisations', 'service sector organisations', 'plug and play methodology', 'case study', 'commerce', 'decision support systems', 'systems re-engineering']), ('A robust H/sub infinity / control approach for induction motors\nThis paper deals with the robustness and stability of an induction motor\ncontrol structure against internal and external disturbances. In the\nproposed control scheme, we have used an H/sub infinity / controller\nwith field orientation and input-output linearization to achieve the\nabove-specified features. Simulation results are included to illustrate\nthe control approach performances\n', ['robust H/sub infinity / control', 'induction motors control', 'robustness', 'stability', 'internal disturbances', 'external disturbances', 'field orientation', 'input-output linearization', 'H/sup infinity / control', 'induction motors', 'linearisation techniques', 'machine control', 'robust control']), ('Documentum completes CM Trifecta\nDaily, people participating in clinical trials for drug companies fill out\nforms describing how they feel physically and emotionally. For some\ntrials, there are hundreds, possibly thousands, of participants. The\ndrug companies must compile all the forms and submit them\nelectronically to the FDA. That\'s where Documentum comes in. "We\'ve\nstreamlined the whole process of managing clinical trial content for\ncompanies, such as Johnson & Johnson, Bristol Myers Squibb, and\nPfizer," notes Documentum\'s president and CEO Dave De Walt. "And by the\nway, the FDA also is one of our customers, as well as the EPA and the\nFAA." And there are about 1,300 other organizations in various\nindustries worldwide that rely on Documentum\'s technologies,\nconsulting, and training services. The company\'s products are designed\nto manage digital content and facilitate online transactions, partner\nand supplier relationships, and ebusiness interactions\n', ['clinical trials', 'drug companies', 'FDA', 'Documentum', 'clinical trial content', 'training services', 'consulting services', 'medical administrative data processing', 'pharmaceutical industry', 'transaction processing']), ('Real-time quasi-2-D inversion of array resistivity logging data using neural\nnetwork\nWe present a quasi-2-D real-time inversion algorithm for a modern galvanic\narray tool via dimensional reduction and neural network simulation.\nUsing reciprocity and superposition, we apply a numerical focusing\ntechnique to the unfocused data. The numerically focused data are much\nless subject to 2-D and layering effects and can be approximated as\nfrom a cylindrical 1-D Earth. We then perform 1-D inversion on the\nfocused data to provide approximate information about the 2-D\nresistivity structure. A neural network is used to perform forward\nmodeling in the 1-D inversion, which is several hundred times faster\nthan conventional numerical forward solutions. Testing our inversion\nalgorithm on both synthetic and field data shows that this fast\ninversion algorithm is useful for providing formation resistivity\ninformation at a well site\n', ['real-time quasi-2-D inversion', 'array resistivity logging data', 'neural network', 'real-time inversion algorithm', 'galvanic array tool', 'dimensional reduction', 'reciprocity', 'superposition', 'numerical focusing technique', 'unfocused data', '1-D inversion', 'focused data', 'forward modeling', 'formation resistivity', 'well site', 'array signal processing', 'Earth crust', 'geophysical prospecting', 'geophysical signal processing', 'inverse problems', 'neural nets', 'terrestrial electricity']), ('Client satisfaction in a feasibility study comparing face-to-face interviews\nwith telepsychiatry\nWe carried out a pilot study comparing satisfaction levels between psychiatric\npatients seen face to face (FTF) and those seen via videoconference.\nPatients who consented were randomly assigned to one of two groups. One\ngroup received services in person (FTF from the visiting psychiatrist)\nwhile the other was seen using videoconferencing at 128 kbit/s. One\npsychiatrist provided all the FTF and videoconferencing assessment and\nfollow-up visits. A total of 24 subjects were recruited. Three of the\nsubjects (13%) did not attend their appointments and two subjects in\neach group were lost to follow-up. Thus there were nine in the FTF\ngroup and eight in the videoconferencing group. The two groups were\nsimilar in most respects. Patient satisfaction with the services was\nassessed using the Client Satisfaction Questionnaire (CSQ-8), completed\nfour months after the initial consultation. The mean scores were 25.3\nin the FTF group and 21.6 in the videoconferencing group. Although\nthere was a trend in favour of the FTF service, the difference was not\nsignificant. Patient satisfaction is only one component of evaluation.\nThe efficacy of telepsychiatry must also be measured relative to that\nof conventional, FTF care before policy makers can decide how\nextensively telepsychiatry should be implemented\n', ['client satisfaction', 'face-to-face interviews', 'telepsychiatry', 'psychiatric patient satisfaction', 'human factors', 'videoconference', 'Client Satisfaction Questionnaire', 'telemedicine', '128 kbit/s', 'human factors', 'medical computing', 'patient treatment', 'psychology', 'teleconferencing', 'telemedicine']), ('Blind source separation applied to image cryptosystems with dual encryption\nBlind source separation (BSS) is explored to add another encryption level\nbesides the existing encryption methods for image cryptosystems. The\ntransmitted images are covered with a noise image by specific mixing\nbefore encryption and then recovered through BSS after decryption.\nSimulation results illustrate the validity of the proposed method\n', ['blind source separation', 'transmitted images', 'noise image', 'image cryptosystems', 'dual encryption', 'cryptography', 'image coding', 'statistical analysis']), ('Estimation of an N-L-N Hammerstein-Wiener model\nEstimation of a single-input single-output block-oriented model is studied. The\nmodel consists of a linear block embedded between two static nonlinear\ngains. Hence, it is called an N-L-N Hammerstein-Wiener model. First,\nthe model structure is motivated and the disturbance model is\ndiscussed. The paper then concentrates on parameter estimation. A\nrelaxation iteration scheme is proposed by making use of a model\nstructure in which the error is bilinear-in-parameters. This leads to a\nsimple algorithm which minimizes the original loss function. The\nconvergence and consistency of the algorithm are studied. In order to\nreduce the variance error, the obtained linear model is further reduced\nusing frequency weighted model reduction. A simulation study is used to\nillustrate the method\n', ['N-L-N Hammerstein-Wiener model', 'single-input single-output block-oriented model', 'linear block', 'static nonlinear gains', 'model structure', 'disturbance model', 'parameter estimation', 'relaxation iteration scheme', 'bilinear-in-parameters error', 'convergence', 'consistency', 'variance error', 'frequency weighted model reduction', 'nonlinear process', 'iterative methods', 'parameter estimation', 'reduced order systems', 'relaxation theory']), ("All change [agile business]\nWhat does it take for an organisation to become an agile business? Its\nemployees probably need to adhere to new procurement policies, work\nmore closely with colleagues in other departments, meet more exacting\nsales targets, and offer higher standards of customer service and\nsupport. In short, they need to change the way they work. Implementing\ntechnologies to support agile business models and underpin new\npractices is a complex task in itself. But getting employees to adopt\nnew practices is far harder, and one that requires careful handling,\nsays Barry O'Connell, general manager of business-to-employee (B2E)\nsolutions at systems vendor Hewlett-Packard (HP)\n", ['agile business', 'corporate transformation', 'organisational change', 'management information systems']), ('A self-organizing context-based approach to the tracking of multiple robot\ntrajectories\nWe have combined competitive and Hebbian learning in a neural network designed\nto learn and recall complex spatiotemporal sequences. In such\nsequences, a particular item may occur more than once or the sequence\nmay share states with another sequence. Processing of repeated/shared\nstates is a hard problem that occurs very often in the domain of\nrobotics. The proposed model consists of two groups of synaptic\nweights: competitive interlayer and Hebbian intralayer connections,\nwhich are responsible for encoding respectively the spatial and\ntemporal features of the input sequence. Three additional mechanisms\nallow the network to deal with shared states: context units, neurons\ndisabled from learning, and redundancy used to encode sequence states.\nThe network operates by determining the current and the next state of\nthe learned sequences. The model is simulated over various sets of\nrobot trajectories in order to evaluate its storage and retrieval\nabilities; its sequence sampling effects; its robustness to noise and\nits tolerance to fault\n', ['self-organizing context-based approach', 'trajectories tracking', 'competitive learning', 'Hebbian learning', 'complex spatiotemporal sequences', 'synaptic weights', 'competitive interlayer connections', 'Hebbian intralayer connections', 'shared states', 'context units', 'sequence states', 'robot trajectories', 'unsupervised learning', 'storage abilities', 'retrieval abilities', 'sequence sampling effects', 'fault tolerance', 'fault tolerance', 'Hebbian learning', 'position control', 'robots', 'self-adjusting systems', 'unsupervised learning']), ('Who Wants To Be A Millionaire(R): The classroom edition\nThis paper introduces a version of the internationally popular television game\nshow Who Wants To Be A Millionaire(R) that has been created for use in\nthe classroom using Microsoft PowerPoint(R). A suggested framework for\nits classroom use is presented, instructions on operating and editing\nthe classroom version of Who Wants To Be A Millionaire(R) are provided,\nand sample feedback from students who have played the classroom version\nof Who Wants To Be A Millionaire(R) is offered\n', ['classroom', 'Who Wants To Be A Millionaire(R)', 'classroom version', 'undergraduate business students', 'student contestants', 'computer aided instruction', 'management education']), ('Multivariable H/sub infinity // mu feedback control design for high-precision\nwafer stage motion\nConventional PID-like SISO controllers are still the most common in industry,\nbut with performance requirements becoming tighter there is a growing\nneed for advanced controllers. For the positioning devices in\nIC-manufacturing, plant interaction is a major performance-limiting\nfactor. MIMO control can be invoked to tackle this problem. A\npractically feasible procedure is presented to design MIMO feedback\ncontrollers for electromechanical positioning devices, using H/sub\ninfinity // mu techniques. Weighting filters are proposed to\nstraightforwardly and effectively impose performance and uncertainty\nspecifications. Experiments show that MIMO control can considerably\nimprove upon the performance with multiloop SISO control. Some problems\nare highlighted that are important for industrial practice, but lacking\na workable solution\n', ['IC manufacture', 'multivariable control systems', 'weighting filters', 'MIMO systems', 'H/sub infinity / control', 'feedback', 'servo systems', 'model uncertainty', 'motion control', 'mechatronics', 'mu synthesis', 'feedback', 'H/sup infinity / control', 'integrated circuit manufacture', 'MIMO systems', 'motion control', 'multivariable control systems', 'servomechanisms']), ("Academic libraries and community: making the connection\nI explore the theme of academic libraries serving and reaching out to the\nbroader community. I highlight interesting projects reported on in the\nliterature (such as the Through Our Parents' Eyes project) and report\non others. I look at challenges to community partnerships and\nrecommendations for making them succeed. Although I focus on links with\nthe broader community, I also took at methods for increasing\ncooperation among various units on campus, so that the needs of campus\ncommunity groups-such as distance education students or disabled\nstudents-are effectively addressed. Though academic libraries are my\nfocus, we can learn a lot from the community building efforts of public\nlibraries\n", ['academic libraries', 'community partnerships', 'campus community groups', 'distance education students', 'disabled students', 'public libraries', 'academic libraries']), ('Robust output feedback model predictive control using off-line linear matrix\ninequalities\nA fundamental question about model predictive control (MPC) is its robustness\nto model uncertainty. In this paper, we present a robust constrained\noutput feedback MPC algorithm that can stabilize plants with both\npolytopic uncertainty and norm-bound uncertainty. The design procedure\ninvolves off-line design of a robust constrained state feedback MPC law\nand a state estimator using linear matrix inequalities (LMIs). Since we\nemploy an off-line approach for the controller design which gives a\nsequence of explicit control laws, we are able to analyze the robust\nstabilizability of the combined control laws and estimator, and by\nadjusting the design parameters, guarantee robust stability of the\nclosed-loop system in the presence of constraints. The algorithm is\nillustrated with two examples\n', ['robust output feedback model predictive control', 'off-line linear matrix inequalities', 'model uncertainty robustness', 'robust constrained output feedback MPC algorithm', 'polytopic uncertainty', 'norm-bound uncertainty', 'controller design procedure', 'robust constrained state feedback MPC law', 'state estimator', 'explicit control law sequence', 'closed-loop system', 'asymptotically stable invariant ellipsoid', 'asymptotic stability', 'closed loop systems', 'control system synthesis', 'matrix algebra', 'predictive control', 'robust control', 'state estimation', 'state feedback']), ('A multi-agent system infrastructure for software component marketplace: an\nontological perspective\nIn this paper, we introduce a multi-agent system architecture and an\nimplemented prototype for a software component marketplace. We\nemphasize the ontological perspective by discussing ontology modeling\nfor the component marketplace, UML extensions for ontology modeling,\nand the idea of ontology transfer which makes the multi-agent system\nadapt itself to dynamically changing ontologies\n', ['multi-agent system architecture', 'software component marketplace', 'ontology modeling', 'UML extensions', 'ontology transfer', 'dynamically changing ontologies', 'adaptation', 'electronic commerce', 'knowledge engineering', 'multi-agent systems', 'object-oriented programming', 'software libraries', 'specification languages']), ('A parallelized indexing method for large-scale case-based reasoning\nCase-based reasoning (CBR) is a problem solving methodology commonly seen in\nartificial intelligence. It can correctly take advantage of the\nsituations and methods in former cases to find out suitable solutions\nfor new problems. CBR must accurately retrieve similar prior cases for\ngetting a good performance. In the past, many researchers proposed\nuseful technologies to handle this problem. However, the performance of\nretrieving similar cases may be greatly influenced by the number of\ncases. In this paper, the performance issue of large-scale CBR is\ndiscussed and a parallelized indexing architecture is then proposed for\nefficiently retrieving similar cases in large-scale CBR. Several\nalgorithms for implementing the proposed architecture are also\ndescribed. Some experiments are made and the results show the\nefficiency of the proposed method\n', ['parallelized indexing method', 'large-scale case-based reasoning', 'problem solving methodology', 'artificial intelligence', 'bitwise indexing', 'similar prior case retrieval', 'performance', 'experiments', 'case-based reasoning', 'database indexing', 'deductive databases', 'parallel processing', 'problem solving']), ("PDF subscriptions bolster revenue\nIn 1999 SD Times offered prospective subscribers the option of receiving their\nissues as Adobe Acrobat PDF files. What set the proposal apart from\nwhat other publishers were doing electronically on the Web was that\nreaders would get the entire version of the paper-including both\nadvertising and editorial just as it looked when it was laid out and\nwent to press. SD Times is only one of a small, but growing, number of\npublications that are taking on the electronic world and finding\nsuccess. In the past six months alone, the New York Times, Popular\nMechanics, trade magazine Electronic Buyers' News, and the Harvard\nBusiness Review have launched digital versions of their newspapers and\nmagazines to augment their online and print versions. The reasons are\nas varied as the publishers themselves. Some companies are finding that\nreaders don't like their Web-based versions either due to poor\nnavigation or missing graphics and images. Others want to expand their\npublications nationally and internationally, but don't want the added\ncost of postage and printing. Still others are looking for ways to give\nadvertisers additional visibility and boost advertising and\nsubscription revenues. No matter what the reason, it's a trend worth\nwatching\n", ['PDF subscriptions', 'SD Times', 'Adobe Acrobat PDF files', 'newspaper', 'electronic issue', 'digital versions', 'magazines', 'electronic publishing']), ("A humanist's legacy in medical informatics: visions and accomplishments of\nProfessor Jean-Raoul Scherrer\nThe objective is to report on the work of Prof. Jean-Raoul Scherrer, and show\nhow his humanist vision, medical skills and scientific background have\nenabled and shaped the development of medical informatics over the last\n30 years. Starting with the mainframe-based patient-centred hospital\ninformation system DIOGENE in the 70s, Prof. Scherrer developed,\nimplemented and evolved innovative concepts of man-machine interfaces,\ndistributed and federated environments, leading the way with\ninformation systems that obstinately focused on the support of care\nproviders and patients. Through a rigorous design of terminologies and\nontologies, the DIOGENE data would then serve as a basis for the\ndevelopment of clinical research, data mining, and lead to innovative\nnatural language processing techniques. In parallel, Prof. Scherrer\nsupported the development of medical image management, ranging from a\ndistributed picture archiving and communication systems (PACS) to\nmolecular imaging of protein electrophoreses. Recognizing the need for\nimproving the quality and trustworthiness of medical information of the\nWeb, Prof. Scherrer created the Health-On-the Net (HON) foundation.\nThese achievements, made possible thanks to his visionary mind, deep\nhumanism, creativity, generosity and determination, have made of Prof.\nScherrer a true pioneer and leader of the human-centered,\npatient-oriented application of information technology for improving\nhealthcare\n", ['Professor Jean-Raoul Scherrer', 'Medical Informatics', 'mainframe based patient centered hospital information system', 'medical image management', 'PACS', 'Internet', 'DIOGENE system', 'man-machine interfaces', 'distributed systems', 'federated systems', 'data mining', 'natural language processing', 'biographies', 'health care', 'history', 'medical image processing', 'medical information systems', 'user interfaces']), ("Bluetooth bites back\nIt is now more than four years since we started to hear about Bluetooth, and\nfrom the user's point of view very little seems to have happened since\nthen. Paul Haddlesey looks at the progress, and the role Bluetooth may\neventually play in your firm's communications strategy\n", ['Bluetooth', 'communications strategy', 'wireless connection', 'mobile', 'mobile computing', 'wireless LAN']), ('Robustness evaluation of a minimal RBF neural network for\nnonlinear-data-storage-channel equalisation\nThe authors present a performance-robustness evaluation of the recently\ndeveloped minimal resource allocation network (MRAN) for equalisation\nin highly nonlinear magnetic recording channels in disc storage\nsystems. Unlike communication systems, equalisation of signals in these\nchannels is a difficult problem, as they are corrupted by\ndata-dependent noise and highly nonlinear distortions. Nair and Moon\n(1997) have proposed a maximum signal to distortion ratio (MSDR)\nequaliser for data storage channels, which uses a specially designed\nneural network, where all the parameters of the neural network are\ndetermined theoretically, based on the exact knowledge of the channel\nmodel parameters. In the present paper, the performance of the MSDR\nequaliser is compared with that of the MRAN equaliser using a magnetic\nrecording channel model, under Conditions that include variations in\npartial erasure, jitter, width and noise power, as well as model\nmismatch. Results from the study indicate that the less complex MRAN\nequaliser gives consistently better performance robustness than the\nMSDR equaliser in terms of signal to distortion ratios (SDRs)\n', ['robustness evaluation', 'minimal resource allocation network', 'highly nonlinear magnetic recording channels', 'disc storage systems', 'nonlinear-data-storage-channel equalisation', 'data-dependent noise', 'highly nonlinear distortions', 'maximum signal to distortion ratio equaliser', 'RBF neural network', 'MRAN equaliser', 'MSDR equaliser', 'digital magnetic recording', 'jitter noise', 'digital magnetic recording', 'equalisers', 'magnetic disc storage', 'magnetic recording noise', 'radial basis function networks']), ("Upper bound analysis of oblique cutting with nose radius tools\nA generalized upper bound model for calculating the chip flow angle in oblique\ncutting using flat-faced nose radius tools is described. The projection\nof the uncut chip area on the rake face is divided into a number of\nelements parallel to an assumed chip flow direction. The length of each\nof these elements is used to find the length of the corresponding\nelement on the shear surface using the ratio of the shear velocity to\nthe chip velocity. The area of each element is found as the cross\nproduct of the length and its width along the cutting edge. Summing up\nthe area of the elements along the shear surface, the total shear\nsurface area is obtained. The friction area is calculated using the\nsimilarity between orthogonal and oblique cutting in the 'equivalent'\nplane that includes both the cutting velocity and chip velocity. The\ncutting power is obtained by summing the shear power and the friction\npower. The actual chip flow angle and chip velocity are obtained by\nminimizing the cutting power with respect to both these variables. The\nshape of the curved shear surface, the chip cross section and the\ncutting force obtained from this model are presented\n", ['upper bound analysis', 'oblique cutting', 'nose radius tools', 'chip flow angle', 'uncut chip area', 'shear surface', 'shear velocity', 'chip velocity', 'friction area', 'cutting', 'friction', 'machine tools', 'machining', 'mechanical engineering', 'shear strength']), ('A better ballot box?\nElection officials are examining technologies to address a wide range of voting\nissues. The problems observed in the November 2000 US election\naccelerated existing trends to get rid of lever machines, punch-cards,\nand hand-counted paper ballots and replace them with mark-sense\nballoting, Internet, and automatic teller machine (ATM) kiosk style\ncomputer-based systems. An estimated US $2-$4 billion will be spent in\nthe United States and Canada to update voting systems during the next\ndecade. Voting online might enable citizens to vote even if they are\nunable to get to the polls. Yet making these methods work right turns\nout to be considerably more difficult than originally thought. New\nelectronic voting systems pose risks as well as solutions. As it turns\nout, many of the voting products currently for sale provide less\naccountability, poorer reliability, and greater opportunity for\nwidespread fraud than those already in use. This paper discusses the\ntechnology available and how to ensure accurate ballots\n', ['ballot box', 'mark-sense balloting', 'automatic teller machine computer-based voting system', 'ATM kiosk style computer-based voting systems', 'electronic voting', 'online voting', 'Internet', 'public administration']), ('Topology-reducing surface simplification using a discrete solid representation\nThis paper presents a new approach for generating coarse-level approximations\nof topologically complex models. Dramatic topology reduction is\nachieved by converting a 3D model to and from a volumetric\nrepresentation. Our approach produces valid, error-bounded models and\nsupports the creation of approximations that do not interpenetrate the\noriginal model, either being completely contained in the input solid or\nbounding it. Several simple to implement versions of our approach are\npresented and discussed. We show that these methods perform\nsignificantly better than other surface-based approaches when\nsimplifying topologically-rich models such as scene parts and complex\nmechanical assemblies\n', ['coarse-level approximations', 'topologically complex models', 'discrete solid representation', 'topology-reducing surface simplification', '3D model', 'volumetric representation', 'error-bounded models', 'scene parts', 'complex mechanical assemblies', 'computational geometry', 'computer graphics']), ('Computation of unmeasured third-generation VCT views from measured views\nWe compute unmeasured cone-beam projections from projections measured by a\nthird-generation helical volumetric computed tomography system by\nsolving a characteristic problem for an ultrahyperbolic differential\nequation [John (1938)]. By working in the Fourier domain, we convert\nthe second-order PDE into a family of first-order ordinary differential\nequations. A simple first-order integration is used to solve the ODES\n', ['unmeasured third-generation VCT views computation', 'measured views', 'cone-beam projections', 'characteristic problem solution', 'ultrahyperbolic differential equation', 'Fourier domain', 'first-order ordinary differential equations', 'simple first-order integration', 'medical diagnostic imaging', 'range conditions', 'third-generation helical volumetric computed tomography system', 'computerised tomography', 'differential equations', 'image reconstruction', 'medical image processing']), ('Domesticating computers and the Internet\nThe people who use computers and the ways they use them have changed\nsubstantially over the past 25 years. In the beginning highly educated\npeople, mostly men, in technical professions used computers for work,\nbut over time a much broader range of people are using computers for\npersonal and domestic purposes. This trend is still continuing, and\nover a shorter time scale has been replicated with the use of the\nInternet. The paper uses data from four national surveys to document\nhow personal computers and the Internet have become increasingly\ndomesticated since 1995 and to explore the mechanisms for this shift.\nNow people log on more often from home than from places of employment\nand do so for pleasure and for personal purposes rather than for their\njobs. Analyses comparing veteran Internet users to novices in 1998 and\n2000 and analyses comparing the change in use within a single sample\nbetween 1995 and 1996 support two complementary explanations for how\nthese technologies have become domesticated. Women, children, and less\nwell-educated individuals are increasingly using computers and the\nInternet and have a more personal set of motives than well-educated\nmen. In addition, the widespread diffusion of the PC and the Internet\nand the response of the computing industry to the diversity in\nconsumers has led to a rich set of personal and domestic services\n', ['computer domestication', 'Internet', 'highly educated people', 'technical professions', 'domestic purposes', 'personal usage', 'national surveys', 'personal computers', 'veteran Internet users', 'novices', 'women', 'children', 'personal motives', 'PC diffusion', 'computing industry', 'domestic services', 'demographics', 'online behavior', 'human factors', 'Internet', 'personal computing', 'social aspects of automation', 'user interfaces']), ('Loop restructuring for data I/O minimization on limited on-chip memory embedded\nprocessors\nIn this paper, we propose a framework for analyzing the flow of values and\ntheir reuse in loop nests to minimize data traffic under the\nconstraints of limited on-chip memory capacity and dependences. Our\nanalysis first undertakes fusion of possible loop nests\nintra-procedurally and then performs loop distribution. The analysis\ndiscovers the closeness factor of two statements which is a\nquantitative measure of data traffic saved per unit memory occupied if\nthe statements were under the same loop nest over the case where they\nare under different loop nests. We then develop a greedy algorithm\nwhich traverses the program dependence graph to group statements\ntogether under the same loop nest legally to promote maximal reuse per\nunit of memory occupied. We implemented our framework in Petit, a tool\nfor dependence analysis and loop transformations. We compared our\nmethod with one based on tiling of fused loop nest and one based on a\ngreedy strategy to purely maximize reuse. We show that our methods work\nbetter than both of these strategies in most cases for processors such\nas TMS320Cxx, which have a very limited amount of on-chip memory. The\nimprovements in data I/O range from 10 to 30 percent over tiling and\nfrom 10 to 40 percent over maximal reuse for JPEG loops\n', ['loop restructuring', 'data I/O minimization', 'on-chip memory', 'data traffic', 'embedded processors', 'loop fusion', 'data locality', 'program dependence graph', 'Petit', 'fused loop nest', 'closeness factor', 'DSP', 'embedded systems', 'graph theory', 'microprocessor chips', 'program control structures', 'storage management chips']), ('Comprehensive encoding and decoupling solution to problems of decoherence and\ndesign in solid-state quantum computing\nProposals for scalable quantum computing devices suffer not only from\ndecoherence due to the interaction with their environment, but also\nfrom severe engineering constraints. Here we introduce a practical\nsolution to these major concerns, addressing solid-state proposals in\nparticular. Decoherence is first reduced by encoding a logical qubit\ninto two qubits, then completely eliminated by an efficient set of\ndecoupling pulse sequences. The same encoding removes the need for\nsingle-qubit operations, which pose a difficult design constraint. We\nfurther show how the dominant decoherence processes can be identified\nempirically, in order to optimize the decoupling pulses\n', ['solid-state quantum computing', 'decoherence', 'logical qubit encoding', 'pulse sequence decoupling', 'engineering constraints', 'decoupling pulse optimization', 'scalable quantum computing devices', 'exchange Hamiltonian', 'encoding', 'exchange interactions (electron)', 'quantum computing', 'semiconductor quantum dots']), ('Study of ambiguities inherent to the spectral analysis of Voigt profiles-a\nmodified Simplex approach\nIn pulsed spectrometries, temporal transients are often analyzed directly in\nthe temporal domain, assuming they consist only of purely exponentially\ndecaying sinusoids. When experimental spectra actually consist of\nGaussian or Voigt profiles (Gauss-Lorentz profiles), we show that the\ndirect methods may erroneously interpret such lines as the sum of two\nor more Lorentzian profiles. Using a Nelder and Mead Simplex method,\nmodified by introducing new means to avoid degeneracies and quenchings\nin secondary minima, we demonstrate that a large number of different\nsolutions can be obtained with equivalent accuracy over the limited\nacquisition time interval, with final peak parameters devoid of\nphysical or chemical meaning\n', ['pulsed spectrometries', 'temporal transients', 'spectral analysis', 'Voigt profiles', 'Gaussian profiles', 'Gauss-Lorentz profiles', 'Nelder and Mead Simplex method', 'accuracy', 'limited acquisition time interval', 'final peak parameters', 'spectral analysis', 'spectral line breadth', 'spectroscopy computing']), ("Soft options for software upgrades?\nSeveral new products claim to take the work out of installing software and\npatches, and even migrating operating systems. Software migration\nproducts fall into two broad categories. The drive imaging type is\ndesigned to make exact copies of a hard disk, either an entire drive or\ncertain directories, so you can use it to back up data. The application\nmanagement type is designed for more incremental upgrades and often\nprovides additional features such as the ability to monitor or control\nusers' access to applications\n", ['software installation', 'software upgrades', 'Microsoft Windows', 'operating systems migration', 'business data processing', 'DP management', 'operating systems (computers)']), ('Community spirit\nIT companies that contribute volunteers, resources or funding to charities and\nlocal groups not only make a real difference to their communities but\nalso add value to their businesses. So says a new coalition of IT\nindustry bodies formed to raise awareness of the options for community\ninvolvement, promote the business case, and publicise examples of best\npractice. The BCS, Intellect (formed from the merger of the Computing\nServices and Software Association and the Federation of the Electronics\nIndustry) and the Worshipful Company of Information Technologists plan\nto run advisory seminars and provide guidelines on how companies of all\nsizes can transform their local communities using their specialist IT\nskills and resources while reaping business benefits\n', ['IT companies', 'volunteer staff', 'resource contribution', 'charity projects', 'community projects', 'staff development', 'business benefits', 'best practice', 'human resource management', 'personnel', 'social aspects of automation']), ('Modeling of torsional vibration induced by extension-twisting coupling of\nanisotropic composite laminates with piezoelectric actuators\nIn this paper we present a dynamic analytical model for the torsional vibration\nof an anisotropic piezoelectric laminate induced by the\nextension-twisting coupling effect. In the present approach, we use the\nHamilton principle and a reduced bending stiffness method for the\nderivation of equations of motion. As a result, the in-plane\ndisplacements are not involved and the out-of-plane displacement of the\nlaminate is the only quantity to be calculated. Therefore, the proposed\nmethod turns the twisting of a laminate with structural coupling into a\nsimplified problem without losing its features. We give analytical\nsolutions of the present model with harmonic excitation. A parametric\nstudy is performed to demonstrate the present approach\n', ['torsional vibration', 'extension -twisting coupling', 'anisotropic composite laminates', 'piezoelectric actuators', 'dynamic analytical model', 'anisotropic piezoelectric laminate', 'extension-twisting coupling effect', 'Hamilton principle', 'reduced bending stiffness', 'equations of motion', 'in-plane displacements', 'out-of-plane displacement', 'twisting', 'structural coupling', 'harmonic excitation', 'parametric study', 'composite laminate', 'material anisotropy', 'PZT', 'bending', 'elastic constants', 'harmonic analysis', 'intelligent actuators', 'intelligent materials', 'laminates', 'piezoelectric actuators', 'piezoelectric materials', 'torsion', 'vibrations']), ('Monitoring the news online\nThe author looks at how we can focus on what we want, finding small stories in\nvast oceans of news. There is no one tool that will scan every news\nresource available and give alerts on new available materials. Every\none has a slightly different focus. Some are paid sources, while many\nare free. If used wisely, an excellent news monitoring system for a\nlarge number of topics can be set up for surprisingly little cost\n', ['news monitoring', 'online news', 'Internet', 'information resources', 'publishing']), ('Surface micromachined paraffin-actuated microvalve\nNormally-open microvalves have been fabricated and tested which use a paraffin\nmicroactuator as the active element. The entire structure with nominal\ndimension of phi 600 mu m * 30 mu m is batch-fabricated by surface\nmicromachining the actuator and channel materials on top of a single\nsubstrate. Gas flow rates in the 0.01-0.1 sccm range have been measured\nfor several devices with actuation powers ranging from 50 to 150 mW on\nglass substrates. Leak rates as low as 500 mu sccm have been measured.\nThe normally-open blocking microvalve structure has been used to\nfabricate a precision flow control system of microvalves consisting of\nfour blocking valve structures. The control valve is designed to\noperate over a 0.01-5.0 sccm flow range at a differential pressure of\n800 torr. Flow rates ranging from 0.02 to 4.996 sccm have been\nmeasured. Leak rates as low as 3.2 msccm for the four valve system have\nbeen measured\n', ['surface micromachined microvalve', 'normally-open microvalves', 'paraffin microactuator', 'active element', 'channel materials', 'gas flow rates', 'actuation powers', 'leak rates', 'blocking valve structures', 'differential pressure', 'flow rates', '600 micron', '30 micron', '50 to 150 mW', '800 torr', 'microactuators', 'microfluidics', 'micromachining', 'microvalves']), ('Control centers are here to stay\nDespite changes with different structures, market rules, and uncertainties, a\ncontrol center must always be in place to maintain the security,\nreliability, and quality of electric service. This article focuses on\nthe energy management system (EMS) control center, identifying the\nmajor functions that have become standard components of every\napplication software package. The two most important control center\nfunctions, security control and load-following control, guarantee the\ncontinuity of electric service, which after all, is the end-product of\nthe utility business. New technology trends in the design of control\ncenter infrastructures are emerging in the liberalized environment of\nthe energy market. An example of a control center infrastructure is\ndescribed. The article ends with a concern for the security of the\ncontrol center itself\n', ['EMS control centers', 'energy management system', 'standard components', 'application software package', 'security control', 'load-following control', 'electric service continuity', 'control center infrastructures', 'liberalized environment', 'energy market', 'control facilities', 'energy management systems', 'load regulation', 'power system control', 'power system security']), ('From information gateway to digital library management system: a case analysis\nThis paper discusses the design, implementation and evolution of the Cornell\nUniversity Library Gateway using the case analysis method. It diagnoses\nthe Gateway within the conceptual framework of definitions and best\npractices associated with information gateways, portals, and emerging\ndigital library management systems, in particular the product ENCompass\n', ['digital library management system', 'Cornell University Library Gateway', 'information gateways', 'portals', 'ENCompass', 'metadata', 'academic libraries', 'digital libraries', 'network servers']), ('Defending against flooding-based distributed denial-of-service attacks: a\ntutorial\nFlooding-based distributed denial-of-service (DDoS) attack presents a very\nserious threat to the stability of the Internet. In a typical DDoS\nattack, a large number of compromised hosts are amassed to send useless\npackets to jam a victim, or its Internet connection, or both. In the\nlast two years, it was discovered that DDoS attack methods and tools\nare becoming more sophisticated, effective, and also more difficult to\ntrace to the real attackers. On the defense side, current technologies\nare still unable to withstand large-scale attacks. The main purpose of\nthis article is therefore twofold. The first one is to describe various\nDDoS attack methods, and to present a systematic review and evaluation\nof the existing defense mechanisms. The second is to discuss a\nlonger-term solution, dubbed the Internet-firewall approach, that\nattempts to intercept attack packets in the Internet core, well before\nreaching the victim\n', ['flooding-based distributed denial-of-service attacks', 'tutorial', 'Internet stability', 'DDoS attack methods', 'DDoS attack tools', 'large-scale attacks', 'Internet firewall', 'attack packets interception', 'reflector attacks', 'distributed attack detection', 'authorisation', 'Internet', 'packet switching', 'telecommunication security']), ('On the accuracy of polynomial interpolation in Hilbert space with disturbed\nnodal values of the operator\nThe interpolation accuracy of polynomial operators in a Hilbert space with a\nmeasure is estimated when nodal values of these operators are given\napproximately\n', ['polynomial interpolation', 'Hilbert space', 'disturbed nodal values', 'polynomial operators', 'Hilbert spaces', 'interpolation', 'polynomials']), ("Modeling the labor market as an evolving institution: model ARTEMIS\nA stylized French labor market is modeled as an endogenously evolving\ninstitution. Boundedly rational firms and individuals strive to\ndecrease the cost or increase utility. The labor market is coordinated\nby a search process and decentralized setting of hiring standards, but\nintermediaries can speed up matching. The model reproduces the dynamics\nof the gross flows and spectacular changes in mobility patterns of some\ndemographic groups when the oil crisis in the 1970's occurred, notably\nthe sudden decline of the integration in good jobs. The internal labor\nmarkets of large firms are shown to increase unemployment if the\nsecondary (temporary or bad) jobs do not exist\n", ['ARTEMIS model', 'French labor market', 'endogenously evolving institution', 'simulation model', 'jobs', 'endogenous intermediary', 'spectacular changes', 'mobility patterns', 'demographic groups', 'demography', 'employment', 'simulation', 'social sciences']), ('A hybrid ML-EM algorithm for calculation of maximum likelihood estimates in\nsemiparametric shared frailty models\nThis paper describes a generalised hybrid ML-EM algorithm for the calculation\nof maximum likelihood estimates in semiparametric shared frailty\nmodels, the Cox proportional hazard models with hazard functions\nmultiplied by a (parametric) frailty random variable. This hybrid\nmethod is much faster than the standard EM method and faster than the\nstandard direct maximum likelihood method (ML, Newton-Raphson) for\nlarge samples. We have previously applied this method to semiparametric\nshared gamma frailty models, and verified by simulations the asymptotic\nand small sample statistical properties of the frailty variance\nestimates. Let theta /sub 0/ be the true value of the frailty variance\nparameter. Then the asymptotic distribution is normal for theta /sub\n0/>0 while it is a 50-50 mixture between a point mass at zero and a\nnormal random variable on the positive axis for theta /sub 0/=0. For\nsmall samples, simulations suggest that the frailty variance estimates\nare approximately distributed as an x-(100-x)% mixture,\n0<or=x<or=50, between a point mass at zero and a normal random\nvariable on the positive axis even for theta /sub 0/>0. We apply\nthis method and verify by simulations these statistical results for\nsemiparametric shared log-normal frailty models. We also apply the\nsemiparametric shared gamma and log-normal frailty models to Busselton\nHealth Study coronary heart disease data\n', ['hybrid ML-EM algorithm', 'maximum likelihood estimates', 'Cox proportional hazard models', 'Busselton Health Study', 'coronary heart disease data', 'data analysis', 'hazard functions', 'simulations', 'frailty variance estimates', 'asymptotic distribution', 'normal distribution', 'normal random variable', 'semiparametric shared log-normal frailty models', 'data analysis', 'log normal distribution', 'maximum likelihood estimation', 'medical administrative data processing', 'normal distribution']), ('A new approach to the decomposition of Boolean functions by the method of\nq-partitions.II. Repeated decomposition\nFor pt.I. see Upr. Sist. Mash., no. 6, p. 29-42 (1999). A new approach to the\ndecomposition of Boolean,functions that depend on n variables and are\nrepresented in various forms is considered. The approach is based on\nthe method of q-partitioning of minterms and on the introduced concept\nof a decomposition clone. The theorem on simple disjunctive\ndecomposition of full and partial functions is formulated. The approach\nproposed is illustrated by examples\n', ['Boolean functions decomposition', 'minterms', 'decomposition clone', 'disjunctive decomposition', 'partial functions', 'logic synthesis', 'q-partitions', 'Boolean functions', 'logic design']), ("Mathematical modelling of the work of the system of wells in a layer with the\nexponential law of permeability variation and the mobile liquid\ninterface\nWe construct and study a two-dimensional model of the work of the system of\nwells in a layer with the mobile boundary between liquids of various\nviscosity. We use a 'plunger' displacement model of liquids. The\nboundaries of the filtration region of these liquids are modelled by\ncurves of the Lyapunov class. Unlike familiar work, we solve\ntwo-dimensonal problems in an inhomogeneous layer when the mobile\nboundary and the boundaries of the filtration region are modelled by\ncurves of the Lyapunov class. We show the practical convergence of the\nnumerical solution of the problems studied\n", ['2D model', 'work', 'well system', 'mathematical modelling', 'exponential law', 'permeability variation', 'mobile liquid interface', 'mobile boundary', 'viscosity', 'plunger displacement model', 'filtration region boundaries', 'Lyapunov class curves', 'inhomogeneous layer', 'convergence', 'numerical solution', 'computational fluid dynamics', 'convergence of numerical methods', 'differential equations', 'elliptic equations', 'filtration', 'flow through porous media', 'Lyapunov methods', 'permeability', 'viscosity']), ("The congenial talking philosophers problem in computer networks\nGroup mutual exclusion occurs naturally in situations where a resource can be\nshared by processes of the same group, but not by processes of\ndifferent groups. For example, suppose data is stored in a CD-jukebox.\nThen, when a disc is loaded for access, users that need data on the\ndisc can concurrently access the disc, while users that need data on a\ndifferent disc have to wait until the current disc is unloaded. The\ndesign issues for group mutual exclusion have been modeled as the\nCongenial Talking Philosophers problem, and solutions for shared memory\nmodels have been proposed (Y.-J. Young, 2000; P. Keane and M. Moir,\n1999). As in ordinary mutual exclusion and many other problems in\ndistributed systems, however, techniques developed for shared memory do\nnot necessarily apply to message passing (and vice versa). We\ninvestigate solutions for Congenial Talking Philosophers in computer\nnetworks where processes communicate by asynchronous message passing.\nWe first present a solution that is a straightforward adaptation from\nG. Ricart and A.K. Agrawala's (1981) algorithm for ordinary mutual\nexclusion. Then we show that the simple modification suffers a severe\nperformance degradation that could cause the system to behave as though\nonly one process of a group can be in the critical section at a time.\nWe then present a more efficient and highly concurrent distributed\nalgorithm for the problem, the first such solution in computer networks\n", ['congenial talking philosophers problem', 'computer networks', 'group mutual exclusion', 'resource sharing', 'shared-memory models', 'distributed systems', 'process communication', 'asynchronous message passing', 'critical section', 'concurrent distributed algorithm', 'computer networks', 'concurrency control', 'distributed algorithms', 'message passing', 'processor scheduling', 'resource allocation', 'shared memory systems']), ('Towards the globalisation of the IS/IT function\nThe IS/IT function has recently emerged from the peripheral aspects of the\nfinance department to the centre of critical organisational change.\nThere is an increasing dependency on its activities as systems extend\nbeyond supporting the internal efficiency of the organisation to\naugmenting global performance. The growth of wide and local networks\nhas resulted in communication possibilities that were not possible a\nfew years ago. E-commerce challenges the achievements of the IS/IT\nfunction and is very prominent in the globalisation of modern\norganisations. The complexity and diversity of electronic exchange is\nalso well documented (Hackney et al., 2000). This has a number of\nimpacts on the development and implementation of IS/IT solutions for\norganisations involved in international trade. It is a conjecture that\nthe IS/IT function is critically important for the alignment of the\nbusiness to meet the demands of global competition, through building\ninternal marketing strategies and creating knowledge based communities.\nThere is clear evidence that IS/IT can lead to improved business\nperformance and potentially for sustained competitive advantage. This\nis obviously true through the advent of new and emerging technologies\nsuch as the Internet\n', ['globalisation', 'IS/IT function', 'local area networks', 'wide area networks', 'e-commerce', 'electronic exchange', 'international trade', 'internal marketing strategies', 'knowledge based communities', 'Internet', 'business communication', 'electronic commerce', 'information systems', 'international trade', 'Internet', 'management of change']), ('Using k-nearest-neighbor classification in the leaves of a tree\nWe construct a hybrid (composite) classifier by combining two classifiers in\ncommon use - classification trees and k-nearest-neighbor (k-NN). In our\nscheme we divide the feature space up by a classification tree, and\nthen classify test set items using the k-NN rule just among those\ntraining items in the same leaf as the test item. This reduces somewhat\nthe computational load associated with k-NN, and it produces a\nclassification rule that performs better than either trees or the usual\nk-NN in a number of well-known data sets\n', ['k-nearest-neighbor classification', 'tree leaves', 'hybrid composite classifier', 'classification trees', 'feature space division', 'computational load', 'data sets', 'k-NN rule', 'learning (artificial intelligence)', 'pattern classification', 'statistical analysis', 'tree data structures']), ('Three-dimensional particle image tracking for dilute particle-liquid flows in a\npipe\nA three-dimensional (3D) particle image tracking technique was used to study\nthe coarse spherical particle-liquid flows in a pipe. The flow images\nfrom both the front view and the normal side view, which was reflected\ninto the front view by a mirror, were recorded with a CCD camera and\ndigitized by a PC with an image grabber card. An image processing\nprogram was developed to enhance and segment the flow image, and then\nto identify the particles. Over 90% of all the particles can be\nidentified and located from the partially overlapped particle images\nusing the circular Hough transform. Then the 3D position of each\ndetected particle was determined by matching its front view image to\nits side view image. The particle velocity was then obtained by pairing\nits images in successive video fields. The measurements for the\nspherical expanded polystyrene particle-oil flows show that the\nparticles, like the spherical bubbles in laminar bubbly flows, tend to\nconglomerate near the pipe wall and to line up to form the particle\nclusters. As liquid velocity decreases, the particle clusters disperse\nand more particles are distributed in the pipe centre region\n', ['three-dimensional particle image tracking', 'dilute particle-liquid flows', 'two-phase flow', 'pipe flow', 'stereo-imaging technique', 'phase distribution', 'CCD camera', '3D position', 'spherical expanded polystyrene particle', 'particle clusters', 'spherical bubble', 'Wiener filter', 'image segmentation', 'region growing technique', 'image recognition', 'image matching', 'Hough transform', 'CCD image sensors', 'flow visualisation', 'Hough transforms', 'image recognition', 'image segmentation', 'mechanical engineering computing', 'optical images', 'physics computing', 'pipe flow', 'stereo image processing', 'tracers', 'two-phase flow', 'velocity measurement']), ('Performance, design and control of a series-parallel (CL/sup 2/-type) resonant\nDC/DC converter\nThe three-element resonant network has various topological alternatives, one of\nwhich, a prospective compound topology, is investigated in detail. The\nconverter uses one capacitor (C) and two inductors (L/sup 2/), to form\na compound type CL/sup 2/ network. Various advantages and limitations\nof the converter are detailed, and a new design procedure for such\nconverters is also introduced. The converter may be controlled by\nvarying the switching frequency or by pulse-width modulation. An\nexperimental prototype has been produced and an excellent performance\nin the lagging power-factor mode has been confirmed\n', ['series-parallel resonant DC/DC power converter', 'three-element resonant network', 'capacitor', 'inductors', 'design procedure', 'switching frequency', 'pulse-width modulation', 'performance', 'lagging power factor mode', 'bridge circuits', 'DC-DC power convertors', 'electric current control', 'PWM power convertors', 'resonant power convertors', 'switching circuits', 'voltage control']), ('Radianz and Savvis look to expand service in wake of telecom scandals [finance]\nWith confidence in network providers waning, Radianz and Savvis try to prove\ntheir stability. Savvis and Radianz, which both specialize in providing\nthe data-extranet components of telecommunication infrastructures, may\nsee more networking doors open at investment banks, brokerage houses,\nexchanges and alternative-trading systems\n', ['network providers', 'Savvis', 'Radianz', 'data-extranet', 'telecommunication infrastructures', 'investment banks', 'brokerage houses', 'exchanges', 'alternative-trading systems', 'electronic trading', 'investment', 'telecommunication']), ('Hermite interpolation by rotation-invariant spatial Pythagorean-hodograph\ncurves\nThe interpolation of first-order Hermite data by spatial Pythagorean-hodograph\ncurves that exhibit closure under arbitrary 3-dimensional rotations is\naddressed. The hodographs of such curves correspond to certain\ncombinations of four polynomials, given by Dietz et al. (1993), that\nadmit compact descriptions in terms of quaternions - an instance of the\n"PH representation map" proposed by Choi et al. (2002). The\nlowest-order PH curves that interpolate arbitrary first-order spatial\nHermite data are quintics. It is shown that, with PH quintics, the\nquaternion representation yields a reduction of the Hermite\ninterpolation problem to three "simple" quadratic equations in three\nquaternion unknowns. This system admits a closed-form solution,\nexpressing all PH quintic interpolants to given spatial Hermite data as\na two-parameter family. An integral shape measure is invoked to fix\nthese two free parameters\n', ['interpolation', 'spatial Pythagorean hodograph curves', 'first-order Hermite data', 'quaternions', 'polynomials', 'closed-form solution', 'integral shape measure', 'interpolation', 'invariance', 'polynomials']), ("Action aggregation and defuzzification in Mamdani-type fuzzy systems\nDiscusses the issues of action aggregation and defuzzification in Mamdani-type\nfuzzy systems. The paper highlights the shortcomings of defuzzification\ntechniques associated with the customary interpretation of the sentence\nconnective 'and' by means of the set union operation. These include\nloss of smoothness of the output characteristic and inaccurate mapping\nof the fuzzy response. The most appropriate procedure for aggregating\nthe outputs of different fuzzy rules and converting them into crisp\nsignals is then suggested. The advantages in terms of increased\ntransparency and mapping accuracy of the fuzzy response are\ndemonstrated\n", ['action aggregation', 'defuzzification', 'Mamdani-type fuzzy systems', 'sentence connective', 'set union operation', 'fuzzy rules', 'fuzzy response', 'crisp signals', 'transparency', 'mapping accuracy', 'fuzzy logic', 'fuzzy set theory', 'fuzzy systems']), ("Banks pin their back-office hopes on successors to screen scrapers\nThe big name in account aggregation has been Yodlee, based in Redwood Shores,\nCA. It pioneered the art of screen scraping, or pulling data off Web\nsites and aggregating it into a single statement. That data, however,\nis a snapshot and does not include a customer's investment history.\nAlso, because Web sites update data at different times, scraping them\ncan provide an inaccurate picture of a customer's financial situation,\nmaking it difficult for reps seeking to provide timely and accurate\nadvice. The objective is to access both fresh and historical data\nacross a client's financial spectrum, from investments to checking\naccounts and loans to insurance policies, a Complete Customer balance\nsheet. At least two technology vendors are progressing in that\ndirection, each coming from different directions. One is Advent, based\nin San Francisco, another is Fincentric, out of Vancouver\n", ['account aggregation', 'Yodlee', 'screen scraping', 'investment', 'Web sites', 'checking', 'loans', 'insurance', 'Advent', 'Fincentric', 'bankers', 'banking', 'insurance', 'investment']), ('Genetic algorithm for input/output selection in MIMO systems based on\ncontrollability and observability indices\nA time domain optimisation algorithm using a genetic algorithm in conjunction\nwith a linear search scheme has been developed to find the smallest or\nnear-smallest subset of inputs and outputs to control a\nmulti-input-multi-output system. Experimental results have shown that\nthis proposed algorithm has a very fast convergence rate and high\ncomputation efficiency\n', ['genetic algorithm', 'input/output selection', 'MIMO systems', 'controllability indices', 'observability indices', 'time domain optimisation algorithm', 'linear search scheme', 'near-smallest subset', 'smallest subset', 'multi-input-multi-output system', 'very fast convergence', 'high computation efficiency', 'multivariable control systems', 'control system analysis', 'controllability', 'convergence of numerical methods', 'genetic algorithms', 'MIMO systems', 'multivariable control systems', 'observability', 'time-domain analysis']), ('Adaptive thinning for bivariate scattered data\nThis paper studies adaptive thinning strategies for approximating a large set\nof scattered data by piecewise linear functions over triangulated\nsubsets. Our strategies depend on both the locations of the data points\nin the plane, and the values of the sampled function at these points -\nadaptive thinning. All our thinning strategies remove data points one\nby one, so as to minimize an estimate of the error that results by the\nremoval of a point from the current set of points (this estimate is\ntermed "anticipated error"). The thinning process generates subsets of\n"most significant" points, such that the piecewise linear interpolants\nover the Delaunay triangulations of these subsets approximate\nprogressively the function values sampled at the original scattered\npoints, and such that the approximation errors are small relative to\nthe number of points in the subsets. We design various methods for\ncomputing the anticipated error at reasonable cost, and compare and\ntest the performance of the methods. It is proved that for data sampled\nfrom a convex function, with the strategy of convex triangulation, the\nactual error is minimized by minimizing the best performing measure of\nanticipated error. It is also shown that for data sampled from certain\nquadratic polynomials, adaptive thinning is equivalent to thinning\nwhich depends only on the locations of the data points - nonadaptive\nthinning. Based on our numerical tests and comparisons, two practical\nadaptive thinning algorithms are proposed for thinning large data sets,\none which is more accurate and another which is faster\n', ['adaptive thinning', 'scattered data', 'piecewise linear functions', 'triangulated subsets', 'error', 'Delaunay triangulations', 'convex function', 'data reduction', 'error analysis', 'interpolation', 'mesh generation', 'piecewise linear techniques']), ('Creating Web-based listings of electronic journals without creating extra work\nCreating up-to-date listings of electronic journals is challenging due to\nfrequent changes in titles available and in URLs for electronic journal\ntitles. However, many library users may want to browse Web pages which\ncontain listings of electronic journals arranged by title and/or\nacademic disciplines. This case study examines the development of a\nsystem which automatically exports data from the online catalog and\nincorporates it into dynamically-generated Web sites. These sites\nprovide multiple access points for journals, include Web-based\ninterfaces enabling subject specialists to manage the list of titles\nwhich appears in their subject area. Because data are automatically\nextracted from the catalog, overlap in updating titles and URLs is\navoided. Following the creation of this system, usage of electronic\njournals dramatically increased and feedback has been positive. Future\nchallenges include developing more frequent updates and motivating\nsubject specialists to more regularly monitor new titles\n', ['Web-based listings', 'electronic journals', 'URL', 'library', 'technical services', 'public services partnerships', 'Web pages', 'case study', 'online catalog', 'Web sites', 'feedback', 'cataloguing', 'electronic publishing', 'information resources', 'Internet', 'library automation']), ('Generalized predictive control for non-uniformly sampled systems\nIn this paper, we study digital control systems with non-uniform updating and\nsampling patterns, which include multirate sampled-data systems as\nspecial cases. We derive lifted models in the state-space domain. The\nmain obstacle for generalized predictive control (GPC) design using the\nlifted models is the so-called causality constraint. Taking into\naccount this design constraint, we propose a new GPC algorithm, which\nresults in optimal causal control laws for the non-uniformly sampled\nsystems. The solution applies immediately to multirate sampled-data\nsystems where rates are integer multiples of some base period\n', ['generalized predictive control design', 'nonuniformly sampled systems', 'digital control systems', 'nonuniform updating patterns', 'nonuniform sampling patterns', 'multirate sampled-data systems', 'state-space models', 'GPC', 'causality constraint', 'optimal causal control laws', 'integer multiples', 'control system synthesis', 'digital control', 'multivariable control systems', 'optimal control', 'predictive control', 'sampled data systems', 'state-space methods']), ('Winning post [mail systems]\nBusinesses that take their mail for granted can end up wasting money as well as\nopportunities. Mike Stecyk, VP of marketing and lines of business at\nPitney Bowes, suggests strategies for making more of a great\nopportunity\n', ['mail', 'Pitney Bowes', 'strategies', 'franking machines', 'folders', 'inserters', 'direct mail shots', 'mailing systems']), ('A second order characteristic finite element scheme for convection-diffusion\nproblems\nA new characteristic finite element scheme is presented for\nconvection-diffusion problems. It is of second order accuracy in time\nincrement, symmetric, and unconditionally stable. Optimal error\nestimates are proved in the framework of L/sup 2/-theory. Numerical\nresults are presented for two examples, which show the advantage of the\nscheme\n', ['second order characteristic finite element scheme', 'convection-diffusion problems', 'second order accuracy', 'optimal error estimates', 'L/sup 2/ -theory', 'error analysis', 'finite element analysis']), ("ClioWeb, ClioRequest, and Clio database: enhancing patron and staff\nsatisfaction\nFaced with increased demand from students and faculty for a speedier and more\nuser-friendly method of obtaining materials from other institutions,\nthe interlibrary loan (ILL) department sought to implement a management\nsystem which would accomplish the task. Students wanted remote\ninterconnectivity to the system and staff wanted increased workflow\nefficiency, reduced paper work, and better data management. This paper\nfocuses on Washington College's experience in selecting and\nimplementing an interlibrary loan system, which would enhance student\nsatisfaction as well as that of the library staff\n", ['Clio database', 'ClioRequest', 'ClioWeb', 'staff satisfaction', 'patron satisfaction', 'faculty', 'students', 'interlibrary loan department', 'user-friendly method', 'management system', 'remote interconnectivity', 'workflow efficiency', 'data management', 'Washington College', 'academic libraries', 'document delivery', 'interlibrary loan', 'management of change', 'software selection']), ('Evaluation of existing and new feature recognition algorithms. 2. Experimental\nresults\nFor pt.1 see ibid., p.839-851. This is the second of two papers investigating\nthe performance of general-purpose feature detection techniques. The\nfirst paper describes the development of a methodology to synthesize\npossible general feature detection face sets. Six algorithms resulting\nfrom the synthesis have been designed and implemented on a SUN\nWorkstation in C++ using ACIS as the geometric modelling system. In\nthis paper, extensive tests and comparative analysis are conducted on\nthe feature detection algorithms, using carefully selected components\nfrom the public domain, mostly from the National Design Repository. The\nresults show that the new and enhanced algorithms identify face sets\nthat previously published algorithms cannot detect. The tests also show\nthat each algorithm can detect, among other types, a certain type of\nfeature that is unique to it. Hence, most of the algorithms discussed\nin this paper would have to be combined to obtain complete coverage\n', ['feature recognition algorithms', 'general-purpose feature detection techniques', 'National Design Repository', 'face sets', 'convex hull', 'concavity', 'feature extraction', 'mechanical engineering']), ('Implementation and performance evaluation of a FIFO queue class library for\ntime warp\nThe authors describe the implementation, use, and performance evaluation of a\nFIFO queue class library by means of a high-performance, easy-to-use\ninterface employed for queuing simulations in parallel discrete\nsimulations based on the time warp method. Various general-purpose\nsimulation libraries and languages have been proposed, and among these\nsome have the advantage of not requiring users to define anything other\nthan the state vector, and not needing awareness of rollback under a\nplatform which performs state control based on copies. However, because\nthe state vectors must be defined as simple data structures without\npointers, dynamic data structures such as a FIFO queue cannot be\nhandled directly. Under the proposed class library, both the platform\nand the user can handle such structures in the same fashion that\nembedded data structures are handled. In addition, instead of all\nstored data, just the operational history can be stored and recovered\nefficiently at an effectively minimal cost by taking advantage of the\nfirst-in-first-out characteristics of the above data structures. When\nthe kernel deletes past state histories during a simulation, garbage\ncollection is also performed transparently using the corresponding\nmethod\n', ['FIFO queue', 'class library', 'time warp simulation', 'performance evaluation', 'easy-to-use interface', 'queuing simulations', 'parallel discrete simulations', 'general-purpose simulation libraries', 'simulation languages', 'state vectors', 'dynamic data structures', 'embedded data structures', 'operational history', 'first-in-first-out characteristics', 'garbage collection', 'object oriented method', 'state management', 'data structures', 'object-oriented programming', 'queueing theory', 'simulation languages', 'software libraries', 'software performance evaluation', 'storage management', 'time warp simulation']), ("Computer mediated communication and university international students\nThe design for the preliminary study presented was based on the experiences of\nthe international students and faculty members of a small southwest\nuniversity being surveyed and interviewed. The data collection\nprocedure blends qualitative and quantitative data. A strong consensus\nwas found that supports the study's premise that there is an\nassociation between the use of computer mediated communication (CMC)\nand teaching and learning performance of international students. Both\ngroups believe CMC to be an effective teaching and learning tool by:\nincreasing the frequency and quality of communication between students\nand instructors; improving language skills through increased writing\nand communication opportunities; allowing students and instructors to\nstay current and to compete effectively; providing alternative teaching\nand learning methods to increase students' confidence in their ability\nto communicate effectively with peers and instructors; and improving\nthe instructors' pedagogical focus and questioning techniques\n", ['computer mediated communication', 'university international students', 'faculty members', 'small southwest university', 'data collection procedure', 'quantitative data', 'qualitative data', 'CMC', 'teaching', 'learning performance', 'language skills', 'communication opportunities', 'instructors', 'student confidence', 'peers', 'pedagogical focus', 'questioning techniques', 'educational technology', 'electronic mail', 'teaching', 'teleconferencing']), ('Global stability of the attracting set of an enzyme-catalysed reaction system\nThe essential feature of enzymatic reactions is a nonlinear dependency of\nreaction rate on metabolite concentration taking the form of saturation\nkinetics. Recently, it has been shown that this feature is associated\nwith the phenomenon of "loss of system coordination" (Liu, 1999). In\nthis paper, we study a system of ordinary differential equations\nrepresenting a branched biochemical system of enzyme-mediated\nreactions. We show that this system can become very sensitive to\nchanges in certain maximum enzyme activities. In particular, we show\nthat the system exhibits three distinct responses: a unique,\nglobally-stable steady-state, large amplitude oscillations, and\nasymptotically unbounded solutions, with the transition between these\nstates being almost instantaneous. It is shown that the appearance of\nlarge amplitude, stable limit cycles occurs due to a "false"\nbifurcation or canard explosion. The subsequent disappearance of limit\ncycles corresponds to the collapse of the domain of attraction of the\nattracting set for the system and occurs due to a global bifurcation in\nthe flow, namely, a saddle connection. Subsequently, almost all\nnonnegative data become unbounded under the action of the dynamical\nsystem and correspond exactly to loss of system coordination. We\ndiscuss the relevance of these results to the possible consequences of\nmodulating such systems\n', ['enzymatic reactions', 'nonlinear dependency', 'metabolite concentration', 'saturation kinetics', 'biochemical system', 'ordinary differential equations', 'enzyme-mediated reactions', 'saddle connection', 'stable limit cycles', 'bifurcation', 'bifurcation', 'biochemistry', 'catalysis', 'differential equations', 'proteins', 'reaction kinetics theory', 'stability']), ("Books on demand: just-in-time acquisitions\nThe Purdue University Libraries Interlibrary Loan unit proposed a pilot project\nto purchase patrons' loan requests from Amazon. com, lend them to the\npatrons, and then add the titles to the collection. Staff analyzed\nprevious monograph loans, developed ordering criteria, implemented the\nproposal as a pilot project for six months, and evaluated the resulting\npatron comments, statistics, and staff perceptions. As a result of\nenthusiastic patron comments and a review of the project statistics,\nthe program was extended\n", ['Purdue University Libraries Interlibrary Loan unit', 'monograph loans', 'ordering criteria', 'staff perceptions', 'patron comments', 'publication on demand', 'academic libraries', 'library automation']), ('Negotiating the semantics of agent communication languages\nThis article presents a formal framework and outlines a method that autonomous\nagents can use to negotiate the semantics of their communication\nlanguage at run-time. Such an ability is needed in open multi-agent\nsystems so that agents can ensure they understand the implications of\nthe utterances that are being made and so that they can tailor the\nmeaning of the primitives to best fit their prevailing circumstances.\nTo this end, the semantic space framework provides a systematic means\nof classifying the primitives along multiple relevant dimensions. This\nclassification can then be used by the agents to structure their\nnegotiation (or semantic fixing) process so that they converge to the\nmutually agreeable semantics that are necessary for coherent social\ninteractions\n', ['autonomous agents', 'multi-agent systems', 'communication language', 'semantic fixing', 'semantic space', 'social interactions', 'formal languages', 'multi-agent systems', 'programming language semantics']), ('Permission grids: practical, error-bounded simplification\nWe introduce the permission grid, a spatial occupancy grid which can be used to\nguide almost any standard polygonal surface simplification algorithm\ninto generating an approximation with a guaranteed geometric error\nbound. In particular, all points on the approximation are guaranteed to\nbe within some user-specified distance from the original surface. Such\nbounds are notably absent from many current simplification methods, and\nare becoming increasingly important for applications in scientific\ncomputing and adaptive level of detail control. Conceptually simple,\nthe permission grid defines a volume in which the approximation must\nlie, and does not permit the underlying simplification algorithm to\ngenerate approximations outside the volume. The permission grid makes\nthree important, practical improvements over current error-bounded\nsimplification methods. First, it works on arbitrary triangular models,\nhandling all manners of mesh degeneracies gracefully. Further, the\nerror tolerance may be easily expanded as simplification proceeds,\nallowing the construction of an error-bounded level of detail hierarchy\nwith vertex correspondences among all levels of detail. And finally,\nthe permission grid has a representation complexity independent of the\nsize of the input model, and a small running time overhead, making it\nmore practical and efficient than current methods with similar\nguarantees\n', ['permission grid', 'spatial occupancy grid', 'polygonal surface simplification algorithm', 'guaranteed geometric error bound', 'approximation', 'error-bounded simplification', 'user-specified distance', 'scientific computing', 'adaptive level of detail control', 'arbitrary triangular models', 'mesh degeneracies', 'error tolerance', 'vertex correspondences', 'representation complexity', 'running time overhead', 'computational complexity', 'computational geometry', 'computer graphics']), ('A digital-to-analog converter based on differential-quad switching\nA high-conversion-rate high-resolution oversampling digital-to-analog converter\n(DAC) for direct digital modulation is addressed in this paper. A new\ntype of switching scheme, called differential-quad switching, is\npresented. To verify the feasibility of this scheme, essential parts\nwith some auxiliary circuitry for interfacing were fabricated in a 0.8-\nmu m CMOS technology. Measured results show that the switching scheme\nprovides 11-b resolution at 100 MSamples/s and 6-b at 1 GSamples/s. The\ndegradation in signal-to-noise ratio is not observed for the variation\nof the supply voltage down to 1.5 V, which means the proposed scheme is\nsuitable for low-voltage applications\n', ['high-conversion-rate DAC', 'high-resolution DAC', 'oversampling DAC', 'digital-to-analog converter', 'CMOS technology', 'direct digital modulation', 'differential-quad switching', 'signal-to-noise ratio', 'SNR', '1.5 V', '0.8 micron', 'CMOS integrated circuits', 'digital-analogue conversion', 'high-speed integrated circuits', 'integrated circuit design', 'modulation', 'switching circuits']), ("Java portability put to the test\nSun Microsystems' recently launched Java Verification Program aims to enable\ncompanies to assess the cross-platform portability of applications\nwritten in Java, and to help software vendors ensure that their\nsolutions can run in heterogenous J2EE application server environments\n", ['Sun Microsystems', 'Java Verification Program', 'cross-platform portability', 'application program interfaces', 'DP management', 'Java']), ('Continuous-time linear systems: folklore and fact\nWe consider a family of continuous input-output maps representing linear\ntime-invariant systems that take a set of signals into itself. It is\nshown that this family contains maps whose impulse response is the zero\nfunction, but which take certain inputs into nonzero outputs. It is\nshown also that this family contains members whose input-output\nproperties are not described by their frequency domain response\nfunctions, and that the maps considered need not even commute\n', ['continuous-time systems', 'linear systems', 'continuous input-output maps', 'time-invariant systems', 'impulse response', 'zero function', 'frequency domain response', 'commutation', 'signal processing', 'continuous time systems', 'frequency-domain analysis', 'linear systems', 'poles and zeros', 'signal processing', 'transient response']), ('A Virtual Test Facility for the simulation of dynamic response in materials\nThe Center for Simulating Dynamic Response of Materials at the California\nInstitute of Technology is constructing a virtual shock physics\nfacility for studying the response of various target materials to very\nstrong shocks. The Virtual Test Facility (VTF) is an end-to-end, fully\nthree-dimensional simulation of the detonation of high explosives (HE),\nshock wave propagation, solid material response to pressure loading,\nand compressible turbulence. The VTF largely consists of a parallel\nfluid solver and a parallel solid mechanics package that are coupled\ntogether by the exchange of boundary data. The Eulerian fluid code and\nLagrangian solid mechanics model interact via a novel approach based on\nlevel sets. The two main computational packages are integrated through\nthe use of Pyre, a problem solving environment written in the Python\nscripting language. Pyre allows application developers to interchange\nvarious computational models and solver packages without recompiling\ncode, and it provides standardized access to several data visualization\nengines and data input mechanisms. In this paper, we outline the main\ncomponents of the VTF, discuss their integration via Pyre, and describe\nsome recent accomplishments in large-scale simulation using the VTF\n', ['virtual shock physics facility', 'Virtual Test Facility', 'high explosives', 'shock wave propagation', 'solid material response', 'pressure loading', 'compressible turbulence', 'parallel fluid solver', 'parallel solid mechanics', 'shock physics simulation', 'data visualization', 'Pyre', 'problem solving environment', 'Python scripting language', 'digital simulation', 'dynamic response', 'materials science', 'shock waves', 'virtual reality']), ("Taking back control [SCADA system]\nMost common way to implement a SCADA system is to go outside. However, in the\nauthor's opinion, to truly take control of a SCADA project, in-house\npersonnel should handle as much of the job as possible. This includes\ndesign, equipment specification, installation, and programming. The\nmore of these tasks one does in-house, the more control and ownership\none has. To accomplish this, we first evaluated the existing SCADA\nsystem and investigated new technologies to establish a list of\nfeatures the new system needed to incorporate\n", ['SCADA', 'supervisory control', 'data acquisition', 'in-house integration', 'compatibility', 'programmable logic controllers', 'process control', 'programmable controllers', 'SCADA systems']), ('Towards strong stability of concurrent repetitive processes sharing resources\nThe paper presents a method for design of stability conditions of concurrent,\nrepetitive processes sharing common resources. Steady-state behaviour\nof the system with m cyclic processes utilising a resource with the\nmutual exclusion is considered. Based on a recurrent equations\nframework necessary and sufficient conditions for the existence of\nmaximal performance steady-state are presented. It was shown that if\nthe conditions hold then the m-process system is marginally stable,\ni.e., a steady-state of the system depends on the perturbations. The\nproblem of finding the relative positions of the processes leading to\nwaiting-free (maximal efficiency) steady-states of the system is\nformulated as a constraint logic programming problem. An example\nillustrating the solving of the problem for a 3-process system using\nobject-oriented, constraint logic programming language Oz is presented.\nA condition sufficient for strong stability of the m-process system is\ngiven. When the condition holds then for any initial phases of the\nprocesses a waiting-free steady-state will be reached\n', ['strong stability', 'concurrent repetitive processes', 'common resources', 'steady-state behaviour', 'cyclic processes', 'mutual exclusion', 'recurrent equations framework', 'necessary and sufficient conditions', 'maximal performance steady-state', 'waiting-free steady-states', 'constraint logic programming', '3-process system', 'Oz language', 'concurrency theory', 'constraint handling', 'logic programming languages', 'object-oriented methods', 'stability']), ("Personality research on the Internet: a comparison of Web-based and traditional\ninstruments in take-home and in-class settings\nStudents, faculty, and researchers have become increasingly comfortable with\nthe Internet, and many of them are interested in using the Web to\ncollect data. Few published studies have investigated the differences\nbetween Web-based data and data collected with more traditional\nmethods. In order to investigate these potential differences, two\nimportant factors were crossed in this study: whether the data were\ncollected on line or not and whether the data were collected in a group\nsetting at a fixed time or individually at a time of the respondent's\nchoosing. The Visions of Morality scale (Shelton and McAdams, 1990) was\nused, and the participants were assigned to one of four conditions:\nin-class Web survey, in-class paper-and-pencil survey; take-home Web\nsurvey, and take-home paper-and-pencil survey. No significant\ndifferences in scores were found for any condition; however, response\nrates were affected by the type of survey administered, with the\ntake-home Web-based instrument having the lowest response rate.\nTherefore, researchers need to be aware that different modes of\nadministration may affect subject attrition and may, therefore,\nconfound investigations of other independent variables\n", ['personality research', 'Internet', 'Web-based instruments', 'data collection', 'Visions of Morality scale', 'in-class Web survey', 'in-class paper-and-pencil survey', 'take-home Web survey', 'take-home paper-and-pencil survey', 'response rates', 'administration', 'subject attrition', 'computer aided instruction', 'Internet', 'psychology']), ('Optimization of cutting conditions for single pass turning operations using a\ndeterministic approach\nAn optimization analysis, strategy and CAM software for the selection of\neconomic cutting conditions in single pass turning operations are\npresented using a deterministic approach. The optimization is based on\ncriteria typified by the maximum production rate and includes a host of\npractical constraints. It is shown that the deterministic optimization\napproach involving mathematical analyses of constrained economic trends\nand graphical representation on the feed-speed domain provides a\nclearly defined strategy that not only provides a unique global optimum\nsolution, but also the software that is suitable for on-line CAM\napplications. A numerical study has verified the developed optimization\nstrategies and software and has shown the economic benefits of using\noptimization\n', ['cutting conditions optimization', 'single pass turning operations', 'deterministic approach', 'CAM software', 'economic cutting conditions', 'maximum production rate', 'mathematical analyses', 'constrained economic trends', 'process planning', 'computer aided production planning', 'computerised numerical control', 'cutting', 'machine tools', 'machining', 'optimisation']), ('Differential algebraic systems anew\nIt is proposed to figure out the leading term in differential algebraic systems\nmore precisely. Low index linear systems with those properly stated\nleading terms are considered in detail. In particular, it is asked\nwhether a numerical integration method applied to the original system\nreaches the inherent regular ODE without conservation, i.e., whether\nthe discretization and the decoupling commute in some sense. In general\none cannot expect this commutativity so that additional difficulties\nlike strong stepsize restrictions may arise. Moreover, abstract\ndifferential algebraic equations in infinite-dimensional Hilbert spaces\nare introduced, and the index notion is generalized to those equations.\nIn particular, partial differential algebraic equations are considered\nin this abstract formulation\n', ['differential algebraic systems', 'low index linear systems', 'numerical integration method', 'inherent regular ODE', 'commutativity', 'stepsize restrictions', 'abstract differential algebraic equations', 'differential equations', 'matrix algebra', 'stability']), ('Learning weights for the quasi-weighted means\nWe study the determination of weights for quasi-weighted means (also called\nquasi-linear means) when a set of examples is given. We consider first\na simple case, the learning of weights for weighted means, and then we\nextend the approach to the more general case of a quasi-weighted mean.\nWe consider the case of a known arbitrary generator f. The paper\nfinishes considering the use of parametric functions that are suitable\nwhen the values to aggregate are measure values or ratio\n', ['quasi-weighted means', 'quasi-linear means', 'learning', 'parametric functions', 'measure values', 'ratio values', 'learning (artificial intelligence)', 'matrix algebra', 'minimisation']), ("2002 in-house fulfillment systems report [publishing]\nCM's 13th annual survey of in-house fulfillment system suppliers brings you up\nto date on the current capabilities of the leading publication software\npackages\n", ['survey', 'in-house fulfillment system', 'suppliers', 'publication software packages', "buyer's guides", 'publishing', 'software packages']), ('BioOne: a new model for scholarly publishing\nThis article describes a unique electronic journal publishing project involving\nthe University of Kansas, the Big 12 Plus Libraries Consortium, the\nAmerican Institute of Biological Sciences, Allen Press, and SPARC, the\nScholarly Publishing and Academic Resources Coalition. This partnership\nhas created BioOne, a database of 40 full-text society journals in the\nbiological and environmental sciences, which was launched in April,\n2001. The genesis and development of the project is described and\nfinancial, technical, and intellectual property models for the project\nare discussed. Collaborative strategies for the project are described\n', ['BioOne full-text society journal database', 'electronic journal publishing project', 'scholarly publishing model', 'University of Kansas', 'Big 12 Plus Libraries Consortium', 'American Institute of Biological Sciences', 'Allen Press', 'SPARC', 'Scholarly Publishing and Academic Resources Coalition', 'biological sciences', 'environmental sciences', 'intellectual property models', 'technical models', 'financial models', 'collaborative strategies', 'academic libraries', 'biology computing', 'electronic publishing', 'environmental science computing', 'full-text databases', 'scientific information systems']), ('A suggestion of fractional-order controller for flexible spacecraft attitude\ncontrol\nA controller design method for flexible spacecraft attitude control is\nproposed. The system is first described by a partial differential\nequation with internal damping. Then the frequency response is\nanalyzed, and the three basic characteristics of the flexible system,\nnamely, average function, lower bound and upper bound are defined. On\nthis basis, a fractional-order controller is proposed, which functions\nas phase stabilization control for lower frequency and smoothly enters\nto amplitude stabilization at higher frequency by proper amplitude\nattenuation. It is shown that the equivalent damping ratio increases in\nproportion to the square of frequency\n', ['fractional-order controller', 'flexible spacecraft attitude control', 'partial differential equation', 'internal damping', 'frequency response', 'phase stabilization control', 'amplitude stabilization', 'damping ratio', 'aerospace instrumentation', 'attitude control', 'controllers', 'damping', 'flexible manipulators', 'frequency response', 'frequency stability', 'nonlinear control systems', 'nonlinear dynamical systems', 'partial differential equations', 'space vehicles']), ('The Information Age interview - Capital One\nCredit card company Capital One attributes its rapid customer growth to the\ninnovative use of cutting-edge technology. European CIO Catherine Doran\ntalks about the systems that have fuelled that runaway success\n', ['credit card company', 'Capital One', 'customer growth', 'cutting-edge technology', 'credit transactions', 'marketing']), ('Influence of advertising expenses on the characteristics of functioning of an\ninsurance company\nThe basic characteristics of the functioning of an insurance company, including\nthe average capital, ruin and survival probabilities, and the\nconditional time before ruin, are examined with allowance for\nadvertising expenses\n', ['advertising expenses influence', 'insurance company functioning characteristics', 'average capital', 'ruin probabilities', 'survival probabilities', 'conditional time', 'advertising', 'costing', 'insurance', 'probability']), ("End-user perspectives on the uptake of computer supported cooperative working\nResearchers in information systems have produced a rich collection of\nmeta-analyses and models to further understanding of factors\ninfluencing the uptake of information technologies. In the domain of\nCSCW, however, these models have largely been neglected, and while\nthere are many case studies, no systematic account of uptake has been\nproduced. We use findings from information systems research to\nstructure a meta-analysis of uptake issues as reported in CSCW case\nstudies, supplemented by a detailed re-examination of one of our own\ncase studies from this perspective. This shows that while there are\nsome factors which seem to be largely specific to CSCW introductions,\nmany of the case study results are very similar to standard IS\nfindings. We conclude by suggesting how the two communities of\nresearchers might build on each other's work, and finally propose\nactivity theory as a means of integrating the two perspectives\n", ['computer supported cooperative work', 'end-user perspectives', 'information systems', 'meta-analyses', 'information technology', 'CSCW', 'activity theory', 'business data processing', 'groupware', 'information systems', 'personal computing']), ('Exploiting randomness in quantum information processing\nWe consider how randomness can be made to play a useful role in quantum\ninformation processing-in particular, for decoherence control and the\nimplementation of quantum algorithms. For a two-level system in which\nthe decoherence channel is non-dissipative, we show that decoherence\nsuppression is possible if memory is present in the channel. Random\nswitching between two potentially harmful noise sources can then\nprovide a source of stochastic control. Such random switching can also\nbe used in an advantageous way for the implementation of quantum\nalgorithms\n', ['quantum information processing', 'randomness', 'decoherence control', 'quantum algorithms', 'two-level system', 'random switching', 'noise', 'stochastic control', 'information theory', 'quantum communication', 'stochastic processes']), ("Emotion and self-control\nA biology-based model of choice is used to examine time-inconsistent\npreferences and the problem of self-control. Emotion is shown to be the\nbiological substrate of choice, in that emotional systems assign value\nto 'goods' in the environment and also facilitate the learning of\nexpectations regarding alternative options for acquiring those goods. A\nthird major function of the emotional choice systems is motivation.\nSelf-control is shown to be the result of a problem with the inhibition\nof the motive force of emotion, where this inhibition is necessary for\nhigher level deliberation\n", ['choice model', 'inhibition', 'learning', 'time-inconsistent preferences', 'self-control', 'emotional choice systems', 'emotion', 'psychology']), ('Knowledge acquisition and ontology modelling for construction of a control and\nmonitoring expert system\nThis paper presents the processes of knowledge acquisition and ontology\ndevelopment for structuring the knowledge base of an expert system.\nOntological engineering is a process that facilitates construction of\nthe knowledge base of an intelligent system. Ontology is the study of\nthe organization and classification of knowledge. Ontological\nengineering in artificial intelligence has the practical goal of\nconstructing frameworks for knowledge that allow computational systems\nto tackle knowledge-intensive problems and it supports knowledge\nsharing and reuse. To illustrate the process of conceptual modelling\nusing the Inferential Modelling Technique as a basis for ontology\nconstruction, the tool and processes are applied to build an expert\nsystem in the domain of monitoring of a petroleum-production facility\n', ['knowledge acquisition', 'knowledge base', 'intelligent system', 'ontological engineering', 'artificial intelligence', 'knowledge reuse', 'inferential modelling technique', 'petroleum-production facility', 'ontology modelling', 'control and monitoring expert system', 'artificial intelligence', 'expert systems', 'knowledge acquisition', 'software reusability']), ('A brief history of electronic reserves\nElectronic reserves has existed as a library service for barely ten years, yet\nits history, however brief, is important as an indicator of the\ndirection being taken by the profession of Librarianship as a whole.\nRecent improvements in technology and a desire to provide better\nservice to students and faculty have resulted in the implementation of\ne-reserves by ever greater numbers of academic libraries. Yet a great\ndeal of confusion still surrounds the issue of copyright compliance.\nNegotiation, litigation, and legislation in particular have framed the\ndebate over the application of fair use to an e-reserves environment,\nand the question of whether or not permission fees should be paid to\nrights holders, but as of yet no definitive answers or standards have\nemerged\n', ['electronic reserves', 'library service', 'librarianship', 'students', 'faculty', 'academic libraries', 'copyright compliance', 'negotiation', 'litigation', 'legislation', 'e-reserves environment', 'permission fees', 'academic libraries', 'copyright', 'history', 'legislation', 'library automation']), ('Causes of the decline of the business school management science course\nThe business school management science course is suffering serious decline. The\ntraditional model- and algorithm-based course fails to meet the needs\nof MBA programs and students. Poor student mathematical preparation is\na reality, and is not an acceptable justification for poor teaching\noutcomes. Management science Ph.D.s are often poorly prepared to teach\nin a general management program, having more experience and interest in\nalgorithms than management. The management science profession as a\nwhole has focused its attention on algorithms and a narrow subset of\nmanagement problems for which they are most applicable. In contrast,\nMBA\'s rarely encounter problems that are suitable for straightforward\napplication of management science tools, living instead in a world\nwhere problems are ill-defined, data is scarce, time is short, politics\nis dominant, and rational "decision makers" are non-existent. The root\ncause of the profession\'s failure to address these issues seems to be\n(in Russell Ackoff\'s words) a habit of professional introversion that\ncaused the profession to be uninterested in what MBA\'s really do on the\njob and how management science can help them\n', ['business school management science course', 'MBA programs', 'MBA students', 'management science', 'profession', 'educational courses', 'management science', 'professional aspects']), ("Center-crossing recurrent neural networks for the evolution of rhythmic\nbehavior\nA center-crossing recurrent neural network is one in which the\nnull(hyper)surfaces of each neuron intersect at their exact centers of\nsymmetry, ensuring that each neuron's activation function is centered\nover the range of net inputs that it receives. We demonstrate that\nrelative to a random initial population, seeding the initial population\nof an evolutionary search with center-crossing networks significantly\nimproves both the frequency and the speed with which high-fitness\noscillatory circuits evolve on a simple walking task. The improvement\nis especially striking at low mutation variances. Our results suggest\nthat seeding with center-crossing networks may often be beneficial,\nsince a wider range of dynamics is more likely to be easily accessible\nfrom a population of center-crossing networks than from a population of\nrandom networks\n", ['center-crossing recurrent neural networks', 'rhythmic behavior evolution', 'null surfaces', 'symmetry', 'activation function', 'evolutionary algorithm', 'random initial population', 'evolutionary search', 'high-fitness oscillatory circuits', 'learning', 'low mutation variance', 'random networks', 'evolutionary computation', 'learning (artificial intelligence)', 'recurrent neural nets', 'search problems']), ("Vendor qualifications for IT staff and networking\nIn some cases, vendor-run accreditation schemes can offer an objective measure\nof a job applicant's skills, but they do not always indicate the true\nextent of practical abilities\n", ['vendor-run accreditation schemes', 'job applicant', 'IT staff', 'network administrators', 'practical abilities', 'accreditation', 'certification', 'computer network management', 'personnel']), ('Optimal time of switching between portfolios of securities\nOptimal time of switching between several portfolios of securities are found\nfor the purpose of profit maximization. Two methods of their\ndetermination are considered. The cases with three and n portfolios are\nstudied in detail\n', ['optimal time', 'portfolios of securities', 'profit maximization', 'commodity trading']), ('Wavelet-based level-of-detail representation of 3D objects\nIn this paper, we propose a 3D object LOD (Level of Detail) modeling system\nthat constructs a mesh from range images and generates the mesh of\nvarious LOD using the wavelet transform. In the initial mesh\ngeneration, we use the marching cube algorithm. We modify the original\nalgorithm to apply it to construct the mesh from multiple range images\nefficiently. To get the base mesh we use the decimation algorithm which\nsimplifies a mesh with preserving the topology. Finally, when\nreconstructing new mesh which is similar to initial mesh we calculate\nthe wavelet coefficients by using the wavelet transform. We solve the\ncritical problem of wavelet-based methods - the surface crease problem\n- by using the mesh simplification as the base mesh generation method\n', ['3D object level of detail modeling system', 'wavelet-based level-of-detail representation', 'range images', 'wavelet transform', 'marching cube algorithm', 'base mesh', 'decimation algorithm', 'wavelet coefficients', 'critical problem', 'surface crease problem', 'mesh simplification', 'hierarchy transformation', 'mesh generation', 'solid modelling', 'topology', 'wavelet transforms']), ('UPSILON: universal programming system with incomplete lazy object notation\nThis paper presents a new model of computation that differs from prior models\nin that it emphasizes data over flow control, has no named variables\nand has an object-oriented flavor. We prove that this model is a\ncomplete and confluent acceptable programming system and has a usable\ntype theory. A new data synchronization primitive is introduced in\norder to achieve the above properties. Subtle variations of the model\nare shown to fall short of having all these necessary properties\n', ['UPSILON', 'universal programming system', 'object-oriented flavor', 'programming system', 'usable type theory', 'data synchronization primitive', 'incomplete lazy object notation', 'computational complexity', 'object-oriented programming', 'type theory']), ('High-performance numerical pricing methods\nThe pricing of financial derivatives is an important field in finance and\nconstitutes a major component of financial management applications. The\nuncertainty of future events often makes analytic approaches infeasible\nand, hence, time-consuming numerical simulations are required. In the\nAurora Financial Management System, pricing is performed on the basis\nof lattice representations of stochastic multidimensional scenario\nprocesses using the Monte Carlo simulation and Backward Induction\nmethods, the latter allowing for the exploitation of shared-memory\nparallelism. We present the parallelization of a Backward Induction\nnumerical pricing kernel on a cluster of SMPs using HPF+, an extended\nversion of High-Performance Fortran. Based on language extensions for\nspecifying a hierarchical mapping of data onto an SMP cluster, the\ncompiler generates a hybrid-parallel program combining\ndistributed-memory and shared-memory parallelism. We outline the\nparallelization strategy adopted by the VFC compiler and present an\nexperimental evaluation of the pricing kernel on an NEC SX-5 vector\nsupercomputer and a Linux SMP cluster, comparing a pure MPI version to\na hybrid-parallel MPI/OpenMP version\n', ['finance', 'financial management', 'Aurora Financial Management System', 'pricing', 'stochastic processes', 'Monte Carlo simulation', 'Backward Induction methods', 'numerical pricing kernel', 'derivative pricing', 'investment strategies', 'financial data processing', 'FORTRAN', 'investment', 'parallel programming']), ('Aggregate bandwidth estimation in stored video distribution systems\nMultimedia applications like video on demand, distance learning, Internet video\nbroadcast, etc. will play a fundamental role in future broadband\nnetworks. A common aspect of such applications is the transmission of\nvideo streams that require a sustained relatively high bandwidth with\nstringent requirements of quality of service. In this paper various\noriginal algorithms for evaluating, in a video distribution system, a\nstatistical estimation of aggregate bandwidth needed by a given number\nof smoothed video streams are proposed and discussed. The variable bit\nrate traffic generated by each video stream is characterized by its\nmarginal distribution and by conditional probabilities between rates of\ntemporary closed streams. The developed iterative algorithms evaluate\nan upper and lower bound of needed bandwidth for guaranteeing a given\nloss probability. The obtained results are compared with simulations\nand with other results, based on similar assumptions, already presented\nin the literature. Some considerations on the developed algorithms are\nmade, in order to evaluate the effectiveness of the proposed methods\n', ['aggregate bandwidth estimation', 'stored video distribution systems', 'multimedia applications', 'video on demand', 'distance learning', 'Internet video broadcast', 'broadband networks', 'video streams transmission', 'quality of service', 'statistical estimation', 'variable bit rate traffic', 'marginal distribution', 'conditional probabilities', 'temporary closed streams', 'iterative algorithms', 'upper bound', 'lower bound', 'loss probability', 'simulations', 'VoD', 'video coding', 'QoS', 'broadband networks', 'data compression', 'Internet', 'iterative methods', 'multimedia communication', 'probability', 'quality of service', 'statistical analysis', 'telecommunication traffic', 'variable rate codes', 'video coding', 'video on demand', 'visual communication']), ('Operational phase-space probability distribution in quantum communication\ntheory\nOperational phase-space probability distributions are useful tools for\ndescribing quantum mechanical systems, including quantum communication\nand quantum information processing systems. It is shown that quantum\ncommunication channels with Gaussian noise and quantum teleportation of\ncontinuous variables are described by operational phase-space\nprobability distributions. The relation of operational phase-space\nprobability distribution to the extended phase-space formalism proposed\nby Chountasis and Vourdas (1998) is discussed\n', ['operational phase-space probability distribution', 'quantum communication theory', 'quantum mechanical systems', 'quantum information processing systems', 'Gaussian noise', 'quantum teleportation', 'continuous variables', 'extended phase-space formalism', 'Gaussian noise', 'information theory', 'probability', 'quantum communication', 'quantum theory']), ('Least load dispatching algorithm for parallel Web server nodes\nA least load dispatching algorithm for distributing requests to parallel Web\nserver nodes is described. In this algorithm, the load offered to a\nnode by a request is estimated based on the expected transfer time of\nthe corresponding reply through the Internet. This loading information\nis then used by the algorithm to identify the least load node of the\nWeb site. By using this algorithm, each request will always be sent for\nservice at the earliest possible time. Performance comparison using\nNASA and ClarkNet access logs between the proposed algorithm and\ncommonly used dispatching algorithms is performed. The results show\nthat the proposed algorithm gives 10% higher throughput than that of\nthe commonly used random and round-robin dispatching algorithms\n', ['least load dispatching algorithm', 'parallel Web server nodes', 'Internet', 'transfer time', 'NASA access logs', 'ClarkNet access logs', 'throughput', 'round-robin dispatching algorithms', 'random dispatching algorithms', 'World Wide Web server', 'client-server systems', 'file servers', 'Internet']), ('Instability phenomena in the gas-metal arc welding self-regulation process\nArc instability is a very important determinant of weld quality. The\ninstability behaviour of the gas-metal arc welding (GMAW) process is\ncharacterized by strong oscillations in arc length and current. In the\npaper, a model of the GMAW process is developed using an exact arc\nvoltage characteristic. This model is used to study stability of the\nself-regulation process and to develop a simulation program that helps\nto understand the transient or dynamic nature of the GMAW process and\nrelationships among current, electrode extension and contact tube-work\ndistance. The process is shown to exhibit instabilities at both long\nelectrode extension and normal extension. Results obtained from\nsimulation runs of the model were also experimentally confirmed by the\npresent author, as reported in this study. In order to explain the\nconcept of the instability phenomena, the metal transfer mode and the\narc voltage-current characteristic were examined. Based on this\nexamination, the conclusion of this study is that their combined\neffects lead to the oscillations in arc current and length\n', ['instability phenomena', 'gas-metal arc welding', 'self-regulation process', 'arc instability', 'weld quality', 'GMAW process', 'exact arc voltage characteristic', 'metal transfer mode', 'adaptive control', 'arc welding', 'process control', 'self-adjusting systems', 'stability']), ('Robust wavelet neuro control for linear brushless motors\nDesign, simulation and experimental implementation of a wavelet basis function\nnetwork learning controller for linear brushless dc motors (LBDCM) are\nconsidered. Stability robustness with position tracking is the primary\nconcern. The proposed controller deals mainly with external\ndisturbances, e.g. nonlinear friction force and payload variation in\nmotion control of linear motors. It consists of two parts, one is a\nstate feedback component, and the other one is a learning feedback\ncomponent. The state feedback controller is designed on the basis of a\nsimple linear model, and the learning feedback component is a wavelet\nneural controller. The attenuation effect of wavelet neural networks on\nfriction force is first verified by the numerical method. The learning\neffect of wavelet neural networks on friction force is also shown in\nthe numerical results. Then, a wavelet neural network is applied on a\nreal LBDCM to on-line suppress the friction force, which may be\nvariable due to the different lubrication. The effectiveness of the\nproposed control schemes is demonstrated by simulated and experimental\nresults\n', ['robust wavelet neuro control', 'linear brushless motors', 'wavelet basis function network', 'LBDCM', 'stability robustness', 'position tracking', 'external disturbances', 'nonlinear friction force', 'payload variation', 'motion control', 'state feedback component', 'learning feedback component', 'attenuation effect', 'friction force', 'lubrication', 'brushless DC motors', 'linear motors', 'neurocontrollers', 'position control', 'robust control', 'state feedback', 'wavelet transforms']), ("Information security policy - what do international information security\nstandards say?\nOne of the most important information security controls, is the information\nsecurity policy. This vital direction-giving document is, however, not\nalways easy to develop and the authors thereof battle with questions\nsuch as what constitutes a policy. This results in the policy authors\nturning to existing sources for guidance. One of these sources is the\nvarious international information security standards. These standards\nare a good starting point for determining what the information security\npolicy should consist of, but should not be relied upon exclusively for\nguidance. Firstly, they are not comprehensive in their coverage and\nfurthermore, tending to rather address the processes needed for\nsuccessfully implementing the information security policy. It is far\nmore important the information security policy must fit in with the\norganisation's culture and must therefore be developed with this in\nmind\n", ['information security policy', 'international information security standards', 'DP management', 'information systems', 'information technology', 'security of data', 'standards']), ('On-line Homework/Quiz/Exam applet: freely available Java software for\nevaluating performance on line\nThe Homework/Quiz/Exam applet is a freely available Java program that can be\nused to evaluate student performance on line for any content authored\nby a teacher. It has database connectivity so that student scores are\nautomatically recorded. It allows several different types of questions.\nEach question can be linked to images and detailed story problems.\nThree levels of feedback are provided to student responses. It allows\nteachers to randomize the sequence of questions and to randomize which\nof several options is the correct answer in multiple-choice questions.\nThe creation and editing of questions involves menu selections, button\npresses, and the typing of content; no programming knowledge is\nrequired. The code is open source in order to encourage modifications\nthat will meet individual pedagogical needs\n', ['online Homework/Quiz/Exam applet', 'freely available Java software', 'online student performance evaluation', 'teacher authored content', 'database connectivity', 'automatic student score recording', 'images', 'detailed story problems', 'feedback', 'randomized question sequence', 'multiple-choice questions', 'question editing', 'question creation', 'menu selections', 'button presses', 'typing content', 'individual pedagogical needs', 'courseware', 'Java', 'public domain software']), ('Deadlock-free scheduling in flexible manufacturing systems using Petri nets\nThis paper addresses the deadlock-free scheduling problem in Flexible\nManufacturing Systems. An efficient deadlock-free scheduling algorithm\nwas developed, using timed Petri nets, for a class of FMSs called\nSystems of Sequential Systems with Shared Resources (S/sup 4/ R). The\nalgorithm generates a partial reachability graph to find the optimal or\nnear-optimal deadlock-free schedule in terms of the firing sequence of\nthe transitions of the Petri net model. The objective is to minimize\nthe mean flow time (MFT). An efficient truncation technique, based on\nthe siphon concept, has been developed and used to generate the minimum\nnecessary portion of the reachability graph to be searched. It has been\nshown experimentally that the developed siphon truncation technique\nenhances the ability to develop deadlock-free schedules of systems with\na high number of deadlocks, which cannot be achieved using standard\nPetri net scheduling approaches. It may be necessary, in some cases, to\nrelax the optimality condition for large FMSs in order to make the\nsearch effort reasonable. Hence, a User Control Factor (UCF) was\ndefined and used in the scheduling algorithm. The objective of using\nthe UCF is to achieve an acceptable trade-off between the solution\nquality and the search effort. Its effect on the MFT and the CPU time\nhas been investigated. Randomly generated examples are used for\nillustration and comparison. Although the effect of UCF did not affect\nthe mean flow time, it was shown that increasing it reduces the search\neffort (CPU time) significantly\n', ['flexible manufacturing systems', 'deadlock-free scheduling', 'Petri nets', 'systems of sequential systems with shared resources', 'partial reachability graph', 'optimal deadlock-free schedule', 'near-optimal deadlock-free schedule', 'Petri net model transitions firing sequence', 'mean flow time minimization', 'siphon truncation technique', 'optimality condition relaxation', 'user control factor', 'CPU time', 'randomly generated examples', 'flexible manufacturing systems', 'minimisation', 'Petri nets', 'production control', 'reachability analysis', 'resource allocation']), ('Extracting linguistic DNA: NStein goes to work for UPI\nIt\'s a tantalizing problem for categorization. United Press International (UPI)\nhas more than 700 correspondents creating thousands of stories every\nweek, running the gamut from business news to sports to entertainment\nto global coverage of America\'s war on terrorism. And while UPI and\nothers news services have mechanisms for adding keywords and\ncategorizing their content, UPI recognized a need to add more\nautomation to the process. With the recent growth and improvement in\ntools for Computer-Aided Indexing (CAI), UPI undertook a process of\nlooking at its needs and evaluating the many CAI tools out there. In\nthe end, they chose technology from Montreal-based NStein Technologies.\n"Our main objective was to acquire the best CAI tool to help improve\nour customers\' access and interaction with our content," says Steve\nSweet, CIO at UPI. "We examined a number of solutions, and NStein\'s\nNServer suite clearly came out on top. The combination of speed,\nscalability, accuracy, and flexibility was what really sold us."\n', ['United Press International', 'UPI', 'electronic archive', 'wire service stories', 'Computer-Aided Indexing', 'NStein Technologies', 'indexing', 'information resources', 'text analysis']), ('Exact controllability of shells in minimal time\nWe prove an exact controllability result for thin cups using the Fourier method\nand recent improvements of Ingham (1936) type theorems\n', ['controllability', 'shells', 'minimal time', 'thin cups', 'partial differential equations', 'Young modulus', 'Hilbert space', 'Fourier method', 'Ingham type theorems', 'controllability', 'eigenvalues and eigenfunctions', 'elasticity', 'Fourier analysis', 'Hilbert spaces', 'partial differential equations', "Young's modulus"]), ("The Canadian National Site Licensing Project\nIn January 2000, a consortium of 64 universities in Canada signed a historic\ninter-institutional agreement that launched the Canadian National Site\nLicensing Project (CNSLP), a three-year pilot project aimed at\nbolstering the research and innovation capacity of the country's\nuniversities. CNSLP tests the feasibility of licensing, on a national\nscale, electronic versions of scholarly publications; in its initial\nphases the project is focused on full-text electronic journals and\nresearch databases in science, engineering, health and environmental\ndisciplines. This article provides an overview of the CNSLP initiative,\nsummarizes organizational and licensing accomplishments to date, and\noffers preliminary observations on challenges and opportunities for\nsubsequent phases of the project\n", ['Canadian National Site Licensing Project', 'inter-institutional agreement', 'research and innovation', 'CNSLP', 'academic libraries', 'information resources', 'electronic scholarly publications', 'full-text electronic journals', 'research databases', 'academic libraries', 'contracts', 'electronic publishing', 'full-text databases', 'information resources', 'library automation']), ("Warranty reserves for nonstationary sales processes\nEstimation of warranty costs, in the event of product failure within the\nwarranty period, is of importance to the manufacturer. Costs associated\nwith replacement or repair of the product are usually drawn from a\nwarranty reserve fund created by the manufacturer. Considering a\nstochastic sales process, first and second moments (and thereby the\nvariance) are derived for the manufacturer's total discounted warranty\ncost of a single sale for single-component items under four different\nwarranty policies from a manufacturer's point of view. These servicing\nstrategies represent a renewable free-replacement, nonrenewable\nfree-replacement, renewable pro-rata, and a nonrenewable minimal-repair\nwarranty plans. The results are extended to determine the mean and\nvariance of total discounted warranty costs for the total sales over\nthe life cycle of the product. Furthermore, using a normal\napproximation, warranty reserves necessary for a certain protection\nlevel, so that reserves are not completely depleted, are found. Results\nand their managerial implications are studied through an extensive\nexample\n", ['nonstationary sales processes', 'warranty reserves', 'warranty costs estimation', 'product failure', 'product replacement', 'product repair', 'stochastic sales process', 'first moments', 'second moments', 'variance', 'total discounted warranty cost', 'single-component items', 'servicing strategies', 'renewable free-replacement', 'nonrenewable free-replacement', 'renewable pro-rata', 'nonrenewable minimal-repair warranty plans', 'total discounted warranty costs', 'product life cycle', 'normal approximation', 'managerial implications', 'life cycle costing', 'probability', 'retailing']), ('An improved self-organizing CPN-based fuzzy system with adaptive\nback-propagation algorithm\nThis paper describes an improved self-organizing CPN-based (Counter-Propagation\nNetwork) fuzzy system. Two self-organizing algorithms IUSOCPN and\nISSOCPN, being unsupervised and supervised respectively, are\nintroduced. The idea is to construct the neural-fuzzy system with a\ntwo-phase hybrid learning algorithm, which utilizes a CPN-based\nnearest-neighbor clustering scheme for both structure learning and\ninitial parameters setting, and a gradient descent method with adaptive\nlearning rate for fine tuning the parameters. The obtained network can\nbe used in the same way as a CPN to model and control dynamic systems,\nwhile it has a faster learning speed than the original back-propagation\nalgorithm. The comparative results on the examples suggest that the\nmethod is fairly efficient in terms of simple structure, fast learning\nspeed, and relatively high modeling accuracy\n', ['self-organizing fuzzy system', 'Counter-Propagation Network', 'neural-fuzzy system', 'hybrid learning', 'gradient descent', 'structure learning', 'initial parameters setting', 'back-propagation learning scheme', 'backpropagation', 'fuzzy neural nets', 'learning (artificial intelligence)', 'self-adjusting systems']), ('Effect of insulation layer on transcribability and birefringence distribution\nin optical disk substrate\nAs the need for information storage media with high storage density increases,\ndigital video disks (DVDs) with smaller recording marks and thinner\noptical disk substrates than those of conventional DVDs are being\nrequired. Therefore, improving the replication quality of land-groove\nor pit structure and reducing the birefringence distribution are\nemerging as important criteria in the fabrication of high-density\noptical disk substrates. We control the transcribability and\ndistribution of birefringence by inserting an insulation layer under\nthe stamper during injection-compression molding of DVD RAM substrates.\nThe effects of the insulation layer on the geometrical and optical\nproperties, such as transcribability and birefringence distribution,\nare examined experimentally. The inserted insulation layer is found to\nbe very effective in improving the quality of replication and leveling\nout the first peak of the gapwise birefringence distribution near the\nmold wall and reducing the average birefringence value, because the\ninsulation layer retarded the growth of the solidified layer\n', ['optical disk substrate', 'transcribability', 'birefringence distribution', 'insulation layer', 'information storage media', 'high storage density', 'digital video disks', 'smaller recording marks', 'thinner optical disk substrates', 'replication quality', 'land-groove', 'pit structure', 'fabrication', 'stamper', 'injection-compression molding', 'DVD RAM substrates', 'geometrical properties', 'optical properties', 'gapwise birefringence distribution', 'mold wall', 'solidified layer growth retardation', 'polyimide thermal insulation layer', 'birefringence', 'moulding', 'optical disc storage', 'optical fabrication', 'replica techniques', 'substrates', 'thermal insulation', 'video discs']), ('Optimization of requantization parameter for MPEG transcoding\nThis paper considers transcoding in which an MPEG stream is converted to a\nlow-bit-rate MPEG stream, and proposes a method in which the\ntranscoding error can be reduced by optimally selecting the\nquantization parameter for each macroblock. In transcoding with a low\ncompression ratio, it is crucial to prohibit transcoding with a\nrequantization parameter which is 1 to 2 times the quantization\nparameter of the input stream. Consequently, as the first step, an\noptimization method for the requantization parameter is proposed which\ncares for the error propagation effect by interframe prediction. Then,\nthe proposed optimization method is extended so that the method can\nalso be applied to the case of a high compression ratio in which the\nrate-distortion curve is approximated for each macroblock in the range\nof requantization parameters larger than 2 times the quantization\nparameter. It is verified by a simulation experiment that the PSNR is\nimproved by 0.5 to 0.8 dB compared to the case in which a 6 Mbit/s MPEG\nstream is not optimized by twofold recompression\n', ['requantization parameter optimization', 'low-bit-rate MPEG stream', 'transcoding error', 'macroblock', 'compression ratio', 'error propagation effect', 'interframe prediction', 'rate-distortion curve', 'PSNR', 'simulation', 'twofold recompression', 'rate conversion', 'rate control', '6 Mbit/s', 'data compression', 'optimisation', 'quantisation (signal)', 'rate distortion theory', 'variable rate codes', 'video coding']), ('Speedera: Web without the wait\nThere\'s no greater testament to the utility of the Internet than the fact that\nhundreds of millions of people worldwide are willing to wait for Web\npages as they build incrementally on screen. But while users may put up\nwith the "World Wide Wait," they definitely don\'t like it. That\'s where\nContent Delivery Networks come in. CDNs can\'t turn a footpath into a\nfreeway, but they can help data in transit take advantage of shortcuts\nand steer clear of traffic jams. And while enhancing the responsiveness\nof Web interaction, CDNs also enhance the prospects of their clients,\nwho need engaged visitors to keep their Web-based business models\nafloat. "Our mission is to improve the quality of the Internet\nexperience for end-users," says Gordon Smith, vice president of\nmarketing at Speedera Networks in Santa Clara, California, "and to\nenable Web-site operators to provide better delivery quality,\nperformance, scalability, and security through an outsourced service\nmodel that slashes IT costs."\n', ['Content Delivery Networks', 'Web-site operators', 'delivery quality', 'scalability', 'security', 'outsourced service model', 'World Wide Web', 'Web interaction', 'Web-based business models', 'Internet experience', 'file servers', 'Internet', 'multimedia communication', 'outsourcing']), ('Data management in location-dependent information services\nLocation-dependent information services have great promise for mobile and\npervasive computing environments. They can provide local and nonlocal\nnews, weather, and traffic reports as well as directory services.\nBefore they can be implemented on a large scale, however, several\nresearch issues must be addressed\n', ['location-dependent information services', 'wireless networks', 'pervasive computing', 'mobile computing', 'news', 'weather', 'traffic reports', 'data management', 'directory services', 'database management systems', 'information services', 'mobile computing']), ('Automated cerebrum segmentation from three-dimensional sagittal brain MR images\nWe present a fully automated cerebrum segmentation algorithm for full\nthree-dimensional sagittal brain MR images. First, cerebrum\nsegmentation from a midsagittal brain MR image is performed utilizing\nlandmarks, anatomical information, and a connectivity-based threshold\nsegmentation algorithm as previously reported. Recognizing that the\ncerebrum in laterally adjacent slices tends to have similar size and\nshape, we use the cerebrum segmentation result from the midsagittal\nbrain MR image as a mask to guide cerebrum segmentation in adjacent\nlateral slices in an iterative fashion. This masking operation yields a\nmasked image (preliminary cerebrum segmentation) for the next lateral\nslice, which may truncate brain region(s). Truncated regions are\nrestored by first finding end points of their boundaries, by comparing\nthe mask image and masked image boundaries, and then applying a\nconnectivity-based algorithm. The resulting final extracted cerebrum\nimage for this slice is then used as a mask for the next lateral slice.\nThe algorithm yielded satisfactory fully automated cerebrum\nsegmentations in three-dimensional sagittal brain MR images, and had\nperformance superior to conventional edge detection algorithms for\nsegmentation of cerebrum from 3D sagittal brain MR images\n', ['fully automated cerebrum segmentation algorithm', 'full 3D sagittal brain MR images', 'midsagittal brain MR image', 'landmarks', 'anatomical information', 'connectivity-based threshold segmentation algorithm', 'laterally adjacent slices', 'masking operation', 'brain region truncation', 'boundary end points', 'masked image boundaries', 'connectivity-based algorithm', 'biomedical MRI', 'brain', 'edge detection', 'image segmentation', 'medical image processing', 'stereo image processing']), ('Identification of states of complex systems with estimation of admissible\nmeasurement errors on the basis of fuzzy information\nThe problem of identification of states of complex systems on the basis of\nfuzzy values of informative attributes is considered. Some estimates of\na maximally admissible degree of measurement error are obtained that\nmake it possible, using the apparatus of fuzzy set theory, to correctly\nidentify the current state of a system\n', ['complex systems states identification', 'admissible measurement errors', 'fuzzy information', 'informative attributes', 'measurement error', 'fuzzy set theory', 'fuzzy logic', 'fuzzy set theory']), ('Embedding of level-continuous fuzzy sets on Banach spaces\nIn this paper we present an extension of the Minkowski embedding theorem,\nshowing the existence of an isometric embedding between the classF/sub\nc/(X) of compact-convex and level-continuous fuzzy sets on a real\nseparable Banach space X and C([0, 1] * B(X*)), the Banach space of\nreal continuous functions defined on the cartesian product between [0,\n1] and the unit ball B(X*) in the dual space X*. Also, by using this\nembedding, we give some applications to the characterization of\nrelatively compact subsets of F/sub c/(X). In particular, an\nAscoli-Arzela type theorem is proved and applied to solving the Cauchy\nproblem x(t) = f(t, x(t)), x(t/sub 0/) = x/sub 0/ on F/sub c/(X)\n', ['isometric embedding', 'level-continuous fuzzy sets', 'compact-convex fuzzy sets', 'real separable Banach space', 'real continuous functions', 'cartesian product', 'unit ball', 'dual space', 'Ascoli-Arzela type theorem', 'Cauchy problem', 'Banach spaces', 'fuzzy set theory']), ("Down up [IT projects]\nDespite the second quarter's gloomy GDP report, savvy CIOs are forging ahead\nwith big IT projects that will position their companies to succeed when\nthe economy soars again\n", ['strategic technology projects', 'Walgreen', 'Ford', 'Caterpillar', "Victoria's Secret", 'Morgan Stanley', 'Staples', 'DP management']), ("Content standards for electronic books: the OEBF publication structure and the\nrole of public interest participation\nIn the emerging world of electronic publishing how we create, distribute, and\nread books will be in a large part determined by an underlying\nframework of content standards that establishes the range of\ntechnological opportunities and constraints for publishing and reading\nsystems. But efforts to develop content standards based on sound\nengineering models must skillfully negotiate competing and sometimes\napparently irreconcilable objectives if they are to produce results\nrelevant to the rapidly changing course of technology. The Open eBook\nForum's Publication Structure, an XML-based specification for\nelectronic books, is an example of the sort of timely and innovative\nproblem solving required for successful real-world standards\ndevelopment. As a result of this effort, the electronic book industry\nwill not only happen sooner and on a larger scale than it would have\notherwise, but the electronic books it produces will be more\nfunctional, more interoperable, and more accessible to all readers.\nPublic interest participants have a critical role in this process\n", ['electronic publishing', 'electronic books', 'content standards', 'OEBF Publication Structure', 'public interest participation', 'Open eBook Forum Publication Structure', 'XML-based specification', 'electronic publishing', 'hypermedia markup languages', 'standards']), ('Knowledge model reuse: therapy decision through specialisation of a generic\ndecision model\nWe present the definition of the therapy decision task and its associated\nHeuristic Multi-Attribute (HM) solving method, in the form of a\nKADS-style specification. The goal of the therapy decision task is to\nidentify the ideal therapy, for a given patient, in accordance with a\nset of objectives of a diverse nature constituting a global\ntherapy-evaluation framework in which considerations such as patient\npreferences and quality-of-life results are integrated. We give a\nhigh-level overview of this task as a specialisation of the generic\ndecision task, and additional decomposition methods for the subtasks\ninvolved. These subtasks possess some reflective capabilities for\nreasoning about self-models, particularly the learning subtask, which\nincrementally corrects and refines the model used to assess the effects\nof the therapies. This work illustrates the process of reuse in the\nframework of AI software development methodologies such as\nKADS-CommonKADS in order to obtain new (more specialised but still\ngeneric) components for the analysis libraries developed in this\ncontext. In order to maximise reuse benefits, where possible, the\ntherapy decision task and HM method have been defined in terms of\nregular components from the earlier-mentioned libraries. To emphasise\nthe importance of using a rigorous approach to the modelling of domain\nand method ontologies, we make extensive use of the semi-formal\nobject-oriented analysis notation UML, together with its associated\nconstraint language OCL, to illustrate the ontology of the decision\nmethod and the corresponding specific one of the therapy decision\ndomain, the latter being a refinement via inheritance of the former\n', ['knowledge model reuse', 'therapy decision task', 'KADS-style specification', 'global therapy-evaluation framework', 'patient preferences', 'reasoning', 'learning subtask', 'software development methodologies', 'CommonKADS', 'ontologies', 'object-oriented analysis notation', 'UML', 'constraint language', 'OCL', 'generic decision model specialisation', 'Heuristic Multi-Attribute solving method', 'decision support systems', 'heuristic programming', 'knowledge acquisition', 'medical expert systems', 'object-oriented methods', 'patient treatment', 'specification languages']), ("Construction of two-sided bounds for initial-boundary value problems\nThis paper extends the bounding operator approach developed for boundary value\nproblems to the case of initial-boundary value problems (IBVPs).\nFollowing the general principle of bounding operators enclosing methods\nfor the case of partial differential equations are discussed. In\nparticular, continuous discretization methods with an appropriate error\nbound controlled shift and monotone extensions of Rothe's method for\nparabolic problems are investigated\n", ['two-sided bounds', 'initial-boundary value problems', 'bounding operator approach', 'bounding operators', 'partial differential equations', 'parabolic problems', 'initial value problems', 'partial differential equations']), ('Experimental investigations on monitoring and control of induction heating\nprocess for semi-solid alloys using the heating coil as sensor\nA method of monitoring the state of metal alloys during induction heating and\ncontrol of the heating process utilizing the heating coil itself as a\nsensor is proposed, and its usefulness and effectiveness were\nexperimentally investigated using aluminium A357 billets for the\nsemi-solid metal (SSM) casting processes. The impedance of the coil\ncontaining the billet was continuously measured by the proposed method\nin the temperature range between room temperature and 700 degrees C. It\nwas found that the reactance component of the impedance varied\ndistinctively according to the billet state and could clearly monitor\nthe deformation of the billet, while the resistance component increased\nwith temperature, reflecting the variation of the resistivity of the\nbillet which has strong correlation to the solid/liquid fraction of the\nbillets. The measured impedance is very sensitive to the billet states\nsuch as temperature, deformation and solid/liquid fraction and could be\nused as a parameter to monitor and control the heating process for SSMs\n', ['induction heating process', 'process monitoring', 'process control', 'semisolid alloys', 'semisolid metal casting', 'heating coil sensor', 'coil impedance', 'reactance component', 'billet state', 'billet deformation', 'resistance component', 'resistivity variation', 'solid/liquid fraction', 'solenoid coil', '20 to 700 C', 'casting', 'electric reactance measurement', 'electric resistance measurement', 'induction heating', 'metallurgical industries', 'process control', 'process monitoring', 'solenoids', 'temperature control']), ("The case for activity based management\nIn today's stormy economic climate businesses need Activity Based Management\n(ABM) more than ever before. In an economic downturn it is a vital tool\nfor pinpointing a business' most profitable customers, products,\nregions or channels, as well as uncovering the costs of individual\nbusiness processes that may need to be improved in order to drive\nhigher profit levels. Changes may be afoot in the ABM market, but\nArmstrong Laing Group CEO Mike Sherratt argues that businesses need\nspecialists with an ABM focus to keep up with their requirements in\nsuch a climate. He looks at what benefits a `best-of-breed' ABM system\ncan offer businesses and contends that businesses must choose carefully\nwhen going down the ABM route - and also ask themselves the question\nwhether 'generalist' organisations will be able to deliver the best\npossible ABM solution\n", ['activity based costing', 'best-of-breed ABM', 'Armstrong Laing Group', 'activity based management', 'accounting', 'costing']), ('Establishing an urban digital cadastre: analytical reconstruction of parcel\nboundaries\nA new method for generating a spatially accurate, legally supportive and\noperationally efficient cadastral database of the urban cadastral\nreality is described. The definition and compilation of an accurate\ncadastral database (achieving a standard deviation smaller than 0.1 m)\nis based on an analytical reconstruction of cadastral boundaries rather\nthan on the conventional field reconstruction process. The new method\nis based on GPS control points and traverse networks for providing the\nframework; the old field books for defining the links between the\nvarious original ground features; and a geometrical and cadastral\nadjustment process as the conceptual basis. A pilot project that was\ncarried out in order to examine and evaluate the new method is\ndescribed\n', ['urban digital cadastre', 'analytical reconstruction', 'parcel boundaries', 'spatially accurate cadastral database', 'urban cadastral reality', 'standard deviation', 'field reconstruction process', 'GPS control points', 'traverse networks', 'old field books', 'ground features', 'cadastral adjustment process', 'land information systems', 'LIS', 'geographic information systems', 'cartography', 'geographic information systems', 'terrain mapping', 'town and country planning', 'visual databases']), ('Bit-serial AB/sup 2/ multiplier using modified inner product\nThis paper presents a new multiplication algorithm and, based on this\nalgorithm, proposes a hardware architecture, called modified\ninner-product multiplier (MIPM), which computes AB/sup 2/\nmultiplication based on a linear feedback shift register (LFSR). The\nalgorithm is based on the property of the irreducible all one\npolynomial (AOP) over the finite field GF(2/sup m/). The proposed\narchitecture reduces the time and space complexity for computing AB/sup\n2/. The proposed architecture has a potential application to\nimplementing exponentiation architecture for a public-key cryptosystem\n', ['bit-serial AB/sup 2/ multiplier', 'modified inner product', 'multiplication algorithm', 'hardware architecture', 'modified inner-product multiplier', 'linear feedback shift register', 'irreducible all one polynomial', 'space complexity', 'time complexity', 'public-key cryptosystem', 'computational complexity', 'cryptography', 'logic design', 'multiplying circuits', 'shift registers']), ('Motion estimation using modified dynamic programming\nA new method for computing precise estimates of the motion vector field of\nmoving objects in a sequence of images is proposed. Correspondence\nvector-field computation is formulated as a matching optimization\nproblem for multiple dynamic images. The proposed method is a heuristic\nmodification of dynamic programming applied to the 2-D optimization\nproblem. Motion-vector-field estimates using real movie images\ndemonstrate good performance of the algorithm in terms of dynamic\nmotion analysis\n', ['modified dynamic programming', 'motion estimation', 'precise estimates', 'motion vector field', 'moving objects', 'image sequence', 'vector-field computation', 'matching optimization problem', 'multiple dynamic images', 'heuristic modification', 'dynamic programming', '2-D optimization problem', 'motion vector field estimates', 'real movie images', 'algorithm', 'dynamic motion analysis', 'dynamic programming', 'image matching', 'image sequences', 'motion estimation', 'robot vision', 'vectors']), ('A new graphical user interface for fast construction of computation phantoms\nand MCNP calculations: application to calibration of in vivo\nmeasurement systems\nReports on a new utility for development of computational phantoms for Monte\nCarlo calculations and data analysis for in vivo measurements of\nradionuclides deposited in tissues. The individual properties of each\nworker can be acquired for a rather precise geometric representation of\nhis (her) anatomy, which is particularly important for low energy gamma\nray emitting sources such as thorium, uranium, plutonium and other\nactinides. The software enables automatic creation of an MCNP input\ndata file based on scanning data. The utility includes segmentation of\nimages obtained with either computed tomography or magnetic resonance\nimaging by distinguishing tissues according to their signal\n(brightness) and specification of the source and detector. In addition,\na coupling of individual voxels within the tissue is used to reduce the\nmemory demand and to increase the calculational speed. The utility was\ntested for low energy emitters in plastic and biological tissues as\nwell as for computed tomography and magnetic resonance imaging scanning\ninformation\n', ['computational phantoms', 'Monte Carlo calculations', 'in vivo measurements', 'radionuclides', 'tissues', 'worker', 'precise geometric representation', 'MCNP input data file', 'scanning data', 'computed tomography', 'brightness', 'graphical user interface', 'computation phantoms', 'calibration', 'in vivo measurement systems', 'Th', 'U', 'Pu', 'signal', 'detector', 'individual voxels', 'memory demand', 'calculational speed', 'plastic', 'biological tissues', 'magnetic resonance imaging scanning information', 'anatomy', 'low energy gamma ray emitting sources', 'actinides', 'software', 'automatic creation', 'biological tissues', 'biomedical MRI', 'calibration', 'computerised tomography', 'graphical user interfaces', 'lung', 'Monte Carlo methods', 'physics computing', 'radioisotopes']), ('Simulation of evacuation processes using a bionics-inspired cellular automaton\nmodel for pedestrian dynamics\nWe present simulations of evacuation processes using a recently introduced\ncellular automaton model for pedestrian dynamics. This model applies a\nbionics approach to describe the interaction between the pedestrians\nusing ideas from chemotaxis. Here we study a rather simple situation,\nnamely the evacuation from a large room with one or two doors. It is\nshown that the variation of the model parameters allows to describe\ndifferent types of behaviour, from regular to panic. We find a\nnon-monotonic dependence of the evacuation times on the coupling\nconstants. These times depend on the strength of the herding behaviour,\nwith minimal evacuation times for some intermediate values of the\ncouplings, i.e., a proper combination of herding and use of knowledge\nabout the shortest way to the exit\n', ['evacuation processes simulation', 'chemotaxis', 'nonmonotonic dependence', 'coupling constants', 'herding behaviour', 'bionics-inspired cellular automaton model', 'pedestrian dynamics', 'biocybernetics', 'cellular automata']), ('Statistical analysis of nonlinearly reconstructed near-infrared tomographic\nimages. II. Experimental interpretation\nFor pt. I see ibid., vol. 21, no. 7, p. 755-63 (2002). Image error analysis of\na diffuse near-infrared tomography (NIR) system has been carried out on\nsimulated data using a statistical approach described in pt. I of this\npaper (Pogue et al., 2002). The methodology is used here with\nexperimental data acquired on phantoms with a prototype imaging system\nintended for characterizing breast tissue. Results show that imaging\nperformance is not limited by random measurement error, but rather by\ncalibration issues. The image error over the entire field of view is\ngenerally not minimized when an accurate homogeneous estimate of the\nphantom properties is available; however, local image error over a\ntarget region of interest (ROI) is reduced. The image reconstruction\nprocess which includes a Levenberg-Marquardt style regularization\nprovides good minimization of the objective function, yet its reduction\nis not always correlated with an overall image error decrease.\nMinimization of the bias in an ROI which contains localized changes in\nthe optical properties can be achieved through five to nine iterations\nof the algorithm. Precalibration of the algorithm through statistical\nevaluation of phantom studies may provide a better measure of the image\naccuracy than that implied by minimization of the standard objective\nfunction\n', ['medical diagnostic imaging', 'nonlinearly reconstructed near-infrared tomographic images', 'image error', 'algorithm precalibration', 'hemoglobin', 'random measurement error', 'target region of interest', 'accurate homogeneous estimate', 'phantom properties', 'Levenberg-Marquardt style regularization', 'bias minimization', 'algorithm iterations', 'objective function minimization', 'calibration', 'image reconstruction', 'infrared imaging', 'measurement errors', 'medical image processing', 'optical tomography', 'statistical analysis']), ('Decomposition of additive cellular automata\nFinite additive cellular automata with fixed and periodic boundary conditions\nare considered as endomorphisms over pattern spaces. A characterization\nof the nilpotent and regular parts of these endomorphisms is given in\nterms of their minimal polynomials. Generalized eigenspace\ndecomposition is determined and relevant cyclic subspaces are described\nin terms of symmetries. As an application, the lengths and frequencies\nof limit cycles in the transition diagram of the automaton are\ncalculated\n', ['cellular automata', 'finite cellular automaton', 'transition diagram', 'endomorphisms', 'computational complexity', 'cellular automata', 'computational complexity']), ('Parallel operation of capacity-limited three-phase four-wire active power\nfilters\nThree-phase four-wire active power filters (APFs) are presented that can be\nparalleled to enlarge the system capacity and reliability. The APF\nemploys the PWM four-leg voltage-source inverter. A decoupling control\napproach for the leg connected to the neutral line is proposed such\nthat the switching of all legs has no interaction. Functions of the\nproposed APF include compensation of reactive power, harmonic current,\nunbalanced power and zero-sequence current of the load. The objective\nis to achieve unity power factor, balanced line current and zero\nneutral-line current. Compensation of all components is\ncapacity-limited, co-operating with the cascaded load current sensing\nscheme. Multiple APFs can be paralleled to share the load power without\nrequiring any control interconnection. In addition to providing the\ntheoretic bases and detailed design of the APFs, two 6 kVA APFs are\nimplemented. The effectiveness of the proposed method is validated with\nexperimental results\n', ['capacity-limited three-phase four-wire active power filters', 'parallel operation', 'PWM four-leg voltage-source inverter', 'decoupling control approach', 'leg switching', 'control design', 'reactive power compensation', 'harmonic current compensation', 'unbalanced power compensation', 'zero-sequence load current compensation', 'unity power factor', 'balanced line current', 'zero neutral-line current', 'load power sharing', 'control performance', '6 kVA', 'active filters', 'compensation', 'control system synthesis', 'DC-AC power convertors', 'electric current control', 'power harmonic filters', 'power supply quality', 'power system control', 'power system harmonics', 'PWM invertors', 'reactive power control']), ("New developments in inductive learning\nAny intelligent system, whether natural or artificial, must have three\ncharacteristics: knowledge, reasoning, and learning. Artificial\nintelligence (AI) studies these three aspects in artificial systems.\nBriefly, we could say that knowledge refers to the system's world\nmodel, and reasoning to the manipulation of this knowledge. Learning is\nslightly more complex; the system interacts with the world and as a\nconsequence it builds onto and modifies its knowledge. This process of\nself-building and self-modifying is known as learning. This thesis is\nset within the field of artificial intelligence and focuses on\nlearning. More specifically, it deals with the inductive learning of\ndecision trees\n", ['inductive learning', 'new developments', 'intelligent system', 'knowledge', 'reasoning', 'artificial intelligence', 'decision trees', 'decision trees', 'learning by example']), ('Robust model-order reduction of complex biological processes\nThis paper addresses robust model-order reduction of a high dimensional\nnonlinear partial differential equation (PDE) model of a complex\nbiological process. Based on a nonlinear, distributed parameter model\nof the same process which was validated against experimental data of an\nexisting, pilot-scale biological nutrient removal (BNR) activated\nsludge plant, we developed a state-space model with 154 state\nvariables. A general algorithm for robustly reducing the nonlinear PDE\nmodel is presented and, based on an investigation of five\nstate-of-the-art model-order reduction techniques, we are able to\nreduce the original model to a model with only 30 states without\nincurring pronounced modelling errors. The singular perturbation\napproximation balanced truncating technique is found to give the lowest\nmodelling errors in low frequency ranges and hence is deemed most\nsuitable for controller design and other real-time applications\n', ['complex biological processes', 'robust model-order reduction', 'high dimensional nonlinear partial differential equation model', 'nonlinear distributed parameter model', 'pilot-scale BNR activated sludge plant', 'state-space model', 'singular perturbation approximation balanced truncating technique', 'modelling errors', 'controller design', 'Hankel singular values', 'biological nutrient removal activated sludge processes', 'biotechnology', 'control system synthesis', 'nonlinear control systems', 'nonlinear differential equations', 'partial differential equations', 'reduced order systems', 'robust control', 'singularly perturbed systems', 'state-space methods', 'water treatment']), ('Managing safety and strategic stocks to improve materials requirements planning\nperformance\nThis paper provides a methodology for managing safety and strategic stocks in\nmaterials requirements planning (MRP) environments to face uncertainty\nin market demand. A set of recommended guidelines suggest where to\nposition, how to dimension and when to replenish both safety and\nstrategic stocks. Trade-offs between stock positioning and dimensioning\nand between stock positioning and replenishment order triggering are\noutlined. The study reveals also that most of the decisions are system\nspecific, so that they should be evaluated in a quantitative manner\nthrough simulation. A case study is reported, where the benefits from\nadopting the new proposed methodology lie in achieving the target\nservice level even under peak demand conditions, with the value of\nsafety stocks as a whole growing only by about 20 per cent\n', ['MRP', 'materials requirements planning', 'market demand', 'strategic stocks', 'safety stocks', 'inventory management', 'variance control', 'stock replenishment', 'service level', 'peak demand', 'management', 'manufacturing resources planning', 'production control', 'stock control']), ('Micro-optical realization of arrays of selectively addressable dipole traps: a\nscalable configuration for quantum computation with atomic qubits\nWe experimentally demonstrate novel structures for the realization of registers\nof atomic qubits: We trap neutral atoms in one- and two-dimensional\narrays of far-detuned dipole traps obtained by focusing a red-detuned\nlaser beam with a microfabricated array of microlenses. We are able to\nselectively address individual trap sites due to their large lateral\nseparation of 125 mu m. We initialize and read out different internal\nstates for the individual sites. We also create two interleaved sets of\ntrap arrays with adjustable separation, as required for many proposed\nimplementations of quantum gate operations\n', ['atomic qubits', 'registers', 'neutral atoms', 'far-detuned dipole traps', 'red-detuned laser beam', 'microfabricated array', 'microlenses', 'internal states', 'quantum gate operations', 'quantum computation', 'scalable configuration', 'quantum computing', 'trapped ions']), ('Six common enterprise programming mistakes\nInstead of giving you tips to use in your programming (at least directly), I\nwant to look at some common mistakes made in enterprise programming.\nInstead of focusing on what to do, I want to look at what you should\nnot do. Most programmers take books like mine and add in the good\nthings, but they leave their mistakes in the very same programs! So I\ntouch on several common errors I see in enterprise programming, and\nthen briefly mention how to avoid those mistakes\n', ['enterprise programming mistakes', 'common errors', 'data store', 'database', 'XML', 'Enterprise JavaBeans', 'vendor-specific programming', 'business data processing', 'programming']), ('Mathematical properties of dominant AHP and concurrent convergence method\nThis study discusses the mathematical structure of the dominant AHP and the\nconcurrent convergence method which were originally developed by\nKinoshita and Nakanishi. They introduced a new concept of a regulating\nalternative into an analyzing tool for a simple evaluation problem with\na criterion set and an alternative set. Although the original idea of\nthe dominant AHP and the concurrent convergence method is unique, the\ndominant AHP and the concurrent convergence method are not sufficiently\nanalyzed in mathematical theory. This study shows that the dominant AHP\nconsists of a pair of evaluation rules satisfying a certain property of\noverall evaluation vectors. This study also shows that the convergence\nof concurrent convergence method is guaranteed theoretically\n', ['dominant AHP', 'concurrent convergence method', 'overall evaluation vectors', 'convergence', 'decision theory']), ("Selecting rail grade crossing investments with a decision support system\nThe Federal Railroad Administration (FRA) has developed a series of rail and\nrail-related analysis tools that assist FRA officials, Metropolitan\nPlanning Organizations (MPOs), state Department of Transportation\n(DOT), and other constituents in evaluating the cost and benefits of\npotential infrastructure projects. To meet agency objectives, the FRA\nwants to add a high-speed rail grade crossing analysis tool to its\npackage of rail and rail-related intermodal software products. This\npaper presents a conceptual decision support system (DSS) that can\nassist officials in achieving this goal. The paper first introduces the\nFRA's objectives and the role of cost benefit analysis in achieving\nthese objectives. Next, there is a discussion of the models needed to\nassess the feasibility of proposed high-speed rail grade crossing\ninvestments and the presentation of a decision support system (DSS)\nthat can deliver these models transparently to users. Then, the paper\nillustrates a system session and examines the potential benefits from\nsystem use\n", ['rail grade crossing investment selection', 'decision support system', 'Federal Railroad Administration', 'Metropolitan Planning Organizations', 'Department of Transportation', 'infrastructure projects', 'high-speed rail grade crossing analysis tool', 'rail-related intermodal software products', 'rail intermodal software products', 'cost benefit analysis', 'cost-benefit analysis', 'decision support systems', 'investment', 'railways', 'town and country planning', 'transportation']), ('An efficient algorithm for sequential generation of failure states in a network\nwith multi-mode components\nIn this work, a new algorithm for the sequential generation of failure states\nin a network with multi-mode components is proposed. The algorithm\npresented in the paper transforms the state enumeration problem into a\nK-shortest paths problem. Taking advantage of the inherent efficiency\nof an algorithm for shortest paths enumeration and also of the\ncharacteristics of the reliability problem in which it will be used, an\nalgorithm with lower complexity than the best algorithm in the\nliterature for solving this problem, was obtained. Computational\nresults will be presented for comparing the efficiency of both\nalgorithms in terms of CPU time and for problems of different size\n', ['multi-mode components reliability', 'sequential failure states generation algorithm', 'network failure states', 'state enumeration problem', 'K-shortest paths problem', 'CPU time', 'engineering computing', 'failure analysis', 'reliability theory']), ("Wireless-retail financial services: adoption can't justify the cost\nSlow adoption by retail investors, costly services and bankrupt vendors has\nprompted banks and brokerage firms to turn off their wireless\napplications\n", ['banks', 'brokerage firms', 'wireless applications', 'banking', 'investment', 'mobile computing']), ('Comparison of automated digital elevation model extraction results using\nalong-track ASTER and across-track SPOT stereo images\nA digital elevation model (DEM) can be extracted automatically from stereo\nsatellite images. During the past decade, the most common satellite\ndata used to extract DEM was the across-track SPOT. Recently, the\naddition of along-track ASTER data, which can be downloaded freely,\nprovides another attractive alternative to extract DEM data. This work\ncompares the automated DEM extraction results using an ASTER stereo\npair and a SPOT stereo pair over an area of hilly mountains in Drum\nMountain, Utah, when compared to a USGS 7.5-min DEM standard product.\nThe result shows that SPOT produces better DEM results in terms of\naccuracy and details, if the radiometric variations between the images,\ntaken on subsequent satellite revolutions, are small. Otherwise, the\nASTER stereo pair is a better choice because of simultaneous\nalong-track acquisition during a single pass. Compared to the USGS\n7.5-min DEM, the ASTER and the SPOT extracted DEMs have a standard\ndeviation of 11.6 and 4.6 m, respectively\n', ['automated digital elevation model extraction', 'along-track ASTER data', 'across-track SPOT stereo images', 'stereo satellite images', 'ASTER stereo pair', 'SPOT stereo image pair', 'radiometric variations', 'simultaneous along-track acquisition', 'geophysical signal processing', 'height measurement', 'image resolution', 'stereo image processing', 'terrain mapping']), ('Yet some more complexity results for default logic\nWe identify several new tractable subsets and several new intractable simple\ncases for reasoning in the propositional version of Reiter\'s default\nlogic. The majority of our findings are related to brave reasoning. By\nmaking some intuitive observations, most classes that we identify can\nbe derived quite easily from some subsets of default logic already\nknown in the literature. Some of the subsets we discuss are subclasses\nof the so-called "extended logic programs". All the tractable subsets\npresented in this paper can be recognized in linear time\n', ['reasoning', 'default logic', 'complexity results', 'complexity classes', 'nonmonotonic reasoning', 'extended logic programs', 'tractable subsets', 'computational complexity', 'formal logic', 'nonmonotonic reasoning']), ('A unifying co-operative Web caching architecture\nNetwork caching of objects has become a standard way of reducing network\ntraffic and latency in the Web. However, Web caches exhibit poor\nperformance with a hit rate of about 30%. A solution to improve this\nhit rate is to have a group of proxies form co-operation where objects\ncan be cached for later retrieval. A cooperative cache system includes\nprotocols for hierarchical and transversal caching. The drawback of\nsuch a system lies in the resulting network load due to the number of\nmessages that need to be exchanged to locate an object. This paper\nproposes a new co-operative Web caching architecture, which unifies\nprevious methods of Web caching. Performance results shows that the\narchitecture achieve up to 70% co-operative hit rate and accesses the\ncached object in at most two hops. Moreover, the architecture is\nscalable with low traffic and database overhead\n', ['co-operative Web caching architecture', 'network caching', 'network traffic reduction', 'network latency reduction', 'co-operative hit rate', 'cooperative cache system', 'protocols', 'hierarchical caching', 'transversal caching', 'network load', 'scalable architecture', 'low traffic overhead', 'low database overhead', 'Web browser', 'World Wide Web', 'cache storage', 'file servers', 'Internet', 'memory protocols', 'online front-ends', 'telecommunication traffic']), ('A fractional-flow model of serial manufacturing systems with rework and its\nreachability and controllability properties\nA dynamic fractional-flow model of a serial manufacturing system incorporating\nrework is considered. Using some results on reachability and\ncontrollability of positive linear systems the ability of serial\nmanufacturing systems with rework to "move in space", that is their\nreachability and controllability properties, are studied. These\nproperties are important not only for optimising the performance of the\nmanufacturing system, possibly off-line, but also to improve its\nfunctioning by using feedback control online\n', ['serial manufacturing systems', 'rework', 'reachability', 'controllability', 'dynamic fractional-flow model', 'positive linear systems', 'performance optimisation', 'feedback control', 'controllability', 'discrete time systems', 'graph theory', 'inspection', 'linear systems', 'Markov processes', 'matrix algebra', 'probability', 'production control', 'reachability analysis']), ('Nonlinear adaptive control via sliding-mode state and perturbation observer\nThe paper presents a nonlinear adaptive controller (NAC) for single-input\nsingle-output feedback linearisable nonlinear systems. A sliding-mode\nstate and perturbation observer is designed to estimate the system\nstates and perturbation which includes the combined effect of system\nnonlinearities, uncertainties and external disturbances. The NAC design\ndoes not require the details of the nonlinear system model and full\nsystem states. It possesses an adaptation capability to deal with\nsystem parameter uncertainties, unmodelled system dynamics and external\ndisturbances. The convergence of the observer and the stability\nanalysis of the controller/observer system are given. The proposed\ncontrol scheme is applied for control of a synchronous generator, in\ncomparison with a state-feedback linearising controller (FLC).\nSimulation study is carried out based on a single-generator\ninfinite-bus power system to show the performance of the\ncontroller/observer system\n', ['nonlinear adaptive control', 'sliding-mode state observer', 'perturbation observer', 'NAC', 'SISO feedback linearisable nonlinear systems', 'parameter uncertainties', 'unmodelled system dynamics', 'external disturbances', 'convergence', 'synchronous generator control', 'state-feedback linearising controller', 'FLC', 'single-generator infinite-bus power system', 'adaptive control', 'control system analysis', 'convergence', 'feedback', 'linearisation techniques', 'nonlinear control systems', 'observers', 'perturbation techniques', 'stability', 'variable structure systems']), ("Trust in online advice\nMany people are now influenced by the information and advice they find on the\nInternet, much of it of dubious quality. This article describes two\nstudies concerned with those factors capable of influencing people's\nresponse to online advice. The first study is a qualitative account of\na group of house-hunters attempting to find worthwhile information\nonline. The second study describes a survey of more than 2,500 people\nwho had actively sought advice over the Internet. A framework for\nunderstanding trust in online advice is proposed in which first\nimpressions are distinguished from more detailed evaluations. Good Web\ndesign can influence the first process, but three key factors-source\ncredibility, personalization, and predictability-are shown to predict\nwhether people actually follow the advice given\n", ['online advice trust', 'Internet', 'survey', 'online mortgage advice', 'Web design', 'source credibility', 'personalization', 'predictability', 'e-commerce', 'house buying advice', 'financial data processing', 'human factors', 'information resources', 'Internet', 'psychology', 'real estate data processing', 'social aspects of automation']), ('Hierarchical neuro-fuzzy quadtree models\nHybrid neuro-fuzzy systems have been in evidence during the past few years, due\nto its attractive combination of the learning capacity of artificial\nneural networks with the interpretability of the fuzzy systems. This\narticle proposes a new hybrid neuro-fuzzy model, named hierarchical\nneuro-fuzzy quadtree (HNFQ), which is based on a recursive partitioning\nmethod of the input space named quadtree. The article describes the\narchitecture of this new model, presenting its basic cell and its\nlearning algorithm. The HNFQ system is evaluated in three well known\nbenchmark applications: the sinc(x, y) function approximation, the\nMackey Glass chaotic series forecast and the two spirals problem. When\ncompared to other neuro-fuzzy systems, the HNFQ exhibits competing\nresults, with two major advantages it automatically creates its own\nstructure and it is not limited to few input variables\n', ['neuro-fuzzy systems', 'fuzzy systems', 'hierarchical neuro-fuzzy quadtree', 'quadtree', 'recursive partitioning', 'learning algorithm', 'Mackey Glass chaotic series', 'fuzzy neural nets', 'learning (artificial intelligence)', 'quadtrees']), ("Using the Small Business Innovation Research Program to turn your ideas into\nproducts\nThe US Government's Small Business Innovation Research Program helps small\nbusinesses transform new ideas into commercial products. The program\nprovides an ideal means for businesses and universities to obtaining\nfunding for cooperative projects. Rules and information for the program\nare readily available, and I will give a few helpful hints to provide\nguidance\n", ['Small Business Innovation Research Program', 'commercial product development', 'businesses', 'universities', 'funding', 'cooperative projects', 'US Government', 'USA', 'government policies', 'product development', 'research initiatives']), ("Friedberg numberings of families of n-computably enumerable sets\nWe establish a number of results on numberings, in particular, on Friedberg\nnumberings, of families of d.c.e. sets. First, it is proved that there\nexists a Friedberg numbering of the family of all d.c.e. sets. We also\nshow that this result, patterned on Friedberg's famous theorem for the\nfamily of all c.e. sets, holds for the family of all n-c.e. sets for\nany n > 2. Second, it is stated that there exists an infinite family\nof d.c.e. sets without a Friedberg numbering. Third, it is shown that\nthere exists an infinite family of c.e. sets (treated as a family of\nd.c.e. sets) with a numbering which is unique up to equivalence.\nFourth, it is proved that there exists a family of d.c.e. sets with a\nleast numbering (under reducibility) which is Friedberg but is not the\nonly numbering (modulo reducibility)\n", ['Friedberg numberings', 'infinite family', 'computability theory', 'families of n-computably enumerable sets', 'computability', 'number theory']), ('The vibration reliability of poppet and contoured actuator valves\nThe problem of selecting the shape of the actuator valve (the final control\nvalve) itself is discussed; the solution to this problem will permit\nappreciable dynamic loads to be eliminated from the moving elements of\nthe steam distribution system of steam turbines under all operating\nconditions\n', ['actuator valve shape selection', 'contoured actuator valves', 'poppet actuator valves', 'dynamic loads elimination', 'moving elements', 'steam distribution system', 'steam turbines', 'vibration reliability', 'actuators', 'reliability', 'steam turbines', 'valves', 'vibrations']), ("MATLAB code for plotting ambiguity functions\nA MATLAB code capable of plotting ambiguity functions of many different radar\nsignals is presented. The program makes use of MATLAB's sparse matrix\noperations, and avoids loops. The program could be useful as a\npedagogical tool in radar courses teaching pulse compression\n", ['MATLAB code', 'ambiguity functions plotting', 'radar signals', 'sparse matrix operations', 'pedagogical tool', 'radar courses', 'pulse compression', 'matched-filter response', 'Doppler-shifted signal version', 'functions', 'pulse compression', 'radar computing', 'radar signal processing', 'sparse matrices']), ('On abelian branched coverings of the sphere\nWe obtain an enumeration formula for the number of weak equivalence classes of\nthe branched (A * B)-covering of the sphere with m-branch points, when\nA and B are finite abelian groups with (|A|, |B|) = 1. From this, we\ncan deduce an explicit formula for enumerating the weak equivalence\nclasses of pseudofree spherical (Zp * Zq)-actions on a given surface,\nwhen p and q are distinct primes\n', ['Abelian branched coverings', 'enumeration formula', 'weak equivalence classes', 'finite abelian groups', 'explicit formula', 'pseudofree spherical', 'equivalence classes', 'graph theory']), ("The visible cement data set\nWith advances in x-ray microtomography, it is now possible to obtain\nthree-dimensional representations of a material's microstructure with a\nvoxel size of less than one micrometer. The Visible Cement Data Set\nrepresents a collection of 3-D data sets obtained using the European\nSynchrotron Radiation Facility in Grenoble, France in September 2000.\nMost of the images obtained are for hydrating portland cement pastes,\nwith a few data sets representing hydrating Plaster of Paris and a\ncommon building brick. All of these data sets are being made available\non the Visible Cement Data Set website at\nhttp://visiblecement.nist.gov. The website includes the raw 3-D\ndatafiles, a description of the material imaged for each data set,\nexample two-dimensional images and visualizations for each data set,\nand a collection of C language computer programs that will be of use in\nprocessing and analyzing the 3-D microstructural images. This paper\nprovides the details of the experiments performed at the ESRF, the\nanalysis procedures utilized in obtaining the data set files, and a few\nrepresentative example images for each of the three materials\ninvestigated\n", ['X-ray microtomography', '3D representations', 'microstructure', 'voxel size', 'European Synchrotron Radiation Facility', 'hydrating portland cement pastes', 'Plaster of Paris', 'building brick', 'cement hydration', 'two-dimensional images', 'microstructural images', 'ESRF', 'cements (building materials)', 'crystal microstructure', 'image processing', 'Internet', 'measurement standards', 'nondestructive testing', 'synchrotron radiation', 'X-ray imaging', 'X-ray topography']), ('Connecting the business without busting the budget\nThe "multi-channel content delivery" model (MCCD) might be a new concept to\nyou, but it is already beginning to replace traditional methods of\nbusiness communications, print and content delivery, argues Darren\nAtkinson, CTO, FormScape\n', ['multi-channel content delivery', 'FormScape', 'documents', 'distributed output management', 'business process management', 'archive', 'retrieval', 'content management', 'document handling', 'management information systems']), ('Approximate confidence intervals for one proportion and difference of two\nproportions\nConstructing a confidence interval for a binomial proportion or the difference\nof two proportions is a routine exercise in daily data analysis. The\nbest-known method is the Wald interval based on the asymptotic normal\napproximation to the distribution of the observed sample proportion,\nthough it is known to have bad performance for small to medium sample\nsizes. Agresti et al. (1998, 2000) proposed an Adding-4 method: 4\npseudo-observations are added with 2 successes and 2 failures and then\nthe resulting (pseudo-)sample proportion is used. The method is simple\nand performs extremely well. Here we propose an approximate method\nbased on a t-approximation that takes account of the uncertainty in\nestimating the variance of the observed (pseudo-)sample proportion. It\nfollows the same line of using a t-test, rather than z-test, in testing\nthe mean of a normal distribution with an unknown variance. For some\ncircumstances our proposed method has a higher coverage probability\nthan the Adding-4 method\n', ['approximate confidence intervals', 'binomial proportion', 'difference of two proportions', 'data analysis', 't-approximation', 'uncertainty', 'variance estimation', 't-test', 'normal distribution', 'coverage probability', 'pseudo-sample proportion', 'approximation theory', 'binomial distribution', 'data analysis', 'normal distribution', 'sampling methods']), ("Simple nonlinear dual-window operator for edge detection\nWe propose a nonlinear edge detection technique based on a\ntwo-concentric-circular-window operator. We perform a preliminary\nselection of edge candidates using a standard gradient and use the\ndual-window operator to reveal edges as zero-crossing points of a\nsimple difference function depending only on the minimum and maximum\nvalues in the two windows. Comparisons with other well-established\ntechniques are reported in terms of visual appearance and computational\nefficiency. They show that detected edges are surely comparable with\nCanny's and Laplacian of Gaussian algorithms, with a noteworthy\nreduction in terms of computational load\n", ['nonlinear dual-window operator', 'edge detection', 'nonlinear edge detection technique', 'two-concentric-circular-window operator', 'standard gradient', 'dual window operator', 'zero-crossing points', 'difference function', 'minimum values', 'maximum values', 'computational efficiency', 'detected edges', 'Laplacian algorithms', 'Gaussian algorithms', "Canny's algorithms", 'computational load', 'nonlinear processing', 'edge detection', 'Laplace equations', 'nonlinear optics']), ('Evolution complexity of the elementary cellular automaton rule 18\nCellular automata are classes of mathematical systems characterized by\ndiscreteness (in space, time, and state values), determinism, and local\ninteraction. Using symbolic dynamical theory, we coarse-grain the\ntemporal evolution orbits of cellular automata. By means of formal\nlanguages and automata theory, we study the evolution complexity of the\nelementary cellular automaton with local rule number 18 and prove that\nits width 1-evolution language is regular, but for every n >or= 2\nits width n-evolution language is not context free but context\nsensitive\n', ['cellular automata', 'symbolic dynamical theory', 'formal languages', 'complexity', 'evolution complexity', 'elementary cellular automaton', 'cellular automata', 'computational complexity', 'evolutionary computation']), ("Mustering motivation to enact decisions: how decision process characteristics\ninfluence goal realization\nDecision scientists tend to focus mainly on decision antecedents, studying how\npeople make decisions. Action psychologists, in contrast, study\npost-decision issues, investigating how decisions, once formed, are\nmaintained, protected, and enacted. Through the research presented\nhere, we seek to bridge these two disciplines, proposing that the\nprocess by which decisions are reached motivates subsequent pursuit and\nbenefits eventual realization. We identify three characteristics of the\ndecision process (DP) as having motivation-mustering potential: DP\neffort investment, DP importance, and DP confidence. Through two field\nstudies tracking participants' decision processes, pursuit and\nrealization, we find that after controlling for the influence of the\nmotivational mechanisms of goal intention and implementation intention,\nthe three decision process characteristics significantly influence the\nsuccessful enactment of the chosen decision directly. The theoretical\nand practical implications of these findings are considered and future\nresearch opportunities are identified\n", ['decision enactment', 'motivation', 'goal realization', 'decision process characteristics', 'action psychologists', 'post-decision issues', 'motivation-mustering potential', 'decision process investment', 'decision process importance', 'decision process confidence', 'goal intention', 'research opportunities', 'decision scientists', 'behavioural sciences', 'decision theory', 'human resource management']), ('The canonical dual frame of a wavelet frame\nWe show that there exist wavelet frames that have nice dual wavelet frames, but\nfor which the canonical dual frame does not consist of wavelets, i.e.,\ncannot be generated by the translates and dilates of a single function\n', ['canonical dual frame', 'wavelet frame', 'Gabor frames', 'multiresolution hierarchy', 'compact support', 'wavelet transforms']), ('Process planning for reliable high-speed machining of moulds\nA method of generating NC programs for the high-speed milling of moulds is\ninvestigated. Forging dies and injection moulds, whether plastic or\naluminium, have a complex surface geometry. In addition they are made\nof steels of hardness as much as 30 or even 50 HRC. Since 1995,\nhigh-speed machining has been much adopted by the die-making industry,\nwhich with this technology can reduce its use of Sinking\nElectrodischarge Machining (SEDM). EDM, in general, calls for longer\nmachining times. The use of high-speed machining makes it necessary to\nredefine the preliminary stages of the process. In addition, it affects\nthe methodology employed in the generation of NC programs, which\nrequires the use of high-level CAM software. The aim is to generate\nerror-free programs that make use of optimum cutting strategies in the\ninterest of productivity and surface quality. The final result is a\nmore reliable manufacturing process. There are two risks in the use of\nhigh-speed milling on hardened steels. One of these is tool breakage,\nwhich may be very costly and may furthermore entail marks on the\nworkpiece. The other is collisions between the tool and the workpiece\nor fixtures, the result of which may be damage to the ceramic bearings\nin the spindles. in order to minimize these risks it is necessary that\nnew control and optimization steps be included in the CAM methodology.\nThere are three things that the firm adopting high-speed methods should\ndo. It should redefine its process engineering, it should systematize\naccess by its CAM programmers to high-speed knowhow, and it should take\nup the use of process simulation tools. In the latter case, it will be\nvery advantageous to use tools for the estimation of cutting forces.\nThe new work methods proposed in this article have made it possible to\nintroduce high speed milling (HSM) into the die industry. Examples are\ngiven of how the technique has been applied with CAM programming\nre-engineered as here proposed, with an explanation of the novel\nfeatures and the results\n', ['moulds', 'reliable high-speed machining', 'process planning', 'NC programs', 'high-speed milling', 'forging dies', 'injection moulds', 'complex surface geometry', 'error-free programs', 'optimum cutting strategies', 'productivity', 'surface quality', 'hardened steels', 'tool breakage', 'tool workpiece collisions', 'ceramic bearings', 'CAM methodology', 'process engineering redefinition', 'process simulation tools', 'CAM programming re-engineering', 'cutting strategies', 'CAD/CAM', 'computer aided production planning', 'computerised numerical control', 'cutting', 'forging', 'machining', 'moulding', 'systems re-engineering']), ('Estimation of the Poisson stream intensity in a multilinear queue with an\nexponential job queue decay\nTimes the busy queue periods start are found for a multilinear queue with an\nexponential job queue decay and uniform resource allocation to\nindividual servers. The stream intensity and the average job are\nestimated from observations of the times the queue busy periods start\n', ['Poisson stream intensity', 'multilinear queue', 'exponential job queue decay', 'busy queue periods start', 'uniform resource allocation', 'stream intensity', 'individual servers', 'queueing theory', 'resource allocation', 'stochastic processes']), ('Evolution of litigation support systems\nFor original paper see ibid., vol. 12, no. 6: "The E-mail of the Species". The\nauthor responds to that paper and argues that printing, scanning and\nimaging E-mails or other electronic (rather than paper) documents prior\nto listing and disclosure seems to be unnecessary, not \'proportionate\'\n(from a costs point of view) and not particularly helpful, to either\nside. He asks how litigation support systems might evolve to help and\nsupport the legal team in their task\n', ['litigation support systems', 'E-mail', 'legal team', 'electronic mail', 'law administration']), ('Extracting straight road structure in urban environments using IKONOS satellite\nimagery\nWe discuss a fully automatic technique for extracting roads in urban\nenvironments. The method has its bases in a vegetation mask derived\nfrom multispectral IKONOS data and in texture derived from panchromatic\nIKONOS data. These two techniques together are used to distinguish road\npixels. We then move from individual pixels to an object-based\nrepresentation that allows reasoning on a higher level. Recognition of\nindividual segments and intersections and the relationships among them\nare used to determine underlying road structure and to then logically\nhypothesize the existence of additional road network components. We\nshow results on an image of San Diego, California. The object-based\nprocessing component may be adapted to utilize other basis techniques\nas well, and could be used to build a road network in any scene having\na straight-line structured topology\n', ['straight road structure', 'urban environments', 'IKONOS satellite imagery', 'fully automatic technique', 'vegetation mask', 'texture', 'panchromatic IKONOS data', 'road pixels', 'object-based representation', 'higher level reasoning', 'individual segment recognition', 'road network components', 'San Diego', 'object-based processing component', 'straight-line structured topology', 'high-resolution imagery', 'large-scale feature extraction', 'vectorized road network', 'civil engineering computing', 'feature extraction', 'image classification', 'image representation', 'image texture', 'remote sensing']), ('Transcripts: bane or boon? [law reporting]\nBecause judge-made law, by its very nature, is less immediately accessible than\nthe law of codified, statutory systems, it calls for an efficient\nsystem of law reporting. Of necessity, any such system will be\nselective, the majority of decisions going unreported. Considerable\npower thereby comes to repose in the hands of the law reporters. The\nauthor shares his invaluable perception and extensive research on the\ndifficulties which arise from the excess of access to judgments\n', ['transcripts', 'law reporting', 'judge-made law', 'judgments', 'information resources', 'law administration']), ('Identification of linear parameter varying models\nWe consider identification of a certain class of discrete-time nonlinear\nsystems known as linear parameter varying system. We assume that\ninputs, outputs and the scheduling parameters are directly measured,\nand a form of the functional dependence of the system coefficients on\nthe parameters is known. We show how this identification problem can be\nreduced to a linear regression, and provide compact formulae for the\ncorresponding least mean square and recursive least-squares algorithms.\nWe derive conditions on persistency of excitation in terms of the\ninputs and scheduling parameter trajectories when the functional\ndependence is of polynomial type. These conditions have a natural\npolynomial interpolation interpretation, and do not require the\nscheduling parameter trajectories to vary slowly. This method is\nillustrated with a simulation example using two different parameter\ntrajectories\n', ['linear parameter varying models', 'identification', 'discrete-time nonlinear systems', 'scheduling parameters', 'functional dependence', 'system coefficients', 'linear regression', 'least mean square algorithms', 'recursive least-squares algorithms', 'persistency of excitation conditions', 'scheduling parameter trajectories', 'polynomial interpolation interpretation', 'parameter trajectories', 'time-varying systems', 'discrete time systems', 'least mean squares methods', 'least squares approximations', 'nonlinear control systems', 'parameter estimation', 'polynomial approximation', 'recursive estimation', 'statistical analysis', 'time-varying systems']), ('Airline base schedule optimisation by flight network annealing\nA system for rigorous airline base schedule optimisation is described. The\narchitecture of the system reflects the underlying problem structure.\nThe architecture is hierarchical consisting of a master problem for\nlogical aircraft schedule optimisation and a sub-problem for schedule\nevaluation. The sub-problem is made up of a number of component\nsub-problems including connection generation, passenger choice\nmodelling, passenger traffic allocation by simulation and revenue and\ncost determination. Schedule optimisation is carried out by means of\nsimulated annealing of flight networks. The operators for the simulated\nannealing process are feasibility preserving and form a complete set of\noperators\n', ['airline base schedule optimisation', 'flight network annealing', 'system architecture', 'hierarchical architecture', 'master problem', 'logical aircraft schedule optimisation', 'schedule evaluation', 'connection generation', 'passenger choice modelling', 'passenger traffic allocation', 'cost determination', 'simulated annealing', 'operators', 'time complexity', 'air traffic', 'mathematical operators', 'scheduling', 'simulated annealing', 'travel industry']), ('The ultimate control group\nEmpirical research on the organization of firms requires that firms be\nclassified on the basis of their control structures. This should be\ndone in a way that can potentially be made operational. It is easy to\nidentify the ultimate controller of a hierarchical organization, and\nthe literature has largely focused on this case. However, many\norganizational structures mix hierarchy with collective choice\nprocedures such as voting, or use circular structures under which\nsuperiors are accountable to their subordinates. The author develops\nsome analytic machinery that can be used to map the authority\nstructures of such organizations, and show that under mild restrictions\nthere is a well-defined ultimate control group. The results are\nconsistent with intuitions about the nature of control in familiar\neconomic settings\n', ['ultimate control group', 'hierarchical organization', 'organizational structures', 'authority structures', 'committees', 'control rights', 'firm organization', 'management', 'management science', 'set theory']), ('Ethnography, customers, and negotiated interactions at the airport\nIn the late 1990s, tightly coordinated airline schedules unraveled owing to\nmassive delays resulting from inclement weather, overbooked flights,\nand airline operational difficulties. As schedules slipped, the delayed\ndepartures and late arrivals led to systemwide breakdowns, customers\nmissed their connections, and airline work activities fell further out\nof sync. In offering possible answers, we emphasize the need to\nconsider the customer as participant, following the human-centered\ncomputing model. Our study applied ethnographic methods to understand\nthe airline system domain and the nature of airline delays, and it\nrevealed the deficiencies of the airline production system model of\noperations. The research insights that led us to shift from a\nproduction and marketing system perspective to a\ncustomer-as-participant view might appear obvious to some readers.\nHowever, we do not know of any airline that designs its operations and\ntechnologies around any other model than the production and marketing\nsystem view. Our human-centered analysis used ethnographic methods to\ngather information, offering new insight into airline delays and\nsuggesting effective ways to improve operations reliability\n', ['human-centered computing model', 'customer trajectories', 'airports', 'employees', 'ethnography', 'negotiated interactions', 'airline delays', 'airline production system operations model', 'customer-as-participant view', 'operations reliability', 'airports', 'marketing data processing', 'negotiation support systems', 'personnel', 'reliability', 'travel industry', 'user centred design']), ("Ride quality evaluation of an actively-controlled stretcher for an ambulance\nThis study considers the subjective evaluation of ride quality during ambulance\ntransportation using an actively-controlled stretcher (ACS). The ride\nquality of a conventional stretcher and an assistant driver's seat is\nalso compared. Braking during ambulance transportation generates\nnegative foot-to-head acceleration in patients and causes blood\npressure to rise in the patient's head. The ACS absorbs the\nfoot-to-head acceleration by changing the angle of the stretcher, thus\nreducing the blood pressure variation. However, the ride quality of the\nACS should be investigated further because the movement of the ACS may\ncause motion sickness and nausea. Experiments of ambulance\ntransportation, including rapid acceleration and deceleration, are\nperformed to evaluate the effect of differences in posture of the\ntransported subject on the ride quality; the semantic differential\nmethod and factor analysis are used in the investigations. Subjects are\ntransported using a conventional stretcher with head forward, a\nconventional stretcher with head backward, the ACS, and an assistant\ndriver's seat for comparison with transportation using a stretcher.\nExperimental results show that the ACS gives the most comfortable\ntransportation when using a stretcher. Moreover, the reduction of the\nnegative foot-to-head acceleration at frequencies below 0.2 Hz and the\nsmall variation of the foot-to-head acceleration result in more\ncomfortable transportation. Conventional transportation with the head\nforward causes the worst transportation, although the characteristics\nof the vibration of the conventional stretcher seem to be superior to\nthat of the ACS\n", ['actively-controlled stretcher', 'ambulance', 'ride quality evaluation', 'subjective evaluation', 'ambulance transportation', 'conventional stretcher', 'assistant driver seat', 'braking', 'negative foot-to-head acceleration', 'blood pressure variation', 'patient head', 'stretcher angle', 'motion sickness', 'nausea', 'rapid acceleration', 'rapid deceleration', 'posture differences', 'transported subject', 'semantic differential method', 'factor analysis', 'head forward', 'head backward', 'comfortable transportation', 'vibration', 'braking', 'haemodynamics', 'patient care', 'position control', 'road vehicles']), ('Repeated games with lack of information on one side: the dual differential\napproach\nWe introduce the dual differential game of a repeated game with lack of\ninformation on one side as the natural continuous time version of the\ndual game introduced by De Meyer (1996). A traditional way to study the\nvalue of differential games is through discrete time approximations.\nHere, we follow the opposite approach: We identify the limit value of a\nrepeated game in discrete time as the value of a differential game.\nNamely, we use the recursive structure for the finitely repeated\nversion of the dual game to construct a differential game for which the\nupper values of the uniform discretization satisfy precisely the same\nproperty. The value of the dual differential game exists and is the\nunique viscosity solution of a first-order derivative equation with a\nlimit condition. We identify the solution by translating viscosity\nproperties in the primal\n', ['repeated games', 'dual differential game', 'repeated game', 'discrete time approximations', 'limit value', 'discrete time', 'viscosity solution', 'limit condition', 'differential games', 'duality (mathematics)']), ('Edison\'s direct current influenced "Broadway" show lighting\nDuring the early decades of the 20 th century, midtown Manhattan in New York\nCity developed an extensive underground direct current (DC) power\ndistribution system. This was a result of the original introduction of\ndirect current by Thomas Edison\'s pioneering Pearl Street Station in\n1882. The availability of DC power in the theater district, led to the\nperpetuation of an archaic form of stage lighting control through\nnearly three-quarters of the 20 th century. This control device was\nknown as a "resistance dimmer." It was essentially a series-connected\nrheostat, but it was wound with a special resistance "taper" so as to\nprovide a uniform change in the apparent light output of typical\nincandescent lamps throughout the travel of its manually operated arm.\nThe development and use of DC powered stage lighting is discussed in\nthis article\n', ['Broadway show lighting', 'Manhattan', 'New York City', 'underground direct current power distribution system', "Thomas Edison's Pearl Street Station", 'theater district', 'stage lighting control', 'resistance dimmer', 'series-connected rheostat', 'resistance taper', 'apparent light output', 'incandescent lamps', 'DC powered stage lighting', 'DC power transmission', 'history', 'lighting control']), ("Web services boost integration\nMicrosoft and IBM have announced products to help their database software\nco-exist with competitors' offerings. The products use web services\ntechnology allowing users to improve integration between databases and\napplication software from rival vendors\n", ['web services technology', 'Microsoft', 'IBM', 'database software', 'database management systems', 'information resources']), ('Limitations of delayed state feedback: a numerical study\nStabilization of a class of linear time-delay systems can be achieved by a\nnumerical procedure, called the continuous pole placement method\n[Michiels et al., 2000]. This method can be seen as an extension of the\nclassical pole placement algorithm for ordinary differential equations\nto a class of delay differential equations. In [Michiels et al., 2000]\nit was applied to the stabilization of a linear time-invariant system\nwith an input delay using static state feedback. In this paper we study\nthe limitations of such delayed state feedback laws. More precisely we\ncompletely characterize the class of stabilizable plants in the\n2D-case. For that purpose we make use of numerical continuation\ntechniques. The use of delayed state feedback in various control\napplications and the effect of its limitations are briefly discussed\n', ['linear time-delay systems', 'continuous pole placement method', 'delay differential equations', 'static state feedback', 'delayed state feedback', 'numerical continuation', 'delay-differential systems', 'differential equations', 'iterative methods', 'state feedback']), ('Dynamics of the firing probability of noisy integrate-and-fire neurons\nCortical neurons in vivo undergo a continuous bombardment due to synaptic\nactivity, which acts as a major source of noise. We investigate the\neffects of the noise filtering by synapses with various levels of\nrealism on integrate-and-fire neuron dynamics. The noise input is\nmodeled by white (for instantaneous synapses) or colored (for synapses\nwith a finite relaxation time) noise. Analytical results for the\nmodulation of firing probability in response to an oscillatory input\ncurrent are obtained by expanding a Fokker-Planck equation for small\nparameters of the problem-when both the amplitude of the modulation is\nsmall compared to the background firing rate and the synaptic time\nconstant is small compared to the membrane time constant. We report the\ndetailed calculations showing that if a synaptic decay time constant is\nincluded in the synaptic current model, the firing-rate modulation of\nthe neuron due to an oscillatory input remains finite in the\nhigh-frequency limit with no phase lag. In addition, we characterize\nthe low-frequency behavior and the behavior of the high-frequency limit\nfor intermediate decay times. We also characterize the effects of\nintroducing a rise time to the synaptic currents and the presence of\nseveral synaptic receptors with different kinetics. In both cases, we\ndetermine, using numerical simulations, an effective decay time\nconstant that describes the neuronal response completely\n', ['firing probability', 'noisy integrate-and-fire neurons', 'cortical neurons', 'synaptic activity', 'noise filtering', 'white noise', 'colored noise', 'Fokker-Planck equation', 'synaptic time constant', 'membrane time constant', 'phase lag', 'synaptic receptors', 'numerical simulation', 'bioelectric potentials', 'Fokker-Planck equation', 'neural nets', 'neurophysiology', 'noise', 'probability']), ('MPEG-4 video object-based rate allocation with variable temporal rates\nIn object-based coding, bit allocation is performed at the object level and\ntemporal rates of different objects may vary. The proposed algorithm\ndeals with these two issues when coding multiple video objects (MVOs).\nThe proposed algorithm is able to successfully achieve the target bit\nrate, effectively code arbitrarily shaped MVOs with different temporal\nrates, and maintain a stable buffer level\n', ['MPEG-4 video coding', 'bit allocation', 'multiple video objects', 'rate-distortion encoding', 'object-based rate allocation', 'variable temporal rates', 'rate distortion theory', 'video coding']), ('The art of the cross-sell [accounting software]\nWith the market for accounting software nearing saturation, vendors are\ntraining resellers in the subtleties of the cross-sell. The rewards can\nbe great. The key is knowing when to focus, and when to partner\n', ['accounting software', 'resellers', 'cross-selling', 'accounting']), ('Training multilayer perceptrons via minimization of sum of ridge functions\nMotivated by the problem of training multilayer perceptrons in neural networks,\nwe consider the problem of minimizing E(x)= Sigma /sub i=1//sup n/\nf/sub i/( xi /sub i/.x), where xi /sub i/ in R/sup S/,\n1<or=i<or=n, and each f/sub i/( xi /sub i/.x) is a ridge\nfunction. We show that when n is small the problem of minimizing E can\nbe treated as one of minimizing univariate functions, and we use the\ngradient algorithms for minimizing E when n is moderately large. For a\nlarge n, we present the online gradient algorithms and especially show\nthe monotonicity and weak convergence of the algorithms\n', ['multilayer perceptrons', 'online gradient algorithms', 'ridge functions', 'minimization', 'neural networks', 'gradient algorithms', 'univariate functions', 'monotonicity', 'weak convergence', 'convergence', 'gradient methods', 'learning (artificial intelligence)', 'minimisation', 'multilayer perceptrons']), ('Development of railway VR safety simulation system\nAbnormal conditions occur in railway transportation due to trouble or accidents\nand it affects a number of passengers. It is very important, therefore,\nto quickly recover and return to normal train operation. For this\npurpose we developed a system, "Computer VR Simulation System for the\nSafety of Railway Transportation." It is a new type simulation system\nto evaluate measures to be taken under abnormal conditions. Users of\nthis simulation system cooperate with one another to correct the\nabnormal conditions that have occurred in virtual reality. This paper\nreports the newly developed simulation system\n', ['railway transportation', 'accidents', 'normal train operation', 'Computer VR Simulation System', 'virtual reality simulation system', 'abnormal conditions correction', 'accidents', 'digital simulation', 'railways', 'traffic engineering computing', 'virtual reality']), ('Scheduling schemes for an integrated flight and propulsion control system\nWe describe two schemes for scheduling an integrated flight and propulsion\ncontrol system for an experimental vertical/short take-off and landing\n(V/STOL) aircraft concept in the acceleration from hover (0-120 kn)\nflight phase. Multivariable integrated flight and propulsion\ncontrollers are designed at several points over the V/STOL envelope and\nimplemented as exact plant observers with state feedback. In the first\nscheduling scheme, the values of the state feedback and observer gain\nmatrices are interpolated between the fixed-point designs as a function\nof aircraft speed. In the second approach, the control signals produced\nby the different fixed-point controllers are blended, allowing a\nsignificant reduction in the order of the scheduled controllers. Both\nscheduling schemes are shown in nonlinear simulation to provide\nexcellent handling qualities as the aircraft accelerates from the hover\n', ['vertical short take-off landing aircraft', 'VSTOL aircraft', 'propulsion control', 'flight control', 'scheduling', 'multivariable control systems', 'observers', 'state feedback', 'fixed-point controllers', 'aerospace propulsion', 'aircraft control', 'multivariable control systems', 'observers', 'scheduling', 'state feedback']), ('Generic simulation approach for multi-axis machining. Part 1: modeling\nmethodology\nThis paper presents a new methodology for analytically simulating multi-axis\nmachining of complex sculptured surfaces. A generalized approach is\ndeveloped for representing an arbitrary cutting edge design, and the\nlocal surface topology of a complex sculptured surface. A NURBS curve\nis used to represent the cutting edge profile. This approach offers the\nadvantages of representing any arbitrary cutting edge design in a\ngeneric way, as well as providing standardized techniques for\nmanipulating the location and orientation of the cutting edge. The\nlocal surface topology of the part is defined as those surfaces\ngenerated by previous tool paths in the vicinity of the current tool\nposition. The local surface topology of the part is represented without\nusing a computationally expensive CAD system. A systematic prediction\ntechnique is then developed to determine the instantaneous tool/part\ninteraction during machining. The methodology employed here determines\nthe cutting edge in-cut segments by determining the intersection\nbetween the NURBS curve representation of the cutting edge and the\ndefined local surface topology. These in-cut segments are then utilized\nfor predicting instantaneous chip load, static and dynamic cutting\nforces, and tool deflection. Part 1 of this paper details the modeling\nmethodology and demonstrates the capabilities of the simulation for\nmachining a complex surface\n', ['multiple axis machining', 'generic modeling', 'tool path specification', 'complex surface machining', 'complex sculptured surfaces', 'systematic prediction', 'cutting edge profile', 'surface topology', 'NURBS curve', 'digital simulation', 'machining', 'path planning', 'production engineering computing']), ('Hypothesis-based concept assignment in software maintenance\nSoftware maintenance accounts for a significant proportion of the lifetime cost\nof a software system. Software comprehension is required in many parts\nof the maintenance process and is one of the most expensive activities.\nMany tools have been developed to help the maintainer reduce the time\nand cost of this task, but of the numerous tools and methods available\none group has received relatively little attention: those using\nplausible reasoning to address the concept assignment problem. We\npresent a concept assignment method for COBOL II: hypothesis-based\nconcept assignment (HB-CA). An implementation of a prototype tool is\ndescribed, and results from a comprehensive evaluation using commercial\nCOBOL II sources are summarised. In particular, we identify areas of a\nstandard maintenance process where such methods would be appropriate,\nand discuss the potential cost savings that may result\n', ['hypothesis-based concept assignment', 'software maintenance', 'lifetime cost', 'COBOL II', 'scalability', 'COBOL', 'software engineering', 'software maintenance']), ('Scalable hybrid computation with spikes\nWe outline a hybrid analog-digital scheme for computing with three important\nfeatures that enable it to scale to systems of large complexity: First,\nlike digital computation, which uses several one-bit precise logical\nunits to collectively compute a precise answer to a computation, the\nhybrid scheme uses several moderate-precision analog units to\ncollectively compute a precise answer to a computation. Second,\nfrequent discrete signal restoration of the analog information prevents\nanalog noise and offset from degrading the computation. Third, a state\nmachine enables complex computations to be created using a sequence of\nelementary computations. A natural choice for implementing this hybrid\nscheme is one based on spikes because spike-count codes are digital,\nwhile spike-time codes are analog. We illustrate how spikes afford easy\nways to implement all three components of scalable hybrid computation.\nFirst, as an important example of distributed analog computation, we\nshow how spikes can create a distributed modular representation of an\nanalog number by implementing digital carry interactions between\nspiking analog neurons. Second, we show how signal restoration may be\nperformed by recursive spike-count quantization of spike-time codes.\nThird, we use spikes from an analog dynamical system to trigger state\ntransitions in a digital dynamical system, which reconfigures the\nanalog dynamical system using a binary control vector; such feedback\ninteractions between analog and digital dynamical systems create a\nhybrid state machine (HSM). The HSM extends and expands the concept of\na digital finite-state-machine to the hybrid domain. We present\nexperimental data from a two-neuron HSM on a chip that implements\nerror-correcting analog-to-digital conversion with the concurrent use\nof spike-time and spike-count codes. We also present experimental data\nfrom silicon circuits that implement HSM-based pattern recognition\nusing spike-time synchrony. We outline how HSMs may be used to perform\nlearning, vector quantization, spike pattern recognition and\ngeneration, and how they may be reconfigured\n', ['scalable hybrid computation', 'spikes', 'hybrid analog-digital scheme', 'moderate-precision analog units', 'frequent discrete signal restoration', 'analog noise', 'spike-count codes', 'finite-state-machine', 'distributed analog computation', 'spike-time codes', 'digital carry interactions', 'binary control vector', 'feedback interactions', 'two neuron hybrid state machine', 'error-correcting analog-to-digital conversion', 'silicon circuits', 'pattern recognition', 'learning', 'vector quantization', 'analogue-digital conversion', 'digital arithmetic', 'error correction', 'finite state machines', 'learning (artificial intelligence)', 'neural chips', 'pattern recognition', 'signal restoration', 'vector quantisation']), ("A Blog in every law firm?\nYou don't know today what you'll want to know next year. Rather than trying to\nsolve that problem, focus on providing simple tools to users that\ncreate valuable content across the firm. Individual contributions will\nbe more visible, and you will have a searchable archive of your\ninstitutional memory and a simplified process for ensuring everyone is\nup to speed. Whether you embrace weblogs for their individual or\ninstitutional benefits, one thing is certain: They will become powerful\ntools for those who seek ways to more efficiently and intelligently\nmanage information\n", ['law firm', 'weblogs', 'institutional memory', 'Web site', 'information resources', 'law administration']), ('Failures and successes: notes on the development of electronic cash\nBetween 1997 and 2001, two mid-sized communities in Canada hosted North\nAmerica\'s most comprehensive experiment to introduce electronic cash\nand, in the process, replace physical cash for casual, low-value\npayments. The technology used was Mondex, and its implementation was\nsupported by all the country\'s major banks. It was launched with an\nextensive publicity campaign to promote Mondex not only in the domestic\nbut also in the global market, for which the Canadian implementation\nwas to serve as a "showcase." However, soon after the start of the\nfirst field test it became apparent that the new technology did not\nwork smoothly. On the contrary, it created a host of controversies, in\nareas as varied as computer security, consumer privacy, and monetary\npolicy. In the following years, few of these controversies could be\nresolved and Mondex could not be established as a widely used payment\nmechanism. In 2001, the experiment was finally terminated. Using the\nconcepts developed in recent science and technology studies (STS), the\narticle analyzes these controversies as resulting from the difficulties\nof fitting electronic cash, a new sociotechnical system, into the\ncomplex setting of the existing payment system. The story of Mondex not\nonly offers lessons on why technologies fail, but also offers insight\ninto how short-term failures can contribute to long-term\ntransformations. This suggests the need to rethink the dichotomy of\nsuccess and failure\n', ['electronic cash', 'Canada', 'low-value payments', 'Mondex', 'major banks', 'publicity campaign', 'global market', 'Canadian implementation', 'computer security', 'consumer privacy', 'monetary policy', 'payment mechanism', 'science and technology studies', 'sociotechnical system', 'short-term failures', 'long-term transformations', 'bibliographies', 'electronic commerce', 'electronic money', 'government policies', 'management of change', 'socio-economic effects']), ('Super high definition image (WHD: Wide/Double HD) transmission system\nThis paper describes a WHD image transmission system constructed from a display\nprojector, CODECs, and a camera system imaging a super high definition\nimage (WHD: Wide/Double HD) corresponding to two screen portions of\ncommon high-vision images. This system was developed as a transmission\nsystem to communicate with or transmit information giving a\nreality-enhanced feeling to a remote location by using images of super\nhigh definition. In addition, the correction processing for the\ndistortions of images occurring due to the structure of the camera\nsystem, an outline of the transmission experiments using the proposed\nsystem, and subjective evaluation experiments using WHD images are\npresented\n', ['super high definition image transmission system', 'WHD image transmission system', 'CODECs', 'camera system imaging', 'reality-enhanced feeling', 'asynchronous transfer mode', 'high definition television', 'image processing']), ('Lossy SPICE models produce realistic averaged simulations\nIn previous averaged models, the state-space averaging technique or switch\nwaveforms analysis were usually applied over perfect elements,\nnon-inclusive of the ohmic losses. However, if these elements play an\nactive role in the DC transfer function, they affect the small-signal\nAC analysis by introducing various damping effects. A model is\nintroduced in a boost voltage-mode application\n', ['lossy SPICE models', 'realistic averaged simulations', 'state-space averaging technique', 'switch waveforms analysis', 'damping effects', 'boost voltage-mode application', 'ohmic losses', 'DC transfer function', 'circuit simulation', 'DC-DC power convertors', 'losses', 'SPICE', 'state-space methods', 'transfer functions', 'waveform analysis']), ('Relevance of Web documents: ghosts consensus method\nThe dominant method currently used to improve the quality of Internet search\nsystems is often called "digital democracy." Such an approach implies\nthe utilization of the majority opinion of Internet users to determine\nthe most relevant documents: for example, citation index usage for\nsorting of search results (google.com) or an enrichment of a query with\nterms that are asked frequently in relation with the query\'s theme.\n"Digital democracy" is an effective instrument in many cases, but it\nhas an unavoidable shortcoming, which is a matter of principle: the\naverage intellectual and cultural level of Internet users is very low;\neveryone knows what kind of information is dominant in Internet query\nstatistics. Therefore, when one searches the Internet by means of\n"digital democracy" systems, one gets answers that reflect an\nunderlying assumption that the user\'s mind potential is very low, and\nthat his cultural interests are not demanding. Thus, it is more correct\nto use the term "digital ochlocracy" to refer to Internet search\nsystems with "digital democracy." Based on the well-known mathematical\nmechanism of linear programming, we propose a method to solve the\nindicated problem\n', ['Internet search systems', 'digital democracy', 'majority opinion', 'citation index usage', 'search results', 'Internet query statistics', 'digital ochlocracy', 'linear programming', 'ghosts consensus method', 'World Wide Web', 'Internet', 'linear programming', 'relevance feedback', 'search engines']), ('Office essentials [stationery suppliers]\nMake purchasing stationery a relatively simple task through effective planning\nand management of stock, and identifying the right supplier\n', ['stationery suppliers', 'purchasing', 'planning', 'management of stock', 'computer stationery', 'paper', 'purchasing', 'stock control']), ('Disability-related special libraries\nOne of the ways that the federal government works to improve services to people\nwith disabilities is to fund disability-related information centers and\nclearinghouses that provide information resources and referrals to\ndisabled individuals, their family members, service providers, and the\ngeneral public. The Teaching Research Division of Western Oregon\nUniversity operates two federally funded information centers for people\nwith disabilities: OBIRN (the Oregon Brain Injury Resource Network) and\nDB-LINK (the National Information Clearinghouse on Children who are\nDeaf-Blind). Both have developed in-depth library collections and\nservices in addition to typical clearinghouse services. The authors\ndescribe how OBIRN and DB-LINK were designed and developed, and how\nthey are currently structured and maintained. Both information centers\nuse many of the same strategies and tools in day-to-day operations, but\ndiffer in a number of ways, including materials and clientele\n', ['disability-related special libraries', 'federal government', 'disability-related information centers', 'disability-related clearinghouses', 'information resources', 'information referrals', 'Western Oregon University', 'OBIRN', 'Oregon Brain Injury Resource Network', 'DB-LINK', 'National Information Clearinghouse on Children who are Deaf-Blind', 'library collections', 'handicapped aids', 'information centres', 'information needs', 'special libraries']), ("On the p-adic Birch, Swinnerton-Dyer Conjecture for non-semistable reduction\nIn this paper, we examine the Iwasawa theory of elliptic curves E with additive\nreduction at an odd prime p. By extending Perrin-Riou's theory to\ncertain nonsemistable representations, we are able to convert Kato's\nzeta-elements into p-adic L-functions. This allows us to deduce the\ncotorsion of the Selmer group over the cyclotomic Z/sub p/-extension of\nQ, and thus prove an inequality in the p-adic Birch and Swinnerton-Dyer\nconjecture at primes p whose square divides the conductor of E\n", ['p-adic Birch', 'Swinnerton-Dyer conjecture', 'nonsemistable reduction', 'lwasawa theory', 'elliptic curves', 'additive reduction', "Perrin-Riou's theory", 'p-adic L-functions', 'cotorsion', 'Selmer group', 'cyclotomic Z/sub p/-extension', 'interpolation', 'number theory']), ('Adaptive image enhancement for retinal blood vessel segmentation\nRetinal blood vessel images are enhanced by removing the nonstationary\nbackground, which is adaptively estimated based on local neighbourhood\ninformation. The result is a much better segmentation of the blood\nvessels with a simple algorithm and without the need to obtain a priori\nillumination knowledge of the imaging system\n', ['adaptive image enhancement', 'retinal blood vessel images', 'local neighbourhood information', 'nonstationary background removal', 'image segmentation', 'personal identification', 'security applications', 'blood vessels', 'eye', 'image enhancement', 'image segmentation']), ('Distribution software: ROI is king\nMiddle-market accounting software vendors are taking to the open road, by way\nof souped-up distribution suites that can track product as it wends its\nway from warehouse floor to customer site. Integration provides\nefficiencies, and cost savings\n', ['accounting software', 'warehouse management', 'distribution', 'accounting', 'goods distribution', 'stock control', 'warehouse automation']), ('Model intestinal microflora in computer simulation: a simulation and modeling\npackage for host-microflora interactions\nThe ecology of the human intestinal microflora and its interaction with the\nhost are poorly understood. Though more and more data are being\nacquired, in part using modern molecular methods, development of a\nquantitative theory has not kept pace with this increase in observing\npower. This is in part due to the complexity of the system and to the\nlack of simulation environments in which to test what the ecological\neffect of a hypothetical mechanism of interaction would be, before\nresorting to laboratory experiments. The MIMICS project attempts to\naddress this through the development of a cellular automaton for\nsimulation of the intestinal microflora. In this paper, the design and\nevaluation of this simulator is discussed\n', ['human intestines', 'intestinal microflora', 'molecular methods', 'observing power', 'system complexity', 'microbial ecology', 'parallel computing', 'host-microflora interactions', 'quantitative theory', 'MIMICS project', 'complex microbial ecosystem', 'biological organs', 'cellular automata', 'digital simulation', 'medical computing', 'microorganisms', 'physiological models', 'software packages']), ('ISCSI poised to lower SAN costs\nIT managers building storage area networks or expanding their capacity may be\nable to save money by using iSCSI and IP systems rather than Fibre\nChannel technologies\n', ['SAN costs', 'storage area networks', 'iSCSI', 'IP systems', 'computer network management', 'digital storage']), ('Ecological interface design: progress and challenges\nEcological interface design (EID) is a theoretical framework for designing\nhuman-computer interfaces for complex socio-technical systems. Its\nprimary aim is to support knowledge workers in adapting to change and\nnovelty. This literature review shows that in situations requiring\nproblem solving, EID improves performance when compared with current\ndesign approaches in industry. EID has been applied to industry-scale\nproblems in a broad variety of application domains (e.g., process\ncontrol, aviation, computer network management, software engineering,\nmedicine, command and control, and information retrieval) and has\nconsistently led to the identification of new information requirements.\nAn experimental evaluation of EID using a full-fidelity simulator with\nprofessional workers has yet to be conducted, although some are\nplanned. Several significant challenges remain as obstacles to the\nconfident use of EID in industry. Promising paths for addressing these\noutstanding issues are identified. Actual or potential applications of\nthis research include improving the safety and productivity of complex\nsocio-technical systems\n', ['ecological interface design', 'human-computer interfaces', 'complex social technical systems', 'industry', 'productivity', 'user interface', 'human factors', 'ergonomics', 'factory automation', 'human factors', 'large-scale systems', 'user interfaces']), ('A survey of interactive mesh-cutting techniques and a new method for\nimplementing generalized interactive mesh cutting using virtual tools\nIn our experience, mesh-cutting methods can be distinguished by how their\nsolutions address the following major issues: definition of the cut\npath, primitive removal and re-meshing, number of new primitives\ncreated, when re-meshing is performed, and representation of the\ncutting tool. Many researchers have developed schemes for interactive\nmesh cutting with the goals of reducing the number of new primitives\ncreated, creating new primitives with good aspect ratios, avoiding a\ndisconnected mesh structure between primitives in the cut path, and\nrepresenting the path traversed by the tool as accurately as possible.\nThe goal of this paper is to explain how, by using a very simple\nframework, one can build a generalized cutting scheme. This method\nallows for any arbitrary cut to be made within a virtual object, and\ncan simulate cutting surface, layered surface or tetrahedral objects\nusing a virtual scalpel, scissors, or loop cautery tool. This method\nhas been implemented in a real-time, haptic-rate surgical simulation\nsystem allowing arbitrary cuts to be made on high-resolution\npatient-specific models\n', ['generalized interactive mesh cutting', 'virtual tools', 'cut path definition', 're-meshing', 'cutting tool', 'disconnected mesh structure', 'virtual object', 'tetrahedral objects', 'layered surface', 'real-time system', 'haptic-rate surgical simulation system', 'high-resolution patient-specific models', 'rendering', 'haptic interfaces', 'cutting', 'digital simulation', 'haptic interfaces', 'medical computing', 'real-time systems', 'rendering (computer graphics)', 'surgery']), ('Four-terminal quantum resistor network for electron-wave computing\nInterconnected ultrathin conducting wires or, equivalently, interconnected\nquasi-one-dimensional electron waveguides, which form a quantum\nresistor network, are presented here in four-terminal configurations.\nThe transmission behaviors through such four-terminal networks are\nevaluated and classified. In addition, we show that such networks can\nbe used as the basic building blocks for a possible massive wave\ncomputing machine in the future. In a network, each interconnection, a\nnode point, is an elastic scatterer that routes the electron wave.\nRouting and rerouting of electron waves in a network is described in\nthe framework of quantum transport from Landauer-Buttiker theory in the\npresence of multiple elastic scatterers. Transmissions through various\ntypes of four-terminal generalized clean Aharonov-Bohm rings are\ninvestigated at zero temperature. Useful logic functions are gathered\nbased on the transmission probability to each terminal with the use of\nthe Buttiker symmetry rule. In the generalized rings, even and odd\nnumbers of terminals can possess some distinctly different transmission\ncharacteristics as we have shown here and earlier. Just as an even or\nodd number of atoms in a ring is an important quantity for classifying\nthe transmission behavior, we show here that whether the number of\nterminals is an even or an odd number is just as important in\nunderstanding the physics of transmission through such a ring.\nFurthermore, we show that there are three basic classes of\nfour-terminal rings and the scaling relation for each class is\nprovided. In particular, the existence of equitransmission among all\nfour terminals is shown here. This particular physical phenomena cannot\nexist in any three-terminal ring. Comparisons and discussions of\ntransmission characteristics between three-terminal and four-terminal\nrings are also presented. The node-equation approach by considering the\nKirchhoff current conservation law at each node point is used for this\nanalysis. Many useful logic functions for electron-wave computing are\nshown here. In particular, we show that a full adder can be constructed\nvery simply using the equitransmission property of the four-terminal\nring. This is in sharp contrast with circuits based on transistor logic\n', ['four-terminal quantum resistor network', 'electron-wave computing', 'interconnected ultrathin conducting wires', 'quasi1D electron waveguides', 'rerouting', 'Landauer-Buttiker theory', 'multiple elastic scatterers', 'Aharonov-Bohm rings', 'logic functions', 'transmission probability', 'Buttiker symmetry rule', 'transmission behavior', 'Kirchhoff current conservation law', 'equitransmission property', 'Aharonov-Bohm effect', 'electron waveguides', 'quantum computing', 'resistors']), ("Santera targets independents in major strategy overhaul [telecom]\nWith big carriers slashing capital expense budgets, Santera Systems is\nbroadening the reach of its next-generation switching platform to\ninclude independent telcos. This week, the vendor will announce that it\nhas signed a deal with Kerman, Calif-based Kerman Telephone Co.\nFurthermore, the company is angling for inclusion in the Rural\nUtilities Service's approved equipment list, hoping to sell its Class 5\nreplacement boxes to the smallest carriers. The move is almost a\ncomplete reversal for the Plano, Texas-based vendor, which previously\nfocused solely on large carriers, including the RBOCs\n", ['Santera Systems', 'switching', 'Kerman Telephone', 'Rural Utilities Service', 'telecommunication switching']), ('Modeling frequently accessed wireless data with weak consistency\nTo reduce the response times of wireless data access in a mobile network,\ncaches are utilized in wireless handheld devices. If the original data\nentry has been updated, the cached data in the handheld device becomes\nstale. Thus, a mechanism is required to predict when the cached copy\nwill expire. This paper studies a weakly consistent data access\nmechanism that computes the time-to-live (TTL) interval to predict the\nexpiration time. We propose an analytic model to investigate this\nTTL-based algorithm for frequently accessed data. The analytic model is\nvalidated against simulation experiments. Our study quantitatively\nindicates how the TTL-based algorithm reduces the wireless\ncommunication cost by increasing the probability of stale accesses.\nDepending on the requirements of the application, appropriate parameter\nvalues can be selected based on the guidelines provided\n', ['frequently accessed wireless data modeling', 'weak consistency', 'response time reduction', 'wireless data access', 'mobile network', 'caches', 'wireless handheld devices', 'data entry', 'time-to-live interval', 'expiration time prediction', 'analytic model', 'simulation experiments', 'wireless communication cost', 'stale access probability', 'cache storage', 'client-server systems', 'mobile communication', 'mobile computing', 'portable computers']), ('Median partitioning: a novel method for the selection of representative subsets\nfrom large compound pools\nA method termed median partitioning (MP) has been developed to select diverse\nsets of molecules from large compound pools. Unlike many other methods\nfor subset selection, the MP approach does not depend on pairwise\ncomparison of molecules and can therefore be applied to very large\ncompound collections. The only time limiting step is the calculation of\nmolecular descriptors for database compounds. MP employs arrays of\nproperty descriptors with little correlation to divide large compound\npools into partitions from which representative molecules can be\nselected. In each of n subsequent steps, a population of molecules is\ndivided into subpopulations above and below the median value of a\nproperty descriptor until a desired number of 2/sup n/ partitions are\nobtained. For descriptor evaluation and selection, an entropy\nformulation was embedded in a genetic algorithm. MP has been applied to\ngenerate a subset of the Available Chemicals Directory, and the results\nhave been compared with cell-based partitioning\n', ['median partitioning', 'large compound pools', 'representative subset selection', 'molecules', 'time limiting step', 'molecular descriptors', 'database compounds', 'property descriptor array', 'entropy formulation', 'genetic algorithm', 'Available Chemicals Directory', 'cell-based partitioning', 'chemistry computing', 'entropy', 'genetic algorithms', 'organic compounds', 'scientific information systems', 'very large databases']), ("Contentment management\nAndersen's William Yarker and Richard Young outline the route to a successful\ncontent management strategy\n", ['Andersen Consulting', 'content management strategy', 'document handling', 'information resources']), ('Reinventing broadband\nMany believe that broadband providers need to change their whole approach. The\nfuture, then, is in reinventing broadband. That means tiered pricing to\nmake broadband more competitive with dial-up access and livelier, more\ndistinct content: video on demand, MP3, and other features exclusive to\nthe fat-pipe superhighway\n', ['MP3', 'business plans', 'video on demand', 'tiered pricing', 'broadband', 'broadband networks', 'costing', 'economics', 'Internet', 'technological forecasting']), ('Precoded OFDM with adaptive vector channel allocation for scalable video\ntransmission over frequency-selective fading channels\nOrthogonal frequency division multiplexing (OFDM) has been applied in broadband\nwireline and wireless systems for high data rate transmission where\nsevere intersymbol interference (ISI) always occurs. The conventional\nOFDM system provides advantages through conversion of an ISI channel\ninto ISI-free subchannels at multiple frequency bands. However, it may\nsuffer from channel spectral nulls and heavy data rate overhead due to\ncyclic prefix insertion. Previously, a new OFDM framework, the precoded\nOFDM, has been proposed to mitigate the above two problems through\nprecoding and conversion of an ISI channel into ISI-free vector\nchannels. In this paper, we consider the application of the precoded\nOFDM system to efficient scalable video transmission. We propose to\nenhance the precoded OFDM system with adaptive vector channel\nallocation to provide stronger protection against errors to more\nimportant layers in the layered bit stream structure of scalable video.\nThe more critical layers, or equivalently, the lower layers, are\nallocated vector channels of higher transmission quality. The channel\nquality is characterized by Frobenius norm metrics; based on channel\nestimation at the receiver. The channel allocation information is fed\nback periodically to the transmitter through a control channel.\nSimulation results have demonstrated the robustness of the proposed\nscheme to noise and fading inherent in wireless channels\n', ['precoded OFDM', 'scalable video transmission', 'frequency-selective fading channels', 'orthogonal frequency division multiplexing', 'channel spectral nulls', 'heavy data rate overhead', 'ISI channel', 'ISI-free vector channels', 'adaptive vector channel allocation', 'layered bit stream structure', 'lower layers', 'critical layers', 'channel quality', 'Frobenius norm metrics', 'channel estimation', 'channel allocation information', 'control channel', 'robustness', 'channel allocation', 'fading channels', 'Internet', 'intersymbol interference', 'mobile computing', 'mobile radio', 'OFDM modulation', 'video coding', 'visual communication']), ('An empirical investigation of the influences of the degree of interactivity on\nuser-outcomes in a multimedia environment\nThe study reported here investigates the influence of "interactivity" on the\nlearning outcomes of users in a multimedia systems environment. Drawing\nfrom past literature base and based on key tenets of three learning\ntheories - behaviorist, cognitivist, and constructivist - the study\nfirst proposes a measurement scheme for "interactivity" and then\nhypothesizes that "interactivity" would influence the learning outcomes\npositively in terms of users\' learning achievement and attitude. Three\nprototypes of a multimedia instructional/training system to represent\nhigh, low, and noninteractive modes of use were developed and\nimplemented and the hypothesized influences were investigated using a\ncontrolled laboratory research design. Multiple analysis of variance\n(MANOVA) results indicate that while interactivity does not necessarily\nenable enhanced gain in user learning, it positively influences\nparticipants\' attitude. The study finds no support for hypothesized\nmoderating effects of learning styles (measured using Kolb\'s Learning\nStyle Inventory scale) on the relationship between interactivity and\nuser outcomes. The results of this study have important implications\nfor both education and corporations\' training efforts and investments.\nImplications and future research directions are discussed\n', ['interactivity', 'user outcomes', 'learning outcomes', 'behaviorist learning theories', 'cognitivist learning theories', 'constructivist learning theories', 'measurement scheme', 'user attitude', 'training system', 'multiple analysis of variance', 'MANOVA', 'Kolb Learning Style Inventory scale', 'education', 'corporations', 'multimedia systems environment', 'business data processing', 'computer based training', 'interactive systems', 'multimedia computing', 'statistical analysis', 'user modelling']), ("FinancialContent. Credibility is king\nIf you went to a site named Financialcontent.com, you'd probably expect to\nfind, well, financial content. Maybe stock prices or company earnings\nor market charts or economic statistics or corporate news reports.\nWell, you'd be partially correct. Financialcontent.com does deal in\nfinancial information, but its main objective is not to distribute its\nfinancial content to individual investors, but to distribute it through\nother Web sites. In other words, FinancialContent is a wholesaler, not\na retailer. As an aggregator, FinancialContent provides partner sites\nwith financial information that is tailored to that individual Web site\n", ['Financialcontent.com', 'financial information', 'Web sites', 'aggregator', 'partner sites', 'financial data processing', 'information resources', 'Internet']), ('Modeling shape and topology of low-resolution density maps of biological\nmacromolecules\nWe develop an efficient way of representing the geometry and topology of\nvolumetric datasets of biological structures from medium to low\nresolution, aiming at storing and querying them in a database\nframework. We make use of a new vector quantization algorithm to select\nthe points within the macromolecule that best approximate the\nprobability density function of the original volume data. Connectivity\namong points is obtained with the use of the alpha shapes theory. This\nnovel data representation has a number of interesting characteristics,\nsuch as (1) it allows us to automatically segment and quantify a number\nof important structural features from low-resolution maps, such as\ncavities and channels, opening the possibility of querying large\ncollections of maps on the basis of these quantitative structural\nfeatures; (2) it provides a compact representation in terms of size;\n(3) it contains a subset of three-dimensional points that optimally\nquantify the densities of medium resolution data; and (4) a general\nmodel of the geometry and topology of the macromolecule (as opposite to\na spatially unrelated bunch of voxels) is easily obtained by the use of\nthe alpha shapes theory\n', ['geometry', 'topology', 'volumetric datasets', 'biological structures', 'database framework', 'vector quantization algorithm', 'low-resolution density maps', 'biological macromolecules', 'modeling', 'probability density function', 'data representation', 'structural features', 'cavities', 'channels', 'connectivity', 'compact representation', 'three-dimensional points', 'medium resolution data', 'general model', 'original volume data', 'alpha shapes theory', 'biology computing', 'data models', 'information services', 'macromolecules', 'molecular biophysics', 'physiological models', 'probability', 'topology', 'vectors']), ('Dynamic spectrum management for next-generation DSL systems\nThe performance of DSL systems is severely constrained by crosstalk due to the\nelectromagnetic coupling among the multiple twisted pairs making up a\nphone cable. In order to reduce performance loss arising from\ncrosstalk, DSL systems are currently designed under the assumption of\nworst-case crosstalk scenarios leading to overly conservative DSL\ndeployments. This article presents a new paradigm for DSL system\ndesign, which takes into account the multi-user aspects of the DSL\ntransmission environment. Dynamic spectrum management (DSM) departs\nfrom the current design philosophy by enabling transceivers to\nautonomously and dynamically optimize their communication settings with\nrespect to both the channel and the transmissions of neighboring\nsystems. Along with this distributed optimization, when an additional\ndegree of coordination becomes available for future DSL deployment, DSM\nwill allow even greater improvement in DSL performance. Implementations\nare readily applicable without causing any performance degradation to\nthe existing DSLs under static spectrum management. After providing an\noverview of the DSM concept, this article reviews two practical DSM\nmethods: iterative water-filling, an autonomous distributed power\ncontrol method enabling great improvement in performance, which can be\nimplemented through software options in some existing ADSL and VDSL\nsystems; and vectored-DMT, a coordinated transmission/reception\ntechnique achieving crosstalk-free communication for DSL systems, which\nbrings within reach the dream of providing universal Internet access at\nspeeds close to 100 Mb/s to 500 m on 1-2 lines and beyond 1 km on 2-4\nlines. DSM-capable DSL thus enables the broadband age\n', ['DSL systems performance', 'DSL system design', 'electromagnetic coupling', 'data transmission', 'twisted pairs', 'phone cable', 'dynamic spectrum management', 'transceivers', 'distributed optimization', 'static spectrum management', 'iterative water-filling', 'autonomous distributed power control method', 'software options', 'VDSL systems', 'ADSL systems', 'vectored-DMT', 'coordinated transmission/reception', 'crosstalk-free communication', 'universal Internet access', 'broadband networks', '100 Mbit/s', '500 m', 'broadband networks', 'crosstalk', 'digital subscriber lines', 'distributed control', 'electromagnetic coupling', 'iterative methods', 'power control', 'telecommunication control', 'twisted pair cables']), ('The development of CASC [automated theorem proving]\nResearchers who make theoretical advances also need some way to demonstrate\nthat an advance really does have general, overall positive consequences\nfor system performance. For this it is necessary to evaluate the system\non a set of problems that is sufficiently large and diverse to be\nsomehow representative of the intended application area as a whole. It\nis only a small step from system evaluation to a communal system\ncompetition. The CADE ATP System Competition (CASC) has been run\nannually since 1996. Any competition is difficult to design and\norganize in the first instance, and to then run over the years. In\norder to obtain the full benefits of a competition, a thoroughly\norganized event, with an unambiguous and motivated design, is\nnecessary. For some issues relevant to the CASC design, inevitable\nconstraints have emerged. For other issues there have been several\nchoices, and decisions have had to be made. This paper describes the\nevolution of CASC, paying particular attention to its design, design\nchanges, and organization\n', ['CASC', 'system performance', 'system evaluation', 'automated deduction', 'CADE ATP System Competition', 'automated theorem proving', 'AI', 'artificial intelligence', 'classical first order logic', 'artificial intelligence', 'formal logic', 'inference mechanisms', 'theorem proving']), ('Pairwise thermal entanglement in the n-qubit (n <or= 5) Heisenberg XX chain\nWe have calculated the concurrence of the pairwise thermal entanglement for the\nfour-qubit and five-qubit Heisenberg XX chain. It is found that there\nis a great difference between the even-qubit and the odd-qubit chain in\nthe aspect of the critical temperature and of the existence of the\nentanglement for the case of the qubit number n no more than 5\n', ['pairwise thermal entanglement', 'four-qubit Heisenberg XX chain', 'five-qubit Heisenberg XX chain', 'even-qubit chain', 'odd-qubit chain', 'critical temperature', 'Heisenberg model', 'quantum computing', 'quantum interference phenomena']), ('Fuzzy non-homogeneous Markov systems\nIn this paper the theory of fuzzy logic and fuzzy reasoning is combined with\nthe theory of Markov systems and the concept of a fuzzy non-homogeneous\nMarkov system is introduced for the first time. This is an effort to\ndeal with the uncertainty introduced in the estimation of the\ntransition probabilities and the input probabilities in Markov systems.\nThe asymptotic behaviour of the fuzzy Markov system and its asymptotic\nvariability is considered and given in closed analytic form. Moreover,\nthe asymptotically attainable structures of the system are estimated\nalso in a closed analytic form under some realistic assumptions. The\nimportance of this result lies in the fact that in most cases the\ntraditional methods for estimating the probabilities can not be used\ndue to lack of data and measurement errors. The introduction of fuzzy\nlogic into Markov systems represents a powerful tool for taking\nadvantage of the symbolic knowledge that the experts of the systems\npossess\n', ['fuzzy nonhomogeneous Markov systems', 'fuzzy logic', 'fuzzy reasoning', 'uncertainty', 'transition probabilities', 'input probabilities', 'asymptotic variability', 'measurement errors', 'symbolic knowledge', 'probability theory', 'fuzzy logic', 'fuzzy systems', 'inference mechanisms', 'Markov processes', 'probability', 'uncertainty handling']), ('On batch-constructing B/sup +/-trees: algorithm and its performance evaluation\nEfficient construction of indexes is very important in bulk-loading a database\nor adding a new index to an existing database since both of them should\nhandle an enormous volume of data. In this paper, we propose an\nalgorithm for batch-constructing the B/sup +/-tree, the most widely\nused index structure in database systems. The main characteristic of\nour algorithm is to simultaneously process all the key values to be\nplaced on each B+-tree page when accessing the page. This avoids the\noverhead due to accessing the same page multiple times, which results\nfrom applying the B+-tree insertion algorithm repeatedly. For\nperformance evaluation, we have analyzed our algorithm in terms of the\nnumber of disk accesses. The results show that the number of disk\naccesses excluding those in the relocation process is identical to the\nnumber of pages belonging to the B/sup +/-tree. Considering that the\nrelocation process is an unavoidable preprocessing step for\nbatch-constructing of B/sup +/-trees, our algorithm requires just one\ndisk access per B+-tree page, and therefore turns out to be optimal. We\nalso present the performance tendency in relation with different\nparameter values via simulation. Finally, we show the performance\nenhancement effect of our algorithm, compared with the one using\nrepeated insertions through experiments\n', ['B+-tree batch construction', 'algorithm performance evaluation', 'database bulk loading', 'index structure', 'B+-tree page', 'page access', 'B+-tree insertion algorithm', 'disk accesses', 'relocation process', 'simulation', 'database indexing', 'tree data structures']), ('Rise of the supercompany [CRM]\nAll the thoughts, conversations and notes of employees help the firm create a\nwider picture of business. Customer relationship management (CRM) feeds\non data, and it is hungry\n', ['customer relationship management', 'central data repository', 'database', 'staff trained', 'database management systems', 'marketing']), ("Take it to the next level [law firm innovation]\nIt's called innovating. Our clients do it. Our culture worships it. Our future\nhinges on it. Why is it so difficult in law firms? How can we make it\neasier? Viva la difference!\n", ['innovation', 'law firms', 'law administration']), ('Dynamic modification of object Petri nets. An application to modelling\nprotocols with fork-join structures\nIn this paper we discuss possibilities of modelling protocols by objects in\nobject-based high-level Petri nets. Some advantages of dynamically\nmodifying the structure of token objects are discussed and the need for\nfurther investigations into mathematically rigorous foundations of\nobject net formalisms incorporating facilities for such operations on\nits token nets is emphasised\n', ['dynamic modification', 'object Petri nets', 'protocols', 'fork-join structures', 'token objects', 'mathematically rigorous foundations', 'object net formalisms', 'formal specification', 'Petri nets', 'protocols', 'synchronisation']), ('Restoration of broadband imagery steered with a liquid-crystal optical phased\narray\nIn many imaging applications, it is highly desirable to replace mechanical\nbeam-steering components (i.e., mirrors and gimbals) with a\nnonmechanical device. One such device is a nematic liquid crystal\noptical phased array (LCOPA). An LCOPA can implement a blazed phase\ngrating to steer the incident light. However, when a phase grating is\nused in a broadband imaging system, two adverse effects can occur.\nFirst, dispersion will cause different incident wavelengths arriving at\nthe same angle to be steered to different output angles, causing\nchromatic aberrations in the image plane. Second, the device will steer\nenergy not only to the first diffraction order, but to others as well.\nThis multiple-order effect results in multiple copies of the scene\nappearing in the image plane. We describe a digital image restoration\ntechnique designed to overcome these degradations. The proposed\npostprocessing technique is based on a Wiener deconvolution filter. The\ntechnique, however, is applicable only to scenes containing objects\nwith approximately constant reflectivities over the spectral region of\ninterest. Experimental results are presented to demonstrate the\neffectiveness of this technique\n', ['broadband imagery', 'liquid-crystal optical phased array steering', 'imaging applications', 'mechanical beam-steering components', 'mirrors', 'gimbals', 'nonmechanical device', 'nematic liquid crystal optical phased array', 'blazed phase grating', 'incident light steering', 'broadband imaging system', 'dispersion', 'incident wavelengths', 'output angles', 'chromatic aberrations', 'image plane', 'optical phased array', 'first diffraction order', 'multiple-order effect', 'halogen lamp', 'multiple copies', 'digital image restoration technique', 'postprocessing technique', 'Wiener deconvolution filter', 'approximately constant reflectivities', 'spectral region of interest', 'aberrations', 'beam steering', 'deconvolution', 'image restoration', 'liquid crystal devices', 'nematic liquid crystals', 'optical arrays', 'reflectivity', 'spatial filters']), ('Project Euclid and the role of research libraries in scholarly publishing\nProject Euclid, a joint electronic journal publishing initiative of Cornell\nUniversity Library and Duke University Press is discussed in the\nbroader contexts of the changing patterns of scholarly communication\nand the publishing scene of mathematics. Specific aspects of the\nproject such as partnerships and the creation of an economic model are\npresented as well as what it takes to be a publisher. Libraries have\ngained important and relevant experience through the creation and\nmanagement of digital libraries, but they need to develop further\nskills if they want to adopt a new role in the life cycle of scholarly\ncommunication\n', ['Project Euclid', 'joint electronic journal publishing initiative', 'Cornell University Library', 'Duke University Press', 'scholarly communication', 'mathematics', 'partnerships', 'economic model', 'scholarly publishing', 'research libraries', 'electronic publishing', 'mathematics computing', 'research libraries']), ('A method for geometrical verification of dynamic intensity modulated\nradiotherapy using a scanning electronic portal imaging device\nIn order to guarantee the safe delivery of dynamic intensity modulated\nradiotherapy (IMRT), verification of the leaf trajectories during the\ntreatment is necessary. Our aim in this study is to develop a method\nfor on-line verification of leaf trajectories using an electronic\nportal imaging device with scanning read-out, independent of the\nmultileaf collimator. Examples of such scanning imagers are electronic\nportal imaging devices (EPIDs) based on liquid-filled ionization\nchambers and those based on amorphous silicon. Portal images were\nacquired continuously with a liquid-filled ionization chamber EPID\nduring the delivery, together with the signal of treatment progress\nthat is generated by the accelerator. For each portal image, the\nprescribed leaf and diaphragm positions were computed from the dynamic\nprescription and the progress information. Motion distortion effects of\nthe leaves are corrected based on the treatment progress that is\nrecorded for each image row. The aperture formed by the prescribed\nleaves and diaphragms is used as the reference field edge, while the\nactual field edge is found using a maximum-gradient edge detector. The\nerrors in leaf and diaphragm position are found from the deviations\nbetween the reference field edge and the detected field edge. Earlier\nmeasurements of the dynamic EPID response show that the accuracy of the\ndetected field edge is better than 1 mm. To ensure that the\nverification is independent of inaccuracies in the acquired progress\nsignal, the signal was checked with diode measurements beforehand. The\nmethod was tested on three different dynamic prescriptions. Using the\ndescribed method, we correctly reproduced the distorted field edges.\nVerifying a single portal image took 0.1 s on an 866 MHz personal\ncomputer. Two flaws in the control system of our experimental dynamic\nmultileaf collimator were correctly revealed with our method. First,\nthe errors in leaf position increase with leaf speed, indicating a\ndelay of approximately 0.8 s in the control system. Second, the\naccuracy of the leaves and diaphragms depends on the direction of\nmotion. In conclusion, the described verification method is suitable\nfor detailed verification of leaf trajectories during dynamic IMRT\n', ['dynamic intensity modulated radiotherapy', 'geometrical verification method', 'dynamic multileaf collimator', 'safe delivery', 'leaf trajectories', 'on-line verification', 'scanning read-out', 'liquid-filled ionization chambers', 'leaf positions', 'diaphragm positions', 'motion distortion effects', 'reference field edge', 'distorted field edges', 'control system', 'dose distributions', 'treatment planning', 'diaphragms', 'dosimetry', 'intensity modulation', 'liquid ionisation chambers', 'medical image processing', 'radiation therapy']), ("Integration is key - an introduction to enterprise application integration\n(EAI) technology\nOver the past few years, numerous organisations have invested in the latest\nsoftware applications to drive their business forward. But many are now\nfinding that these systems are becoming redundant on their own. The key\nto staying ahead of the competition in today's current climate is now\nto integrate all of these systems, says Justin Opie, Portfolio Director\nat Imark Communications\n", ['enterprise application integration', 'Imark Communications', 'management information systems']), ('The development and evaluation of SHOKE2000: the PCI-based FPGA card\nThis paper describes a PCI-based FPGA card, SHOKE2000, which was developed in\norder to study reconfigurable computing. Since the latest field\nprogrammable gate arrays (FPGA) consist of input/output (I/O)\nconfigurable blocks as well as internal configurable logic blocks, they\nnot only realize various user logic circuits but also connect with\npopular I/O standards easily. These features enable FPGA to connect\nseveral devices with different interfaces, and thus new reconfigurable\nsystems would be realizable by connecting the FPGA with devices such as\ndigital signal processors (DSP) and analog devices. This paper\ndescribes the basic functions of SHOKE2000, which was developed for\nrealizing hybrid reconfigurable systems consisting of FPGA, DSP, and\nanalog devices. We also present application examples of SHOKE2000,\nincluding a simple image recognition application, a distributed shared\nmemory computer cluster, and teaching materials for computer education\n', ['PCI', 'FPGA card', 'SHOKE2000', 'reconfigurable computing', 'field programmable gate arrays', 'FPGA', 'I/O standard', 'interfaces', 'digital signal processors', 'DSP', 'analog devices', 'hybrid reconfigurable systems', 'image recognition application', 'distributed shared memory computer cluster', 'teaching materials', 'computer education', 'intellectual property', 'user logic circuits', 'analogue integrated circuits', 'computer science education', 'digital signal processing chips', 'distributed shared memory systems', 'field programmable gate arrays', 'image recognition', 'industrial property', 'reconfigurable architectures', 'system buses', 'teaching', 'workstation clusters']), ('On the monotonicity conservation in numerical solutions of the heat equation\nIt is important to choose such numerical methods in practice that mirror the\ncharacteristic properties of the described process beyond the stability\nand convergence. The investigated qualitative property in this paper is\nthe conservation of the monotonicity in space of the initial heat\ndistribution. We prove some statements about the monotonicity\nconservation and total monotonicity of one-step vector-iterations.\nThen, applying these results, we consider the numerical solutions of\nthe one-dimensional heat equation. Our main theorem formulates the\nnecessary and sufficient condition of the uniform monotonicity\nconservation. The sharpness of the conditions is demonstrated by\nnumerical examples\n', ['monotonicity conservation', 'numerical solutions', 'heat equation', 'characteristic properties', 'qualitative property', 'one-step vector-iterations', 'necessary and sufficient condition', 'differential equations', 'heat transfer', 'matrix algebra']), ('Combining PC control and HMI\nIntegrating PC-based control with human machine interface (HMI) technology can\nbenefit a plant floor system. However, before one decides on PC-based\ncontrol, there are many things one should consider, especially when\nusing a soft programmable logic controller (PLC) to command the\ninput/output. There are three strategies to integrate a PC-based\ncontrol system with an HMI: treat the PC running the control\napplication as if it were a PLC, integrate the system using standard PC\ninterfaces; or using application programming interfaces\n', ['PC-based control system', 'application programming interfaces', 'PC interfaces', 'human machine interface', 'shop floor system', 'programmable logic controller', 'man-machine systems', 'manufacturing processes', 'microcomputer applications', 'peripheral interfaces', 'process control', 'programmable controllers', 'user interfaces']), ('Neural networks for web content filtering\nWith the proliferation of harmful Internet content such as pornography,\nviolence, and hate messages, effective content-filtering systems are\nessential. Many Web-filtering systems are commercially available, and\npotential users can download trial versions from the Internet. However,\nthe techniques these systems use are insufficiently accurate and do not\nadapt well to the ever-changing Web. To solve this problem, we propose\nusing artificial neural networks to classify Web pages during content\nfiltering. We focus on blocking pornography because it is among the\nmost prolific and harmful Web content. However, our general framework\nis adaptable for filtering other objectionable Web material\n', ['artificial neural networks', 'Intelligent Classification Engine', 'learning capabilities', 'pornographic/nonpornographic Web page differentiation', 'Web content filtering', 'violence', 'Web page classification', 'harmful Web content', 'classification', 'information resources', 'Internet', 'learning (artificial intelligence)', 'online front-ends', 'social aspects of automation']), ("Putting pen to screen on Tablet PCs\nWith the release of the first Tablet PCs produced to Microsoft Corp.'s general\nspecifications, handheld computers may be about to leap into the ring\nwith today's laptops. They will be about the size of the smaller\nlaptops, will be at least as powerful, and maybe their biggest selling\npoint-will be able to handle handwritten text. The Tablet PCs will be\namply configured, general-purpose machines with more than enough power\nto run the full-blown Windows XP operating system. In particular, they\nwill allow handwritten text to be entered onto a digitizing tablet and\nrecognized, a functionality that's called pen-based computing. The\nTablet PC will far outpace the computing power of existing small\ndevices such as PDAs (personal digital assistants), including those\nvariants based on Microsoft's own Pocket PC operating system\n", ['Tablet PC', 'Microsoft', 'handheld computers', 'handwritten text', 'Windows XP operating system', 'digitizing tablet', 'pen-based computing', 'handwriting recognition', 'notebook computers']), ("ConChat: a context-aware chat program\nConChat is a context-aware chat program that enriches electronic communication\nby providing contextual information and resolving potential semantic\nconflicts between users.ConChat uses contextual information to improve\nelectronic communication. Using contextual cues, users can infer during\na conversation what the other person is doing and what is happening in\nhis or her immediate surroundings. For example, if a user learns that\nthe other person is talking with somebody else or is involved in some\nurgent activity, he or she knows to expect a slower response.\nConversely, if the user learns that the other person is sitting in a\nmeeting directly related to the conversation, he or she then knows to\nrespond more quickly. Also, by informing users about the other person's\ncontext and tagging potentially ambiguous chat messages, ConChat\nexplores how context can improve electronic communication by reducing\nsemantic conflicts\n", ['context-aware chat program', 'ConChat', 'contextual information', 'semantic conflicts', 'contextual cues', 'groupware', 'teleconferencing']), ('Differential calculus for p-norms of complex-valued vector functions with\napplications\nFor complex-valued n-dimensional vector functions t to s(t), supposed to be\nsufficiently smooth, the differentiability properties of the mapping t\nto ||s(t)||/sub p/ at every point t = t/sub 0/ epsilon R/sub 0//sup\n+/:= {t epsilon R | t >or= 0} are investigated, where || . ||/sub p/\nis the usual vector norm in C/sup n/ resp. R/sup n/, for p epsilon [1,\no infinity ]. Moreover, formulae for the first three right derivatives\nD/sub +//sup k/||s(t)||/sub p/, k = 1, 2,3 are determined. These\nformulae are applied to vibration problems by computing the best upper\nbounds on ||s(t)||/sub p/ in certain classes of bounds. These results\ncannot be obtained by the methods used so far. The systematic use of\nthe differential calculus for vector norms, as done here for the first\ntime, could lead to major advances also in other branches of\nmathematics and other sciences\n', ['differential calculus', 'vector functions', 'mapping', 'vibration problems', 'vector norms', 'differentiation', 'functions', 'vectors']), ('The creation of a high-fidelity finite element model of the kidney for use in\ntrauma research\nA detailed finite element model of the human kidney for trauma research has\nbeen created directly from the National Library of Medicine Visible\nHuman Female (VHF) Project data set. An image segmentation and organ\nreconstruction software package has been developed and employed to\ntransform the 2D VHF images into a 3D polygonal representation.\nNonuniform rational B-spline (NURBS) surfaces were then mapped to the\npolygonal surfaces, and were finally utilized to create a robust 3D\nhexahedral finite element mesh within a commercially available meshing\nsoftware. The model employs a combined viscoelastic and hyperelastic\nmaterial model to successfully simulate the behaviour of biological\nsoft tissues. The finite element model was then validated for use in\nbiomechanical research\n', ['high-fidelity finite element model', 'kidney', 'trauma research', 'National Library of Medicine', 'Visible Human Female project', 'medical data set', 'image segmentation', 'organ reconstruction', 'physically based animation', 'software package', '3D polygonal representation', '2D VHF images', 'nonuniform rational B-spline surfaces', 'NURBS', 'polygonal surfaces', '3D hexahedral finite element mesh', 'viscoelastic model', 'hyperelastic material model', 'biological soft tissues', 'biomechanical research', 'computer animation', 'finite element analysis', 'image reconstruction', 'image representation', 'image segmentation', 'kidney', 'medical image processing', 'software packages', 'splines (mathematics)']), ("Virtual engineering office: a state-of-the-art platform for engineering\ncollaboration\nA sales force in Latin America, the design department in Europe, and production\nin Asia? Arrangements of this kind are the new business reality for\ntoday's global manufacturing companies. But how are such global\noperations to be effectively coordinated? ABB's answer was to develop\nand implement a new platform for high-performance, real-time\ncollaboration. Globally distributed engineering teams can now work\ntogether, regardless of time, location or the CAD system they use,\nmaking ABB easier to do business with, for customers as well as\nsuppliers\n", ['virtual engineering office', 'state-of-the-art', 'engineering collaboration platform', 'business', 'global manufacturing companies', 'ABB', 'CAD system', 'globally distributed engineering teams', 'business data processing', 'engineering computing', 'groupware', 'management information systems']), ('Securing the Internet routing infrastructure\nThe unprecedented growth of the Internet over the last years, and the\nexpectation of an even faster increase in the numbers of users and\nnetworked systems, resulted in the Internet assuming its position as a\nmass communication medium. At the same time, the emergence of an\nincreasingly large number of application areas and the evolution of the\nnetworking technology suggest that in the near future the Internet may\nbecome the single integrated communication infrastructure. However, as\nthe dependence on the networking infrastructure grows, its security\nbecomes a major concern, in light of the increased attempt to\ncompromise the infrastructure. In particular, the routing operation is\na highly visible target that must be shielded against a wide range of\nattacks. The injection of false routing information can easily degrade\nnetwork performance, or even cause denial of service for a large number\nof hosts and networks over a long period of time. Different approaches\nhave been proposed to secure the routing protocols, with a variety of\ncountermeasures, which, nonetheless, have not eradicated the\nvulnerability of the routing infrastructure. In this article, we survey\nthe up-to-date secure routing schemes. that appeared over the last few\nyears. Our critical point of view and thorough review of the literature\nare an attempt to identify directions for future research on an indeed\ndifficult and still largely open problem\n', ['Internet routing infrastructure security', 'networked systems', 'networking technology', 'integrated communication infrastructure', 'networking infrastructure', 'false routing information', 'network performance', 'routing protocols', 'countermeasures', 'routing infrastructure', 'secure routing schemes', 'research', 'preventive security mechanisms', 'link state protocols', 'Internet', 'security of data', 'telecommunication network routing', 'telecommunication security', 'transport protocols']), ('Fast, accurate and stable simulation of power electronic systems using virtual\nresistors and capacitors\nSimulation of power electronic circuits remains a problem due to the high level\nof stiffness brought about by the modelling of switches as biresistors\ni.e. very low turn-on resistance and very high turn-off resistance. The\nmerits and drawbacks of two modelling methods that address this problem\nare discussed. A modelling solution for ensuring numerically stable,\naccurate and fast simulation of power electronic systems is proposed.\nThe solution enables easy connectivity between power electronic\nelements in the simulation model. It involves the modelling of virtual\ncapacitance at switching nodes to soften voltage discontinuity due to\nthe switch current suddenly going to zero. Undesirable ringing effects\nthat may arise due to the interaction between the virtual capacitance\nand circuit inductance are eliminated by modelling virtual damping\nresistors in parallel to inductors that are adjacent to switching\nelements. A midpoint configuration method is also introduced for\nmodelling shunt capacitors. A DC traction system is simulated using\nthis modelling strategy and the results are included. Simulation\nresults obtained using this modelling strategy are validated by\ncomparison with the established mesh analysis technique of modelling.\nThe simulation performance is also compared with the Power System\nBlockset commercial software\n', ['power electronic systems simulation', 'virtual resistors', 'virtual capacitors', 'turn-on resistance', 'high turn-off resistance', 'switch modelling', 'switching nodes', 'voltage discontinuity softening', 'ringing effects', 'DC traction system', 'mesh analysis technique', 'Power System Blockset software', 'computer simulation', 'capacitors', 'circuit simulation', 'power electronics', 'power engineering computing', 'resistors', 'switching circuits']), ('The simulated emergence of distributed environmental control in evolving\nmicrocosms\nThis work continues investigation into Gaia theory (Lovelock, The ages of Gaia,\nOxford University Press, 1995) from an artificial life perspective\n(Downing, Proceedings of the 7th International Conference on Artificial\nLife, p. 90-99, MIT Press, 2000), with the aim of assessing the general\ncompatibility of emergent distributed environmental control with\nconventional natural selection. Our earlier system, GUILD (Downing and\nZvirinsky, Artificial Life, 5, p.291-318, 1999), displayed emergent\nregulation of the chemical environment by a population of metabolizing\nagents, but the chemical model underlying those results was trivial,\nessentially admitting all possible reactions at a single energy cost.\nThe new model, METAMIC, utilizes abstract chemistries that are both (a)\nconstrained to a small set of legal reactions, and (b) grounded in\nbasic fundamental relationships between energy, entropy, and biomass\nsynthesis/breakdown. To explore the general phenomena of emergent\nhomeostasis, we generate 100 different chemistries and use each as the\nbasis for several METAMIC runs, as part of a Gaia hunt. This search\ndiscovers 20 chemistries that support microbial populations capable of\nregulating a physical environmental factor within their growth-optimal\nrange, despite the extra metabolic cost. Case studies from the Gaia\nhunt illustrate a few simple mechanisms by which real biota might\nexploit the underlying chemistry to achieve some control over their\nphysical environment. Although these results shed little light on the\nquestion of Gaia on Earth, they support the possibility of emergent\nenvironmental control at the microcosmic level\n', ['simulated emergence', 'evolving microcosms', 'natural selection', 'GUILD system', 'metabolizing agents', 'chemical model', 'METAMIC model', 'emergent homeostasis', 'Gaia hunt', 'genetic algorithms', 'artificial chemistry', 'artificial metabolisms', 'Gaia theory', 'artificial life', 'emergent distributed environmental control', 'artificial life', 'genetic algorithms', 'living systems']), ('Active vibration control of piezolaminated smart beams\nThis paper deals with the active vibration control of beam like structures with\ndistributed piezoelectric sensor and actuator layers bonded on top and\nbottom surfaces of the beam. A finite element model based on\nEuler-Bernoulli beam theory has been developed. The contribution of the\npiezoelectric sensor and actuator layers on the mass and stiffness of\nthe beam is considered. Three types of classical control strategies,\nnamely direct proportional feedback, constant-gain negative velocity\nfeedback and Lyapunov feedback and an optimal control strategy, linear\nquadratic regulator (LQR) scheme are applied to study their control\neffectiveness. Also, the control performance with different types of\nloading, such as impulse loading, step loading, harmonic and random\nloading is studied\n', ['active vibration control', 'piezolaminated smart beams', 'beam like structures', 'distributed piezoelectric sensor layers', 'distributed piezoelectric actuator layers', 'top surfaces', 'bottom surfaces', 'finite element model', 'Euler-Bernoulli beam theory', 'mass', 'stiffness', 'direct proportional feedback', 'constant-gain negative velocity feedback', 'Lyapunov feedback', 'optimal control strategy', 'linear quadratic regulator', 'control effectiveness', 'impulse loading', 'step loading', 'harmonic loading', 'random loading', 'feedback', 'finite element analysis', 'intelligent control', 'linear quadratic control', 'Lyapunov methods', 'piezoelectric actuators', 'vibration control']), ('On the emergence of rules in neural networks\nA simple associationist neural network learns to factor abstract rules (i.e.,\ngrammars) from sequences of arbitrary input symbols by inventing\nabstract representations that accommodate unseen symbol sets as well as\nunseen but similar grammars. The neural network is shown to have the\nability to transfer grammatical knowledge to both new symbol\nvocabularies and new grammars. Analysis of the state-space shows that\nthe network learns generalized abstract structures of the input and is\nnot simply memorizing the input strings. These representations are\ncontext sensitive, hierarchical, and based on the state variable of the\nfinite-state machines that the neural network has learned.\nGeneralization to new symbol sets or grammars arises from the spatial\nnature of the internal representations used by the network, allowing\nnew symbol sets to be encoded close to symbol sets that have already\nbeen learned in the hidden unit space of the network. The results are\ncounter to the arguments that learning algorithms based on weight\nadaptation after each exemplar presentation (such as the long term\npotentiation found in the mammalian nervous system) cannot in principle\nextract symbolic knowledge from positive examples as prescribed by\nprevailing human linguistic theory and evolutionary psychology\n', ['associationist neural network', 'learns', 'abstract rules', 'neural network', 'state-space', 'symbolic knowledge', 'cognitive neurosciences', 'associationist learning', 'associative processing', 'learning (artificial intelligence)', 'neural nets']), ("Axioms for branching time\nLogics of general branching time, or historical necessity, have long been\nstudied but important axiomatization questions remain open. Here the\ndifficulties of finding axioms for such logics are considered and ideas\nfor solving some of the main open problems are presented. A new, more\nexpressive logical account is also given to support Peirce's\nprohibition on truth values being attached to the contingent future\n", ['axioms', 'branching time', 'truth values', 'temporal logic', 'decidability', 'temporal logic']), ('A new identification approach for FIR models\nThe identification of stochastic discrete systems disturbed with noise is\ndiscussed in this brief. The concept of general prediction error (GPE)\ncriterion is introduced for the time-domain estimate with optimal\nfrequency estimation (OFE) introduced for the frequency-domain\nestimate. The two estimation methods are combined to form a new\nidentification algorithm, which is called the empirical\nfrequency-domain optimal parameter (EFOP) estimate, for the finite\nimpulse response (FIR) model interfered by noise. The algorithm\ntheoretically provides the global optimum of the model frequency-domain\nestimate. Some simulation examples are given to illustrate the new\nidentification method\n', ['identification approach', 'FIR models', 'stochastic discrete systems', 'general prediction error criterion', 'time-domain estimate', 'optimal frequency estimation', 'frequency-domain estimate', 'empirical frequency-domain optimal parameter estimate', 'discrete systems', 'frequency-domain analysis', 'parameter estimation', 'prediction theory', 'stochastic systems', 'time-domain analysis']), ('Discreteness and relevance: a reply to Roman Poznanski\nIn reply to Poznanski (see ibid., p.435, 2002) on discreteness and relevance,\nEliasmith claims that all of the concerns voiced by Poznanski in his\nreply fail to offer a serious challenge to the idea that continuity is\nirrelevant to a good understanding of cognitive systems. Eliasmith\nhopes that it is evident that he does not claim that the process in\nneural systems is discrete, but rather that a complete characterization\nof the process can be discrete; these of course are significantly\ndifferent claims\n', ['discreteness', 'relevance', 'continuity', 'cognitive systems', 'neural systems', 'cognitive systems', 'neural nets', 'neurophysiology']), ("A hybrid model for smoke simulation\nA smoke simulation approach based on the integration of traditional particle\nsystems and density functions is presented in this paper. By attaching\na density function to each particle as its attribute, the diffusion of\nsmoke can be described by the variation of particles' density\nfunctions, along with the effect on airflow by controlling particles'\nmovement and fragmentation. In addition, a continuous density field for\nrealistic rendering can be generated quickly through the look-up tables\nof particle's density functions. Compared with traditional particle\nsystems, this approach can describe smoke diffusion, and provide a\ncontinuous density field for realistic rendering with much less\ncomputation. A quick rendering scheme is also presented in this paper\nas a useful preview tool for tuning appropriate parameters in the smoke\nmodel\n", ['hybrid model', 'smoke simulation', 'density functions', 'continuous density field', 'rendering', 'look-up tables', 'rendering (computer graphics)', 'table lookup']), ('Comparison between discrete STFT and wavelets for the analysis of power quality\nevents\nThis paper deals with the comparison of signal processing tools for power\nquality analysis. Two signal processing techniques are considered: the\nwavelet filters and the discrete short-time Fourier transforms (STFT).\nThen, examples of the two most frequent disturbances met in the power\nsystem are chosen. An adjustable speed drive with a six-pulse converter\nusing EMTP/ATP is designed and normal energizing of utility capacitors\nis presented . The analysis is tested on a system consisting of 13\nbuses and is representative of a medium-sized industrial plant.\nFinally, each kind of electrical disturbance is analyzed with examples\nrepresenting each tool. A qualitative comparison of results shows the\nadvantages and drawbacks of each signal processing technique applied to\npower quality analysis\n', ['discrete STFT', 'wavelets', 'power quality events', 'signal processing tools', 'signal processing techniques', 'wavelet filters', 'discrete short-time Fourier transforms', 'adjustable speed drive', 'six-pulse converter', 'EMTP/ATP', 'utility capacitors', 'medium-sized industrial plant', 'electrical disturbance', 'short-time Fourier transforms', 'EMTP', 'Fourier transforms', 'power capacitors', 'power supply quality', 'signal processing', 'variable speed drives', 'wavelet transforms']), ('HeLIN pilot mentoring scheme\nThe health care libraries unit coordinates, facilitates, and promotes\ncontinuing personal development for all staff in the Health Libraries\nand Information Network (HeLIN) of the Oxford Deanery (UK). It supports\nthe development of a culture of lifelong learning and recognizes that\nCPD should help deliver organizational objectives, as well as enabling\nall staff to expand and fulfill their potential. A major emphasis for\n2000 was to investigate ways of improving support for individual\nlearning within the workplace. The group identified a need to build on\nexisting informal support networks in order to provide additional\nlearning opportunities and decided to investigate the feasibility of\npiloting a mentoring scheme. The objectives of the pilot were to\nincrease understanding and knowledge of mentoring as a tool for CPD; to\ninvestigate existing mentoring schemes and their applicability for\nHeLIN; to develop a pilot mentoring scheme for HeLIN incorporating a\nprogram for accreditation of mentors; and to evaluate the scheme and\ndisseminate the results. In order to identify current practice in this\narea, a literature review was carried out, and colleagues with an\ninterest in or existing knowledge of mentoring schemes were contacted\nwhere possible. In the absence of clearly defined appraisal tools, all\nabstracts were read, and articles that met the following criteria were\nobtained and distributed to the group for review\n', ['HeLIN pilot mentoring scheme', 'health care libraries unit', 'continuing personal development', 'staff', 'Health Libraries and Information Network', 'lifelong learning', 'informal support networks', 'accreditation', 'literature review', 'midcareer librarians', 'continuing education', 'employment', 'human resource management', 'libraries', 'personnel', 'professional aspects']), ('Solution of a class of two-dimensional integral equations\nThe two-dimensional integral equation 1/ pi integral integral /sub D/( phi (r,\ntheta )/R/sup 2/)dS=f(r/sub 0/, theta /sub 0/) defined on a circular\ndisk D: r/sub 0/<or=a, 0<or= theta /sub 0/<or=2 pi , is\nconsidered in the present paper. Here R in the kernel denotes the\ndistance between two points P(r, theta ) and P/sub 0/(r/sub 0/, theta\n/sub 0/) in D, and 0< alpha <2 or 2< alpha <4. Based on\nsome known results of Bessel functions, integral representations of the\nkernel are established for 0< alpha <2 and 2< alpha <4,\nrespectively, and employed to solve the corresponding two-dimensional\nintegral equation. The solutions of the weakly singular integral\nequation for 0< alpha <2 and of the hypersingular integral\nequation for 2< alpha <4 are obtained, respectively\n', ['2D integral equations', 'circular disk', 'kernel', 'Bessel functions', 'integral representations', 'weakly singular integral equation', 'hypersingular integral equation', 'Bessel functions', 'integral equations']), ('A switching synchronization scheme for a class of chaotic systems\nIn this Letter, we propose an observer-based synchronization scheme for a class\nof chaotic systems. This class of systems are given by piecewise-linear\ndynamics. By using some properties of such systems, we give a procedure\nto construct the gain of the observer. We prove various stability\nresults and comment on the robustness of the proposed scheme. We also\npresent some simulation results\n', ['switching synchronization scheme', 'chaotic systems', 'piecewise-linear dynamics', 'robustness', 'state observers', 'asymptotic stability', 'chaos', 'nonlinear control systems', 'piecewise linear techniques', 'robust control', 'synchronisation']), ('The use of subtypes and stereotypes in the UML model\nBased on users\' experiences of Version 1.3 of the Unified Modeling Language\n(UML) of the Object Management Group (OMG), a Request For Information\nin 1999 elicited several responses which were asked to identify\n"problems" but not to offer any solutions. One of these responses is\nexamined for "problems" relating to the UML metamodel and here some\nsolutions to the problems identified there are proposed. Specifically,\nwe evaluate the metamodel relating to stereotypes versus subtypes; the\nvarious kinds of Classifier (particularly Types, Interfaces and\nClasses); the introduction of a new subtype for the whole part\nrelationship; as well as identifying areas in the metamodel where the\nUML seems to have been used inappropriately in the very definition of\nthe UML\'s metamodel\n', ['subtypes', 'stereotypes', 'UML model', 'Unified Modeling Language', 'Object Management Group', 'Request For Information', 'Classifier', 'whole part relationship', 'formal specification', 'object-oriented databases', 'object-oriented programming', 'specification languages']), ('Supply chain infrastructures: system integration and information sharing\nThe need for supply chain integration (SCI) methodologies has been increasing\nas a consequence of the globalization of production and sales, and the\nadvancement of enabling information technologies. In this paper, we\ndescribe our experience with implementing and modeling SCIs. We present\nthe integration architecture and the software components of our\nprototype implementation. We then discuss a variety of information\nsharing methodologies. Then, within the framework of a multi-echelon\nsupply chain process model spanning multiple organizations, we\nsummarize research on the benefits of intraorganizational knowledge\nsharing, and we discuss performance scalability\n', ['supply chain integration', 'supply chain infrastructures', 'system integration', 'information sharing', 'globalization', 'production', 'sales', 'software components', 'multi-echelon supply chain process model', 'multiple organizations', 'intraorganizational knowledge sharing', 'performance scalability', 'client-server systems', 'decision support systems', 'electronic data interchange', 'manufacturing resources planning', 'production control']), ("How closely can a personal computer clock track the UTC timescale via the\nInternet?\nNowadays many software packages allow you to keep the clock of your personal\ncomputer synchronized to time servers spread over the internet. We\npresent how a didactic laboratory can evaluate, in a statistical sense,\nthe minimum synch error of this process (the other extreme, the\nmaximum, is guaranteed by the code itself). The measurement set-up\nutilizes the global positioning system satellite constellation in\n'common view' between two similar timing stations: one acts as a time\nserver for the other, so the final timing difference at the second\nstation represents the total synch error through the internet. Data\nrecorded over batches of 10000 samples show a typical RMS value of 35\nms. This measurement configuration allows students to obtain a much\nbetter understanding of the synch task and pushes them, at all times,\nto look for an experimental verification of data results, even when\nthey come from the most sophisticated 'black boxes' now readily\navailable off the shelf\n", ['personal computer clock', 'UTC timescale', 'internet', 'software packages', 'time servers', 'didactic laboratory', 'statistical sense', 'global positioning system satellite constellation', 'final timing difference', 'synch error', 'black boxes', 'clocks', 'computer aided instruction', 'Internet', 'teaching']), ('Agents in e-commerce: state of the art\nThis paper surveys the state of the art of agent-mediated electronic commerce\n(e-commerce), especially in business-to-consumer (B2C) e-commerce and\nbusiness-to-business (B2B) e-commerce. From the consumer buying\nbehaviour perspective, the roles of agents in B2C e-commerce are:\nproduct brokering, merchant brokering, and negotiation. The\napplications of agents in B2B e-commerce are mainly in supply chain\nmanagement. Mobile agents, evolutionary agents, and data-mining agents\nare some special techniques which can be applied in agent-mediated\ne-commerce. In addition, some technologies for implementation are\nbriefly reviewed. Finally, we conclude this paper by discussions on the\nfuture directions of agent-mediated e-commerce\n', ['state of the art', 'agent-mediated electronic commerce', 'business-to-consumer e-commerce', 'consumer buying behaviour', 'product brokering', 'merchant brokering', 'negotiation', 'supply chain management', 'multi-agent systems', 'mobile agents', 'evolutionary agents', 'data-mining agents', 'business-to-business e-commerce', 'data mining', 'distributed programming', 'electronic commerce', 'multi-agent systems', 'production control']), ("Project-based learning: teachers learning and using high-tech to preserve Cajun\nculture\nUsing project-based learning pedagogy in EdTc 658 Advances in Educational\nTechnology, the author has trained inservice teachers in Southwestern\nLouisiana with an advanced computer multimedia program called\nDirector(R) (Macromedia, Inc.). The content of this course focused on\nmodeling the project-based learning pedagogy and researching Acadian's\ntraditions and legacy. With the multi-functions of microcomputers, new\ntechnologies were used to preserve and celebrate the local culture with\nsuperiority of text, graphics, animation, sound, and video. The article\ndescribes how several groups of school teachers in the surrounding\nareas of a regional state university of Louisiana learned computer\nmultimedia using project-based learning and integrated their learning\ninto local cultural heritage\n", ['project-based learning', 'teachers', 'Cajun culture', 'project-based learning pedagogy', 'EdTc 658 Advances in Educational Technology', 'inservice teachers', 'advanced computer multimedia program', 'Director', 'Acadian traditions', 'Macromedia', 'new technologies', 'local culture', 'school teachers', 'regional state university', 'computer multimedia', 'local cultural heritage', 'authoring systems', 'computer literacy', 'courseware', 'multimedia computing', 'social sciences computing', 'teacher training', 'teaching']), ("Gifts to a science academic librarian\nGifts, by their altruistic nature, perfectly fit into the environment of\nuniversities and academic libraries. As a university's community and\ngeneral public continue to donate materials, libraries accept donations\nwillingly, both in-kind and monetary. Eight steps of gift processing\nare listed in the paper. Positive and negative aspects of gift\nacceptance are discussed. Gifts bring value for academic libraries.\nGifts can be considered additional routes to contribute to library\ncollections without direct purchases, options to add money to the\nlibrary budget, and the cement of social relationships. But,\nunfortunately, large donations are time-consuming, labor-intensive and\ncostly to process. Great amounts of staff time and processing space are\ntwo main negative aspects that cause concern and put the value of gift\nacceptance under consideration by librarians. Some strategies in\nhandling gifts are recommended. To be effective, academic science\nlibrarians need to approach gifts as an investment. Librarians are not\nto be forced by moral and public notions and should be able to make\nprofessional decisions in evaluating proposed collections\n", ['science academic librarian', 'academic libraries', 'donations', 'gift processing', 'library collections', 'budget', 'staff time', 'professional decisions', 'research libraries', 'acquisitions', 'gift books', 'academic libraries', 'library automation', 'research libraries']), ('How should team captains order golfers on the final day of the Ryder Cup\nmatches?\nI used game theory to examine how team captains should select their slates for\nthe final day of the Ryder Cup matches. Under the assumption that\ngolfers have different abilities and are not influenced by pressure or\nmomentum, I found that drawing names from a hat will do no worse than\nany other strategy\n', ['golf', 'golfer ordering', 'Ryder Cup final day', 'game theory', 'slate', 'scheduling', 'sport']), ('Strain contouring using Gabor filters: principle and algorithm\nMoire interferometry is a powerful technique for high sensitivity in-plane\ndeformation contouring. However, from an engineering viewpoint, the\nderivatives of displacement, i.e., strain, are the desired parameter.\nThus there is a need to differentiate the displacement field. Optical\nand digital methods have been proposed for this differentiation.\nOptical methods provide contours that still need to be quantified,\nwhile digital methods suffer from drawbacks inherent in the digital\ndifferentiation process. We describe a novel approach of strain\nsegmentation for the moire pattern using a multichannel Gabor filter.\nAppropriate filter design allows for user-specific segmentation, which\nis essentially in engineering design and analysis\n', ['strain contouring', 'Gabor filters', 'algorithm', 'moire interferometry', 'high sensitivity in-plane deformation contouring', 'displacement', 'displacement field', 'digital methods', 'optical methods', 'differentiation', 'digital differentiation process', 'strain segmentation', 'multichannel Gabor filter', 'filter design', 'user-specific segmentation', 'engineering design', 'engineering analysis', 'image segmentation', 'spatial filters', 'deformation', 'image segmentation', 'light interferometry', 'moire fringes', 'nondestructive testing', 'spatial filters', 'strain measurement']), ('JPEG2000: standard for interactive imaging\nJPEG2000 is the latest image compression standard to emerge from the Joint\nPhotographic Experts Group (JPEG) working under the auspices of the\nInternational Standards Organization. Although the new standard does\noffer superior compression performance to JPEG, JPEG2000 provides a\nwhole new way of interacting with compressed imagery in a scalable and\ninteroperable fashion. This paper provides a tutorial-style review of\nthe new standard, explaining the technology on which it is based and\ndrawing comparisons with JPEG and other compression standards. The\npaper also describes new work, exploiting the capabilities of JPEG2000\nin client-server systems for efficient interactive browsing of images\nover the Internet\n', ['JPEG2000', 'interactive imaging', 'image compression', 'Joint Photographic Experts Group', 'International Standards Organization', 'scalable compression', 'interoperable compression', 'review', 'client-server systems', 'data compression', 'image coding', 'ISO standards']), ('Application of time-frequency principal component analysis to text-independent\nspeaker identification\nWe propose a formalism, called vector filtering of spectral trajectories, that\nallows the integration of a number of speech parameterization\napproaches (cepstral analysis, Delta and Delta Delta parameterizations,\nauto-regressive vector modeling, ...) under a common formalism. We then\npropose a new filtering, called contextual principal components (CPC)\nor time-frequency principal components (TFPC). This filtering consists\nin extracting the principal components of the contextual covariance\nmatrix, which is the covariance matrix of a sequence of vectors\nexpanded by their context. We apply this new filtering in the framework\nof closed-set speaker identification, using a subset of the POLYCOST\ndatabase. When using speaker-dependent TFPC filters, our results show a\nrelative improvement of approximately 20% compared to the use of the\nclassical cepstral coefficients augmented by their Delta -coefficients,\nwhich is significantly better with a 90% confidence level\n', ['time-frequency principal component analysis', 'text-independent speaker identification', 'vector filtering', 'spectral trajectories', 'speech parameterization', 'cepstral analysis', 'Delta Delta parameterization', 'Delta parameterization', 'auto-regressive vector modeling', 'contextual principal components', 'contextual covariance matrix', 'closed-set speaker identification', 'POLYCOST database', 'cepstral coefficients', 'confidence level', 'Delta -coefficients', 'covariance matrices', 'filtering theory', 'principal component analysis', 'speaker recognition', 'time-frequency analysis']), ("The effects of work pace on within-participant and between-participant keying\nforce, electromyography, and fatigue\nA laboratory study was conducted to determine the effects of work pace on\ntyping force, electromyographic (EMG) activity, and subjective\ndiscomfort. We found that as participants typed faster, their typing\nforce and finger flexor and extensor EMG activity increased linearly.\nThere was also an increase in subjective discomfort, with a sharp\nthreshold between participants' self-selected pace and their maximum\ntyping speed. The results suggest that participants self-select a\ntyping pace that maximizes typing speed and minimizes discomfort. The\nfastest typists did not produce significantly more finger flexor EMG\nactivity but did produce proportionately less finger extensor EMG\nactivity compared with the slower typists. We hypothesize that fast\ntypists may use different muscle recruitment patterns that allow them\nto be more efficient than slower typists at striking the keys. In\naddition, faster typists do not experience more discomfort than slow\ntypists. These findings show that the relative pace of typing is more\nimportant than actual typing speed with regard to discomfort and muscle\nactivity. These results suggest that typists may benefit from skill\ntraining to increase maximum typing speed. Potential applications of\nthis research includes skill training for typists\n", ['work pace effect', 'EMG activity', 'subjective discomfort', 'finger flexor', 'typing speed', 'discomfort', 'typists', 'muscle recruitment patterns', 'keying force', 'skill training', 'electromyography', 'ergonomics', 'human factors', 'man-machine systems']), ("A method for correlations analysis of coordinates: applications for molecular\nconformations\nWe describe a new method to analyze multiple correlations between subsets of\ncoordinates that represent a sample. The correlation is established\nonly between specific regions of interest at the coordinates. First,\nthe region(s) of interest are selected at each molecular coordinate.\nNext, a correlation matrix is constructed for the selected regions. The\nmatrix is subject to further analysis, illuminating the\nmultidimensional structural characteristics that exist in the\nconformational space. The method's abilities are demonstrated in\nseveral examples: it is used to analyze the conformational space of\ncomplex molecules, it is successfully applied to compare related\nconformational spaces, and it is used to analyze a diverse set of\nprotein folding trajectories\n", ['multiple correlation analysis', 'regions of interest', 'correlation matrix', 'molecular coordinate', 'multidimensional structural characteristics', 'complex molecules', 'conformational spaces', 'protein folding trajectories', 'molecular conformations', 'biology computing', 'macromolecules', 'molecular biophysics', 'molecular configurations', 'proteins']), ('Information systems project failure: a comparative study of two countries\nMany organizations, regardless of size, engage in at least one, and often many\ninformation system projects each year. Many of these projects consume\nmassive amounts of resources, and may cost as little as a few thousand\ndollars to ten, and even hundreds of millions of dollars. Needless to\nsay, the investment of time and resources into these ventures are of\nsignificant concern to chief information officers (CIOs), executives\nstaff members, project managers, and others in leadership positions.\nThis paper describes the results of a survey performed between\nAustralia and the United States regarding factors leading to IS project\nfailure. The findings suggest that, among other things, end user\ninvolvement and executive management leadership are key indicators\ninfluencing IS project failure\n', ['information systems project failure', 'Australia', 'United States', 'end user involvement', 'executive management leadership', 'information systems', 'project management', 'software development management', 'systems analysis']), ('Portal dose image prediction for dosimetric treatment verification in\nradiotherapy. II. An algorithm for wedged beams\nA method is presented for calculation of a two-dimensional function, T/sub\nwedge/(x,y), describing the transmission of a wedged photon beam\nthrough a patient. This in an extension of the method that we have\npublished for open (nonwedged) fields [Med. Phys. 25, 830-840 (1998)].\nTransmission functions for open fields are being used in our clinic for\nprediction of portal dose images (PDI, i.e., a dose distribution behind\nthe patient in a plane normal to the beam axis), which are compared\nwith PDIs measured with an electronic portal imaging device (EPID). The\ncalculations are based on the planning CT scan of the patient and on\nthe irradiation geometry as determined in the treatment planning\nprocess. Input data for the developed algorithm for wedged beams are\nderived from (the already available) measured input data set for\ntransmission prediction in open beams, which is extended with only a\nlimited set of measurements in the wedged beam. The method has been\ntested for a PDI plane at 160 cm from the focus, in agreement with the\napplied focus-to-detector distance of our fluoroscopic EPIDs. For low\nand high energy photon beams (6 and 23 MV) good agreement (~1%) has\nbeen found between calculated and measured transmissions for a slab and\na thorax phantom\n', ['portal dose image prediction', 'dosimetric treatment verification', 'radiotherapy', 'transmission dosimetry', 'wedged beams algorithm', 'two-dimensional function', 'wedged photon beam', 'planning CT scan', 'irradiation geometry', 'low energy photon beams', 'high energy photon beams', 'slab phantom', 'thorax phantom', 'in vivo dosimetry', 'electronic portal imaging devices', 'fluoroscopic CCD camera', 'pencil beam algorithm', 'CadPlan planning system', 'open beams', 'virtual wedges', '6 MV', '23 MV', 'computerised tomography', 'dosimetry', 'medical image processing', 'Monte Carlo methods', 'radiation therapy']), ('Optimal learning for patterns classification in RBF networks\nThe proposed modifying of the structure of the radial basis function (RBF)\nnetwork by introducing the weight matrix to the input layer (in\ncontrast to the direct connection of the input to the hidden layer of a\nconventional RBF) so that the training space in the RBF network is\nadaptively separated by the resultant decision boundaries and class\nregions is reported. The training of this weight matrix is carried out\nas for a single-layer perceptron together with the clustering process.\nIn this way the network is capable of dealing with complicated\nproblems, which have a high degree of interference in the training\ndata, and achieves a higher classification rate over the current\nclassifiers using RBF\n', ['pattern classification', 'optimal learning', 'RBF networks', 'radial basis function network', 'weight matrix training', 'input layer', 'training space', 'decision boundaries', 'class regions', 'single-layer perceptron', 'clustering process', 'classification rate improvement', 'learning (artificial intelligence)', 'pattern classification', 'radial basis function networks']), ('Mammogram synthesis using a 3D simulation. II. Evaluation of synthetic\nmammogram texture\nWe have evaluated a method for synthesizing mammograms by comparing the texture\nof clinical and synthetic mammograms. The synthesis algorithm is based\nupon simulations of breast tissue and the mammographic imaging process.\nMammogram texture was synthesized by projections of simulated adipose\ntissue compartments. It was hypothesized that the synthetic and\nclinical texture have similar properties, assuming that the mammogram\ntexture reflects the 3D tissue distribution. The size of the projected\ncompartments was computed by mathematical morphology. The texture\nenergy and fractal dimension were also computed and analyzed in terms\nof the distribution of texture features within four different tissue\nregions in clinical and synthetic mammograms. Comparison of the\ncumulative distributions of the mean features computed from 95\nmammograms showed that the synthetic images simulate the mean features\nof the texture of clinical mammograms. Correlation of clinical and\nsynthetic texture feature histograms, averaged over all images, showed\nthat the synthetic images can simulate the range of features seen over\na large group of mammograms. The best agreement with clinical texture\nwas achieved for simulated compartments with radii of 4-13.3 mm in\npredominantly adipose tissue regions, and radii of 2.7-5.33 and 1.3-2.7\nmm in retroareolar and dense fibroglandular tissue regions,\nrespectively\n', ['mammogram synthesis', '3D simulation', 'synthetic mammogram texture', 'breast tissue simulation', 'adipose tissue compartments', '3D tissue distribution', 'mathematical morphology', 'fractal dimension', 'cumulative distributions', 'synthetic images', 'dense fibroglandular tissue regions', 'retroareolar tissue regions', 'X-ray image acquisition', 'computationally compressed phantom', 'biological tissues', 'computer graphics', 'diagnostic radiography', 'fractals', 'image registration', 'image texture', 'mammography', 'mathematical morphology', 'medical image processing', 'physiological models']), ('Successive expansion method of network planning applying symbolic analysis\nmethod\nThe conventional power system network successive expansion planning method is\ndiscussed in the context of the new paradigm of competitive electric\npower, energy and service market. In sequel, the paper presents an\napplication of the conceptually new computer program, based on the\nsymbolic analysis of load flows in power system networks. The network\nparameters and variables are defined as symbols. The symbolic analyzer,\nwhich models analytically the power system DC load flows, enables the\nsensitivity analysis of the power system to parameter and variable\nvariations (costs, transfers, injections), a valuable tool for the\nexpansion planning analysis. That virtue could not be found within the\nconventional approach, relying on compensation methods, precalculated\ndistribution factors, and so on. This novel application sheds some\nlight on the traditional power system network expansion planning\nmethod, as well as on its possible application within the system\nnetwork expansion planning in the new environment assuming the\ncompetitive electric power market\n', ['power system network successive expansion planning', 'competitive electric power market', 'competitive electric energy market', 'competitive electric service market', 'computer program', 'symbolic analysis', 'load flows', 'symbolic analyzer', 'power system DC load flows', 'sensitivity analysis', 'compensation methods', 'precalculated distribution factors', 'power system network expansion planning method', 'load flow', 'power system analysis computing', 'power system planning', 'symbol manipulation']), ("A knowledge-based approach for managing urban infrastructures\nThis paper presents a knowledge e-based approach dedicated to the efficient\nmanagement, regulation, interactive and dynamic monitoring of urban\ninfrastructures. This approach identifies the data and related\ntreatments common to several municipal activities and defines the\nrequirements and functionalities of the computer tools developed to\nimprove the delivery and coordination of municipal services to the\npopulation. The resulting cooperative system called SIGIU is composed\nof a set of integrated operating systems (SYDEX) and the global\nplanning and coordination system (SYGEC). The objective is to integrate\nthe set of SYDEX and the SYGEC into a single coherent system for all\nthe SIGIU's users according to their tasks, their roles, and their\nresponsibilities within the municipal administration. SIGIU is provided\nby different measurement and monitoring instruments installed on some\nsystem's elements to be supervised. In this context, the information\ncan be presented in different forms: video, pictures, data and alarms.\nOne of SIGIU's objectives is the real-time management of urban\ninfrastructures' control mechanisms. To carry out this process, the\nalarm control agent creates a mobile agent associated with the alarm,\nwhich is sent to a mobile station and warns an operator. Preliminary\nimplementation results show that SIGIU supports effectively and\nefficiently the decision making process related to managing urban\ninfrastructures\n", ['knowledge-based approach', 'urban infrastructure management', 'regulation', 'dynamic monitoring', 'municipal activities', 'cooperative system', 'SIGIU', 'integrated operating systems', 'SYDEX', 'global planning system', 'coordination system', 'SYGEC', 'video', 'real-time management', 'alarm control agent', 'mobile agent', 'intelligent decision support system', 'urban planning', 'multi-agent systems', 'decision support systems', 'multi-agent systems', 'operating systems (computers)', 'planning (artificial intelligence)', 'real-time systems', 'town and country planning']), ("Keeping Web accessibility in mind: I&R services for all\nAfter presenting three compelling reasons for making Web sites accessible to\npersons with a broad range of disabilities (it's the morally right\nthing to do, it's the smart thing to do from an economic perspective,\nand it's required by law), the author discusses design issues that\nimpact persons with particular types of disabilities. She presents\npractical advice for assessing and addressing accessibility problems.\nAn extensive list of resources for further information is appended, as\nis a list of sites which simulate the impact of specific accessibility\nproblems on persons with disabilities\n", ['Web site accessibility', 'information and referral services', 'disabilities', 'handicapped aids', 'information resources', 'information retrieval']), ('Strategies for high throughput, templated zeolite synthesis\nThe design and redesign of high throughput experiments for zeolite synthesis\nare addressed. A model that relates materials function to the chemical\ncomposition of the zeolite and the structure directing agent is\nintroduced. Using this model, several Monte Carlo-like design protocols\nare evaluated. Multi-round protocols are bound to be effective, and\nstrategies that use a priori information about the structure-directing\nlibraries are found to be the best\n', ['templated zeolite synthesis', 'high throughput strategies', 'materials function', 'chemical composition', 'structure directing agent', 'Monte Carlo-like design protocols', 'multi-round protocols', 'a priori information', 'catalytic activity', 'catalytic selectivity', 'organo-cation template molecules', 'combinatorial methods', 'random energy model', 'figure of merit', 'material discovery', 'small molecule design', 'Voronoi diagram', 'phase-dependent random Gaussian variables', 'Metropolis-type method', 'ligand libraries', 'reflecting boundary conditions', 'catalysts', 'chemical structure', 'chemistry computing', 'combinatorial mathematics', 'computational geometry', 'design of experiments', 'materials preparation', 'Monte Carlo methods', 'random processes', 'zeolites']), ('Search for efficient solutions of multi-criterion problems by target-level\nmethod\nThe target-level method is considered for solving continuous multi-criterion\nmaximization problems. In the first step, the decision-maker specifies\na target-level point (the desired criterion values); then in the set of\nvector evaluations we seek points that are closest to the target point\nin the Chebyshev metric. The vector evaluations obtained in this way\nare in general weakly efficient. To identify the efficient evaluations,\nthe second step maximizes the sum of the criteria on the set generated\nin step 1. We prove the relationship between the evaluations and\ndecisions obtained by the proposed procedure, on the one hand, and the\nefficient (weakly efficient) evaluations and decisions, on the other\nhand. If the Edgeworth-Pareto hull of the set of vector evaluations is\nconvex, the set of efficient vector evaluations can be approximated by\nthe proposed method\n', ['multi-criterion problems', 'target-level method', 'continuous multi-criterion maximization problems', 'target-level point', 'Chebyshev metric', 'Edgeworth-Pareto hull', 'Chebyshev approximation', 'operations research']), ('Genetic algorithm-neural network estimation of Cobb angle from torso asymmetry\nin scoliosis\nScoliosis severity, measured by the Cobb angle, was estimated by artificial\nneural network from indices of torso surface asymmetry using a genetic\nalgorithm to select the optimal set of input torso indices. Estimates\nof the Cobb angle were accurate within 5 degrees in two-thirds, and\nwithin 10 degrees in six-sevenths, of a test set of 115 scans of 48\nscoliosis patients, showing promise for future longitudinal studies to\ndetect scoliosis progression without use of X-rays\n', ['Cobb angle', 'artificial neural network', 'torso surface asymmetry', 'genetic algorithm', 'input torso indices', 'scoliosis patients', 'scoliosis progression', 'angular measurement', 'bone', 'diagnostic radiography', 'diseases', 'genetic algorithms', 'neural nets']), ('Design of a stroke dependent damper for the front axle suspension of a truck\nusing multibody system dynamics and numerical optimization\nA stroke dependent damper is designed for the front axle suspension of a truck.\nThe damper supplies extra damping for inward deflections rising above 4\ncm. In this way the damper should reduce extreme suspension deflections\nwithout deteriorating the comfort of the truck. But the question is\nwhich stroke dependent damping curve yields the best compromise between\nsuspension deflection working space and comfort. Therefore an\noptimization problem is defined to minimize the maximum inward\nsuspension deflection subject to constraints on the chassis\nacceleration for three typical road undulations. The optimization\nproblem is solved using sequential linear programming (SLP) and\nmultibody dynamics simulation software. Several optimization runs have\nbeen carried out for a small two degree of freedom vehicle model and a\nlarge full-scale model of the truck semi-trailer combination. The\nresults show that the stroke dependent damping can reduce large\ndeflections at incidental road disturbances, but that the optimum\nstroke dependent damping curve is related to the acceleration bound. By\nmeans of vehicle model simulation and numerical optimization we have\nbeen able to quantify this trade-off between suspension deflection\nworking space and truck comfort\n', ['stroke dependent damper', 'front axle suspension', 'multibody system dynamics', 'numerical optimization', 'inward deflections', 'extreme suspension deflections', 'chassis acceleration', 'road undulations', 'sequential linear programming', 'full-scale model', 'truck semi-trailer combination', 'damping', 'incidental road disturbances', 'acceleration bound', 'vehicle model simulation', 'truck comfort', 'damping', 'digital simulation', 'dynamics', 'linear programming', 'mechanical variables control', 'road vehicles']), ('From a biological to a computational model for the autonomous behavior of an\nanimat\nEndowing an autonomous system like a robot with intelligent behavior is\ndifficult for several reasons. First, behavior is such a wide topic\nthat a general framework paradigm of inspiration must be chosen in\norder to obtain a consistent model. Such a framework can be, for\nexample, biological modeling or an artificial intelligence approach.\nSecond, a general framework is not sufficient to determine a fully\nspecified program to be implemented in a robot. Many choices, tuning\nand tests must be carried out before obtaining a robust system. A\nbiological model is presented, based on the definition of cortex-like\nautomata, representing elementary functions in the perceptive, motor or\nassociative domain. These automata are connected in a network whose\narchitecture, functioning and learning rules are described in a\ncortical framework. Second, the computational model derived from that\nbiological model is specified. The way units exchange and compute\nvariables through links is explained, with reference to corresponding\nbiological elements. It is then easier to report experiments allowing\nan autonomous system to learn regularities of a simple environment and\nto exploit them to satisfy some internal drives. Even if additional\nbiological hints can be added, this model allow us to better understand\nhow a biological model can be implemented and how biological properties\ncan emerge from a distributed set of units\n', ['autonomous system', 'robot', 'autonomous behavior', 'intelligent behavior', 'animat', 'tuning', 'tests', 'robust system', 'biological model', 'cortex-like automata', 'elementary functions', 'perceptive domain', 'associative domain', 'motor domain', 'learning rules', 'architecture', 'computational model', 'variable computation', 'variable exchange', 'links', 'regularity learning', 'simple environment', 'internal drives', 'artificial life', 'automata theory', 'learning (artificial intelligence)']), ('Solutions for cooperative games\nA new concept of the characteristic function is defined. It matches cooperative\ngames far better than the classical characteristic function and is\nuseful in reducing the number of decisions that can be used as the\nunique solution of a game\n', ['cooperative games', 'characteristic function', 'decisions', 'unique solution', 'transferrable utility', 'decision theory', 'functions', 'game theory']), ('The evolution of information systems: Their impact on organizations and\nstructures\nInformation systems and organization structures have been highly interconnected\nwith each other. Over the years, information systems architectures as\nwell as organization structures have evolved from centralized to more\ndecentralized forms. This research looks at the evolution of both\ninformation systems and organization structures. In the process, it\nlooks into the impact of computers on organizations, and examines the\nways organization structures have changed, in association with changes\nin information system architectures. It also suggests logical linkages\nbetween information system architectures and their "fit" with certain\norganization structures and strategies. It concludes with some\nimplications for emerging and future organizational forms, and provides\na quick review of the effect of the Internet on small businesses\ntraditionally using stand-alone computers\n', ['information systems evolution', 'information system architectures', 'DP management', 'information systems', 'social aspects of automation']), ('Cache invalidation and replacement strategies for location-dependent data in\nmobile environments\nMobile location-dependent information services (LDISs) have become increasingly\npopular in recent years. However, data caching strategies for LDISs\nhave thus far received little attention. In this paper, we study the\nissues of cache invalidation and cache replacement for\nlocation-dependent data under a geometric location model. We introduce\na new performance criterion, called caching efficiency, and propose a\ngeneric method for location-dependent cache invalidation strategies. In\naddition, two cache replacement policies, PA and PAID, are proposed.\nUnlike the conventional replacement policies, PA and PAID take into\nconsideration the valid scope area of a data value. We conduct a series\nof simulation experiments to study the performance of the proposed\ncaching schemes. The experimental results show that the proposed\nlocation-dependent invalidation scheme is very effective and the PA and\nPAID policies significantly outperform the conventional replacement\npolicies\n', ['mobile computing', 'location-dependent information', 'cache replacement', 'cache invalidation', 'semantic caching', 'performance evaluation', 'mobile location-dependent information services', 'data caching', 'cache storage', 'mobile computing', 'performance evaluation', 'storage management']), ('Deriving model parameters from field test measurements [generator control\nsimulation]\nA major component of any power system simulation is the generating plant. The\npurpose of DeriveAssist is to speed up the parameter derivation process\nand to allow engineers less versed in parameter matching and\nidentification to get involved in the process of power plant electric\ngenerator modelling\n', ['DeriveAssist', 'parameter derivation process', 'parameter matching', 'parameter identification', 'power system simulation', 'turbine/governor', 'power system stability analysis', 'computer simulation', 'generator parameter derivation process', 'steady-state parameters derivation', 'control simulation', 'control system analysis computing', 'electric generators', 'electric machine analysis computing', 'machine control', 'machine testing', 'machine theory', 'parameter estimation', 'power station control', 'power system stability']), ('An intelligent fuzzy decision system for a flexible manufacturing system with\nmulti-decision points\nThis paper describes an intelligent fuzzy decision support system for real-time\nscheduling and dispatching of parts in a flexible manufacturing system\n(FMS), with alternative routing possibilities for all parts. A fuzzy\nlogic approach is developed to improve the system performance by\nconsidering multiple performance measures and at multiple decision\npoints. The characteristics of the system status, instead of parts, are\nfed back to assign priority to the parts waiting to be processed. A\nsimulation model is developed and it is shown that the proposed\nintelligent fuzzy decision support system keeps all performance\nmeasures at a good level. The proposed intelligent system is a\npromising tool for dealing with scheduling FMSs, in contrast to\ntraditional rules\n', ['flexible manufacturing system', 'FMS', 'fuzzy logic', 'multiple decision points', 'intelligent decision support system', 'real-time system', 'scheduling', 'simulation', 'computer aided production planning', 'decision support systems', 'flexible manufacturing systems', 'fuzzy logic', 'production control', 'real-time systems']), ('Critical lines identification on voltage collapse analysis\nThis paper deals with critical lines identification on voltage collapse\nanalysis. It is known, from the literature, that voltage collapse is a\nlocal phenomenon that spreads around an initial neighborhood Therefore,\nidentifying the system critical bus plays an important role on voltage\ncollapse prevention. For this purpose, the system critical transmission\nlines should also be identified In this paper, these issues are\naddressed, yielding reliable results in a short computational time.\nTests are done with the help of the IEEE-118 bus and the Southeastern\nBrazilian systems\n', ['power system voltage collapse analysis', 'critical transmission lines identification', 'local phenomenon', 'system critical bus identification', 'IEEE-118 bus', 'computer simulation', 'Brazil', 'control system analysis computing', 'power system analysis computing', 'power system dynamic stability', 'power system identification', 'power transmission lines', 'transmission line theory', 'transmission network calculations']), ('Time-varying properties of renal autoregulatory mechanisms\nIn order to assess the possible time-varying properties of renal\nautoregulation, time-frequency and time-scaling methods were applied to\nrenal blood flow under broad-band forced arterial blood pressure\nfluctuations and single-nephron renal blood flow with spontaneous\noscillations obtained from normotensive (Sprague-Dawley, Wistar, and\nLong-Evans) rats, and spontaneously hypertensive rats. Time-frequency\nanalyses of normotensive and hypertensive blood flow data obtained from\neither the whole kidney or the single-nephron show that indeed both the\nmyogenic and tubuloglomerular feedback (TGF) mechanisms have\ntime-varying characteristics. Furthermore, we utilized the Renyi\nentropy to measure the complexity of blood-flow dynamics in the\ntime-frequency plane in an effort to discern differences between\nnormotensive and hypertensive recordings. We found a clear difference\nin Renyi entropy between normotensive and hypertensive blood flow\nrecordings at the whole kidney level for both forced (p < 0.037) and\nspontaneous arterial pressure fluctuations (p < 0.033), and at the\nsingle-nephron level (p < 0.008). Especially at the single-nephron\nlevel, the mean Renyi entropy is significantly larger for hypertensive\nthan normotensive rats, suggesting more complex dynamics in the\nhypertensive condition. To further evaluate whether or not the\nseparation of dynamics between normotensive and hypertensive rats is\nfound in the prescribed frequency ranges of the myogenic and TGF\nmechanisms, we employed multiresolution wavelet transform. Our analysis\nrevealed that exclusively over scale ranges corresponding to the\nfrequency intervals of the myogenic and TGF mechanisms, the widths of\nthe blood flow wavelet coefficients fall into disjoint sets for\nnormotensive and hypertensive rats. The separation of the scales at the\nmyogenic and TGF frequency ranges is distinct and obtained with 100%\naccuracy. However, this observation remains valid only for the whole\nkidney blood pressure/flow data. The results suggest that understanding\nof the time-varying properties of the two mechanisms is required for a\ncomplete description of renal autoregulation\n', ['time-varying properties', 'Sprague-Dawley rats', 'Wistar rats', 'Long-Evans rats', 'whole kidney', 'single-nephron', 'Renyi entropy', 'spontaneous arterial pressure fluctuations', 'hypertensive rats', 'normotensive rats', 'renal autoregulatory mechanisms', 'broad-band forced arterial blood pressure fluctuations', 'single-nephron renal blood flow', 'spontaneous oscillations', 'biocontrol', 'blood flow measurement', 'entropy', 'kidney', 'medical signal processing', 'time-frequency analysis', 'wavelet transforms']), ('Synthesis of the control systems via reflection onto auxiliary surfaces\nAn approach to robust control systems synthesis, both linear and nonlinear, and\nnonstationary is offered. The control is carried out, providing the\ngiven phase constraints varied in acceptable limits, in view of\nconstraints on its value and incompleteness of the information about\nfunctioning disturbances. The approach is based on the introduction of\nauxiliary integral surfaces, on which the initial moving is projected.\nAs a result the reduced equivalent moving is formed, being described by\nthe scalar equation which in many important cases can be integrated\ndirectly. On the basis of the equation obtained solving a synthesis\ntask is carried out and can be reduced to algebraic or integral\ninequalities. The final relations defined for linear equivalent moving\nare presented\n', ['control systems synthesis', 'auxiliary surfaces', 'robust control systems', 'linear systems', 'nonlinear systems', 'algebraic inequalities', 'integral inequalities', 'linear equivalent moving', 'algebra', 'control system synthesis', 'integral equations', 'linear systems', 'nonlinear control systems', 'robust control']), ('Design and manufacture of a lightweight piezo-composite curved actuator\nIn this paper we are concerned with the design, manufacture and performance\ntest of a lightweight piezo-composite curved actuator (called LIPCA)\nusing a top carbon fiber composite layer with near-zero coefficient of\nthermal expansion (CTE), a middle PZT ceramic wafer, and a bottom\nglass/epoxy layer with a high CTE. The main point of the design for\nLIPCA is to replace the heavy metal layers of THUNDER TM by lightweight\nfiber reinforced plastic layers without losing the capabilities for\ngenerating high force and large displacement. It is possible to save up\nto about 40% of the weight if we replace the metallic backing material\nby the light fiber composite layer. We can also have design flexibility\nby selecting the fiber direction and the size of prepreg layers. In\naddition to the lightweight advantage and design flexibility, the\nproposed device can be manufactured without adhesive layers when we use\nan epoxy resin prepreg system. Glass/epoxy prepregs, a ceramic wafer\nwith electrode surfaces, and a carbon prepreg were simply stacked and\ncured at an elevated temperature (177 degrees C) after following an\nautoclave bagging process. We found that the manufactured composite\nlaminate device had a sufficient curvature after being detached from a\nflat mould. An analysis method using the classical lamination theory is\npresented to predict the curvature of LIPCA after curing at an elevated\ntemperature. The predicted curvatures are in quite good agreement with\nthe experimental values. In order to investigate the merits of LIPCA,\nperformance tests of both LIPCA and THUNDER TM have been conducted\nunder the same boundary conditions. From the experimental actuation\ntests, it was observed that the developed actuator could generate\nlarger actuation displacement than THUNDER TM\n', ['performance test', 'lightweight piezo-composite curved actuator', 'LIPCA', 'carbon fiber composite layer', 'near-zero coefficient of thermal expansion', 'PZT ceramic wafer', 'glass/epoxy layer', 'fiber reinforced plastic layers', 'predicted curvatures', 'performance tests', 'boundary conditions', 'THUNDER', '177 degrees C', '177 degC', 'glass fibre reinforced composites', 'intelligent actuators', 'intelligent structures', 'piezoelectric actuators', 'thermal expansion']), ("A three-source model for the calculation of head scatter factors\nAccurate determination of the head scatter factor S/sub c/ is an important\nissue, especially for intensity modulated radiation therapy, where the\nsegmented fields are often very irregular and much less than the\ncollimator jaw settings. In this work, we report an S/sub c/\ncalculation algorithm for symmetric, asymmetric, and irregular open\nfields shaped by the tertiary collimator (a multileaf collimator or\nblocks) at different source-to-chamber distance. The algorithm was\nbased on a three-source model, in which the photon radiation to the\npoint of calculation was treated as if it originated from three\neffective sources: one source for the primary photons from the target\nand two extra-focal photon sources for the scattered photons from the\nprimary collimator and the flattening filter, respectively. The field\nmapping method proposed by Kim et al. [Phys. Med. Biol. 43, 1593-1604\n(1998)] was extended to two extra-focal source planes and the scatter\ncontributions were integrated over the projected areas (determined by\nthe detector's eye view) in the three source planes considering the\nsource intensity distributions. The algorithm was implemented using\nMicrosoft Visual C/C++ in the MS Windows environment. The only input\ndata required were head scatter factors for symmetric square fields,\nwhich are normally acquired during machine commissioning. A large\nnumber of different fields were used to evaluate the algorithm and the\nresults were compared with measurements. We found that most of the\ncalculated S/sub c/'s agreed with the measured values to within 0.4%.\nThe algorithm can also be easily applied to deal with irregular fields\nshaped by a multileaf collimator that replaces the upper or lower\ncollimator jaws\n", ['three-source model', 'head scatter factors', 'intensity modulated radiation therapy', 'segmented fields', 'collimator jaw settings', 'calculation algorithm', 'symmetric', 'fields', 'asymmetric', 'fields', 'irregular open fields', 'tertiary collimator', 'multileaf collimator', 'blocks', 'source-to-chamber distance', 'photon radiation', 'target', 'extra-focal photon sources', 'scattered photons', 'primary collimator', 'flattening filter', 'field mapping method', 'extra-focal source planes', 'source intensity distributions', 'MS Windows environment', 'input data', 'symmetric square fields', 'machine commissioning', 'upper collimator jaws', 'lower collimator jaws', 'algorithm theory', 'dosimetry', 'intensity modulation', 'physiological models', 'radiation therapy', 'X-ray scattering']), ('Quantum computing with solids\nScience and technology could be revolutionized by quantum computers, but\nbuilding them from solid-state devices will not be easy. The author\noutlines the challenges in scaling up the technology from lab\nexperiments to practical devices\n', ['quantum computers', 'solid-state devices', 'quantum computing']), ("Plenoptic image editing\nThis paper presents a new class of interactive image editing operations\ndesigned to maintain consistency between multiple images of a physical\n3D scene. The distinguishing feature of these operations is that edits\nto any one image propagate automatically to all other images as if the\n(unknown) 3D scene had itself been modified. The modified scene can\nthen be viewed interactively from any other camera viewpoint and under\ndifferent scene illuminations. The approach is useful first as a\npower-assist that enables a user to quickly modify many images by\nediting just a few, and second as a means for constructing and editing\nimage-based scene representations by manipulating a set of photographs.\nThe approach works by extending operations like image painting,\nscissoring, and morphing so that they alter a scene's plenoptic\nfunction in a physically-consistent way, thereby affecting scene\nappearance from all viewpoints simultaneously. A key element in\nrealizing these operations is a new volumetric decomposition technique\nfor reconstructing an scene's plenoptic function from an incomplete set\nof camera viewpoints\n", ['interactive image editing operations', 'multiple images', 'physical 3D scene', 'modified scene', 'camera viewpoint', 'image-based scene representations', 'image painting', 'scissoring', 'morphing', 'plenoptic function', 'volumetric decomposition technique', 'plenoptic image editing', 'image morphing', 'rendering (computer graphics)']), ('A high-resolution high-frequency monolithic top-shooting microinjector free of\nsatellite drops - part II: fabrication, implementation, and\ncharacterization\nFor pt. I, see ibid., vol. 11, no. 5, p. 427-36 (2002). Describes the\nfabrication, implementation and characterization of a thermal driven\nmicroinjector, featuring a bubble check valve and monolithic\nfabrication. Microfabrication of this microinjector is based on\nbulk/surface-combined micromachining of the silicon wafer, free of the\nbonding process that is commonly used in the fabrication of commercial\nprinting head, so that even solvents and fuels can be ejected. Droplet\nejection sequences of two microinjectors have been studied along with a\ncommercial inkjet printhead for comparison. The droplet ejection of our\nmicroinjector with 10 mu m diameter nozzle has been characterized at a\nfrequency over 35 kHz, at least 3 times higher than those of commercial\ncounterparts. The droplet volume from this device is smaller than 1 pl,\n10 times smaller than those of commercial inkjets employed in the\nconsumer market at the time of testing. Visualization results have\nverified that our design, although far from being optimized, operates\nin the frequency several times higher than those of commercial products\nand reduces the crosstalk among neighboring chambers\n', ['monolithic top-shooting microinjector', 'satellite drops', 'thermal driven microinjector', 'bubble check valve', 'bulk/surface-combined micromachining', 'bonding process', 'inkjet printhead', 'nozzle', 'droplet volume', 'consumer market', 'crosstalk', '10 micron', '35 kHz', 'bubbles', 'crosstalk', 'drops', 'ink jet printers', 'microfluidics', 'micromachining', 'nozzles']), ('A variable-stepsize variable-order multistep method for the integration of\nperturbed linear problems\nG. Scheifele (1971) wrote the solution of a perturbed oscillator as an\nexpansion in terms of a new set of functions, which extends the\nmonomials in the Taylor series of the solution. Recently, P. Martin and\nJ.M. Ferrandiz (1997) constructed a multistep code based on the\nScheifele technique, and it was generalized by D.J. Lopez and P. Martin\n(1998) for perturbed linear problems. However, the remarked codes are\nconstant steplength methods, and efficient integrators must be able to\nchange the steplength. In this paper we extend the ideas of F.T. Krogh\n(1974) from Adams methods to the algorithm proposed by Lopez and\nMartin, and we show the advantages of the new code in perturbed\nproblems\n', ['variable-stepsize variable-order multistep method', 'perturbed linear problems integration', 'perturbed oscillator', 'monomials', 'Taylor series', 'multistep code', 'constant steplength methods', 'Adams methods', 'codes', 'extrapolation', 'iterative methods', 'series (mathematics)']), ('Geometric source separation: merging convolutive source separation with\ngeometric beamforming\nConvolutive blind source separation and adaptive beamforming have a similar\ngoal-extracting a source of interest (or multiple sources) while\nreducing undesired interferences. A benefit of source separation is\nthat it overcomes the conventional cross-talk or leakage problem of\nadaptive beamforming. Beamforming on the other hand exploits geometric\ninformation which is often readily available but not utilized in blind\nalgorithms. We propose to join these benefits by combining cross-power\nminimization of second-order source separation with geometric linear\nconstraints used in adaptive beamforming. We find that the geometric\nconstraints resolve some of the ambiguities inherent in the\nindependence criterion such as frequency permutations and degrees of\nfreedom provided by additional sensors. We demonstrate the new method\nin performance comparisons for actual room recordings of two and three\nsimultaneous acoustic sources\n', ['geometric source separation', 'geometric beamforming', 'convolutive blind source separation', 'adaptive beamforming', 'cross-talk', 'leakage problem', 'blind algorithms', 'cross-power minimization', 'second-order source separation', 'geometric linear constraints', 'frequency permutations', 'degrees of freedom', 'sensors', 'room recordings', 'acoustic sources', 'acoustic signal processing', 'array signal processing', 'convolution', 'minimisation']), ('Human factors research on data modeling: a review of prior research, an\nextended framework and future research directions\nThis study reviews and synthesizes human factors research on conceptual data\nmodeling. In addition to analyzing the variables used in earlier\nstudies and summarizing the results of this stream of research, we\npropose a new framework to help with future efforts in this area. The\nstudy finds that prior research has focused on issues that are relevant\nwhen conceptual models are used for communication between systems\nanalysts and developers (Analyst Developer models) whereas the issues\nimportant for models that are used to facilitate communication between\nanalysts and users (User-Analyst models) have received little attention\nand, hence, require a significantly stronger role in future research.\nIn addition, we emphasize the importance of building a strong\ntheoretical foundation and using it to guide future empirical work in\nthis area\n', ['human factors', 'conceptual data modeling', 'future efforts', 'Analyst Developer models', 'User-Analyst models', 'database', 'data models', 'database management systems', 'human factors']), ("Reaching for five nines: ActiveWatch and SiteSeer\nEvery Web admin's dream is achieving the fabled five nines-99.999 percent\nuptime. To attain such availability, your Web site must be down no more\nthan about five minutes per year. Technologies like RAID, clustering,\nand load balancing make this easier, but to actually track uptime,\nmaintain auditable records, and discover patterns in failures to\nprevent downtime in the future, you'll need to set up external\nmonitoring. Because your Internet connection is a key factor in\nmeasuring uptime, you must monitor your site from the Internet itself,\nbeyond your firewall. You could monitor with custom software on remote\nhosts, or you could use one of the two reasonably priced services\navailable: Mercury Interactive's ActiveWatch and Freshwater Software's\nSiteSeer. (Freshwater Software has been a subsidiary of Mercury\nInteractive for about a year now.) The two services offer a slightly\ndifferent mix of features and target different markets. Both services\noffer availability and performance monitoring from several remote\nlocations, alerts to email or pager, and periodic reports. They differ\nin what's most easily monitored, and in the way you interact with the\nservices\n", ['Web site', 'uptime tracking', 'auditable records', 'failure pattern discovery', 'downtime', 'external monitoring', 'Internet connection', 'Mercury Interactive ActiveWatch', 'Freshwater Software SiteSeer', 'performance monitoring', 'availability monitoring', 'remote locations', 'email alerts', 'pager alerts', 'periodic reports', 'Internet', 'system monitoring', 'system recovery']), ('Psychology and the Internet\nThis article presents an overview of the way that the Internet is being used to\nassist psychological research and mediate psychological practice. It\nshows how psychologists are using the Internet to examine the\ninteractions between people and computers, and highlights some of the\nways that this research is important to the design and development of\nuseable and acceptable computer systems. In particular, this\nintroduction reviews the research presented at the International\nConference on Psychology and the Internet held in the United Kingdom.\nThe final part introduces the eight articles in this special edition.\nThe articles are representative of the breadth of research being\nconducted on psychology and the Internet: there are two on\nmethodological issues, three on group processes, one on organizational\nimplications, and two on social implications of Internet use\n', ['Internet', 'psychological research', 'human-computer interactions', 'usability', 'social implications', 'psychology', 'organizational implications', 'group processes', 'methodological issues', 'online research', 'human factors', 'Internet', 'psychology', 'user interfaces']), ("Two efficient algorithms for the generalized maximum balanced flow problem\nMinoux (1976) considered the maximum balanced flow problem, i.e. the problem of\nfinding a maximum flow in a two-terminal network N = (V,A) with source\ns and sink t satisfying the constraint that any arc-flow of N is\nbounded by a fixed proportion of the total flow value from s to t,\nwhere V is vertex set and A is arc set. As a generalization, we focus\non the problem of maximizing the total flow value of a generalized flow\nin N with gains gamma (a) > 0 (a in A) where any arc-flow is bounded\nby a fixed proportion of the total flow value, where gamma (a)f(a)\nunits arrive at the vertex w for each arc-flow f(a) (a identical to (\nupsilon , w) in A) entering vertex upsilon in a generalized flow. Our\nmain results are to propose two polynomial algorithms for this problem.\nThe first algorithm runs in O(mM(n, m, B') log B) time, where B is the\nmaximum absolute value among integral values used by an instance of the\nproblem, and M(n, m, B') denotes the complexity of solving a\ngeneralized maximum flow problem in a network with n vertices, and m\narcs, and a rational instance expressed with integers between 1 and B'.\nIn the second algorithm, using a parameterized technique, runs in\nO({M(n, m, B')}/sup 2/) time\n", ['generalized maximum balanced flow problem', 'two-terminal network', 'polynomial algorithms', 'parameterized technique', 'computational complexity', 'graph theory', 'optimisation']), ('Recursive state estimation for multiple switching models with unknown\ntransition probabilities\nThis work considers hybrid systems with continuous-valued target states and\ndiscrete-valued regime variable. The changes (switches) of the regime\nvariable are modeled by a finite state Markov chain with unknown and\nrandom transition probabilities following Dirichlet distributions. Our\nwork analytically derives the marginal posterior distribution of the\nstates and regime variables, the transition probabilities being\nintegrated out. This leads to a variety of recursive hybrid state\nestimation schemes which are an appealing intuitive and straightforward\nextension of standard algorithms. Their performance is illustrated by a\nmaneuvering target tracking example\n', ['recursive state estimation', 'multiple switching models', 'unknown transition probabilities', 'hybrid systems', 'continuous-valued target states', 'discrete-valued regime variable', 'finite state Markov chain', 'random transition probabilities', 'Dirichlet distributions', 'marginal posterior distribution', 'maneuvering target tracking', 'Bayes methods', 'Markov processes', 'probability', 'state estimation', 'target tracking']), ('Low-voltage DRAM sensing scheme with offset-cancellation sense amplifier\nA novel bitline sensing scheme is proposed for low-voltage DRAM to achieve low\npower dissipation and compatibility with low-voltage CMOS. One of the\nmajor obstacles in low-voltage DRAM is the degradation of\ndata-retention time due to low signal level at the memory cell, which\nrequires power-consuming refresh operations more frequently. This paper\nproposes an offset-cancellation sense-amplifier scheme (OCSA) that\nimproves data-retention time significantly even at low supply voltage.\nIt also improves die efficiency, because the proposed scheme reduces\nthe number of sense amplifiers by supporting more cells in each sense\namplifier. Measurements show that the data-retention time of the\nproposed scheme at 1.5-V supply voltage is 2.4 times of the\nconventional scheme at 2.0 V\n', ['LV DRAM sensing scheme', 'low-voltage sensing scheme', 'offset-cancellation sense amplifier scheme', 'bitline sensing scheme', 'low power dissipation', 'low-voltage CMOS compatibility', 'data-retention time', 'memory cell', 'differential amplifier configuration', 'power-consuming refresh operations', 'sensing margin', '1.5 V', 'CMOS memory circuits', 'differential amplifiers', 'DRAM chips', 'low-power electronics']), ('Generating code at run time with Reflection.Emit\nThe .NET framework SDK includes several tools that convert source code into\nexecutable code-the C# and VB.NET compilers get most of the attention,\nbut there are others. The Regex class (in the\nSystem.Text.RegularExpressions namespace) has the ability to compile\nfavorite regular expressions into a .NET assembly. In fact, the NET\nCommon Language Runtime (CLR) contains a whole namespace full of\nclasses to help us build assemblies, define types, and emit their\nimplementations, all at run time. These classes, which comprise the\nSystem.Reflection.Emit namespace, are known collectively as Reflection.\nEmit\n', ['.NET framework SDK', 'runtime code generation', 'Regex class', '.NET Common Language Runtime', 'assemblies', 'types', 'System.Reflection.Emit namespace', 'Reflection.Emit', 'network operating systems', 'program compilers']), ('Car-caravan snaking. 1. The influence of pintle pin friction\nA brief review of knowledge of car-caravan snaking is carried out. Against the\nbackground described, a fairly detailed mathematical model of a\ncontemporary car-trailer system is constructed and a baseline set of\nparameter values is given. In reduced form, the model is shown to give\nresults in accordance with literature. The properties of the baseline\ncombination are explored using both linear and non-linear versions of\nthe model. The influences of damping at the pintle joint and of several\nother design parameters on the stability of the linear system in the\nneighbourhood of the critical snaking speed are calculated and\ndiscussed. Coulomb friction damping at the pintle pin is then included\nand simulations are used to indicate the consequent amplitude-dependent\nbehaviour. The friction damping, especially when its level has to be\nchosen by the user, is shown to give dangerous characteristics, despite\nhaving some capacity for stabilization of the snaking motions. It is\nconcluded that pintle pin friction damping does not represent a\nsatisfactory solution to the snaking problem. The paper sets the scene\nfor the development of an improved solution\n', ['car-caravan snaking', 'pintle pin friction', 'mathematical model', 'car-trailer system', 'linear system', 'amplitude-dependent behaviour', 'critical snaking speed', 'Coulomb friction damping', 'automobiles', 'dynamics', 'friction', 'motion control', 'vibration control']), ('The treatment of fear of flying: a controlled study of imaginal and virtual\nreality graded exposure therapy\nThe goal of this study was to determine if virtual reality graded exposure\ntherapy (VRGET) was equally efficacious, more efficacious, or less\nefficacious, than imaginal exposure therapy in the treatment of fear of\nflying. Thirty participants (Age=39.8+or-9.7) with confirmed DSM-IV\ndiagnosis of specific phobia fear of flying were randomly assigned to\none of three groups: VRGET with no physiological feedback (VRGETno),\nVRGET with physiological feedback (VRGETpm), or systematic\ndesensitization with imaginal exposure therapy (IET). Eight sessions\nwere conducted once a week. During each session, physiology was\nmeasured to give an objective measurement of improvement over the\ncourse of exposure therapy. In addition, self-report questionnaires,\nsubjective ratings of anxiety (SUDs), and behavioral observations\n(included here as flying behavior before beginning treatment and at a\nthree-month posttreatment followup) were included. In the analysis of\nresults, the Chi-square test of behavioral observations based on a\nthree-month posttreatment followup revealed a statistically significant\ndifference in flying behavior between the groups [ chi /sup\n2/(4)=19.41, p<0.001]. Only one participant (10%) who received IET,\neight of the ten participants (80%) who received VRGETno, and ten out\nof the ten participants (100%) who received VRGETpm reported an ability\nto fly without medication or alcohol at three-month followup. Although\nthis study included small sample sizes for the three groups, the\nresults showed VRGET was more effective than IET in the treatment of\nflying. It also suggests that physiological feedback may add to the\nefficacy of VR treatment\n', ['flying fear', 'virtual reality graded exposure therapy', 'imaginal exposure therapy', 'phobia', 'physiological feedback', 'physiology', 'questionnaires', 'subjective ratings of anxiety', 'behavioral observations', 'Chi-square test', 'patient treatment', 'medical computing', 'patient treatment', 'psychology', 'user interfaces', 'virtual reality']), ("Quick media response averts PR disaster\nSometimes it's not what you do, but how you do it. After hackers broke the\nblocking code on the home version of its popular Cyber Patrol Internet\nfiltering software and posted it on the Internet, marketers at\nMicrosystems Software pulled out a playbook of standard crisis\nmanagement and PR techniques. But the Cyber Patrol PR team including\noutside PR counsel and the company's outside law firm, used those tools\naggressively in order to turn the tide of public and media opinion away\nfrom the hackers, who initially were hailed as folk heroes, and in\nfavor of the company's interests, to save the product's and the\ncompany's reputations and inherent value. And the entire team managed\nto move at Internet speed: The crisis was essentially over in about\nthree weeks\n", ['media response', 'Cyber Patrol Internet filtering software', 'Microsystems Software', 'crisis management', 'public relations', 'Internet', 'marketing']), ('Use of fuzzy weighted autocorrelation function for pitch extraction from noisy\nspeech\nAn investigation is presented into the feasibility of incorporating a fuzzy\nweighting scheme into the calculation of an autocorrelation function\nfor pitch extraction. Simulation results reveal that the proposed\nmethod provides better robustness against background noise than the\nconventional approaches for extracting pitch period in a noisy\nenvironment\n', ['pitch extraction', 'noisy speech', 'fuzzy weighting scheme', 'autocorrelation function', 'simulation results', 'background noise', 'speech analysis-synthesis system', 'average magnitude difference function', 'cepstrum method', 'AWGN', 'correlation methods', 'fuzzy systems', 'speech processing']), ('Inverse problems for a mathematical model of ion exchange in a compressible ion\nexchanger\nA mathematical model of ion exchange is considered, allowing for ion exchanger\ncompression in the process of ion exchange. Two inverse problems are\ninvestigated for this model, unique solvability is proved, and\nnumerical solution methods are proposed. The efficiency of the proposed\nmethods is demonstrated by a numerical experiment\n', ['inverse problems', 'mathematical model', 'ion exchange', 'compressible ion exchanger', 'ion exchanger compression', 'unique solvability', 'numerical solution methods', 'computability', 'inverse problems', 'ion exchange', 'mathematical programming']), ('Analytic PCA construction for theoretical analysis of lighting variability in\nimages of a Lambertian object\nWe analyze theoretically the subspace best approximating images of a convex\nLambertian object taken from the same viewpoint, but under different\ndistant illumination conditions. We analytically construct the\nprincipal component analysis for images of a convex Lambertian object,\nexplicitly taking attached shadows into account, and find the principal\neigenmodes and eigenvalues with respect to lighting variability. Our\nanalysis makes use of an analytic formula for the irradiance in terms\nof spherical-harmonic coefficients of the illumination and shows, under\nappropriate assumptions, that the principal components or eigenvectors\nare identical to the spherical harmonic basis functions evaluated at\nthe surface normal vectors. Our main contribution is in extending these\nresults to the single-viewpoint case, showing how the principal\neigenmodes and eigenvalues are affected when only a limited subset (the\nupper hemisphere) of normals is available and the spherical harmonics\nare no longer orthonormal over the restricted domain. Our results are\nvery close, both qualitatively and quantitatively, to previous\nempirical observations and represent the first essentially complete\ntheoretical explanation of these observations\n', ['analytic principal component analysis', 'spherical harmonics', 'lighting variability', 'five-dimensional subspace', 'convex Lambertian object', 'surface normal vectors', 'principal eigenmodes', 'principal eigenvalues', 'radiance', 'irradiance', 'computer vision', 'eigenvalues and eigenfunctions', 'lighting', 'object recognition', 'principal component analysis']), ("Social presence in telemedicine\nWe studied consultations between a doctor, emergency nurse practitioners (ENPs)\nand their patients in a minor accident and treatment service (MATS). In\nthe conventional consultations, all three people were located at the\nmain hospital. In the teleconsultations, the doctor was located in a\nhospital 6 km away from the MATS and used a videoconferencing link\nconnected at 384 kbit/s. There were 30 patients in the conventional\ngroup and 30 in the telemedical group. The presenting problems were\nsimilar in the two groups. The mean duration of teleconsultations was\n951 s and the mean duration of face-to-face consultations was 247 s. In\ndoctor-nurse communication there was a higher rate of turn taking in\nteleconsultations than in face-to-face consultations; there were also\nmore interruptions, more words and more `backchannels' (e.g. `mhm',\n`uh-huh') per teleconsultation. In doctor-patient communication there\nwas a higher rate of turn taking, more words, more interruptions and\nmore backchannels per teleconsultation. In patient-nurse communication\nthere was. relatively little difference between the two modes of\nconsulting the doctor. Telemedicine appeared to empower the patient to\nask more questions of the doctor. It also seemed that the doctor took\ngreater care in a teleconsultation to achieve coordination of beliefs\nwith the patient than in a face-to-face consultation\n", ['social presence', 'telemedicine', 'doctor', 'emergency nurse practitioners', 'patients', 'minor accident and treatment service', 'teleconsultations', 'videoconferencing link', 'face-to-face consultations', 'doctor-nurse communication', 'interruptions', 'backchannels', 'words', 'turn taking', 'patient-nurse communication', 'belief coordination', '384 kbit/s', '951 s', '247 s', 'biomedical communication', 'patient care', 'social aspects of automation', 'teleconferencing', 'telemedicine']), ('eLeaders make the Web work\nSome companies are making the most of back-office/Web integration. Here are\nsome winners\n', ['back-office/Web integration', 'e-commerce', 'Visual Integrator', 'Accpac eTransact', 'accounting', 'electronic commerce']), ('Solution of the safe problem on (0,1)-matrices\nA safe problem with mn locks is studied. It is reduced to a system of linear\nequations in the modulo 2 residue class. There are three possible\nvariants defined by the numbers m and n evenness, with only one of them\nhaving a solution. In two other cases, correction of the initial state\nof the safe insuring a solution is proposed\n', ['safe problem', 'mn locks', 'linear equations', 'modulo 2 residue class', '(0;1)-matrices', 'computer games', 'linear Diophantine equations', 'computer games', 'matrix algebra']), ('Bivariate fractal interpolation functions on rectangular domains\nNon-tensor product bivariate fractal interpolation functions defined on gridded\nrectangular domains are constructed. Linear spaces consisting of these\nfunctions are introduced. The relevant Lagrange interpolation problem\nis discussed. A negative result about the existence of affine fractal\ninterpolation functions defined on such domains is obtained\n', ['bivariate fractal interpolation functions', 'rectangular domains', 'gridded rectangular domains', 'linear spaces', 'Lagrange interpolation problem', 'affine fractal interpolation functions', 'fractals', 'interpolation']), ('Modeling and simulating practices, a work method for work systems design\nWork systems involve people engaging in activities over time-not just with each\nother, but also with machines, tools, documents, and other artifacts.\nThese activities often produce goods, services, or-as is the case in\nthe work system described in this article-scientific data. Work systems\nand work practice evolve slowly over time. The integration and use of\ntechnology, the distribution and collocation of people, organizational\nroles and procedures, and the facilities where the work occurs largely\ndetermine this evolution\n', ['work practice simulation', 'work practice modeling', 'work system design method', 'complex system interactions', 'human activity', 'communication', 'collaboration', 'teamwork', 'tool usage', 'workspace usage', 'problem solving', 'learning behavior', 'digital simulation', 'user centred design']), ('Evaluating the complexity of index sets for families of general recursive\nfunctions in the arithmetic hierarchy\nThe complexity of index sets of families of general recursive functions is\nevaluated in the Kleene-Mostowski arithmetic hierarchy\n', ['index sets complexity', 'general recursive functions', 'arithmetic hierarchy', 'Kleene-Mostowski arithmetic hierarchy', 'computational complexity', 'recursive functions']), ('The perceived utility of human and automated aids in a visual detection task\nAlthough increases in the use of automation have occurred across society,\nresearch has found that human operators often underutilize (disuse) and\noverly rely on (misuse) automated aids (Parasuraman-Riley (1997)).\nNearly 275 Cameron University students participated in 1 of 3\nexperiments performed to examine the effects of perceived utility\n(Dzindolet et al. (2001)) on automation use in a visual detection task\nand to compare reliance on automated aids with reliance on humans.\nResults revealed a bias for human operators to rely on themselves.\nAlthough self-report data indicate a bias toward automated aids over\nhuman aids, performance data revealed that participants were more\nlikely to disuse automated aids than to disuse human aids. This\ndiscrepancy was accounted for by assuming human operators have a\n"perfect automation" schema. Actual or potential applications of this\nresearch include the design of future automated decision aids and\ntraining procedures for operators relying on such aids\n', ['automated aids', 'visual detection task', 'human operators', 'automated decision aids', 'social process', 'automation', 'human factors', 'man-machine systems', 'social aspects of automation']), ("Recent researches of human science on railway systems\nThis paper presents research of human science on railway systems at RTRI. They\nare roughly divided into two categories: research to improve safety and\nthose to improve comfort. On the former subject, for the safeguard\nagainst accidents caused by human errors, we have promoted studies of\npsychological aptitude test, various research to evaluate train\ndrivers' working conditions and environments, and new investigations to\nminimize the risk of passenger casualties at train accidents. On the\nlatter subject, we have developed new methods to evaluate the riding\ncomfort including that of tilt train, and started research on the\nimprovement of railway facilities for the aged and the disabled from\nthe viewpoint of universal design\n", ['human science', 'RTRI', 'safety improvement', 'comfort improvement', 'railway systems', 'accidents', 'human errors', 'psychological aptitude test', "train drivers' working conditions", "train drivers' working environments", 'passenger casualties risk minimisation', 'train accidents', 'riding comfort', 'tilt train', 'railway facilities', 'aged persons', 'disabled persons', 'sight impaired', 'wakefulness level', 'ergonomics', 'accidents', 'ergonomics', 'human factors', 'psychology', 'railways', 'risk management', 'safety']), ('Data extraction from the Web based on pre-defined schema\nWith the development of the Internet, the World Wide Web has become an\ninvaluable information source for most organizations. However, most\ndocuments available from the Web are in HTML form which is originally\ndesigned for document formatting with little consideration of its\ncontents. Effectively extracting data from such documents remains a\nnontrivial task. In this paper, we present a schema-guided approach to\nextracting data from HTML pages. Under the approach, the user defines a\nschema specifying what to be extracted and provides sample mappings\nbetween the schema and the HTML page. The system will induce the\nmapping rules and generate a wrapper that takes the HTML page as input\nand produces the required data in the form of XML conforming to the\nuser-defined schema. A prototype system implementing the approach has\nbeen developed. The preliminary experiments indicate that the proposed\nsemi-automatic approach is not only easy to use but also able to\nproduce a wrapper that extracts required data from inputted pages with\nhigh accuracy\n', ['data extraction', 'wrapper generation', 'data integration', 'Internet', 'information source', 'schema', 'HTML', 'distributed database', 'queries', 'distributed databases', 'hypermedia markup languages', 'information resources', 'query processing']), ("A meteorological fuzzy expert system incorporating subjective user input\nWe present a fuzzy expert system, MEDEX, for forecasting gale-force winds in\nthe Mediterranean basin. The most successful local wind forecasting in\nthis region is achieved by an expert human forecaster with access to\nnumerical weather prediction products. That forecaster's knowledge is\nexpressed as a set of 'rules-of-thumb'. Fuzzy set methodologies have\nproved well suited for encoding the forecaster's knowledge, and for\naccommodating the uncertainty inherent in the specification of rules,\nas well as in subjective and objective input. MEDEX uses fuzzy set\ntheory in two ways: as a fuzzy rule base in the expert system, and for\nfuzzy pattern matching to select dominant wind circulation patterns as\none input to the expert system. The system was developed, tuned, and\nverified over a two-year period, during which the weather conditions\nfrom 539 days were individually analyzed. Evaluations of MEDEX\nperformance for both the onset and cessation of winter and summer winds\nare presented, and demonstrate that MEDEX has forecasting skill\ncompetitive with the US Navy's regional forecasting center in Rota,\nSpain\n", ['meteorological fuzzy expert system', 'subjective user input', 'MEDEX', 'gale-force wind forecasting', 'Mediterranean basin', 'numerical weather prediction products', 'rules-of-thumb', 'fuzzy set theory', 'subjective variables', 'uncertainty', 'rule specification', 'fuzzy rule base', 'fuzzy pattern matching', 'wind circulation patterns', 'expert systems', 'fuzzy logic', 'fuzzy set theory', 'geophysics computing', 'pattern matching', 'uncertainty handling', 'weather forecasting']), ('Driving the NKK Smartswitch.2. Graphics and text\nWhether your message is one of workplace safety or world peace, the long nights\nof brooding over ways to tell the world are over. Part 1 described the\nbasic interface to drive the Smartswitch. Part 2 adds the bells and\nwhistles to allow both text and messages to be placed anywhere on the\nscreen. It considers character generation, graphic generation and the\nuser interface\n', ['NKK Smartswitch', 'computer graphics', 'text', 'messages', 'character generation', 'graphic generation', 'user interface', 'character sets', 'computer graphics', 'user interfaces']), ('A distributed mobile agent framework for maintaining persistent distance\neducation\nMobile agent techniques involve distributed control if communication is\nrequired among different types of agents, especially when mobile agents\ncan migrate from station to station. This technique can be implemented\nin a distributed distance learning environment, which allows students\nor instructors to login from anywhere to a central server in an\neducation center while still retaining the look-and-feel of personal\nsetups. In this research paper, we propose a distributed agent\nframework along with its communication messages to facilitate mobile\npersonal agents, which serve three different groups of distance\neducation users: instructors, students, and system administrators. We\npropose an agent communication framework as well as agent evolution\nstates of mobile agents. The communication architecture and message\ntransmission protocols are illustrated. The system is implemented on\nthe Windows platform to support nomadic accessibility of remote\ndistance learning users. Personal data also migrate with the mobile\nagents, allowing users to maintain accessibility to some extent even\nwhen the Internet connection is temperately disconnected. Using\nuser-friendly personal agents, a distance education platform can\ninclude different tools to meet different needs for users\n', ['distributed mobile agent framework', 'persistent distance education', 'distributed control', 'central server', 'distributed agent framework', 'message transmission protocols', 'user-friendly personal agents', 'computer aided instruction', 'distance learning', 'Internet', 'software agents']), ('Fidelity of quantum teleportation through noisy channels\nWe investigate quantum teleportation through noisy quantum channels by solving\nanalytically and numerically a master equation in the Lindblad form. We\ncalculate the fidelity as a function of decoherence rates and angles of\na state to be teleported. It is found that the average fidelity and the\nrange of states to be accurately teleported depend on types of noises\nacting on quantum channels. If the quantum channels are subject to\nisotropic noise, the average fidelity decays to 1/2, which is smaller\nthan the best possible value of 2/3 obtained only by the classical\ncommunication. On the other hand, if the noisy quantum channel is\nmodeled by a single Lindblad operator, the average fidelity is always\ngreater than 2/3\n', ['fidelity', 'quantum teleportation', 'noisy quantum channels', 'analytical solution', 'numerical solution', 'quantum channels', 'classical communication', 'Lindblad operator', 'Alice', 'Bob', 'sender', 'recipient', 'dual classical channels', 'eigenstate', 'isotropic noise', 'quantum communication']), ('A typed representation for HTML and XML documents in Haskell\nWe define a family of embedded domain specific languages for generating HTML\nand XML documents. Each language is implemented as a combinator library\nin Haskell. The generated HTML/XML documents are guaranteed to be\nwell-formed. In addition, each library can guarantee that the generated\ndocuments are valid XML documents to a certain extent (for HTML only a\nweaker guarantee is possible). On top of the libraries, Haskell serves\nas a meta language to define parameterized documents, to map structured\ndocuments to HTML/XML, to define conditional content, or to define\nentire Web sites. The combinator libraries support element-transforming\nstyle, a programming style that allows programs to have a visual\nappearance similar to HTML/XML documents, without modifying the syntax\nof Haskell\n', ['typed representation', 'HTML documents', 'combinator library', 'software libraries', 'meta language', 'parameterized documents', 'conditional content', 'Web sites', 'element-transforming style', 'functional programming', 'syntax', 'XML documents', 'Haskell', 'embedded domain specific languages', 'functional languages', 'functional programming', 'hypermedia markup languages', 'software libraries', 'type theory']), ("Evaluating the best main battle tank using fuzzy decision theory with\nlinguistic criteria evaluation\nIn this paper, experts' opinions are described in linguistic terms which can be\nexpressed in trapezoidal (or triangular) fuzzy numbers. To make the\nconsensus of the experts consistent, we utilize the fuzzy Delphi method\nto adjust the fuzzy rating of every expert to achieve the consensus\ncondition. For the aggregate of many experts' opinions, we take the\noperation of fuzzy numbers to get the mean of fuzzy rating, x/sub ij/\nand the mean of weight, w/sub .j/. In multi-alternatives and\nmulti-attributes cases, the fuzzy decision matrix X=[x/sub ij/]/sub\nm*n/ is constructed by means of the fuzzy rating, x/sub ij/. Then, we\ncan derive the aggregate fuzzy numbers by multiplying the fuzzy\ndecision matrix with the corresponding fuzzy attribute weights. The\nfinal results become a problem of ranking fuzzy numbers. We also\npropose an easy procedure of using fuzzy numbers to rank aggregate\nfuzzy numbers A/sub i/. In this way, we can obtain the best selection\nfor evaluating the system. For practical application, we propose an\nalgorithm for evaluating the best main battle tank by fuzzy decision\ntheory and comparing it with other methods\n", ['battle tank evaluation', 'fuzzy group decision making', 'fuzzy decision theory', 'linguistic criteria evaluation', 'multiple criteria problems', 'group decision making', 'subjective-objective backgrounds', 'trapezoidal fuzzy numbers', 'triangular fuzzy numbers', 'fuzzy Delphi method', 'fuzzy rating', 'consensus condition', 'fuzzy number ranking', 'fuzzy decision matrix', 'aggregate fuzzy numbers', 'fuzzy attribute weights', 'computational linguistics', 'decision theory', 'fuzzy set theory', 'group decision support systems', 'military computing', 'military systems']), ('Estimation of error in curvature computation on multi-scale free-form surfaces\nA novel technique for multi-scale curvature computation on a free-form 3-D\nsurface is presented. This is achieved by convolving local\nparametrisations of the surface with 2-D Gaussian filters iteratively.\nIn our technique, semigeodesic coordinates are constructed at each\nvertex of the mesh. Smoothing results are shown for 3-D surfaces with\ndifferent shapes indicating that surface noise is eliminated and\nsurface details are removed gradually. A number of evolution properties\nof 3-D surfaces are described. Next, the surface Gaussian and mean\ncurvature values are estimated accurately at multiple scales which are\nthen mapped to colours and displayed directly on the surface. The\nperformance of the technique when selecting different directions as an\narbitrary direction for the geodesic at each vertex are also presented.\nThe results indicate that the error observed for the estimation of\nGaussian and mean curvatures is quite low after only one iteration.\nFurthermore, as the surface is smoothed iteratively, the error is\nfurther reduced. The results also show that the estimation error of\nGaussian curvature is less than that of mean curvature. Our experiments\ndemonstrate that estimation of smoothed surface curvatures are very\naccurate and not affected by the arbitrary direction of the first\ngeodesic line when constructing semigeodesic coordinates. Our technique\nis independent of the underlying triangulation and is also more\nefficient than volumetric diffusion techniques since 2-D rather than\n3-D convolutions are employed. Finally, the method presented here is a\ngeneralisation of the Curvature Scale Space method for 2-D contours.\nThe CSS method has outperformed comparable techniques within the MPEG-7\nevaluation framework. As a result, it has been selected for inclusion\nin the MPEG-7 package of standards\n', ['multi-scale curvature computation', 'free-form 3D surface', 'local parametrisations', '2D Gaussian filters', 'surface noise', 'evolution properties', 'surface Gaussian values', 'mean curvature values', 'semigeodesic coordinates', 'underlying triangulation', 'volumetric diffusion techniques', 'convolutions', 'Curvature Scale Space method', 'MPEG-7 evaluation framework', 'computational geometry', 'computer vision', 'object recognition']), ('Relativistic constraints on the distinguishability of orthogonal quantum states\nThe constraints imposed by special relativity on the distinguishability of\nquantum states are discussed. An explicit expression relating the\nprobability of an error in distinguishing two orthogonal single-photon\nstates to their structure, the time t at which a measurement starts,\nand the interval of time T elapsed from the start of the measurement\nuntil the time at which the outcome is obtained by an observer is given\nas an example\n', ['relativistic constraints', 'orthogonal quantum states', 'special relativity', 'orthogonal single-photon states', 'time interval', 'observer', 'nonrelativistic quantum information theory', 'quantum communication channels', 'quantum-state distinguishability', 'information theory', 'quantum communication', 'quantum theory', 'special relativity']), ('Convergence of Runge-Kutta methods for nonlinear parabolic equations\nWe study time discretizations of fully nonlinear parabolic differential\nequations. Our analysis uses the fact that the linearization along the\nexact solution is a uniformly sectorial operator. We derive smooth and\nnonsmooth-data error estimates for the backward Euler method, and we\nprove convergence for strongly A (v)-stable Runge-Kutta methods. For\nthe latter, the order of convergence for smooth solutions is\nessentially determined by the stage order of the method. Numerical\nexamples illustrating the convergence estimates are presented\n', ['Runge-Kutta method convergence', 'time discretizations', 'linearization', 'uniformly sectorial operator', 'nonsmooth-data error estimates', 'data error estimates', 'backward Euler method', 'nonlinear parabolic differential equations', 'convergence of numerical methods', 'error analysis', 'nonlinear differential equations', 'parabolic equations', 'Runge-Kutta methods']), ('Sampled-data implementation of a gain scheduled controller\nA continuous-time gain-scheduled controller must be transformed to a\ncorresponding discrete-time controller for sampled-data implementation.\nWe show that certain linearization properties of a continuous-time gain\nscheduled controller are inherited by its sampled-data implementation.\nWe also show that a similar relationship exists for multi-rate gain\nscheduled controllers arising in flight control applications\n', ['gain scheduled controller', 'sampled-data implementation', 'continuous-time gain-scheduled controller', 'discrete-time controller', 'linearization properties', 'multi-rate gain scheduled controllers', 'flight control applications', 'aerospace control', 'continuous time systems', 'control system synthesis', 'discrete time systems', 'linearisation techniques', 'nonlinear control systems', 'sampled data systems']), ('Determinantal solutions of solvable chaotic systems\nIt is shown that two solvable chaotic systems, the arithmetic-harmonic mean\n(ARM) algorithm and the Ulam-von Neumann (UvN) map, have determinantal\nsolutions. An additional formula for certain determinants and Riccati\ndifference equations play a key role in both cases. Two infinite\nhierarchies of solvable chaotic systems are presented which have\ndeterminantal solutions\n', ['determinantal solutions', 'arithmetic-harmonic mean algorithm', 'solvable chaotic systems', 'Ulam-von Neumann map', 'determinants', 'Riccati difference equations', 'Chebyshev polynomial', 'Chebyshev approximation', 'determinants', 'difference equations', 'polynomials', 'Riccati equations']), ('New projection-type methods for monotone LCP with finite termination\nIn this paper we establish two new projection-type methods for the solution of\nthe monotone linear complementarity problem (LCP). The methods are a\ncombination of the extragradient method and the Newton method, in which\nthe active set strategy is used and only one linear system of equations\nwith lower dimension is solved at each iteration. It is shown that\nunder the assumption of monotonicity, these two methods are globally\nand linearly convergent. Furthermore, under a nondegeneracy condition\nthey have a finite termination property. Finally, the methods are\nextended to solving the monotone affine variational inequality problem\n', ['projection-type methods', 'monotone LCP', 'finite termination', 'monotone linear complementarity problem', 'extragradient method', 'Newton method', 'active set strategy', 'linear system of equations', 'iteration', 'monotonicity', 'convergence', 'nondegeneracy condition', 'monotone affine variational inequality problem', 'matrix', 'vector', 'convergence of numerical methods', 'gradient methods', 'matrix algebra', 'Newton method', 'set theory', 'vectors']), ('Single-phase half-bridge converter topology for power quality compensation\nA high power factor half-bridge rectifier with neutral point switch clamped\nscheme is proposed. Three power switches are employed in the proposed\nrectifier. Two PWM control schemes are used to draw a sinusoidal line\ncurrent with low current distortion. The control signals of the power\nswitches are derived from the DC link voltage balance compensator, line\ncurrent controller and DC link voltage regulator. The hysteresis\ncurrent control scheme is employed to track the line current command.\nThe proposed control scheme and the circuit configuration can be\napplied to the active power filter to eliminate the harmonic currents\nand compensate the reactive power generated from the nonlinear load.\nAnalytical and experimental results are included to illustrate the\nvalidity and effectiveness of the proposed control scheme\n', ['single-phase half-bridge rectifier topology', 'neutral point switch clamped scheme', 'PWM control schemes', 'power quality compensation', 'sinusoidal line current', 'current distortion', 'power switches control signals', 'DC link voltage balance compensator', 'line current controller', 'DC link voltage regulator', 'hysteresis current control scheme', 'line current command tracking', 'harmonic currents elimination', 'circuit configuration', 'AC-DC power convertors', 'bridge circuits', 'electric current control', 'harmonic distortion', 'power conversion harmonics', 'power supply quality', 'power system control', 'PWM power convertors', 'rectifying circuits', 'switching circuits', 'voltage control']), ('Selective representing and world-making\nWe discuss the thesis of selective representing-the idea that the contents of\nthe mental representations had by organisms are highly constrained by\nthe biological niches within which the organisms evolved. While such a\nthesis has been defended by several authors elsewhere, our primary\nconcern here is to take up the issue of the compatibility of selective\nrepresenting and realism. We hope to show three things. First, that the\nnotion of selective representing is fully consistent with the realist\nidea of a mind-independent world. Second, that not only are these two\nconsistent, but that the latter (the realist conception of a\nmind-independent world) provides the most powerful perspective from\nwhich to motivate and understand the differing perceptual and cognitive\nprofiles themselves. Third, that the (genuine and important) sense in\nwhich organism and environment may together constitute an integrated\nsystem of scientific interest poses no additional threat to the realist\nconception\n', ['world-making', 'selective representing', 'mental representations', 'organisms', 'realism', 'mind-independent world', 'cognitive profiles', 'artificial intelligence', 'cognitive systems']), ('Approximation of pathwidth of outerplanar graphs\nThere exists a polynomial time algorithm to compute the pathwidth of\nouterplanar graphs, but the large exponent makes this algorithm\nimpractical. In this paper, we give an algorithm that, given a\nbiconnected outerplanar graph G, finds a path decomposition of G of\npathwidth at most twice the pathwidth of G plus one. To obtain the\nresult, several relations between the pathwidth of a biconnected\nouterplanar graph and its dual are established\n', ['pathwidth approximation', 'outerplanar graphs', 'polynomial time algorithm', 'biconnected outerplanar graph', 'path decomposition', 'approximation theory', 'computational geometry', 'graph theory']), ('Hot controllers\nOver the last few years, the semiconductor industry has put much emphasis on\nways to improve the accuracy of thermal mass flow controllers (TMFCs).\nAlthough issues involving TMFC mounting orientation and pressure\neffects have received much attention, little has been done to address\nthe effect of changes in ambient temperature or process gas\ntemperature. Scientists and engineers at Qualiflow have succeeded to\nsolve the problem using a temperature correction algorithm for digital\nTMFCs. Using an in situ environmental temperature compensation\ntechnique, we calculated correction factors for the temperature effect\nand obtained satisfactory results with both the traditional sensor and\nthe new, improved thin-film sensors\n', ['semiconductor manufacturing', 'process gas flow', 'thermal mass flow controller', 'temperature correction algorithm', 'in situ environmental temperature compensation', 'compensation', 'controllers', 'flow control', 'process control', 'semiconductor device manufacture']), ("Maybe it's not too late to join the circus: books for midlife career management\nMidcareer librarians looking for career management help on the bookshelf face\nthousands of choices. This article reviews thirteen popular career\nself-help books. The reviewed books cover various aspects of career\nmanagement and provide information on which might be best suited for\nparticular goals, including career change, career tune-up, and personal\nand professional self-evaluation. The comments reflect issues of\ninterest to midcareer professionals\n", ['midlife career management', 'librarians', 'career self-help books', 'career change', 'professional self-evaluation', 'personal self-evaluation', 'libraries', 'employment', 'human resource management', 'information science', 'libraries', 'personnel', 'professional aspects']), ("Anti-spam suit attempts to hold carriers accountable\nA lawsuit alleges that Sprint has violated Utah's new anti-spam act. The action\ncould open the door to new regulations on telecom service providers\n", ['Sprint', 'telecom service providers', 'regulations', 'anti-spam act', 'lawsuit', 'electronic mail', 'legislation', 'telecommunication']), ('The dynamic aspect of land administration: an often-forgotten component in\nsystem design\nAlthough the establishment of a land administration system is enough of a\nchallenge as it is, the task of keeping the system up to date with\ndevelopments in society is even more challenging. Initial adjudication\nand cadastral mapping basically record land tenure as it exists at a\ngiven moment, i.e. the static situation. The paper aims to analyse the\ndevelopments that might occur in a society with respect to tenure,\nvalue and use of land. These developments constitute a dynamic\ncomponent of land administration. As land administration systems have\nto serve society on a long-term basis and normally have a long-term\nreturn on investment, the author recommends taking into account both\nthe static and dynamic component when designing land administration\nsystems\n', ['dynamic aspect', 'system design', 'land administration system', 'societal developments', 'adjudication', 'cadastral mapping', 'land tenure', 'static situation', 'return on investment', 'static component', 'dynamic component', 'cartography', 'socio-economic effects', 'systems analysis', 'town and country planning']), ('PKI: coming to an enterprise near you?\nFor many years public key infrastructure (PKI) deployments were the provenance\nof governments and large, security-conscious corporations and financial\ninstitutions. These organizations have the financial and human\nresources necessary to successfully manage the complexities of a public\nkey system. Lately however, several forces have converged to encourage\na broader base of enterprises to take a closer look at PKI. These\nforces are discussed. PKI vendors are now demonstrating to customers\nhow they can make essential business applications faster and more\nefficient by moving them to the Internet-without sacrificing security.\nThose applications usually include secure remote access, secure\nmessaging, electronic document exchange, transaction validation, and\nnetwork authentication. After a brief discussion of PKI basics the\nauthor reviews various products available on the market\n', ['PKI', 'business-critical applications', 'public key infrastructure', 'e-commerce', 'IPSec VPNs', 'PKI vendors', 'security', 'secure remote access', 'secure messaging', 'electronic document exchange', 'transaction validation', 'network authentication', 'Baltimore Technologies', 'Entrust', 'GeoTrust', 'RSA Security', 'VeriSign', 'business communication', 'Internet', 'public key cryptography', 'telecommunication security']), ('Self-describing Turing machines\nAfter a sketchy historical account on the question of self-describeness and\nself-reproduction, and after discussing the definition of suitable\nencodings for self-describeness, we give the construction of several\nself-describing Turing machines, namely self-describing machines with,\nrespectively, 350, 267, 224 and 206 instructions\n', ['self-describing Turing machines', 'self-describeness', 'self-reproduction', 'encodings', 'Turing machines']), ('New lower bounds of the size of error-correcting codes for the Z-channel\nOptimization problems on graphs are formulated to obtain new lower bounds of\nthe size of error-correcting codes for the Z-channel\n', ['lower bounds', 'error-correcting codes', 'Z-channel', 'optimization problems', 'graphs', 'error correction codes', 'optimisation']), ('A notion of non-interference for timed automata\nThe non-interference property of concurrent systems is a security property\nconcerning the flow of information among different levels of security\nof the system. In this paper we introduce a notion of timed\nnon-interference for real-time systems specified by timed automata. The\nnotion is presented using an automata based approach and then it is\ncharacterized also by operations and equivalence between timed\nlanguages. The definition is applied to an example of a time-critical\nsystem modeling a simplified control of an airplane\n', ['timed automata', 'noninterference notion', 'concurrent systems', 'security property', 'real-time systems', 'time-critical system', 'automata theory', 'process algebra', 'real-time systems']), ("A phytography of WALDMEISTER\nThe architecture of the WALDMEISTER prover for unit equational deduction is\nbased on a strict separation of active and passive facts. After an\ninspection of the system's proof procedure, the representation of each\nof the central data structures is outlined, namely indexing for the\nactive facts, compression for the passive facts, successor sets for the\nhypotheses, and minimal recording of inference steps for the proof\nobject. In order to cope with large search spaces, specialized\nredundancy criteria are employed, and the empirically gained control\nknowledge is integrated to ease the use of the system. The paper\nconcludes with a quantitative comparison of the WALDMEISTER versions\nover the years, and a view of the future prospects\n", ['WALDMEISTER', 'theorem prover', 'unit equational deduction', 'passive facts', 'active facts', 'data structures', 'indexing', 'hypotheses', 'phytography', 'CADE ATP System Competition', 'inference', 'large search spaces', 'redundancy', 'future prospects', 'data structures', 'formal logic', 'inference mechanisms', 'search problems', 'theorem proving']), ('Numerical modeling of the flow in stenosed coronary artery. The relationship\nbetween main hemodynamic parameters\nThe severity of coronary arterial stenosis is usually measured by either simple\ngeometrical parameters, such as percent diameter stenosis, or\nhemodynamically based parameters, such as the fractional flow reserve\n(FFR) or coronary flow reserve (CFR). The present study aimed to\nestablish a relationship between actual hemodynamic conditions and the\nparameters that define stenosis severity in the clinical setting. We\nused a computational model of the blood flow in a vessel with a blunt\nstenosis and an autoregulated vascular bed to simulate a stenosed blood\nvessel. A key point in creating realistic simulations is to properly\nmodel arterial autoregulation. A constant flow regulation mechanism\nresulted in CFR and FFR values that were within the physiological\nrange, while a constant wall-shear stress model yielded unrealistic\nvalues. The simulation tools developed in the present study may be\nuseful in the clinical assessment of single and multiple stenoses by\nmeans of minimally invasive methods\n', ['numerical modeling', 'hemodynamic parameters', 'coronary arterial stenosis', 'stenosis severity', 'clinical setting', 'computational model', 'blood flow', 'blunt stenosis', 'autoregulated vascular bed', 'simulation', 'stenosed blood vessel', 'arterial autoregulation', 'constant flow regulation mechanism', 'physiological range', 'constant wall shear stress model', 'minimally invasive methods', 'blood vessels', 'cardiovascular system', 'computational fluid dynamics', 'digital simulation', 'diseases', 'haemodynamics', 'medical computing', 'Navier-Stokes equations', 'numerical analysis']), ('Compatibility comparison and performance evaluation for Japanese HPF compilers\nusing scientific applications\nThe lack of compatibility of High-Performance Fortran (HPF) between vender\nimplementations has been disheartening scientific application users so\nas to hinder the development of portable programs. Thus parallel\ncomputing is still unpopular in the computational science community,\neven though parallel programming is common to the computer science\ncommunity. As users would like to run the same source code on parallel\nmachines with different architectures as fast as possible, we have\ninvestigated the compatibility of source codes for Japanese HPF\ncompilers (NEC, Fujitsu and Hitachi) with two real-world applications:\na 3D fluid code and a 2D particle code. We have found that the\nsource-level compatibility between Japanese HPF compilers is almost\npreserved, but more effort will be needed to sustain complete\ncompatibility. We have also evaluated parallel performance and found\nthat HPF can achieve good performance for the 3D fluid code with almost\nthe same source code. For the 2D particle code, good results have also\nbeen obtained with a small number of processors, but some changes in\nthe original source code and the addition of interface blocks is\nrequired\n', ['High-Performance Fortran', 'HPF', 'source compatability', 'portable programs', 'parallel programming', 'compilers', 'parallel performance', 'FORTRAN', 'parallel programming', 'program compilers', 'software performance evaluation', 'software portability']), ('A note on multi-index polynomials of Dickson type and their applications in\nquantum optics\nWe discuss the properties of a new family of multi-index Lucas type\npolynomials, which are often encountered in problems of intracavity\nphoton statistics. We develop an approach based on the integral\nrepresentation method and show that this class of polynomials can be\nderived from recently introduced multi-index Hermite like polynomials\n', ['Lucas type polynomials', 'multi-index polynomials', 'quantum optics', 'intracavity photon statistics', 'integral representation', 'generating functions', 'polynomials', 'quantum optics']), ('A scalable and lightweight QoS monitoring technique combining passive and\nactive approaches: on the mathematical formulation of CoMPACT monitor\nTo make a scalable and lightweight QoS monitoring system, we (2002) have\nproposed a new QoS monitoring technique, called the change-of-measure\nbased passive/active monitoring (CoMPACT Monitor), which is based on\nthe change-of-measure framework and is an active measurement\ntransformed by using passively monitored data. This technique enables\nus to measure detailed QoS information for individual users,\napplications and organizations, in a scalable and lightweight manner.\nIn this paper, we present the mathematical foundation of CoMPACT\nMonitor. In addition, we show its characteristics through simulations\nin terms of typical implementation issues for inferring the delay\ndistributions. The results show that CoMPACT Monitor gives accurate QoS\nestimations with only a small amount of extra traffic for active\nmeasurement\n', ['quality of service', 'change-of-measure', 'passive monitoring', 'active monitoring', 'CoMPACT Monitor', 'delay distributions', 'Internet', 'QoS monitoring', 'network performance', 'Internet', 'monitoring', 'quality of service', 'telecommunication traffic']), ("Satellite image collection optimization\nImaging satellite systems represent a high capital cost. Optimizing the\ncollection of images is critical for both satisfying customer orders\nand building a sustainable satellite operations business. We describe\nthe functions of an operational, multivariable, time dynamic\noptimization system that maximizes the daily collection of satellite\nimages. A graphical user interface allows the operator to quickly see\nthe results of what if adjustments to an image collection plan. Used\nfor both long range planning and daily collection scheduling of Space\nImaging's IKONOS satellite, the satellite control and tasking (SCT)\nsoftware allows collection commands to be altered up to 10 min before\nupload to the satellite\n", ['satellite image collection optimization', 'imaging satellite systems', 'multivariable time dynamic optimization system', 'graphical user interface', 'image collection plan', 'long range planning', 'daily collection scheduling', 'Space Imaging IKONOS satellite', 'satellite control tasking software', 'collection commands', 'geophysics computing', 'graphical user interfaces', 'image processing', 'optimisation', 'remote sensing']), ('The road to perpetual progress [retail inventory management]\nWith annual revenues increasing 17.0% to 20.0% consistently over the last three\nyears and more than 2,500 new stores opened from 1998 through 2001,\nDollar General is on the fast track. However, the road to riches could\nhave easily become the road to ruin had the retailer not exerted\ncontrol over its inventory management\n', ['Dollar General', 'retailer', 'inventory management', 'retailing', 'stock control']), ('Model theory for hereditarily finite superstructures\nWe study model-theoretic properties of hereditarily finite superstructures over\nmodels of not more than countable signatures. A question is answered in\nthe negative inquiring whether theories of hereditarily finite\nsuperstructures which have a unique (up to isomorphism) hereditarily\nfinite superstructure can be described via definable functions. Yet\ntheories for such superstructures admit a description in terms of\niterated families TF and SF. These are constructed using a definable\nunion taken over countable ordinals in the subsets which are unions of\nfinitely many complete subsets and of finite subsets, respectively.\nSimultaneously, we describe theories that share a unique (up to\nisomorphism) countable hereditarily finite superstructure\n', ['model theory', 'model-theoretic properties', 'countable signatures', 'iterated families', 'definable union', 'finitely many complete subsets', 'countable hereditarily finite superstructure', 'formal logic']), ('Allan variance and fractal Brownian motion\nNoise filtering is the subject of a voluminous literature in radio engineering.\nThe methods of filtering require knowledge of the frequency response,\nwhich is usually unknown. D.W. Allan (see Proc. IEEE, vol.54, no.2,\np.221-30, 1966; IEEE Trans. Instr. Measur., vol.IM-36, p.646-54, 1987)\nproposed a simple method of determining the interval between equally\naccurate observations which does without this information. In this\nmethod, the variances of the increments of noise and signal are equal,\nso that, in observations with a greater step, the variations caused by\nnoise are smaller than those caused by the signal. This method is the\nstandard accepted by the USA metrology community. The present paper is\ndevoted to a statistical analysis of the Allan method and acquisition\nof additional information\n', ['Allan variance', 'fractal Brownian motion', 'noise filtering', 'radio engineering', 'frequency response', 'USA metrology community', 'statistical analysis', 'white noise', 'Brownian motion', 'filtering theory', 'fractals', 'measurement', 'random noise', 'signal processing', 'statistical analysis']), ('Positional control of pneumatic manipulators for construction tasks\nThis paper describes solutions that can be applied to pneumatic manipulator\nproblems in positioning, both for angle trajectories and for long\nlinear trajectories, used in construction tasks. Optimal positioning of\na pneumatic manipulator along angle trajectories with minimum control\nenergy consumption is given. The implementation of the control system\nis presented. Control algorithms for a long linear trajectory\nmanipulator based on two-phase and three-phase motion modes of the\nend-effector are investigated. Conventional and fuzzy logic controls of\na pneumatic manipulator were applied and experimental testing was\ncarried out. The obtained results allow widening the application range\nof pneumatic manipulators in construction, particularly in gantry type\nmachines\n', ['pneumatic manipulators', 'construction tasks', 'positioning', 'positional control', 'angle trajectories', 'long linear trajectory manipulator', 'two-phase motion modes', 'three-phase motion modes', 'fuzzy logic controls', 'gantry type machines', 'civil engineering', 'fuzzy control', 'industrial manipulators', 'pneumatic control equipment', 'position control']), ("Happily ever after: plateauing as a means for long-term career satisfaction\nLittle did I know when I attended Judith Bardwick's presentation on plateauing\nat the ALA annual convention in 1988 that it would turn out to be one\nof the most valuable sessions I would attend at any library conference,\nsince it has enabled me to understand the phenomenon of plateauing and\nto use the strategies she suggested to rejuvenate my career and\npersonal life continually. Key concepts and solutions from her book and\nfrom other literature on plateauing are summarized and examples given\nas to how I incorporated them into my life\n", ['plateauing', 'long-term career satisfaction', 'librarians', 'personal life', 'employment', 'information science', 'library automation', 'personnel']), ('Quantum-controlled measurement device for quantum-state discrimination\nWe propose a "programmable" quantum device that is able to perform a specific\ngeneralized measurement from a certain set of measurements depending on\na quantum state of a "program register." In particular, we study a\nsituation when the programmable measurement device serves for the\nunambiguous discrimination between nonorthogonal states. The particular\npair of states that can be unambiguously discriminated is specified by\nthe state of a program qubit. The probability of successful\ndiscrimination is not optimal for all admissible pairs. However, for\nsome subsets it can be very close to the optimal value\n', ['quantum-controlled measurement device', 'quantum-state discrimination', 'programmable quantum device', 'quantum state', 'program register', 'nonorthogonal states', 'program qubit', 'quantum computing', 'quantum theory']), ('Licensing experiences in the Netherlands\nThe licensing strategy of university libraries in the Netherlands is closely\nconnected with university policies to develop document servers and to\nmake research publications available on the Web. National agreements\nhave been made with major publishers, such as Elsevier Science and\nKluwer Academic, to provide access to a wide range of scientific\ninformation and to experiment with new ways of providing information\nand new business models\n', ['licensing strategy', 'university libraries', 'Netherlands', 'university policies', 'document servers', 'research publications', 'Web', 'Elsevier Science', 'Kluwer Academic', 'scientific information', 'business models', 'academic libraries', 'electronic publishing', 'library automation', 'research libraries', 'scientific information systems']), ('Structural interpretation of matched pole-zero discretisation\nDeals with matched pole-zero discretisation, which has been used in practice\nfor hand calculations in the digital redesign of continuous-time\nsystems but available only in the transfer-function form. Since this\nform is inconvenient for characterising the time-domain properties of\nsampled-data loops and for computerising the design of such systems, a\nstate-space formulation is developed. Under the new interpretation, the\nmatched pole-zero model is shown to be structurally identical to a\nhold-equivalent discrete-time model, where the generalised hold takes\nintegral part, thus unifying the most widely used discretisation\napproaches. An algorithm for obtaining the generalised hold function is\npresented. The hold-equivalent structure of the matched pole-zero model\nclarifies several discrete-time system properties, such as\ncontrollability and observability, and their preservation or loss with\na matched pole-zero discretisation. With the proposed formulation, the\nmatched pole-zero, hold-equivalent, and mapping models can now all be\nconstructed with a single schematic model\n', ['structural interpretation', 'matched pole-zero discretisation', 'continuous-time systems', 'time-domain properties', 'sampled-data loops', 'state-space formulation', 'hold-equivalent discrete-time model', 'controllability', 'observability', 'closed-loop system', 'digital simulations', 'closed loop systems', 'controllability', 'discrete time systems', 'observability', 'poles and zeros', 'sampled data systems']), ('Dihedral congruence primes and class fields of real quadratic fields\nWe show that for a real quadratic field F the dihedral congruence primes with\nrespect to F for cusp forms of weight k and quadratic nebentypus are\nessentially the primes dividing expressions of the form epsilon /sub\n+//sup k-1/+or-1 where epsilon /sub +/ is a totally positive\nfundamental unit of F. This extends work of Hida. Our results allow us\nto identify a family of (ray) class fields of F which are generated by\ntorsion points on modular abelian varieties\n', ['dihedral congruence primes', 'class fields', 'real quadratic fields', 'quadratic nebentypus', 'torsion points', 'modular abelian varieties', 'class field theory', 'number theory']), ("Verona Lastre: consolidation provides opening for a new plate vendor\nFewer companies than ever are manufacturing CTP plates. The market has become\nglobalized, with just four big firms dominating the picture. To the\nSamor Group, however, globalization looked like an opportunity; it\nreasoned that many a national and local distributor would welcome a\nsmall, competitive, regional manufacturer. A couple of years ago it\nformed a company, Verona Lastre, to exploit that opportunity. Now Vela,\nas it's familiarly called, has launched its line of high-quality\nthermal plates and is busily lining up dealers in Europe and the\nAmericas\n", ['Verona Lastre', 'Vela', 'CTP plates', 'computer controlled typesetting', 'printing industry', 'publishing']), ('Brightness-independent start-up routine for star trackers\nInitial attitude acquisition by a modern star tracker is investigated here.\nCriteria for efficient organization of the on-board database are\ndiscussed with reference to a brightness-independent initial\nacquisition algorithm. Star catalog generation preprocessing is\ndescribed, with emphasis on the identification of minimum star\nbrightness for detection by a sensor based on a charge coupled device\n(CCD) photodetector. This is a crucial step for proper evaluation of\nthe attainable sky coverage when selecting the stars to be included in\nthe on-board catalog. Test results are also reported, both for\nreliability and accuracy, even if the former is considered to be the\nprimary target. Probability of erroneous solution is 0.2% in the case\nof single runs of the procedure, while attitude determination accuracy\nis in the order of 0.02 degrees in the average for the computation of\nthe inertial pointing of the boresight axis\n', ['brightness-independent start-up routine', 'star trackers', 'initial attitude acquisition', 'on-board database', 'star catalog generation preprocessing', 'gyroless spacecraft', 'minimum star brightness', 'charge coupled device photodetector', 'reliability', 'boresight axis', 'attitude control', 'CCD image sensors', 'computerised navigation', 'optical tracking', 'space vehicles']), ('TCRM: diagnosing tuple inconsistency for granulized datasets\nMany approaches to granularization have been presented for knowledge discovery.\nHowever, the inconsistent tuples that exist in granulized datasets are\nhardly ever revealed. We developed a model, tuple consistency\nrecognition model (TCRM) to help efficiently detect inconsistent tuples\nfor datasets that are granulized. The main outputs of the developed\nmodel include explored inconsistent tuples and consumed processing\ntime. We further conducted an empirical test where eighteen continuous\nreal-life datasets granulized by the equal width interval technique\nthat embedded S-plus histogram binning algorithm (SHBA) and largest\nbinning size algorithm (LBSA) binning algorithms were diagnosed.\nRemarkable results: almost 40% of the granulized datasets contain\ninconsistent tuples and 22% have the amount of inconsistent tuples more\nthan 20%\n', ['TCRM', 'tuple inconsistency', 'granulized datasets', 'granularization', 'knowledge discovery', 'tuple consistency recognition model', 'relational database', 'large database', 'processing time', 'equal width interval technique', 'S-plus histogram binning algorithm', 'largest binning size algorithm', 'SQL', 'data integrity', 'data mining', 'relational databases', 'SQL', 'very large databases']), ('A note on an axiomatization of the core of market games\nAs shown by Peleg (1993), the core of market games is characterized by\nnonemptiness, individual rationality, superadditivity, the weak reduced\ngame property, the converse reduced game property, and weak symmetry.\nIt was not known whether weak symmetry was logically independent. With\nthe help of a certain transitive 4-person TU game, it is shown that\nweak symmetry is redundant in this result. Hence, the core on market\ngames is axiomatized by the remaining five properties, if the universe\nof players contains at least four members\n', ['market game core axiomatization', 'nonempty games', 'individual rationality', 'superadditive games', 'weak reduced game property', 'converse reduced game property', 'weak symmetry', 'transitive 4-person TU game', 'redundant', 'cooperative systems', 'economic cybernetics', 'game theory']), ('P2P is dead, long live P2P\nPicture the problem: a sprawling multinational has hundreds of offices,\nthousands of workers, and countless amounts of intellectual property\nscattered here, there, everywhere. In Kuala Lumpur an executive needs\nto see an internally-generated report on oil futures in central\nAsia-but where is it? London? New York? Moscow? With a few clicks of\nthe mouse-and the right P2P technology deployed in-house-that executive\nwill find and retrieve the report. Without P2P that might be\nimpossible-certainly it would be time-consuming-and, right there, the\nargument for P2P implementations inside enterprises becomes clear. Who\nare the players? No companies have managed to stake out clear leads and\nthe fact is that the P2P marketplace now is up for grabs-but the\nexciting news is that a range of small and startup businesses are\ntrying to grab turf and quite probably, if the analysts are right, a\nfew of these now little-known companies will emerge as digital content\nstars within the next few years. Cases in point: Groove Networks,\nAvaki, WorldStreet, Yaga, NextPage, and Kontiki. Very different\ncompanies-their approach to the markets radically differ-but, say the\nanalysts, each is worth a close look because among them they are\ndefining the future of P2P\n', ['P2P technology', 'digital content', 'businesses', 'Groove Networks', 'Avaki', 'WorldStreet', 'NextPage', 'Kontiki', 'Yaga', 'content owners', 'business data processing', 'electronic data interchange']), ("Modeling daily realized futures volatility with singular spectrum analysis\nUsing singular spectrum analysis (SSA), we model the realized volatility and\nlogarithmic standard deviations of two important futures return series.\nThe realized volatility and logarithmic standard deviations are\nconstructed following the methodology of Andersen et al. [J. Am. Stat.\nAss. 96 (2001) 42-55] using intra-day transaction data. We find that\nSSA decomposes the volatility series quite well and effectively\ncaptures both the market trend (accounting for about 34-38% of the\ntotal variance in the series) and, more importantly, a number of\nunderlying market periodicities. Reliable identification of any\nperiodicities is extremely important for options pricing and risk\nmanagement and we believe that SSA can be a useful addition to the\nfinancial practitioners' toolbox\n", ['daily realized futures volatility', 'singular spectrum analysis', 'SSA', 'logarithmic standard deviations', 'return series', 'intraday transaction data', 'market trend', 'market periodicities', 'risk management', 'options pricing', 'financial practitioners', 'econophysics', 'asset return', 'economics', 'fluctuations', 'probability', 'statistical mechanics', 'stock markets']), ('Using the Web to answer legal reference questions\nIn an effort to help non-law librarians with basic legal reference questions,\nthe author highlights three basic legal Web sites and outlines useful\nsubject-specific Web sites that focus on statutes and regulations, case\nlaw and attorney directories\n', ['World Wide Web', 'legal reference questions', 'nonlaw librarians', 'case law', 'attorney directories', 'information resources', 'law administration']), ('Quantum phase gate for photonic qubits using only beam splitters and\npostselection\nWe show that a beam splitter of reflectivity one-third can be used to realize a\nquantum phase gate operation if only the outputs conserving the number\nof photons on each side are postselected\n', ['quantum phase gate', 'photonic qubits', 'postselection', 'multiqubit networks', 'postselected quantum gate', 'optical quantum gate operations', 'reflectivity', 'quantum phase gate operation', 'outputs', 'photon number conservation', 'postselected photon number conserving outputs', 'quantum computation', 'quantum information processing', 'postselected quantum phase gate', 'polarization beam splitters', 'optical beam splitters', 'quantum gates', 'quantum optics', 'quantum theory', 'reflectivity']), ('The BIOGENES system for knowledge-based bioprocess control\nThe application of knowledge-based control systems in the area of\nbiotechnological processes has become increasingly popular over the\npast decade. This paper outlines the structure of the advanced\nknowledge-based part of the BIOGENES Copyright control system for the\ncontrol of bioprocesses such as the fed-batch Saccharomyces cerevisiae\ncultivation. First, a brief overview of all the tasks implemented in\nthe knowledge-based level including process data classification,\nqualitative process state identification and supervisory process\ncontrol is given. The procedures performing the on-line identification\nof metabolic states and supervisory process control (setpoint\ncalculation and control strategy selection) are described in more\ndetail. Finally, the performance of the system is discussed using\nresults obtained from a number of experimental cultivation runs in a\nlaboratory unit\n', ['BIOGENES system', 'knowledge-based bioprocess control', 'biotechnological processes', 'fed-batch Saccharomyces cerevisiae cultivation', 'process data classification', 'qualitative process state identification', 'supervisory process control', 'online identification', 'metabolic states', 'experiment', 'biocontrol', 'biotechnology', 'identification', 'intelligent control', 'process control']), ('Non-linear analysis of nearly saturated porous media: theoretical and numerical\nformulation\nA formulation for a porous medium saturated with a compressible fluid\nundergoing large elastic and plastic deformations is presented. A\nconsistent thermodynamic formulation is proposed for the two-phase\nmixture problem; thus preserving a straightforward and robust numerical\nscheme. A novel feature is the specification of the fluid\ncompressibility in terms of a volumetric logarithmic strain, which is\nenergy conjugated to the fluid pressure in the entropy inequality. As a\nresult, the entropy inequality is used to separate three different\nmechanisms representing the response: effective stress response\naccording to Terzaghi in the solid skeleton, fluid pressure response to\ncompressibility of the fluid, and dissipative Darcy flow representing\nthe interaction between the two phases. The paper is concluded with a\ncouple of numerical examples that display the predictive capabilities\nof the proposed formulation. In particular, we consider results for the\nkinematically linear theory as compared to the kinematically non-linear\ntheory\n', ['nearly saturated porous media', 'nonlinear analysis', 'compressible fluid', 'large elastic deformations', 'large plastic deformations', 'consistent thermodynamic formulation', 'two-phase mixture problem', 'robust numerical scheme', 'fluid compressibility', 'volumetric logarithmic strain', 'fluid pressure', 'entropy inequality', 'effective stress response', 'solid skeleton', 'fluid pressure response', 'dissipative Darcy flow', 'predictive capabilities', 'kinematically linear theory', 'kinematically nonlinear theory', 'compressibility', 'elastic deformation', 'finite element analysis', 'mechanical engineering computing', 'plastic deformation', 'porous materials', 'thermodynamics']), ('Three-dimensional optimum design of the cooling lines of injection moulds based\non boundary element design sensitivity analysis\nA three-dimensional numerical simulation using the boundary element method is\nproposed, which can predict the cavity temperature distributions in the\ncooling stage of injection moulding. Then, choosing the radii and\npositions of cooling lines as design variables, the boundary integral\nsensitivity formulations are deduced. For the optimum design of cooling\nlines, the squared difference between the objective temperature and\ntemperature of the cavity is taken as the objective function. Based on\nthe optimization techniques with design sensitivity analysis, an\niterative algorithm to reach the minimum value of the objective\nfunction is introduced, which leads to the optimum design of cooling\nlines at the same time\n', ['injection moulding', '3D numerical simulation', 'boundary element method', 'cavity temperature distributions', 'cooling stage', 'boundary integral sensitivity analysis', 'iterative algorithm', 'heat conduction', 'objective function', 'optimization', 'boundary-elements methods', 'cooling', 'iterative methods', 'moulding', 'optimisation', 'plastics industry', 'process control', 'sensitivity analysis', 'solid modelling']), ("Supervisory control design based on hybrid systems and fuzzy events detection.\nApplication to an oxichlorination reactor\nThis paper presents a supervisory control scheme based on hybrid systems theory\nand fuzzy events detection. The fuzzy event detector is a linguistic\nmodel, which synthesizes complex relations between process variables\nand process events incorporating experts' knowledge about the process\noperation. This kind of detection allows the anticipation of\nappropriate control actions, which depend upon the selected membership\nfunctions used to characterize the process under scrutiny. The proposed\nsupervisory control scheme was successfully implemented for an\noxichlorination reactor in a vinyl monomer plant\n", ['supervisory control design', 'hybrid systems', 'events detection. fuzzy', 'oxichlorination reactor', 'linguistic model', 'complex relations', 'process variables', 'process events', 'expert knowledge', 'process operation', 'control actions', 'membership functions', 'vinyl monomer plant', 'reactor stability', 'raw material consumption', 'discrete events systems', 'reactive systems', 'finite state machines', 'chemical technology', 'continuous time systems', 'control system synthesis', 'discrete event systems', 'finite state machines', 'fuzzy control', 'fuzzy set theory', 'identification', 'process control']), ('Modifier formula on mean square convergence of LMS algorithm\nIn describing the mean square convergence of the LMS algorithm, the update\nformula based on independence assumption will bring explicit errors,\nespecially when step-size is large. A modifier formula that describes\nthe convergence well, is proposed. Simulations support the proposed\nformula in different conditions\n', ['LMS algorithm', 'mean square convergence', 'update formula', 'independence assumption', 'modifier formula', 'adaptive filtering', 'LMS filter', 'adaptive filters', 'convergence of numerical methods', 'filtering theory', 'least mean squares methods', 'mean square error methods', 'signal processing']), ('Positive productivity, better billing [health care]\nWorkflow software provides the right communication solution for hospital\nspecialists, and delivers an unexpected financial boost too\n', ['health care', 'San Francisco General Hospital', 'ProVation MD', 'workflow software', 'health care', 'invoicing', 'workflow management software']), ('Fuzzy control of multivariable process by modified error decoupling\nIn this paper, a control concept for the squared (equal number of inputs and\noutputs) multivariable process systems is given. The proposed control\nsystem consists of two parts, single loop fuzzy controllers in each\nloop and a centralized decoupling unit. The fuzzy control system uses\nfeedback control to minimize the error in the loop and the decoupler\nuses an adaptive technique to mitigate loop interactions. The decoupler\npredicts the interacting loop changes and modifies the input (error) of\nthe loop controller. The controller was tested on the simulation model\nof "single component vaporizer" process\n', ['multivariable process', 'modified error decoupling', 'squared multivariable process systems', 'square multivariable process systems', 'single-loop fuzzy controllers', 'centralized decoupling unit', 'feedback control', 'error minimization', 'loop interaction mitigation', 'single component vaporizer process', 'set point changes', 'load changes', 'adaptive control', 'feedback', 'fuzzy control', 'multivariable control systems', 'optimal control', 'process control']), ('Asymptotic analysis of (3, 2, 1)-Shell Sort\nWe analyze the (3, 2, 1)-Shell Sort algorithm under the usual random\npermutation model\n', ['Shell Sort', 'sorting', 'algorithm', 'moderate-sized lists', 'random permutation', 'additive functional', 'local limit theorem', 'Markov chain', 'Poissonization', 'asymptotic analysis', 'algorithm theory', 'Markov processes', 'sorting']), ('A study on meaning processing of dialogue with an example of development of\ntravel consultation system\nThis paper describes an approach to processing meaning instead of processing\ninformation in computing. Human intellectual activity is supported by\nlinguistic activities in the brain. Therefore, processing the meaning\nof language instead of processing information should allow us to\nrealize human intelligence on a computer. As an example of the proposed\nframework for processing meaning, we build a travel consultation\ndialogue system which can understand utterance by a user and retrieve\ninformation through dialogue. Through a simulation example of the\nsystem, we show that both information processing and language\nprocessing are integrated\n', ['meaning processing', 'human intellectual activity', 'linguistic activities', 'travel consultation dialogue system', 'user utterance understanding', 'information retrieval', 'information processing', 'language processing', 'computational linguistics', 'information retrieval', 'natural language interfaces', 'speech-based user interfaces', 'travel industry']), ("Breast cancer: effectiveness of computer-aided diagnosis-observer study with\nindependent database of mammograms\nEvaluates the effectiveness of a computerized classification method as an aid\nto radiologists reviewing clinical mammograms for which the diagnoses\nwere unknown to both the radiologists and the computer. Six\nmammographers and six community radiologists participated in an\nobserver study. These 12 radiologists interpreted, with and without the\ncomputer aid, 110 cases that were unknown to both the 12 radiologist\nobservers and the trained computer classification scheme. The\nradiologists' performances in differentiating between benign and\nmalignant masses without and with the computer aid were evaluated with\nreceiver operating characteristic (ROC) analysis. Two-tailed P values\nwere calculated for the Student t test to indicate the statistical\nsignificance of the differences in performances with and without the\ncomputer aid. When the computer aid was used, the average performance\nof the 12 radiologists improved, as indicated by an increase in the\narea under the ROC curve (A/sub z/) from 0.93 to 0.96 (P<.001), by\nan increase in partial area under the ROC curve (/sub 0.9/0A'/sub z/)\nfrom 0.56 to 0.72 (P<.001), and by an increase in sensitivity from\n94% to 98% (P=.022). No statistically significant difference in\nspecificity was found between readings with and those without computer\naid ( Delta +-0.014; P=.46; 95% Cl: -0.054, 0.026), where Delta is\ndifference in specificity. When we analyzed results from the\nmammographers and community radiologists as separate groups, a larger\nimprovement was demonstrated for the community radiologists.\nComputer-aided diagnosis can potentially help radiologists improve\ntheir diagnostic accuracy in the task of differentiating between benign\nand malignant masses seen on mammograms\n", ['computerized classification method', 'clinical mammograms', 'observer study', 'breast cancer', 'computer-aided diagnosis', 'independent database', 'trained computer classification scheme', 'radiologist observers', 'benign masses', 'malignant masses', 'receiver operating characteristic analysis', 'two-tailed P values', 'Student t test', 'statistical significance', 'performances', 'average performance', 'receiver operating characteristic curve', 'diagnostic accuracy', 'computer aid', 'mammographers', 'community radiologists', 'cancer', 'diagnostic radiography', 'image classification', 'mammography', 'medical image processing', 'radiology', 'statistical databases']), ('Integration, the Web are key this season [tax]\nIntegration and the Web are driving many of the enhancements planned by tax\npreparation software vendors for this coming season\n', ['accounting packages', 'tax packages', 'software integration', 'Internet', 'CCH', 'TaxWorks', "People's Choice", 'Visual Tax', 'GoSystem Tax RS', 'Drake', 'NetConnection', 'ATX', 'CPASoftware', 'Intuit', 'Petz', 'TaxSimple', 'RIA', 'accounting', 'software packages', 'tax preparation']), ("Using virtual reality to teach disability awareness\nA desktop virtual reality (VR) program was designed and evaluated to teach\nchildren about the accessibility and attitudinal barriers encountered\nby their peers with mobility impairments. Within this software,\nchildren sitting in a virtual wheelchair experience obstacles such as\nstairs, narrow doors, objects too high to reach, and attitudinal\nbarriers such as inappropriate comments. Using a collaborative research\nmethodology, 15 youth with mobility impairments assisted in developing\nand beta-testing the software. The effectiveness of the program was\nthen evaluated with 60 children in Grades 4-6 using a controlled\npretest/posttest design. The results indicated that the program was\neffective for increasing children's knowledge of accessibility\nbarriers. Attitudes, grade level, familiarity with individuals with a\ndisability, and gender were also investigated\n", ['virtual reality', 'disability awareness teaching', 'children', 'accessibility', 'virtual wheelchair', 'collaborative research methodology', 'mobility impairments', 'software beta-testing', 'collaborative software development', 'computer aided instruction', 'software effectiveness', 'gender', 'computer aided instruction', 'gender issues', 'handicapped aids', 'human factors', 'user interfaces', 'virtual reality']), ("Stock market trading rule discovery using technical charting heuristics\nIn this case study in knowledge engineering and data mining, we implement a\nrecognizer for two variations of the 'bull flag' technical charting\nheuristic and use this recognizer to discover trading rules on the NYSE\nComposite Index. Out-of-sample results indicate that these rules are\neffective\n", ['stock market trading', 'rule discovery', 'technical charting heuristics', 'financial expert system', 'case study', 'knowledge engineering', 'data mining', 'NYSE Composite Index', 'out-of-sample results', 'data mining', 'expert systems', 'financial data processing', 'heuristic programming', 'knowledge engineering', 'stock markets']), ("A new merging algorithm for constructing suffix trees for integer alphabets\nA new approach for constructing a suffix tree T/sub s/ for a given string S is\nto construct recursively a suffix tree T/sub o/ for odd positions,\nconstruct a suffix, tree T/sub e/ for even positions from T/sub o/ and\nthen merge T/sub o/ and T/sub e/ into T/sub s/. To construct suffix\ntrees for integer alphabets in linear time had been a major open\nproblem on index data structures. Farach used this approach and gave\nthe first linear-time algorithm for integer alphabets. The hardest part\nof Farach's algorithm is the merging step. In this paper we present a\nnew and simpler merging algorithm based on a coupled BFS (breadth-first\nsearch). Our merging algorithm is more intuitive than Farach's coupled\nDFS (depth-first search) merging, and thus it can be easily extended to\nother applications\n", ['merging algorithm', 'recursive construction', 'index data structures', 'coupled BFS', 'breadth-first search', 'suffix trees', 'integer alphabets', 'linear time', 'computational complexity', 'merging', 'string matching', 'tree data structures', 'tree searching']), ('Modeling and simulation of adaptive available bit rate voice over asynchronous\ntransfer mode networks\nThis article presents a modeling and simulation methodology to analyze the\nperformance of voice quality when sent over the available bit rate\nservice in asynchronous transfer mode networks. Sources can modify the\nrate at which they send traffic to the network based on the feedback\ncarried in the resource management cells. This is achieved by changing\nthe encoding level. As the contention increases to network\nresources-bandwidth in this case-sources start reducing the rate at\nwhich they generate and send traffic. The efficiency of the scheme\nunder different scheduling/drop policies and other operating conditions\nand environments is evaluated using simulation modeling. Furthermore,\nsensitivity analysis is applied to different parameters, such as queue\nsize and averaging interval length, to investigate their impact on the\nperformance metrics. Results show that limiting the load to 41% of the\nlink capacity results in an acceptable quality\n', ['simulation', 'modeling', 'performance analysis', 'voice quality', 'traffic', 'feedback', 'resource management cells', 'encoding level', 'bandwidth contention', 'scheduling/drop policies', 'queue size', 'averaging interval length', 'performance metrics', 'link capacity', 'adaptive available bit rate voice', 'ATM networks', 'asynchronous transfer mode', 'digital simulation', 'speech coding', 'telecommunication computing', 'telecommunication traffic', 'voice communication']), ('Simulation of physicochemical processes of erosion-corrosion of metals in\ntwo-phase flows\nA computational model for the erosion-corrosion of the metals used in power\nequipment in two-phase flows (RAMEK-2) was developed. The results of\ncalculations of the dependency of the intensity of the\nerosion-corrosion of structural steels as a function of the\nthermodynamic, hydrodynamic and water chemistry parameters of these\nflows in the working paths of thermal power stations and nuclear power\nstations are presented in a three-dimensional space. On the basis of\nmathematical models, application software was created for forecasting\nthe erosion-corrosion resource and for optimizing the rules on\ndiagnosis and protective maintenance of erosion-corrosion of the\nelements of the wet-steam path in power stations\n', ['erosion-corrosion computational model', 'two-phase flows', 'RAMEK-2', 'computer simulation', 'structural steels', 'thermodynamic parameters', 'hydrodynamic parameters', 'water-chemistry parameters', 'three-dimensional space', 'thermal power plants', 'nuclear power plants', 'application software', 'protective maintenance', 'fault diagnosis', 'wet-steam path', 'corrosion', 'flow simulation', 'physics computing', 'power apparatus', 'power engineering computing', 'power stations', 'two-phase flow']), ('Innovative manufacture of impulse turbine blades for wave energy power\nconversion\nAn innovative approach to the manufacture of impulse turbine blades using rapid\nprototyping, fused decomposition modelling (FDM), is presented. These\nblades were designed and manufactured by the Wave Energy Research Team\n(WERT) at the University of Limerick for the experimental analysis of a\n0.6 m impulse turbine with fixed guide vanes for wave energy power\nconversion. The computer aided design/manufacture (CAD/CAM) package\nPro-Engineer 2000i was used for three-dimensional solid modelling of\nthe individual blades. A detailed finite element analysis of the blades\nunder centrifugal loads was performed using Pro-Mechanica. based on\nthis analysis and FDM machine capabilities, blades were redesigned.\nFinally, Pro-E data were transferred to an FDM machine for the\nmanufacture of turbine blades. The objective of this paper is to\npresent the innovative method used to design, modify and manufacture\nblades in a time and cost effective manner using a concurrent\nengineering approach\n', ['CAD/CAM', 'impulse turbine blades', 'wave energy power conversion', 'fused decomposition modelling', 'rapid prototyping', 'manufacturing', 'concurrent engineering', 'University of Limerick', 'solid modelling', 'finite element analysis', 'CAD/CAM', 'concurrent engineering', 'finite element analysis', 'rapid prototyping (industrial)', 'solid modelling']), ('Automated post bonding inspection by using machine vision techniques\nInspection plays an important role in the semiconductor industry. In this\npaper, we focus on the inspection task after wire bonding in packaging.\nThe purpose of wire bonding (W/B) is to connect the bond pads with the\nlead fingers. Two major types of defects are (1) bonding line missing\nand (2) bonding line breakage. The numbers of bonding lines and bonding\nballs are used as the features for defect classification. The proposed\nmethod consists of image preprocessing, orientation determination,\nconnection detection, bonding line detection, bonding ball detection,\nand defect classification. The proposed method is simple and fast. The\nexperimental results show that the proposed method can detect the\ndefects effectively\n', ['semiconductor industry', 'IC manufacturing', 'automated post bonding inspection', 'machine vision', 'wire bonding', 'packaging', 'bond pad connection', 'lead fingers', 'bonding line missing', 'bonding line breakage', 'bonding balls', 'defect classification', 'image preprocessing', 'orientation determination', 'connection detection', 'bonding line detection', 'bonding ball detection', 'computer vision', 'flaw detection', 'inspection', 'integrated circuit manufacture', 'integrated circuit packaging', 'lead bonding']), ('Note on "Deterministic inventory lot-size models under inflation with shortages\nand deterioration for fluctuating demand" by Yang et al\nFor original paper see H.-L. Yang et al., ibid., vol.48, p.144-58 (2001). Yang\net al. extended the lot-size models to allow for inflation and\nfluctuating demand. For this model they proved that the optimal\nreplenishment schedule exists and is unique. They also proposed an\nalgorithm to find the optimal policy. The present paper provides\nexamples, which show that the optimal replenishment schedule and\nconsequently the overall optimal policy may not exist\n', ['deterministic inventory lot-size models', 'inflation', 'fluctuating demand', 'optimal replenishment schedule', 'optimal policy algorithm', 'optimal scheduling parameters', 'deterministic algorithms', 'optimisation', 'stock control']), ('Elimination of zero-order diffraction in digital holography\nA simple method to suppress the zero-order diffraction in the reconstructed\nimage of digital holography is presented. In this method, the Laplacian\nof a detected hologram is used instead of the hologram itself for\nnumerical reconstruction by computing the discrete Fresnel integral.\nThis method can significantly improve the image quality and give better\nresolution and higher accuracy of the reconstructed image. The main\nadvantages of this method are its simplicity in experimental\nrequirements and convenience in data processing\n', ['zero-order diffraction suppression', 'reconstructed image', 'Laplacian', 'detected hologram', 'numerical image reconstruction', 'discrete Fresnel integral', 'image quality', 'image resolution', 'accuracy', 'data processing', 'image. processing', 'digital holography', 'electronic speckle pattern interferometry', 'holographic interferometry', 'image reconstruction', 'image resolution', 'integral equations', 'light diffraction', 'measurement errors']), ('New hub gears up for algorithmic exchange\nWarwick University in the UK is on the up and up. Sometimes considered a\ntypical 1960s, middle-of-the-road redbrick institution-not known for\ntheir distinction the 2001 UK Research Assessment Exercise (RAE) shows\nits research to be the fifth most highly-rated in the country, with\noutstanding standards in the sciences. This impressive performance has\nrightly given Warwick a certain amount of muscle, which it is flexing\nrather effectively, aided by a snappy approach to making things happen\nthat leaves some older institutions standing. The result is a brand new\nCentre for Scientific Computing (CSC), launched within a couple of\nyears of its initial conception\n', ['Warwick University Centre for Scientific Computing', 'natural sciences computing']), ('Construction of double sampling s-control charts for agile manufacturing\nDouble sampling (DS) X-control charts are designed to allow quick detection of\na small shift of process mean and provides a quick response in an agile\nmanufacturing environment. However, the DS X-control charts assume that\nthe process standard deviation remains unchanged throughout the entire\ncourse of the statistical process control. Therefore, a complementary\nDS chart that can be used to monitor the process variation caused by\nchanges in process standard deviation should be developed. In this\npaper, the development of the DS s-charts for quickly detecting small\nshift in process standard deviation for agile manufacturing is\npresented. The construction of the DS s-charts is based on the same\nconcepts in constructing the DS X-charts and is formulated as an\noptimization problem and solved with a genetic algorithm. The\nefficiency of the DS s-control chart is compared with that of the\ntraditional s-control chart. The results show that the DS s-control\ncharts can be a more economically preferable alternative in detecting\nsmall shifts than traditional s-control charts\n', ['double sampling s-control charts', 'agile manufacturing', 'double sampling X-control charts', 'process mean shift detection', 'process standard deviation', 'statistical process control', 'genetic algorithm', 'genetic algorithms', 'manufacturing processes', 'statistical process control']), ('PageFlex + MediaRich = PageRich\nLayout and graphics innovators collaborate on fully variable combination.\nPageflex and Equilibrium have melded their respective EDIT and\nMediaRich technologies to make a variable-data composition engine with\na Web interface. Though a first-generation effort, it shows substantial\npromise\n', ['graphics', 'layout', 'PageFlex', 'MediaRich', 'PageRich', 'composition', 'software houses', 'publishing']), ('Physical quantum algorithms\nI review the differences between classical and quantum systems, emphasizing the\nconnection between no-hidden variable theorems and superior\ncomputational power of quantum computers. Using quantum lattice gas\nautomata as examples, I describe possibilities for efficient simulation\nof quantum and classical systems with a quantum computer. I conclude\nwith a list of research directions\n', ['physical quantum algorithms', 'classical systems', 'no-hidden variable theorems', 'quantum computers', 'quantum lattice gas automata', 'cellular automata', 'lattice gas', 'quantum computing']), ('The road to recovery [disaster planning]\nSeptember 11 stripped us of our innocence, forcing corporations to recognize\nthat disaster planning is a business necessity\n', ['disaster planning', 'recovery', 'risk management', 'disasters', 'planning', 'system recovery']), ('Self-validating integration and approximation of piecewise analytic functions\nLet an analytic or a piecewise analytic function on a compact interval be\ngiven. We present algorithms that produce enclosures for the integral\nor the function itself. Under certain conditions on the representation\nof the function, this is done with the minimal order of numbers of\noperations. The integration algorithm is implemented and numerical\ncomparisons to non-validating integration software are presented\n', ['self-validating integration', 'self-validating approximation', 'compact interval', 'enclosures', 'minimal order', 'integration algorithm', 'complex interval arithmetic', 'piecewise analytic functions', 'approximation theory', 'differentiation', 'integration', 'interpolation']), ('Dementing disorders: volumetric measurement of cerebrospinal fluid to\ndistinguish normal from pathologic finding - feasibility study\nWe have demonstrated that automated methods to describe the severity and\ndistribution of cerebral atrophy are capable of providing diagnostic\ninformation in the classification of neurodegenerative diseases\n', ['cerebrospinal fluid volumetric measurement', 'magnetic resonance imaging technique', 'medical diagnostic imaging', 'healthy subjects', 'dementing disorders', 'normal-pathologic findings distinguishing', 'diagnostic information', 'neurodegenerative diseases classification', 'automated methods', 'cerebral atrophy distribution', 'cerebral atrophy severity', 'biomedical MRI', 'brain', 'diseases', 'medical image processing', 'volume measurement']), ("Work in progress: Developing policies for access to government information in\nthe New South Africa\nFollowing South Africa's transition to democracy in 1994, the SA government has\nadopted policies supporting freedom of expression and freedom of access\nto information. The Bill of Rights in the new Constitution includes a\nconstitutional right of access to information held by the state. Since\n1994 various initiatives have been taken by government and other bodies\nto promote such access. These include moves to reorganize government\nprinting and publishing, restructure the government's public\ninformation services, make government information available on the\nInternet, and extend telephony and Internet access to poor communities.\nSA's new Legal Deposit Act, (1997) makes provision for the creation of\nofficial publications depositories. The Promotion of Access to\nInformation Act, (2000) was enacted to ensure access to information\nheld by the state and public bodies. However, despite much activity, it\nhas proved difficult to translate principles into practical and\nwell-coordinated measures to improve access to government information.\nA specific concern is the failure of policy-makers to visualize a role\nfor libraries\n", ['government information', 'South Africa', 'freedom of expression', 'freedom of access to information', 'Bill of Rights', 'constitutional right of access', 'government printing', 'government publishing', 'public information services', 'Internet', 'official publications depositories', 'public bodies', 'libraries', 'government data processing', 'information dissemination']), ('Games machines play\nIndividual rationality, or doing what is best for oneself, is a standard model\nused to explain and predict human behavior, and von Neumann-Morgenstern\ngame theory is the classical mathematical formalization of this theory\nin multiple-agent settings. Individual rationality, however, is an\ninadequate model for the synthesis of artificial social systems where\ncooperation is essential, since it does not permit the accommodation of\ngroup interests other than as aggregations of individual interests.\nSatisficing game theory is based upon a well-defined notion of being\ngood enough, and does accommodate group as well as individual interests\nthrough the use of conditional preference relationships, whereby a\ndecision maker is able to adjust its preferences as a function of the\npreferences, and not just the options, of others. This new theory is\noffered as an alternative paradigm to construct artificial societies\nthat are capable of complex behavior that goes beyond exclusive self\ninterest\n', ['individual rationality', 'human behavior', 'game theory', 'multiple-agent', 'artificial social systems', 'cooperation', 'conditional preference relationships', 'artificial societies', 'self interest', 'decision theory', 'group rationality', 'artificial intelligence', 'cognitive systems', 'decision theory', 'game theory']), ('Government budget and accounting information policy and practice in Taiwan\nThe principal government budget and accounting information policies in Taiwan\nare founded on the ability to provide integrated, consistent, and\ntimely information for government managers to make more rational\ndecisions concerning national resource allocation and evaluation. A\nspecific accounting organization system has been designed for this\npurpose. This paper analyzes information policies and practices\naccording to the relevant laws and regulations, identifies issues\nregarding the policies, and presents strategies to resolve the issues\n', ['Government budget', 'accounting information policy', 'Taiwan', 'government managers', 'rational decisions', 'national resource allocation', 'national resource evaluation', 'Generally Accepted Accounting Principles', 'budgeting data processing', 'decision support systems', 'government data processing']), ('Non-asymptotic confidence ellipsoids for the least-squares estimate\nWe consider the finite sample properties of least-squares system\nidentification, and derive non-asymptotic confidence ellipsoids for the\nestimate. The shape of the confidence ellipsoids is similar to the\nshape of the ellipsoids derived using asymptotic theory, but unlike\nasymptotic theory, they are valid for a finite number of data points.\nThe probability that the estimate belongs to a certain ellipsoid has a\nnatural dependence on the volume of the ellipsoid, the data generating\nmechanism, the model order and the number of data points available\n', ['nonasymptotic confidence ellipsoids', 'least-squares estimate', 'finite sample properties', 'least-squares system identification', 'probability', 'data generating mechanism', 'model order', 'data points', 'identification', 'least squares approximations', 'linear systems', 'probability']), ('Bigger is better: the influence of physical size on aesthetic preference\njudgments\nThe hypothesis that the physical size of an object can influence aesthetic\npreferences was investigated. In a series of four experiments,\nparticipants were presented with pairs of abstract stimuli and asked to\nindicate which member of each pair they preferred. A preference for\nlarger stimuli was found on the majority of trials using various types\nof stimuli, stimuli of various sizes, and with both adult and\n3-year-old participants. This preference pattern was disrupted only\nwhen participants had both stimuli that provided a readily accessible\nalternative source of preference-evoking information and sufficient\nattentional resources to make their preference judgments\n', ['aesthetic preference judgments', 'physical size influence', 'decision making', 'preference formation', 'judgment cues', 'abstract stimuli', 'adult participants', 'child participants', 'preference pattern', 'preference-evoking information', 'attentional resources', 'decision theory', 'psychology']), ('A context-aware decision engine for content adaptation\nBuilding a good content adaptation service for mobile devices poses many\nchallenges. To meet these challenges, this quality-of-service-aware\ndecision engine automatically negotiates for the appropriate adaptation\ndecision for synthesizing an optimal content version\n', ['content adaptation', 'mobile devices', 'quality-of-service-aware', 'decision engine', 'optimal content version', 'adaptation decision', 'client-server systems', 'mobile computing', 'quality of service']), ('Active pitch control in larger scale fixed speed horizontal axis wind turbine\nsystems. I. linear controller design\nThis paper reviews and addresses the principles of linear controller design of\nthe fixed speed wind turbine system in above rated wind speed, using\npitch angle control of the blades and applying modern control theory.\nFirst, the nonlinear equations of the system are built in under some\nreasonable suppositions. Then, the nonlinear equations are linearised\nat set operating point and digital simulation results are shown in this\npaper. Finally, a linear quadratic optimal feedback controller is\ndesigned and the dynamics of the closed circle system are simulated\nwith digital calculation. The advantages and disadvantages of the\nassumptions and design method are also discussed. Because of the\ninherent characteristics of the linear system control theory, the\nperformance of the linear controller is not sufficient for operating\nwind turbines, as is discussed\n', ['fixed speed wind turbine system', 'pitch angle control', 'control theory', 'nonlinear equations', 'digital simulation', 'linear quadratic optimal feedback controller', 'closed circle system', 'linear system control theory', 'wind turbines', 'horizontal axis wind turbine systems', 'active pitch control', 'linear controller design', 'aerodynamics', 'drive train dynamics', 'aerodynamics', 'control system synthesis', 'feedback', 'linear quadratic control', 'nonlinear equations', 'power generation control', 'wind turbines']), ('Packet promises past & present [IP switching]\nWith the death of the competitive carrier market and the significant slashing\nof RBOC capex budgets, softswitch vendors have been forced to retrench.\nNow instead of focusing primarily on limited Internet off-load\napplications, packet-based softswitches are set to gel around real user\nneeds for services such as voice over IP and IP Centrex\n', ['softswitch vendors', 'voice over IP', 'IP Centrex', 'Internet telephony', 'telecommunication switching']), ('Sensorless control of induction motor drives\nControlled induction motor drives without mechanical speed sensors at the motor\nshaft have the attractions of low cost and high reliability. To replace\nthe sensor the information on the rotor speed is extracted from\nmeasured stator voltages and currents at the motor terminals.\nVector-controlled drives require estimating the magnitude and spatial\norientation of the fundamental magnetic flux waves in the stator or in\nthe rotor. Open-loop estimators or closed-loop observers are used for\nthis purpose. They differ with respect to accuracy, robustness, and\nsensitivity against model parameter variations. Dynamic performance and\nsteady-state speed accuracy in the low-speed range can be achieved by\nexploiting parasitic effects of the machine. The overview in this paper\nuses signal flow graphs of complex space vector quantities to provide\nan insightful description of the systems used in sensorless control of\ninduction motors\n', ['sensorless control', 'induction motor drives', 'reliability', 'stator voltages', 'stator currents', 'vector-controlled drives', 'magnitude', 'spatial orientation', 'fundamental magnetic flux waves', 'open-loop estimators', 'closed-loop observers', 'robustness', 'sensitivity', 'model parameter variations', 'steady-state speed accuracy', 'parasitic effects', 'signal flow graphs', 'space vector quantities', 'closed loop systems', 'induction motor drives', 'reliability', 'robust control', 'signal flow graphs', 'stators']), ('Strong active solution in non-cooperative games\nFor the non-cooperative games and the problems of accepting or rejecting a\nproposal, a new notion of equilibrium was proposed, its place among the\nknown basic equilibria was established, and its application to the\nstatic and dynamic game problems was demonstrated\n', ['strong active solution', 'noncooperative games', 'static game problems', 'dynamic game problems', 'functional equations', 'game theory', 'matrix algebra', 'set theory']), ('Schema evolution in data warehouses\nWe address the issues related to the evolution and maintenance of data\nwarehousing systems, when underlying data sources change their schema\ncapabilities. These changes can invalidate views at the data\nwarehousing system. We present an approach for dynamically adapting\nviews according to schema changes arising on source relations. This\ntype of maintenance concerns both the schema and the data of the data\nwarehouse. The main issue is to avoid the view recomputation from\nscratch especially when views are defined from multiple sources. The\ndata of the data warehouse is used primarily in organizational\ndecision-making and may be strategic. Therefore, the schema of the data\nwarehouse can evolve for modeling new requirements resulting from\nanalysis or data-mining processing. Our approach provides means to\nsupport schema evolution of the data warehouse independently of the\ndata sources\n', ['schema evolution', 'data warehouses', 'system maintenance', 'containment', 'data sources', 'source relations', 'structural view maintenance', 'view adaptation', 'SQL query', 'organizational decision-making', 'data analysis', 'business data processing', 'data warehouses', 'query processing', 'relational databases', 'SQL']), ('Look into the future of content management\nPredictions of consolidation in the Content Management (CM) vendor arena have\nappeared in nearly every major industry prognosis over the past two\nyears. Gartner Group, for example, recently reiterated its prediction\nthat half the CM vendors in existence in mid-2001 would leave the\nmarketplace by the end of 2002. Analysts consistently advise\nprospective CM buyers to tread carefully because their vendor may not\nstick around. But fortunately, the story goes, fewer vendor choices\nwill finally bring greater clarity and sharper differentiators to this\notherwise very messy product landscape. In fact, the number of CM\nvendors continues to rise. Industry growth has come through greater\ndemand among CM buyers, but also expanding product functionality as\nwell as successful partnerships. The marketplace certainly cannot\nsustain its current breadth of vendors in the long run, yet it remains\nunclear when and how any serious industry consolidation will occur. In\nthe meantime, evolving business models and feature sets have created\njust the kind of clearer segmentation and transparent product\ndifferences that were supposed to emerge following an industry\ncontraction\n', ['content management', 'enterprise systems', 'product functionality', 'partnerships', 'industry consolidation', 'information industry', 'information resources', 'Internet']), ('Affine invariants of convex polygons\nIn this correspondence, we prove that the affine invariants, for image\nregistration and object recognition, proposed recently by Yang and\nCohen (see ibid., vol.8, no.7, p.934-46, July 1999) are algebraically\ndependent. We show how to select an independent and complete set of the\ninvariants. The use of this new set leads to a significant reduction of\nthe computing complexity without decreasing the discrimination power\n', ['affine invariants', 'convex polygons', 'algebraically dependent. invariants', 'complexity reduction', 'image registration', 'object recognition', 'convex quadruplet', 'feature vector', 'computational complexity', 'feature extraction', 'image registration', 'object recognition']), ('GK-DEVS: Geometric and kinematic DEVS formalism for simulation modeling of\n3-dimensional multi-component systems\nA combined discrete/continuous simulation methodology based on the DEVS\n(discrete event system specification) formalism is presented in this\npaper that satisfies the simulation requirements of 3-dimensional and\ndynamic systems with multi-components. We propose a geometric and\nkinematic DEVS (GK-DEVS) formalism that is able to describe the\ngeometric and kinematic structure of a system and its continuous state\ndynamics as well as the interaction among the multi-components. To\nestablish one model having dynamic behavior and a particular\nhierarchical structure, the atomic and the coupled model of the\nconventional DEVS are merged into one model in the proposed formalism.\nFor simulation of the continuous motion of 3-D components, the\nsequential state set is partitioned into the discrete and the\ncontinuous state set and the rate of change function over the\ncontinuous state set is employed. Although modified from the\nconventional DEVS formalism, the GK-DEVS formalism preserves a\nhierarchical, modular modeling fashion and a coupling scheme.\nFurthermore, for the GK-DEVS model simulation, we propose an abstract\nsimulation algorithm, called a GK-Simulator, in which data and control\nare separated and events are scheduled not globally but hierarchically\nso that an object-oriented principle is satisfied. The proposed GK-DEVS\nformalism and the GK-Simulator algorithm have been applied to the\nsimulation of a flexible manufacturing system consisting of a 2-axis\nlathe, a 3-axis milling machine, and a vehicle-mounted robot\n', ['GK-DEVS', 'kinematic DEVS', 'geometric DEVS', 'simulation modeling', '3 dimensional multi-component systems', 'combined discrete/continuous simulation methodology', 'simulation requirements', 'continuous state dynamics', 'dynamic behavior', 'continuous motion', 'sequential state set', 'abstract simulation algorithm', 'GK-Simulator', 'object-oriented principle', 'flexible manufacturing system', '2-axis lathe', '3-axis milling machine', 'vehicle-mounted robot', 'computerised numerical control', 'discrete event simulation', 'flexible manufacturing systems', 'object-oriented programming']), ('Complexity transitions in global algorithms for sparse linear systems over\nfinite fields\nWe study the computational complexity of a very basic problem, namely that of\nfinding solutions to a very large set of random linear equations in a\nfinite Galois field modulo q. Using tools from statistical mechanics we\nare able to identify phase transitions in the structure of the solution\nspace and to connect them to the changes in the performance of a global\nalgorithm, namely Gaussian elimination. Crossing phase boundaries\nproduces a dramatic increase in memory and CPU requirements necessary\nfor the algorithms. In turn, this causes the saturation of the upper\nbounds for the running time. We illustrate the results on the specific\nproblem of integer factorization, which is of central interest for\ndeciphering messages encrypted with the RSA cryptosystem\n', ['complexity transitions', 'global algorithms', 'sparse linear systems', 'finite fields', 'random linear equations', 'finite Galois field', 'statistical mechanics', 'Gaussian elimination', 'phase boundaries', 'integer factorization', 'message deciphering', 'encryption', 'RSA cryptosystem', 'disordered systems', 'combinatorial mathematics', 'computational complexity', 'cryptography', 'entropy', 'Galois fields', 'random processes', 'sparse matrices', 'statistical mechanics']), ('E-mail and the legal profession\nThe widespread use of E-mail can be found in all areas of commerce, and the\nlegal profession is one that has embraced this new medium of\ncommunication. E-mail is not without its drawbacks, however. Due to the\nnature of the technologies behind the medium, it is a less secure form\nof communication than many of those traditionally used by the legal\nprofession, including DX, facsimile, and standard and registered post.\nThere are a number of ways in which E-mails originating from the\npractice may be protected, including software encryption, hardware\nencryption and various methods of controlling and administering access\nto the E-mails\n', ['E-mail', 'legal profession', 'secure communication', 'software encryption', 'hardware encryption', 'access control', 'authorisation', 'cryptography', 'electronic mail', 'law administration', 'telecommunication security']), ('FC++: Functional tools for object-oriented tasks\nFC++ is a library for programming functionally in C++. Compared to other C++\nfunctional programming libraries, FC++ is distinguished by its powerful\ntype system which allows the manipulation of parametrically polymorphic\nfunctions (e.g., passing them as arguments to other functions and\nreturning them as results). In this paper, we show how FC++ can be used\nin common object-oriented programming tasks. We demonstrate FC++\nimplementations of several common design patterns (Adapter, Builder,\nCommand, and more). Compared to conventional C++ implementations of\nthese patterns, our implementations are either simpler (in that fewer\nclasses/dependencies are needed), more efficient, or more type-safe\n(thanks to parametric polymorphism and type inference)\n', ['library', 'functional programming', 'FC++', 'object-oriented programming', 'parametric polymorphism', 'C++', 'C++ language', 'functional programming', 'object-oriented programming', 'software libraries']), ("Teaching psychology as a laboratory science in the age of the Internet\nFor over 30 years, psychologists have relied on computers to teach experimental\npsychology. With the advent of experiment generators, students can\ncreate well-designed experiments and can test sophisticated hypotheses\nfrom the start of their undergraduate training. Characteristics of new\nNet-based experiment generators are discussed and compared with\ntraditional stand-alone generators. A call is made to formally evaluate\nthe instructional effectiveness of the wide range of experiment\ngenerators now available. Specifically, software should be evaluated in\nterms of known learning outcomes, using appropriate control groups. The\nmany inherent differences between any two software programs should be\nmade clear. The teacher's instructional method should be fully\ndescribed and held constant between comparisons. Finally, the often\ncomplex interaction between the teacher's instructional method and the\npedagogical details of the software must be considered\n", ['experimental psychology teaching', 'laboratory science', 'Internet', 'computers', 'well-designed experiments', 'hypothesis testing', 'undergraduate training', 'Net-based experiment generators', 'stand-alone generators', 'instructional effectiveness', 'software', 'known learning outcomes', 'control groups', 'teacher instructional method', 'pedagogical details', 'courseware', 'Internet', 'psychology', 'student experiments', 'teaching']), ('Novel TCP congestion control scheme and its performance evaluation\nA novel self-tuning proportional and derivative (ST-PD) control based TCP\ncongestion control scheme is proposed. The new scheme approaches the\ncongestion control problem from a control-theoretical perspective and\novercomes several Important limitations associated with existing TCP\ncongestion control schemes, which are heuristic based. In the proposed\nscheme, a PD controller is employed to keep the buffer occupancy of the\nbottleneck node on the connection path at an ideal operating level, and\nit adjusts the TCP window accordingly. The control gains of the PD\ncontroller are tuned online by a fuzzy logic controller based on the\nperceived bandwidth-delay product of the TCP connection. This scheme\ngives ST-PD TCP several advantages over current TCP implementations.\nThese include rapid response to bandwidth variations, insensitivity to\nbuffer sizes, and significant improvement of TCP throughput over lossy\nlinks by decoupling congestion control and error control functions of\nTCP\n', ['TCP congestion control scheme', 'performance evaluation', 'self-tuning proportional-derivative control', 'control-theoretical perspective', 'PD controller', 'buffer occupancy', 'bottleneck node', 'connection path', 'fuzzy logic controller', 'bandwidth-delay product', 'lossy links', 'fuzzy control', 'performance evaluation', 'telecommunication congestion control', 'transport protocols', 'tuning', 'two-term control']), ('A fundamental investigation into large strain recovery of one-way shape memory\nalloy wires embedded in flexible polyurethanes\nShape memory alloys (SMAs) are being embedded in or externally attached to\nsmart structures because of the large amount of actuation deformation\nand force that these materials are capable of producing when they are\nheated. Previous investigations have focused primarily on using single\nor opposing SMA wires exhibiting the two-way shape memory effect (SME)\nbecause of the simplicity with which the repeatable actuation behavior\nof the structure can be predicted. This repeatable actuation behavior\nis achieved at the expense of reduced levels of recoverable\ndeformation. Alternatively, many potential smart structure applications\nwill employ multiple SMA wires exhibiting a permanent one-way SME to\nsimplify fabrication and increase the recoverable strains in the\nstructure. To employ the one-way wires, it is necessary to investigate\nhow they affect the recovery of large strains when they are embedded in\na structure. In this investigation, the large strain recovery of a\none-way SMA wire embedded in a flexible polyurethane is characterized\nusing the novel deformation measurement technique known as digital\nimage correlation. These results are compared with a simple actuation\nmodel and a three-dimensional finite element analysis of the structure\nusing the Brinson model for describing the thermomechanical behavior of\nthe SMA. Results indicate that the level of actuation strain in the\nstructure is substantially reduced by the inelastic behavior of the\none-way SMA wires, and there are significant differences between the\ndeformations of the matrix material adjacent to the SMA wires and in\nthe region surrounding it. The transformation behavior of the SMA wires\nwas also determined to be volume preserving, which had a significant\neffect on the transverse strain fields\n', ['strain recovery', 'one-way shape memory', 'flexible polyurethanes', 'alloy wires', 'SMA wires', 'actuation deformation', 'two-way shape memory effect', 'recoverable strains', 'flexible polyurethane', 'embedded sensor', 'smart structures', 'three-dimensional finite element analysis', 'actuation strain', 'deformations', 'matrix material', 'transverse strain fields', 'composite materials', 'finite element analysis', 'intelligent actuators', 'intelligent control', 'intelligent structures', 'shape memory effects', 'strain control', 'strain measurement']), ('On-line robust processing techniques for elimination of measurement drop-out\nWhen processing measurement data, it is usually assumed that some amount of\nnormally distributed measurement noise is present. In some situations,\noutliers are present in the measurements and consequently the noise is\nfar from normally distributed. In this case classical least-squares\nprocedures for estimating Fourier spectra (or derived quantities like\nthe frequency response function) can give results which are inaccurate\nor even useless. In this paper, a novel technique for the on-line\nprocessing of measurement outliers will be proposed. Both the\ncomputation speed and the accuracy of the technique presented will be\ncompared with different classical approaches for handling outliers in\nmeasurement data (i.e. filtering techniques, outlier rejection\ntechniques and robust regression techniques). In particular, all\nprocessing techniques will be validated by applying them to the problem\nof speckle drop-out in optical vibration measurements (performed with a\nlaser Doppler vibrometer), which typically causes outliers in the\nmeasurements\n', ['on-line robust processing techniques', 'measurement dropout elimination', 'normally distributed measurement noise', 'classical least-squares procedures', 'Fourier spectra', 'frequency response function', 'measurement outliers', 'computation speed', 'speckle dropout', 'optical vibration measurements', 'laser Doppler vibrometer', 'laser interferometer', 'modal analysis', 'vibration velocity', 'iterative technique', 'low-pass filtering', 'median filtering', 'robust regression', 'signal sampling', 'order statistics', 'sinusoidal excitation', 'broadband excitation', 'frequency spectra', 'filtering theory', 'frequency response', 'iterative methods', 'laser velocimetry', 'least squares approximations', 'light interferometry', 'low-pass filters', 'measurement errors', 'median filters', 'modal analysis', 'random noise', 'signal sampling', 'statistical analysis', 'vibration measurement']), ("Taxonomy's role in content management\nA taxonomy is simply a way of classifying things. Still, there is a rapidly\ngrowing list of vendors offering taxonomy software and related\napplications. They promise many benefits, especially to enterprise\ncustomers: Content management will be more efficient. Corporate portals\nwill be enhanced by easily created Yahoo!-like directories of internal\ninformation. And the end-user experience will be dramatically improved\nby more successful content retrieval and more effective knowledge\ndiscovery. But today's taxonomy products represent emerging\ntechnologies. They are not out-of-the-box solutions. And even the most\nautomated systems require some manual assistance from people who know\nhow to classify content\n", ['taxonomy software', 'taxonomy applications', 'enterprise customers', 'content management', 'corporate portals', 'internal information', 'effective knowledge discovery', 'classification', 'information retrieval', 'online front-ends', 'search engines']), ("An agent-oriented and service-oriented environment for deploying dynamic\ndistributed systems\nThis paper presents JASE, a Java-based Agent-oriented and Service-oriented\nEnvironment for deploying dynamic distributed systems. JASE utilizes\ntwo important concepts in the field of distributed computing: the\nconcept of services and remote programming with mobile agents. In JASE,\nmobile agents are used to support applications, and service interface\nagents are used to wrap services. Service inter-face agents can\ndynamically register their services in Service Server. Mobile agent\nlocates a specific service interface agent by submitting requests to\nthe Service Server with descriptions of required services. JASE uses\nXML to describe both service descriptions and the mobile agent's\nqueries. JASE supports two kinds of communication facility: tuple space\nand asynchronous messages. In this paper, the design and implementation\nof JASE are described. An application shows the suitability and the\neffectiveness of the JASE and performance evaluation is also made.\nFinally, related work and some conclusions are given\n", ['agent-oriented environment', 'service-oriented environment', 'dynamic distributed systems', 'JASE', 'Java-based agent-oriented and service-oriented environment', 'remote programming', 'mobile agents', 'performance evaluation', 'hypermedia markup languages', 'Java', 'object-oriented programming', 'software agents', 'standards']), ('Cat and class: what use are these skills to the new legal information\nprofessional?\nThis article looks at the cataloguing and classification skills taught on\ninformation studies courses and the use these skills are to new legal\ninformation professionals. The article is based on the opinions of nine\nnew legal information professionals from both academic and law firm\nlibraries\n', ['legal information professional', 'cataloguing', 'classification', 'information studies courses', 'law firm libraries', 'academic libraries', 'academic libraries', 'cataloguing', 'classification', 'information science', 'law administration']), ("Pioneering women in computer science\nAlthough their contributions are not well documented, women have played an\nimportant role in the development of computer science. A survey of\nwomen pioneers demonstrates their influence in designing and\nprogramming the first electronic computers and languages, while laying\nthe groundwork for women's expanding involvement in science\n", ['pioneering women', 'computer science development', 'electronic computers', 'programming languages', 'history', 'computer science', 'gender issues', 'history', 'programming languages']), ('E - a brainiac theorem prover\nWe describe the superposition-based theorem prover E. E is a sound and complete\nprover for clausal first order logic with equality. Important\nproperties of the prover include strong redundancy elimination\ncriteria, the DISCOUNT loop proof procedure, a very flexible interface\nfor specifying search control heuristics, and an efficient inference\nengine. We also discuss the strengths and weaknesses of the system\n', ['brainiac theorem prover', 'CASC', 'superposition-based theorem prover', 'E automatic theorem prover', 'rewriting', 'completeness', 'soundness', 'clausal first order logic', 'equality', 'strong redundancy elimination criteria', 'DISCOUNT', 'CADE ATP System Competitions', 'loop proof procedure', 'search control heuristics', 'inference engine', 'formal logic', 'inference mechanisms', 'rewriting systems', 'search problems', 'theorem proving']), ('Account aggregation: shaping up portfolios\nCPA providers of financial planning services are providing clients with a\nunified view of their investments\n', ['financial planning services', 'account aggregation', 'accounting', 'investment']), ('Use of periodic and monotonic activation functions in multilayer feedforward\nneural networks trained by extended Kalman filter algorithm\nThe authors investigate the convergence and pruning performance of multilayer\nfeedforward neural networks with different types of neuronal activation\nfunctions in solving various problems. Three types of activation\nfunctions are adopted in the network, namely, the traditional sigmoid\nfunction, the sinusoidal function and a periodic function that can be\nconsidered as a combination of the first two functions. To speed up the\nlearning, as well as to reduce the network size, the extended Kalman\nfilter (EKF) algorithm conjunct with a pruning method is used to train\nthe network. The corresponding networks are applied to solve five\ntypical problems, namely, 4-point XOR logic function, parity\ngeneration, handwritten digit recognition, piecewise linear function\napproximation and sunspot series prediction. Simulation results show\nthat periodic activation functions perform better than monotonic ones\nin solving multicluster classification problems. Moreover, the combined\nperiodic activation function is found to possess the fast convergence\nand multicluster classification capabilities of the sinusoidal\nactivation function while keeping the robustness property of the\nsigmoid function required in the modelling of unknown systems\n', ['multilayer feedforward neural networks', 'extended Kalman filter algorithm', 'monotonic activation functions', 'periodic activation functions', 'convergence', 'pruning performance', 'neuronal activation functions', 'sigmoid function', 'sinusoidal function', 'EKF algorithm', '4-point XOR logic function', 'parity generation', 'handwritten digit recognition', 'piecewise linear function approximation', 'sunspot series prediction', 'multicluster classification problems', 'convergence', 'feedforward neural nets', 'handwritten character recognition', 'Kalman filters', 'learning (artificial intelligence)', 'pattern classification', 'piecewise linear techniques', 'transfer functions']), ('Nonlinearities in NARX polynomial models: representation and estimation\nIt is shown how nonlinearities are mapped in NARX polynomial models. General\nexpressions are derived for the gain and eigenvalue functions in terms\nof the regressors and coefficients of NARX models. Such relationships\nare useful in grey-box identification problems. The results are\nillustrated using simulated and real data\n', ['NARX polynomial model nonlinearities', 'nonlinearity representation', 'nonlinearity estimation', 'gain functions', 'eigenvalue functions', 'regressors', 'grey-box identification problems', 'nonlinear autoregressive exogenous-input polynomial model', 'autoregressive processes', 'eigenvalues and eigenfunctions', 'identification', 'modelling', 'polynomials']), ('Clausal resolution in a logic of rational agency\nA resolution based proof system for a Temporal Logic of Possible Belief is\npresented. This logic is the combination of the branching-time temporal\nlogic CTL (representing change over time) with the modal logic KD45\n(representing belief). Such combinations of temporal or dynamic logics\nand modal logics are useful for specifying complex properties of\nmulti-agent systems. Proof methods are important for developing\nverification techniques for these complex multi-modal logics.\nSoundness, completeness and termination of the proof method are shown\nand simple examples illustrating its use are given\n', ['resolution based proof system', 'temporal logic', 'branching-time temporal logic', 'CTL', 'modal logic', 'KD45', 'belief', 'dynamic logics', 'multi-agent systems', 'multi-modal logics', 'rational agents', 'formal logic', 'multi-agent systems', 'temporal logic']), ("Model selection in electromagnetic source analysis with an application to VEFs\nIn electromagnetic source analysis, it is necessary to determine how many\nsources are required to describe the electroencephalogram or\nmagnetoencephalogram adequately. Model selection procedures (MSPs) or\ngoodness of fit procedures give an estimate of the required number of\nsources. Existing and new MSPs are evaluated in different source and\nnoise settings: two sources which are close or distant and noise which\nis uncorrelated or correlated. The commonly used MSP residual variance\nis seen to be ineffective, that is it often selects too many sources.\nAlternatives like the adjusted Hotelling's test, Bayes information\ncriterion and the Wald test on source amplitudes are seen to be\neffective. The adjusted Hotelling's test is recommended if a\nconservative approach is taken and MSPs such as Bayes information\ncriterion or the Wald test on source amplitudes are recommended if a\nmore liberal approach is desirable. The MSPs are applied to empirical\ndata (visual evoked fields)\n", ['model selection', 'electromagnetic source analysis', 'noise settings', 'residual variance', 'Wald test', "adjusted Hotelling's test", 'empirical data', 'VEFs', 'MEG source analysis', 'EEG source analysis', 'goodness-of-fit', 'source localization', 'visual evoked fields', 'brain models', 'electroencephalography', 'magnetoencephalography', 'medical signal processing', 'vision']), ('Changes in the entropy and the Tsallis difference information during\nspontaneous decay and self-organization of nonextensive systems\nA theoretical-information description of self-organization processes during\nstimulated transitions between stationary states of open nonextensive\nsystems is presented. S/sub q/- and I/sub q/-theorems on changes of the\nentropy and Tsallis difference information measures in the process of\nevolution in the space of control parameters are proved. The entropy\nand the Tsallis difference information are derived and their new\nextreme properties are discussed\n', ['entropy', 'Tsallis difference information', 'spontaneous decay', 'self-organization', 'nonextensive systems', 'stimulated transitions', 'information measures', 'control parameters', 'nonextensive statistical mechanics', 'entropy', 'information theory', 'phase transformations', 'self-organised criticality', 'statistical mechanics']), ('A digital fountain approach to asynchronous reliable multicast\nThe proliferation of applications that must reliably distribute large, rich\ncontent to a vast number of autonomous receivers motivates the design\nof new multicast and broadcast protocols. We describe an ideal, fully\nscalable protocol for these applications that we call a digital\nfountain. A digital fountain allows any number of heterogeneous\nreceivers to acquire content with optimal efficiency at times of their\nchoosing. Moreover, no feedback channels are needed to ensure reliable\ndelivery, even in the face of high loss rates. We develop a protocol\nthat closely approximates a digital fountain using two new classes of\nerasure codes that for large block sizes are orders of magnitude faster\nthan standard erasure codes. We provide performance measurements that\ndemonstrate the feasibility of our approach and discuss the design,\nimplementation, and performance of an experimental system\n', ['digital fountain', 'asynchronous reliable multicast', 'autonomous receivers', 'broadcast protocols', 'multicast protocol', 'scalable protocol', 'heterogeneous receivers', 'optimal efficiency', 'high loss rates', 'erasure codes', 'large block size', 'performance measurements', 'experimental system performance', 'Internet', 'FEC codes', 'forward error correction', 'RS codes', 'Tornado codes', 'Luby transform codes', 'bulk data distribution', 'IP multicast', 'simulation results', 'interoperability', 'content distribution methods', 'Reed-Solomon codes', 'decoder', 'decoding', 'error correction codes', 'forward error correction', 'Internet', 'multicast communication', 'performance evaluation', 'Reed-Solomon codes', 'transport protocols']), ('Chemical production in the superlative [formaldehyde plant process control\nsystem and remote I/O system]\nBASF commissioned the largest formaldehyde production plant in the world, in\nDecember 2000, with an annual capacity of 180000 t. The new plant,\nbuilt to meet the growing demand for formaldehyde, sets new standards.\nIts size, technology and above all its cost-effectiveness give it a\nleading position internationally. To maintain such high standards by\nthe automation technology, in addition to the trail-blazing Simatic PCS\n7 process control system from Siemens, BASF selected the innovative\nremote I/O system I.S.1 from R. STAHL Schaltgerate GmbH to record and\nto output field signals in hazardous areas Zone 1 and 2. This\ncombination completely satisfied all technical requirements and also\nhad the best price-performance ratio of all the solutions. 25 remote\nI/O field stations were designed and matched to the needs of the\nformaldehyde plant\n', ['chemical production', 'formaldehyde production plant construction', 'superlative', 'BASF', 'cost-effective plant', 'automation technology', 'process control system', 'trail-blazing Simatic PCS 7', 'Siemens', 'remote I/O system I.S.1', 'R. STAHL Schaltgerate GmbH', 'signal recording', 'Zone 1 hazardous area', 'Zone 2 hazardous area', 'price-performance ratio', 'remote I/O field station design', 'chemical industry', 'industrial plants', 'process control', 'production control', 'telecontrol']), ('Remember e-commerce? Yeah, well, it\'s still here\nSandy Kemper, the always outspoken CEO of successful e-commerce company eScout,\noffers his views on the purported demise of "commerce" in e-commerce,\nand what opportunities lie ahead for those bankers bold enough to act\nin a market turned tentative by early excesses\n', ['e-commerce', 'bankers', 'eScout', 'banking', 'electronic commerce']), ('International news sites in English\nWeb access to news sites all over the world allows us the opportunity to have\nan electronic news stand readily available and stocked with a variety\nof foreign (to us) news sites. A large number of currently available\nforeign sites are English-language publications or English language\nversions of non-North American sites. These sites are quite varied in\nterms of quality, coverage, and style. Finding them can present a\nchallenge. Using them effectively requires critical-thinking skills\nthat are a part of media awareness or digital literacy\n', ['Web access', 'international news sites', 'English-language publications', 'non North American sites', 'critical-thinking skills', 'media awareness', 'digital literacy', 'information resources', 'information retrieval', 'information science']), ("HPF/JA: extensions of High Performance Fortran for accelerating real-world\napplications\nThis paper presents a set of extensions on High Performance Fortran (HPF) to\nmake it more usable for parallelizing real-world production codes. HPF\nhas been effective for programs that a compiler can automatically\noptimize efficiently. However, once the compiler cannot, there have\nbeen no ways for the users to explicitly parallelize or optimize their\nprograms. In order to resolve the situation, we have developed a set of\nHPF extensions (HPF/JA) to give the users more control over\nsophisticated parallelization and communication optimizations. They\ninclude parallelization of loops with complicated reductions,\nasynchronous communication, user-controllable shadow, and communication\npattern reuse for irregular remote data accesses. Preliminary\nexperiments have proved that the extensions are effective at increasing\nHPF's usability\n", ['High Performance Fortran', 'HPF', 'parallel processing', 'compiler', 'data parallel language', 'supercomputer', 'parallelization of loops', 'parallel programming', 'FORTRAN', 'parallelising compilers']), ("Web-based intelligent helpdesk-support environment\nWith the advent of Internet technology, it is now feasible to provide effective\nand efficient helpdesk service over the global Internet to meet\ncustomers' requirements and satisfaction. In this research, we have\ndesigned and developed a Web-based intelligent helpdesk-support\nenvironment, WebHotLine, to support the customer service centre of a\nlarge multinational corporation in the electronics industry. The paper\ndescribes the basic architecture of the environment that supports the\nmajor functions of Web-based fault information retrieval, online\nmultilingual translation capability, different operating modes of\nvideo-conferencing for enhanced support and direct intelligent fault\ndiagnosis by customers or customer support engineers. As a result,\nWebHotLine helps to save cost in eliminating the expensive overseas\ntelephone charges, reduction in machine down time and number of on-site\nvisits by service engineers as in traditional helpdesk environment\n", ['Web-based intelligent helpdesk-support environment', 'Internet technology', 'WebHotLine', 'customer service centre', 'Web-based fault information retrieval', 'online multilingual translation capability', 'videoconferencing', 'help systems', 'information resources', 'technical support services']), ('Fault-tolerant computer-aided control systems with multiversion-threshold\nadaptation: adaptation methods, reliability estimation, and choice of\nan architecture\nFor multiversion majority-redundant computer-aided control systems,\nsystematization of adaptation methods that are stable to hardware and\nsoftware failures, a method for estimating their reliability from an\nevent graph model, and a method for selecting a standard architecture\nwith regard for reliability requirements are studied\n', ['fault-tolerant computer-aided control systems', 'multiversion-threshold adaptation', 'reliability estimation', 'architecture', 'multiversion majority-redundant computer-aided control systems', 'hardware failure stability', 'software failure stability', 'event graph model', 'computerised control', 'fault tolerant computing', 'graph theory']), ("Efficient allocation of knowledge in distributed business structures\nAccelerated business processes demand new concepts and realizations of\ninformation systems and knowledge databases. This paper presents the\nconcept of the collaborative information space (CIS), which supplies\nthe necessary tools to transform individual knowledge into collective\nuseful information. The creation of 'information objects' in the CIS\nallows an efficient allocation of information in all business process\nsteps at any time. Furthermore, the specific availability of\nheterogeneous, distributed data is realized by a Web-based user\ninterface, which enables effective search by a multidimensionally\nhierarchical composition\n", ['distributed business structures', 'efficient knowledge allocation', 'accelerated business processes', 'information systems', 'knowledge databases', 'collaborative information space', 'information objects', 'business process steps', 'heterogeneous distributed data', 'Web-based user interface', 'multidimensionally hierarchical composition', 'interactive system', 'business data processing', 'distributed databases', 'groupware', 'information resources', 'interactive systems', 'user interfaces']), ('Exact analytical model for the AEP of control signals\nAn exact analytical model for the aliasing error probability (AEP) in the\nsignature analysis of control signals using a modified signature\nanalyser is presented. The signature analyser used comprises a\ngeneral-structure two-input compacting module (TICM), which simplifies\nthe motherboard VLSI design by providing a flexible geometry, which\ncould be easily integrated with neighbouring structures. The use of the\nmodified data probe eliminates the ambiguity introduced by the\nhigh-impedance state and at the same time retains the same signature of\nthe binary stream. The model specifies algebraically the effects of the\nTICM architecture, the test pattern length, and the control stream\nerror probabilities. It is proved that the (hardware) criterion used\nfor calculating the AEP for the internal- and external exclusive-OR\ntwo-input shift registers is not valid for the general case and a new\ncriterion is provided. The results obtained are augmented by two\nspecial cases, a case study, and associated simulation\n', ['analytical model', 'AEP', 'control signals', 'aliasing error probability', 'signature analysis', 'signature analyser', 'two-input compacting module', 'motherboard VLSI design', 'flexible geometry', 'high-impedance state', 'binary stream', 'test pattern length', 'control stream error probabilities', 'two-input shift registers', 'antialiasing', 'automatic testing', 'error statistics', 'logic testing', 'shift registers', 'VLSI']), ("Information architecture for the Web: The IA matrix approach to designing\nchildren's portals\nThe article presents a matrix that can serve as a tool for designing the\ninformation architecture of a Web portal in a logical and systematic\nmanner. The information architect begins by inputting the portal's\nobjective, target user, and target content. The matrix then determines\nthe most appropriate information architecture attributes for the portal\nby filling in the Applied Information Architecture portion of the\nmatrix. The article discusses how the matrix works using the example of\na children's Web portal to provide access to museum information\n", ['information architecture', 'target user', 'target content', "children's Web portal", 'museum information', 'computer aided instruction', 'electronic publishing', 'exhibitions', 'humanities', 'information resources']), ('A self-adjusting quality of service control scheme\nWe propose and analyze a self-adjusting Quality of Service (QoS) control scheme\nwith the goal of optimizing the system reward as a result of servicing\ndifferent priority clients with varying workload, QoS and\nreward/penalty requirements. Our scheme is based on resource\npartitioning and designated "degrade QoS areas" such that system\nresources are partitioned into priority areas each of which is reserved\nspecifically to serve only clients in a corresponding class with no QoS\ndegradation, plus one "degraded QoS area" into which all clients can be\nadmitted with QoS adjustment being applied only to the lowest priority\nclients. We show that the best partition is dictated by the workload\nand the reward/penalty characteristics of clients in difference\npriority classes. The analysis results can be used by a QoS manager to\noptimize the system total reward dynamically in response to changing\nworkloads at run time. We demonstrate the validity of our scheme by\nmeans of simulation and comparing the proposed QoS self-adjusting\nscheme with those that do not use resource partitioning or designated\ndegraded QoS areas\n', ['self-adjusting quality of service control scheme', 'priority clients', 'resource partitioning', 'simulation', 'multimedia systems', 'performance evaluation', 'resource reservation', 'multimedia systems', 'quality of service']), ("Dueling platforms [healthcare network servers]\nMany large hospitals and healthcare systems have grown accustomed to the\nreliability of mainframe architecture, although tighter operating\nbudgets, coupled with advances in client/server technology, have led to\nmore office and clinical applications being moved off mainframes. But\nEvanston Northwestern Healthcare wasn't ready to get rid of its IBM OS\n390 mainframe just yet. While a number of new clinical applications are\nbeing installed on two brand new IBM servers, Evanston Northwestern\nHealthcare will retain its favored hospital billing system and let it\nreside on the organization's mainframe, as it has since 1982\n", ['network servers', 'Evanston Northwestern Healthcare', 'IBM OS 390 mainframe', 'Leapfrog Group', 'computerized physician order entry system', 'health care', 'mainframes', 'network servers']), ("To classify or not to classify, that is the question?\nIn addressing classification issues, the librarian needs to decide what best\nsuits the purpose and requirements of the user group and the\norganisation they work in. The author has used the well-established\nMoys Classification Scheme. This gives the level of detail required for\ncurrent stock and allows for the incorporation of new material as the\nfirm's specialisations develop. The scheme is widely used in other\nfirms as well as in the local law society library, so it will be\nfamiliar to many users\n", ['Moys Classification Scheme', 'law society library', 'classification', 'digital libraries', 'law administration', 'library automation']), ('Computing failure probabilities. Applications to reliability analysis\nThe paper presents one method for calculating failure probabilities with\napplications to reliability analysis. The method is based on\ntransforming the initial set of variables to a n-dimensional uniform\nrandom variable in the unit hypercube, together with the limit\ncondition set and calculating the associated probability using a\nrecursive method based on the Gauss-Legendre quadrature formulas to\ncalculate the resulting multiple integrals. An example of application\nis used to illustrate the proposed method\n', ['failure probabilities computation', 'reliability analysis applications', 'n-dimensional uniform random variable', 'unit hypercube', 'limit condition', 'recursive method', 'Gauss-Legendre quadrature formulae', 'multiple integrals calculation', 'tail approximation', 'failure analysis', 'probability', 'recursive estimation', 'reliability theory']), ('On average depth of decision trees implementing Boolean functions\nThe article considers the representation of Boolean functions in the form of\ndecision trees. It presents the bounds on average time complexity of\ndecision trees for all classes of Boolean functions that are closed\nover substitution, and the insertion and deletion of unessential\nvariables. The obtained results are compared with the results developed\nby M.Ju. Moshkov (1995) that describe the worst case time complexity of\ndecision trees\n', ['average depth', 'decision trees', 'Boolean functions', 'average time complexity', 'worst case time complexity', 'Boolean functions', 'computational complexity', 'decision trees']), ('Truss topology optimization by a modified genetic algorithm\nThis paper describes the use of a stochastic search procedure based on genetic\nalgorithms for developing near-optimal topologies of load-bearing truss\nstructures. Most existing cases these publications express the truss\ntopology as a combination of members. These methods, however, have the\ndisadvantage that the resulting topology may include needless members\nor those which overlap other members. In addition to these problems,\nthe generated structures are not necessarily structurally stable. A new\nmethod, which resolves these problems by expressing the truss topology\nas a combination of triangles, is proposed in this paper. Details of\nthe proposed methodology are presented as well as the results of\nnumerical examples that clearly show the effectiveness and efficiency\nof the method\n', ['stochastic search procedure', 'modified genetic algorithm', 'near-optimal topologies', 'load-bearing truss structures', 'truss topology optimization', 'triangles', 'genetic algorithms', 'structural engineering computing']), ('Minimizing blossoms under symmetric linear constraints\nIn this paper, we show that there exists a close dependence between the control\npolygon of a polynomial and the minimum of its blossom under symmetric\nlinear constraints. We consider a given minimization problem P, for\nwhich a unique solution will be a point delta on the Bezier curve. For\nthe minimization function f, two sufficient conditions exist that\nensure the uniqueness of the solution, namely, the concavity of the\ncontrol polygon of the polynomial and the characteristics of the Polya\nfrequency-control polygon where the minimum coincides with a critical\npoint of the polynomial. The use of the blossoming theory provides us\nwith a useful geometrical interpretation of the minimization problem.\nIn addition, this minimization approach leads us to a new method of\ndiscovering inequalities about the elementary symmetric polynomials\n', ['control polygon', 'polynomial', 'blossom minimization', 'symmetric linear constraints', 'Bezier curve', 'concavity', 'Polya frequency-control polygon', 'critical point', 'geometrical interpretation', 'inequalities', 'elementary symmetric polynomials', 'computational geometry', 'minimisation', 'polynomials']), ("Aggregators versus disintermediators: battling it out in the information\nsuperhighstreet\nPerhaps the future of large-scale content aggregators is now no longer in doubt\nbut this was not the case 10 years ago, when many leading industry\nexperts were much more pessimistic in their predictions. In the year\nthat Dialog celebrates its thirtieth anniversary as the world's oldest\nand largest professional online information service, it is appropriate\nto look back at these changing perceptions, the reasons for these\nchanges, and why the experts got it wrong. We also look at the present\nday; the value that large-scale content aggregators bring to the\ninformation supply chain; and we discuss why users would choose to use\naggregators as opposed to going directly to the publishers\n", ['large-scale content aggregators', 'online information service', 'information supply chain', 'disintermediators', 'information industry', 'information services']), ('A conceptual framework for evaluation of information technology investments\nThe decision to acquire a new information technology poses a number of serious\nevaluation and selection problems to technology managers, because the\nnew system must not only meet current information requirements of the\norganisation, but also the needs for future expansion. Tangible and\nintangible benefits factors, as well as risks factors, must be\nidentified and evaluated. The paper provides a review of ten major\nevaluation categories and available models, which fall under each\ncategory, showing their advantages and disadvantages in handling the\nabove difficulties. This paper describes strategic implications\ninvolved in the selection decision, and the inherent difficulties in:\n(1) choosing or developing a model, (2) obtaining realistic inputs for\nthe model, and (3) making tradeoffs among the conflicting factors. It\nproposes a conceptual framework to help the decision maker in choosing\nthe most appropriate methodology in the evaluation process. It also\noffers a new model, called GAHP, for the evaluation problem combining\ninteger goal linear programming and analytic hierarchy process (AHP) in\na single hybrid multiple objective multi-criteria model. A goal\nprogramming methodology, with zero-one integer variables and mixed\ninteger constraints, is used to set goal target values against which\ninformation technology alternatives are evaluated and selected. AHP is\nused to structure the evaluation process providing pairwise comparison\nmechanisms to quantify subjective, nonmonetary, intangible benefits and\nrisks factors, in deriving data for the model. A case illustration is\nprovided showing how GAHP can be formulated and solved\n', ['information technology investments', 'technology managers', 'information requirements', 'risks factors', 'evaluation categories', 'selection decision', 'tradeoffs', 'decision maker', 'analytic hierarchy process', 'hybrid multiple objective multi-criteria model', 'goal programming methodology', 'zero-one integer variables', 'mixed integer constraints', 'goal target values', 'information technology alternatives', 'pairwise comparison mechanisms', 'nonmonetary benefits', 'intangible benefits', 'group decision process', 'information technology', 'investment', 'mathematical programming']), ('Design, analysis and testing of some parallel two-step W-methods for stiff\nsystems\nParallel two-step W-methods are linearly-implicit integration methods where the\ns stage values can be computed in parallel. We construct methods of\nstage order q = s and order p = s with favourable stability properties.\nGeneralizations for the concepts of A- and L-stability are proposed and\nconditions for stiff accuracy are given. Numerical comparisons on a\nshared memory computer show the efficiency of the methods, especially\nin combination with Krylov-techniques for large stiff systems\n', ['parallel two-step W-methods', 'large stiff systems', 'linearly-implicit integration methods', 'stage order', 'stability', 'shared memory computer', 'differential equations', 'convergence analysis', 'Krylov-techniques', 'differential equations', 'integration', 'mathematics computing', 'numerical stability', 'parallel processing', 'shared memory systems']), ('Optical setup and analysis of disk-type photopolymer high-density holographic\nstorage\nA relatively simple scheme for disk-type photopolymer high-density holographic\nstorage based on angular and spatial multiplexing is described. The\neffects of the optical setup on the recording capacity and density are\nstudied. Calculations and analysis show that this scheme is more\neffective than a scheme based on the spatioangular multiplexing for\ndisk-type photopolymer high-density holographic storage, which has a\nlimited medium thickness. Also an optimal beam recording angle exists\nto achieve maximum recording capacity and density\n', ['disk-type photopolymer high-density holographic storage', 'optical setup', 'angular multiplexing', 'spatial multiplexing', 'recording capacity', 'recording density', 'spatio-angular multiplexing', 'limited medium thickness', 'optimal beam recording angle', 'maximum recording capacity', 'maximum density', 'holographic storage', 'multiplexing', 'optical disc storage', 'optical polymers', 'optimisation']), ('A nonlinear time-optimal control problem\nSufficient conditions for the existence of an optimal control in a time-optimal\ncontrol problem with fixed ends for a smooth nonlinear control system\nare formulated. The properties of this system for characterizing the\noptimal control switching points are studied\n', ['nonlinear time-optimal control problem', 'sufficient existence conditions', 'smooth nonlinear control system', 'optimal control switching points', 'boundary-value problems', 'nonlinear control systems', 'time optimal control']), ("Riccati-based preconditioner for computing invariant subspaces of large\nmatrices\nThis paper introduces and analyzes the convergence properties of a method that\ncomputes an approximation to the invariant subspace associated with a\ngroup of eigenvalues of a large not necessarily diagonalizable matrix.\nThe method belongs to the family of projection type methods. At each\nstep, it refines the approximate invariant subspace using a linearized\nRiccati's equation which turns out to be the block analogue of the\ncorrection used in the Jacobi-Davidson method. The analysis conducted\nin this paper shows that the method converges at a rate quasi-quadratic\nprovided that the approximate invariant subspace is close to the exact\none. The implementation of the method based on multigrid techniques is\nalso discussed and numerical experiments are reported\n", ['Riccati-based preconditioner', 'invariant subspaces', 'large matrices', 'eigenvalues', 'diagonalizable matrix', 'projection type methods', 'Jacobi-Davidson method', 'multigrid techniques', 'differential equations', 'eigenvalues and eigenfunctions', 'matrix algebra', 'Riccati equations']), ('Tablet PCs on the way [publishing markets]\nPreviews of hardware and software look promising for publishing markets\n', ['publishing markets', 'Tablet PC', 'notebook computers', 'publishing']), ('Control of integral processes with dead-time. 2. Quantitative analysis\nFor part 1, see ibid., p.285-90, (2002). Several different control schemes for\nintegral processes with dead time resulted in the same disturbance\nresponse. It has already been shown that such a response is subideal.\nHence, it is necessary to quantitatively analyse the achievable\nspecifications and the robust stability regions. The control parameter\ncan be quantitatively determined with a compromise between the\ndisturbance response and the robustness. Four specifications:\n(normalised) maximum dynamic error, maximum decay rate, (normalised)\ncontrol action bound and approximate recovery time are used to\ncharacterise the step-disturbance response. It is shown that any\nattempt to obtain a (normalised) dynamic error less than tau /sub m/ is\nimpossible and a sufficient condition on the (relative)\ngain-uncertainty bound is square root (3)/2\n', ['integral processes', 'dead-time', 'quantitative analysis', 'disturbance response', 'robust stability regions', 'robustness', 'maximum dynamic error', 'maximum decay rate', 'control action bound', 'approximate recovery time', 'step-disturbance response', 'sufficient condition', 'gain-uncertainty bound', 'control system analysis', 'observers', 'robust control']), ("World's biggest battery helps to stabilise Alaska\nIn this paper, the author describes a battery energy storage system which is\nunder construction to provide voltage compensation in support of\nAlaska's 138 kV Northern Intertie\n", ['power system stabilisation', 'battery energy storage system', 'voltage compensation', 'USA', 'interconnected power systems', '138 kV', '77 MW', 'battery storage plants', 'power system interconnection', 'power system stability', 'power transmission control', 'power transmission lines']), ('An augmented spatial digital tree algorithm for contact detection in\ncomputational mechanics\nBased on the understanding of existing spatial digital tree-based contact\ndetection approaches, and the alternating digital tree (ADT) algorithm\nin particular, a more efficient algorithm, termed the augmented spatial\ndigital tree (ASDT) algorithm, is proposed in the present work. The\nASDT algorithm adopts a different point representation scheme that uses\nonly the lower comer vertex to represent a (hyper-)rectangle, with the\nupper comer vertex serving as the augmented information. Consequently,\nthe ASDT algorithm can keep the working space the same as the original\nn-dimensional space and, in general, a much better balanced tree can be\nexpected. This, together with the introduction of an additional\nbounding subregion for the rectangles associated with each tree node,\nmakes it possible to significantly reduce the number of node visits in\nthe region search, although each node visit may be slightly more\nexpensive. Three examples arising in computational mechanics are\npresented to provide an assessment of the performance of the ASDT. The\nnumerical results indicate that the ASDT is, at least, over 3.9 times\nfaster than the ADT\n', ['augmented spatial digital tree algorithm', 'contact detection', 'computational mechanics', 'alternating digital tree algorithm', 'upper comer vertex', 'augmented data structure', 'spatial binary tree-based contact detection approaches', 'digital simulation', 'finite element analysis', 'mechanical contact', 'physics computing']), ('Correction to construction of panoramic image mosaics with global and local\nalignment\nFor original paper see ibid., vol. 36, no. 2, p. 101-30 (2000). The authors had\ngiven a method for the construction of panoramic image mosaics with\nglobal and local alignment. Unfortunately a mistake had led to an\nincorrect equation which whilst making little difference in many cases,\nfor faster (and assured) convergence, the correct formulae given here\nshould be used\n', ['panoramic image mosaics', 'global alignment', 'local alignment', 'resampled image', 'computer vision', 'motion estimation']), ('Rapid Cauer filter design employing new filter model\nThe exact three-dimensional (3D) design of a coaxial Cauer filter employing a\nnew filter model, a 3D field simulator and a circuit simulator, is\ndemonstrated. Only a few iterations between the field simulator and the\ncircuit simulator are necessary to meet a given specification\n', ['Cauer filter', 'filter design', 'filter model', '3D design', 'coaxial filter', 'field simulator', 'circuit simulator', 'iterations', 'bandpass filters', 'band-pass filters', 'circuit CAD', 'circuit simulation', 'microwave filters', 'passive filters', 'resonator filters']), ('Silicon debug of a PowerPC TM microprocessor using model checking\nWhen silicon is available, newly designed microprocessors are tested in\nspecially equipped hardware laboratories, where real applications can\nbe run at hardware speeds. However, the large volumes of code being\nrun, plus the limited access to the internal nodes of the chip, make it\nvery difficult to characterize the nature of any failures that occur.\nWe describe how temporal logic model checking was used to quickly\ncharacterize a design error exhibited during hardware testing of a\nPowerPC microprocessor. We outline the conditions under which model\nchecking can efficiently characterize such failures, and show how the\nparticular error we detected could have been revealed early in the\ndesign cycle, by model checking a short and simple correctness\nspecification. We discuss the implications of this for verification\nmethodologies over the full design cycle\n', ['PowerPC microprocessor', 'model checking', 'temporal logic', 'circuit design error', 'hardware testing', 'correctness specification', 'verification methodologies', 'Computation Tree Logic', 'circuit debugging', 'circuit CAD', 'computer debugging', 'formal verification', 'integrated circuit testing', 'microprocessor chips', 'temporal logic']), ("The Malaysian model\nJapan's first third generation service, Foma, is unlikely to be truly\nattractive to consumers until 2005. That still falls well within the\nfinancial planning of its operator Docomo. But where does that leave\nEuropean 3G operators looking for reassurance? Malaysia, says Simon\nMarshall\n", ['3G operators', 'Malaysia', 'Maxis Communications', 'Telekom Malaysia', 'cellular radio', 'mobile computing']), ('A new algebraic modelling approach to distributed problem-solving in MAS\nThis paper is devoted to a new algebraic modelling approach to distributed\nproblem-solving in multi-agent systems (MAS), which is featured by a\nunified framework for describing and treating social behaviors, social\ndynamics and social intelligence. A conceptual architecture of\nalgebraic modelling is presented. The algebraic modelling of typical\nsocial behaviors, social situation and social dynamics is discussed in\nthe context of distributed problem-solving in MAS. The comparison and\nsimulation on distributed task allocations and resource assignments in\nMAS show more advantages of the algebraic approach than other\nconventional methods\n', ['algebraic modelling approach', 'distributed problem-solving', 'multi-agent systems', 'unified framework', 'social behaviors', 'social dynamics', 'social intelligence', 'distributed task allocations', 'resource assignments', 'artificial intelligence', 'multi-agent systems', 'problem solving', 'software agents']), ("Disappointment reigns [retail IT]\nCPFR remains at the forefront of CIOs' minds, but a number of barriers, such as\nsecretive corporate cultures and spotty data integrity, stand between\nretail organizations and true supply-chain collaboration. CIOs remain\nvexed at these obstacles, as was evidenced at a roundtable discussion\nby retail and consumer-goods IT leaders at the Retail Systems 2002\nconference, held in Chicago by the consultancy MoonWatch Media Inc.,\nNewton Upper Falls, Mass. Other annoyances discussed by retail CIOs\ninclude poorly designed business processes and retail's poor image with\nthe IT talent emerging from school into the job market\n", ['retail', 'MoonWatch Media', 'Retail Systems 2002 conference', 'CIOs', 'collaborative planning forecasting and replenishment', 'retailing']), ('Agent-based product-support logistics system using XML and RDF\nThe capability of the timely provision of maintenance services and service\nparts is critical to the competitiveness of industrial systems. To\nenhance the timely operations in a product-support logistics chain,\nbusiness partners (equipment manufacturers, parts distributors,\ncustomers) may have to collaborate for the efficient exchange of\nrelevant information. We propose the architecture of an agent-based\nproduct-support logistics system. Emphasis is placed on the problems of\nsharing and exchanging information through agent communication. We\nadopt the Resource Description Framework (RDF) schema for information\nmodelling in product-support logistics domain. The eXtensible Markup\nLanguage (XML) serialization generates messages for agent\ncommunication. The use of XML and RDF enables software agents to\nunderstand the contents of messages correctly and consistently. We\ndemonstrate the feasibility of our agent architecture using a scenario\nin logistical support processes. We believe that the approach can\nprovide a promising way to the automation of business processes in\nproduct-support logistics through seamless communication among the\npartners\n', ['agent-based product-support logistics system', 'XML', 'RDF', 'service parts', 'industrial systems', 'maintenance services', 'Resource Description Framework schema', 'information modelling', 'eXtensible Markup Language', 'software agents', 'software agents', 'software maintenance']), ('Phase conditions for Schur polynomials\nThe rate of change of phase of a real or complex Schur polynomial, evaluated\nalong the unit circle traversed counterclockwise, is strictly positive.\nFor polynomials with real coefficients, this bound can be tightened.\nThese and some other fundamental bounds on the rate of change of phase\nare derived here, using the Tchebyshev representation of the image of a\nreal polynomial evaluated on the unit circle\n', ['phase conditions', 'Schur polynomial', 'real coefficients', 'rate of change of phase', 'Tchebyshev representation', 'phase monotonicity', 'robust stability', 'discrete-time control systems', 'stabilization', 'Chebyshev approximation', 'polynomials', 'stability']), ('Blended implementation of block implicit methods for ODEs\nIn this paper we further develop a new approach for naturally defining the\nnonlinear splittings needed for the implementation of block implicit\nmethods for ODEs, which has been considered by Brugnano [J. Comput.\nAppl. Math. 116 (2000) 41] and by Brugnano and Trigiante [in: Recent\nTrends in Numerical Analysis, Nova Science, New York, 2000, pp.\n81-105]. The basic idea is that of defining the numerical method as the\ncombination (blending) of two suitable component methods. By carefully\nchoosing such methods, it is shown that very efficient implementations\ncan be obtained. Moreover, some of them, characterized by a diagonal\nsplitting, are well suited for parallel computers. Numerical tests\ncomparing the performances of the proposed implementation with existing\nones are also presented, in order to make evident the potential of the\napproach\n', ['nonlinear splittings', 'block implicit methods', 'blended implementation', 'ODEs', 'numerical method', 'diagonal splitting', 'parallel computers', 'numerical tests', 'differential equations']), ("The open-source HCS project\nDespite the rumors, the HCS II project is not dead. In fact, HCS has been\nlicensed and is now an open-source project. In this article, the author\nbrings us up to speed on the HCS II project's past, present, and\nfuture. The HCS II is an expandable, standalone, network-based\n(RS-485), intelligent-node, industrial-oriented supervisory control\n(SC) system intended for demanding home control applications. The HCS\nincorporates direct and remote digital inputs and outputs, direct and\nremote analog inputs and outputs, real time or Boolean decision event\ntriggering, X10 transmission and reception, infrared remote control\ntransmission and reception, remote LCDs, and a master console. Its\nprogram is compiled on a PC with the XPRESS compiler and then\ndownloaded to the SC where it runs independently of the PC\n", ['HCS II', 'supervisory control system', 'home control', 'network-based', 'home automation', 'home computing']), ('Multilayered image representation: application to image compression\nThe main contribution of this work is a new paradigm for image representation\nand image compression. We describe a new multilayered representation\ntechnique for images. An image is parsed into a superposition of\ncoherent layers: piecewise smooth regions layer, textures layer, etc.\nThe multilayered decomposition algorithm consists in a cascade of\ncompressions applied successively to the image itself and to the\nresiduals that resulted from the previous compressions. During each\niteration of the algorithm, we code the residual part in a lossy way:\nwe only retain the most significant structures of the residual part,\nwhich results in a sparse representation. Each layer is encoded\nindependently with a different transform, or basis, at a different\nbitrate, and the combination of the compressed layers can always be\nreconstructed in a meaningful way. The strength of the multilayer\napproach comes from the fact that different sets of basis functions\ncomplement each others: some of the basis functions will give\nreasonable account of the large trend of the data, while others will\ncatch the local transients, or the oscillatory patterns. This\nmultilayered representation has a lot of beautiful applications in\nimage understanding, and image and video coding. We have implemented\nthe algorithm and we have studied its capabilities\n', ['image compression', 'multilayered representation', 'image representation', 'piecewise smooth regions layer', 'textures layer', 'multilayered decomposition algorithm', 'residual part', 'sparse representation', 'basis functions', 'wavelet transforms', 'cosine transforms', 'transform coding', 'data compression', 'image coding', 'image representation', 'transform coding']), ('Regularization of linear regression problems\nThe study considers robust estimation of linear regression parameters by the\nregularization method, the pseudoinverse method, and the Bayesian\nmethod allowing for correlations and errors in the data. Regularizing\nalgorithms are constructed and their relationship with pseudoinversion,\nthe Bayesian approach, and BLUE is investigated\n', ['linear regression problems regularization', 'robust estimation', 'linear regression parameters', 'pseudoinverse method', 'Bayesian method', 'pseudoinversion', 'Bayesian approach', 'BLUE', 'Bayes methods', 'estimation theory', 'probability', 'statistical analysis']), ("Software vendors' failure fuels consolidation theories [telecom interconnection\nand billing]\nAs independent software vendors like AP Engines fall by the wayside as\nindependent entities, attrition could accelerate consolidation in the\nOSS space\n", ['telecom interconnection', 'telecom billing', 'operations supports systems', 'Quintessent', 'consolidation', 'AP Engines', 'DP industry', 'telecommunication']), ('Access to information for blind and visually impaired clients\nThis article guides I&R providers in establishing effective communication\ntechniques for working with visually impaired consumers. The authors\ndiscuss common causes of vision impairment and the functional\nimplications of each and offer information on disability etiquette and\neffective voice, accessible media and in-person communication. There is\nan overview of assistive technologies used by people who are visually\nimpaired-to facilitate written and electronic communications as well as\nlow-tech solutions for producing large-print and Braille materials\nin-house. Providers who implement these communication techniques will\nbe well equipped to serve visually-impaired consumers, and consumers\nwill be more likely to avail themselves of these services when\nproviders make them easily accessible\n', ['information access', 'blind clients', 'visually impaired clients', 'information and referral systems', 'communication techniques', 'disability etiquette', 'effective voice', 'accessible media', 'in-person communication', 'assistive technologies', 'electronic communications', 'written communications', 'large-print materials', 'Braille materials', 'handicapped aids', 'information retrieval', 'information services']), ('I-WAP: an intelligent WAP site management system\nThe popularity regarding wireless communications is such that more and more WAP\nsites have been developed with wireless markup language (WML).\nMeanwhile, to translate hypertext markup language (HTML) pages into\nproper WML ones becomes imperative since it is difficult for WAP users\nto read most contents designed for PC users via their mobile phone\nscreens. However, for those sites that have been maintained with\nhypertext markup language (HTML), considerable time and manpower costs\nwill be incurred to rebuild them with WML. In this paper, we propose an\nintelligent WAP site management system to cope with these problems.\nWith the help of the intelligent management system, the original\ncontents of HTML Web sites can be automatically translated to proper\nWAP content in an efficient way. As a consequence, the costs associated\nwith maintaining WAP sites could be significantly reduced. The\nmanagement system also allows the system manager to define the\nrelevance of numerals and keywords for removing unimportant or\nmeaningless contents. The original contents will be reduced and\nreorganized to fit the size of mobile phone screens, thus reducing the\ncommunication cost and enhancing readability. Numerical results gained\nthrough various experiments have evinced the effective performance of\nthe WAP management system\n', ['intelligent WAP site management system', 'I-WAP', 'wireless communication', 'wireless markup language', 'hypertext markup language', 'HTML pages', 'mobile phone', 'communication cost', 'readability', 'wireless mobile Internet', 'computer network management', 'hypermedia markup languages', 'Internet', 'mobile computing', 'mobile radio']), ('The exact solution of coupled thermoelectroelastic behavior of piezoelectric\nlaminates\nExact solutions for static analysis of thermoelectroelastic laminated plates\nare presented. In this analysis, a new concise procedure for the\nanalytical solution of composite laminated plates with piezoelectric\nlayers is developed. A simple eigenvalue formula in real number form is\ndirectly developed from the basic coupled piezoelectric differential\nequations and the difficulty of treating imaginary eigenvalues is\navoided. The solution is defined in the trigonometric series and can be\napplied to thin and thick plates. Numerical studies are conducted on a\nfive-layer piezoelectric plate and the complexity of stresses and\ndeformations under combined loading is illustrated. The results could\nbe used as a benchmark for assessing any numerical solution by\napproximate approaches such as the finite element method while also\nproviding useful physical insight into the behavior of piezoelectric\nplates in a thermal environment\n', ['exact solution', 'coupled thermoelectroelastic behavior', 'piezoelectric laminates', 'thermoelectroelastic laminated plates', 'analytical solution', 'composite laminated plates', 'piezoelectric layers', 'eigenvalue formula', 'real number form', 'coupled piezoelectric differential equations', 'trigonometric series', 'thin plates', 'thick plates', 'five-layer piezoelectric plate', 'numerical study', 'stresses', 'deformations', 'combined loading', 'finite element method', 'thermal environment', 'deformation', 'differential equations', 'eigenvalues and eigenfunctions', 'finite element analysis', 'intelligent structures', 'laminates', 'piezoelectric materials', 'structural engineering computing', 'thermoelasticity']), ('Control of thin film growth in chemical vapor deposition manufacturing systems:\na feasibility study\nA study is carried out to design and optimize chemical vapor deposition (CVD)\nsystems for material fabrication. Design and optimization of the CVD\nprocess is necessary to satisfying strong global demand and ever\nincreasing quality requirements for thin film production. Advantages of\ncomputer aided optimization include high design turnaround time,\nflexibility to explore a larger design space and the development and\nadaptation of automation techniques for design and optimization. A CVD\nreactor consisting of a vertical impinging jet at atmospheric pressure,\nfor growing titanium nitride films, is studied for thin film\ndeposition. Numerical modeling and simulation are used to determine the\nrate of deposition and film uniformity over a wide range of design\nvariables and operating conditions. These results are used for system\ndesign and optimization. The optimization procedure employs an\nobjective function characterizing film quality, productivity and\noperational costs based on reactor gas flow rate, susceptor temperature\nand precursor concentration. Parameter space mappings are used to\ndetermine the design space, while a minimization algorithm, such as the\nsteepest descent method, is used to determine optimal operating\nconditions for the system. The main features of computer aided design\nand optimization using these techniques are discussed in detail\n', ['chemical vapor deposition', 'material fabrication', 'optimization', 'operational costs', 'film quality', 'titanium nitride films', 'thin film growth', 'reactor gas flow rate', 'susceptor temperature', 'precursor concentration', 'parameter space mappings', 'TiN', 'chemical vapour deposition', 'optimisation', 'parameter space methods', 'process control', 'quality control', 'thin films']), ('High-voltage transistor scaling circuit techniques for high-density\nnegative-gate channel-erasing NOR flash memories\nIn order to scale high-voltage transistors for high-density negative-gate\nchannel-erasing NOR flash memories, two circuit techniques were\ndeveloped. A proposed level shifter with low operating voltage is\ncomposed of three parts, a latch holding the negative erasing voltage,\ntwo coupling capacitors connected with the latched nodes in the latch,\nand high-voltage drivers inverting the latch, resulting in reduction of\nthe maximum internal voltage by 0.5 V. A proposed high-voltage\ngenerator adds a path-gate logic to a conventional high-voltage\ngenerator to realize both low noise and low ripple voltage, resulting\nin a reduction of the maximum internal voltage by 0.5 V. As a result,\nthese circuit techniques along with high coupling-ratio cell technology\ncan scale down the high-voltage transistors by 15% and can realize\nhigher density negative-gate channel-erase NOR flash memories in\ncomparison with the source-erase NOR flash memories\n', ['HV transistor scaling circuit techniques', 'high-density NOR flash memories', 'negative-gate channel-erasing flash memories', 'level shifter', 'low operating voltage shifter', 'high-voltage drivers', 'high-voltage generator', 'path-gate logic', 'HV generator', 'low noise', 'low ripple voltage', 'high coupling-ratio cell technology', 'CMOS memory circuits', 'flash memories', 'NOR circuits', 'reference circuits', 'signal generators']), ('Passing the image test [imaging accreditation]\nAccredited imaging qualifications, hot on the heels of Microsoft, Cisco and\nothers, are taking off in the USA. Dave Tyler looks at the CDIA+\nqualification that looks likely to become the exam of choice for the DM\nindustry\n', ['accreditation', 'imaging qualifications', 'CDIA+', 'accreditation', 'document image processing', 'training']), ('On Implicit Euler for high-order high-index DAEs\nThe Implicit Euler method is seldom used to solve differential-algebraic\nequations (DAEs) of differential index r >or= 3, since the method in\ngeneral fails to converge in the first r - 2 steps after a change of\nstepsize. However, if the differential equation is of order d = r - 1\n>or= 1, an alternative variable-step version of the Euler method can\nbe shown uniformly convergent. For d = r - 1, this variable-step method\nis equivalent to the Implicit Euler except for the first r - 2 steps\nafter a change of stepsize. Generalization to DAEs with differential\nequations of order d > r - 1 >or= 1, and to variable-order\nformulas is discussed\n', ['Implicit Euler method', 'differential-algebraic equations', 'convergence', 'stepsize change', 'variable-step method', 'variable-order formulas', 'linear multistep method', 'backward differentiation formula', 'initial value problem', 'differential index', 'convergence of numerical methods', 'differential equations', 'initial value problems', 'iterative methods']), ("Choice preferences without inferences: subconscious priming of risk attitudes\nWe present a procedure for subconscious priming of risk attitudes. In\nExperiment 1, we were reliably able to induce risk-seeking or\nrisk-averse preferences across a range of decision scenarios using this\npriming procedure. In Experiment 2, we showed that these priming\neffects can be reversed by drawing participants' attention to the\npriming event. Our results support claims that the formation of risk\npreferences can be based on preconscious processing, as for example\npostulated by the affective primacy hypothesis, rather than rely on\ndeliberative mental operations, as posited by several current models of\njudgment and decision making\n", ['subconscious priming', 'risk attitudes', 'risk-seeking preferences', 'risk-averse preferences', 'decision scenarios', 'preconscious processing', 'affective primacy hypothesis', 'deliberative mental operations', 'choice preferences', 'decision theory', 'psychology']), ('Evaluating the performance of a distributed database of repetitive elements in\ncomplete genomes\nThe original version of the Repeat Sequence Database (RSDB) was created based\non centralized database systems (CDBSs). RSDB presently includes an\nenormous amount of data, with the amount of biological data increasing\nrapidly. Distributed RSDB (DRSDB) is developed to yield better\nperformance. This study proposed many approaches to data distribution\nand experimentally determines the best approach to obtain good\nperformance of our database. Experimental results indicate that DRSDB\nperforms well for particular types of query\n', ['distributed Repeat Sequence Database', 'biological data', 'data distribution', 'queries', 'complete genomes', 'repetitive elements', 'performance evaluation', 'biology computing', 'distributed databases', 'genetics', 'query processing', 'scientific information systems', 'sequences', 'software performance evaluation']), ('Symbiosis or alienation: advancing the university press/research library\nrelationship through electronic scholarly communication\nUniversity presses and research libraries have a long tradition of\ncollaboration. The rapidly expanding electronic scholarly communication\nenvironment offers important new opportunities for cooperation and for\ninnovative new models of publishing. The economics of libraries and\nscholarly publishers have strained the working relationship and\npromoted debates on important information policy issues. This article\nexplores the context for advancing the partnership, cites examples of\njoint efforts in electronic publishing, and presents an action plan for\nworking together\n', ['university press/research library relationship', 'electronic scholarly communication', 'economics', 'information policy', 'electronic publishing', 'academic libraries', 'electronic publishing', 'research libraries']), ('Angular disparity in ETACT scintimammography\nEmission tuned aperture computed tomography (ETACT) has been previously shown\nto have the potential for the detection of small tumors (<1 cm) in\nscintimammography. However, the optimal approach to the application of\nETACT in the clinic has yet to be determined. Therefore, we sought to\ndetermine the effect of the angular disparity between the ETACT\nprojections on image quality through the use of a computer simulation.\nA small, spherical tumor of variable size (5, 7.5 or 10 mm) was placed\nat the center of a hemispherical breast (15 cm diameter). The tumor to\nnontumor ratio was either 5:1 or 10:1. The detector was modeled to be a\ngamma camera fitted with a 4-mm-diam pinhole collimator. The\npinhole-to-detector and the pinhole-to-tumor distances were 25 and 15\ncm, respectively. A ray tracing technique was used to generate three\nsets of projections (10 degrees , 15 degrees , and 20 degrees , angular\ndisparity). These data were blurred to a resolution consistent with the\n4 mm pinhole. The TACT reconstruction method was used to reconstruct\nthese three image sets. The tumor contrast and the axial spatial\nresolution was measured. Smaller angular disparity led to an\nimprovement in image contrast but at a cost of degraded axial spatial\nresolution. The improvement in contrast is due to a slight improvement\nin the in-plane spatial resolution. Since improved contrast should lead\nto better tumor detectability, smaller angular disparity should be\nused. However, the difference in contrast between 10 degrees and 15\ndegrees was very slight and therefore a reasonable clinical choice for\nangular disparity is 15 degrees\n', ['emission tuned aperture computed tomography scintimammography', 'angular disparity', 'small tumors', 'image quality', 'computer simulation', 'spherical tumor', 'hemispherical breast', 'gamma camera', 'pinhole collimator', 'pinhole-to-detector distances', 'pinhole-to-tumor distances', 'ray tracing technique', 'tuned aperture computed tomography reconstruction method', 'image sets', 'axial spatial resolution', 'in-plane spatial resolution', 'clinical choice', 'cancer', 'digital simulation', 'emission tomography', 'image reconstruction', 'mammography', 'medical image processing', 'tumours']), ("A three-tier technology training strategy in a dynamic business environment\nAs end-user training becomes increasingly important in today's\ntechnology-intensive business environment, progressive companies remain\nalert to find ways to provide their end users with timely training and\nresources. This paper describes an innovative training strategy adopted\nby one midsize organization to provide its end users with adequate,\nflexible, and responsive training. The paper then compares the\nthree-tier strategy with other models described in technology training\nliterature. Managers who supervise technology end users in\norganizations comparable to the one in the study may find the\nthree-tier strategy workable and may want to use it in their own\ntraining programs to facilitate training and improve end-user skills.\nResearchers and scholars may find that the idea of three-tier training\ngenerates new opportunities for research\n", ['three-tier technology training strategy', 'dynamic business environment', 'end-user training', 'technology-intensive business environment', 'companies', 'innovative training strategy', 'midsize organization', 'organizations', 'business data processing', 'information technology', 'personal computing', 'training']), ('A linear time algorithm for recognizing regular Boolean functions\nA positive (or monotone) Boolean function is regular if its variables are\nnaturally ordered, left to fight, by decreasing strength, so that\nshifting the nonzero component of any true vector to the left always\nyields another true vector. This paper considers the problem of\nrecognizing whether a positive function f is regular, where f is given\nby min T(f) (the set of all minimal true vectors of f). We propose a\nsimple linear time (i.e., O(n|min T(f)|)-time) algorithm for it. This\nimproves upon the previous algorithm by J.S. Provan and M.O. Ball\n(1988) which requires O(n/sup 2/|min T(f)|) time. As a corollary, we\nalso present an O(n(n+|min T(f)|))-time algorithm for the recognition\nproblem of 2-monotonic functions\n', ['linear time algorithm', 'regular Boolean functions', 'monotone Boolean function', 'nonzero component', 'true vector', 'positive function', '2-monotonic functions', 'Boolean functions', 'threshold logic']), ('Analysis and operation of hybrid active filter for harmonic elimination\nThis paper presents a hybrid active filter topology and its control to suppress\nthe harmonic currents from entering the power source. The adopted\nhybrid active filter consists of one active filter and one passive\nfilter connected in series. By controlling the equivalent output\nvoltage of active filter, the harmonic currents generated by the\nnonlinear load are blocked and flowed into the passive filter. The\npower rating of the converter is reduced compared with the pure active\nfilters to filter the harmonic currents. The harmonic current detecting\napproach and DC-link voltage regulation are proposed to obtain\nequivalent voltage of active filter. The effectiveness of the adopted\ntopology and control scheme has been verified by the computer\nsimulation and experimental results in a scaled-down laboratory\nprototype\n', ['hybrid active filter', 'harmonic elimination', 'harmonic currents suppression', 'active filter', 'passive filter', 'equivalent output voltage', 'harmonic currents', 'nonlinear load', 'converter power rating reduction', 'DC-link voltage regulation', 'active filter equivalent voltage', 'computer simulation', 'scaled-down laboratory prototype', 'voltage source inverter', 'active filters', 'invertors', 'passive filters', 'power harmonic filters', 'voltage control']), ('Strong and weak points of the MUSCADET theorem prover-examples from CASC-JC\nMUSCADET is a knowledge-based theorem prover based on natural deduction. It has\nparticipated in CADE Automated theorem proving System Competitions. The\nresults show its complementarity with regard to resolution-based\nprovers. This paper presents some of its crucial methods and gives some\nexamples of MUSCADET proofs from the last competition (CASC-JC in IJCAR\n2001)\n', ['MUSCADET', 'CASC-JC', 'knowledge-based theorem prover', 'natural deduction', 'CADE Automated theorem proving System Competitions', 'resolution-based provers', 'formal logic', 'inference mechanisms', 'knowledge based systems', 'theorem proving']), ("Real-time implementation of a new low-memory SPIHT image coding algorithm using\nDSP chip\nAmong all algorithms based on wavelet transform and zerotree quantization, Said\nand Pearlman's (1996) set partitioning in hierarchical trees (SPIHT)\nalgorithm is well-known for its simplicity and efficiency. This paper\ndeals with the real-time implementation of SPIHT algorithm using DSP\nchip. In order to facilitate the implementation and improve the codec's\nperformance, some relative issues are thoroughly discussed, such as the\noptimization of program structure to speed up the wavelet\ndecomposition. SPIHT's high memory requirement is a major drawback for\nhardware implementation. In this paper, we modify the original SPIHT\nalgorithm by presenting two new concepts-number of error bits and\nabsolute zerotree. Consequently, the memory cost is significantly\nreduced. We also introduce a new method to control the coding process\nby number of error bits. Our experimental results show that the\nimplementation meets common requirement of real-time video coding and\nis proven to be a practical and efficient DSP solution\n", ['SPIHT algorithm', 'real-time implementation', 'wavelet transform', 'zerotree quantization', 'codec', 'wavelet decomposition', 'number of error bits', 'absolute zerotree', 'DSP chip', 'set partitioning in hierarchical trees', 'memory cost reduction', 'video coding', 'data compression', 'digital signal processing chips', 'image coding', 'real-time systems', 'transform coding', 'tree data structures', 'video codecs', 'video coding', 'wavelet transforms']), ("Standards for service discovery and delivery\nFor the past five years, competing industries and standards developers have\nbeen hotly pursuing automatic configuration, now coined the broader\nterm service discovery. Jini, Universal Plug and Play (UPnP),\nSalutation, and Service Location Protocol are among the front-runners\nin this new race. However, choosing service discovery as the topic of\nthe hour goes beyond the need for plug-and-play solutions or support\nfor the SOHO (small office/home office) user. Service discovery's\npotential in mobile and pervasive computing environments motivated my\nchoice\n", ['service discovery', 'Jini', 'Universal Plug and Play', 'Salutation', 'Service Location Protocol', 'mobile computing', 'pervasive computing', 'Internet', 'mobile computing', 'protocols', 'standards']), ('Steady-state mean-square error analysis of the cross-correlation and constant\nmodulus algorithm in a MIMO convolutive system\nThe cross-correlation and constant modulus algorithm (CC-CMA) has been proven\nto be an effective approach in the problem of joint blind equalisation\nand source separation in a multi-input and multi-output system. In the\npaper, the steady-state mean-square error performance of CC-CMA in a\nnoise-free environment is studied, and a new expression is derived\nbased on the energy preservation approach of Mai and Sayed (2000).\nSimulation studies are undertaken to support the analysis\n', ['MIMO convolutive system', 'Steady-state mean-square error analysis', 'cross-correlation', 'constant modulus algorithm', 'joint blind equalisation', 'source separation', 'multi-input multi-output system', 'noise-free environment', 'energy preservation approach', 'CC-CMA', 'blind equalisers', 'convolution', 'correlation methods', 'mean square error methods', 'MIMO systems']), ('Identifying multivariate discordant observations: a computer-intensive approach\nThe problem of identifying multiple outliers in a multivariate normal sample is\napproached via successive testing using P-values rather than tabled\ncritical values. Caroni and Prescott (Appl. Statist. 41, p.355, 1992)\nproposed a generalization of the EDR-ESD procedure of Rosner\n(Technometrics, 25, 1983)). Venter and Viljoen (Comput. Statist. Data\nAnal. 29, p.261, 1999) introduced a computer intensive method to\nidentify outliers in a univariate outlier situation. We now generalize\nthis method to the multivariate outlier situation and compare this new\nprocedure with that of Caroni and Prescott (Appl. Statist. 4, p.355,\n1992)\n', ['multivariate discordant observations', 'computer-intensive approach', 'multiple outliers', 'multivariate normal sample', 'P-values', 'tabled critical values', 'data analysis', 'EDR-EHD procedure', 'univariate outlier', 'stepwise testing approach', 'multivariate outlier', 'data analysis', 'probability', 'statistical analysis']), ('One and two facility network design revisited\nThe one facility one commodity network design problem (OFOC) with nonnegative\nflow costs considers the problem of sending d units of flow from a\nsource to a destination where arc capacity is purchased in batches of C\nunits. The two facility problem (TFOC) is similar, but capacity can be\npurchased either in batches of C units or one unit. Flow costs are\nzero. These problems are known to be NP-hard. We describe an exact\nO(n/sup 3/3/sup n/) algorithm for these problems based on the repeated\nuse of a bipartite matching algorithm. We also present a better lower\nbound of Omega (n/sup 2k*/) for an earlier Omega (n/sup 2k/) algorithm\ndescribed in the literature where k = [d/C] and k* = min{k, [(n 2)/2]}.\nThe matching algorithm is faster than this one for k >or= [(n -\n2)/2]. Finally, we provide another reformulation of the problem that is\nquasi integral. This property could be useful in designing a modified\nversion of the simplex method to solve the problem using a sequence of\npivots with integer extreme solutions, referred to as the integral\nsimplex method in the literature\n', ['one facility one commodity network design problem', 'two facility network design', 'nonnegative flow costs', 'flow costs', 'NP-hard problems', 'exact algorithm', 'bipartite matching algorithm', 'lower bound', 'quasi integral', 'pivots', 'integral simplex method', 'computational complexity', 'directed graphs', 'integer programming', 'linear programming', 'operations research']), ('Coordination [crisis management]\nCommunications during a crisis, both internal and external, set the tone during\nresponse and carry a message through recovery. The authors describe how\nto set up a system for information coordination to make sure the right\npeople get the right message, and the organization stays in control\n', ['crisis management', 'communications process', 'information coordination', 'disasters', 'risk management']), ('Parallel interior point schemes for solving multistage convex programming\nThe predictor-corrector interior-point path-following algorithm is promising in\nsolving multistage convex programming problems. Among many other\ngeneral good features of this algorithm, especially attractive is that\nthe algorithm allows the possibility to parallelise the major\ncomputations. The dynamic structure of the multistage problems\nspecifies a block-tridiagonal system at each Newton step of the\nalgorithm. A wrap-around permutation is then used to implement the\nparallel computation for this step\n', ['parallel interior point schemes', 'multistage convex programming', 'predictor-corrector interior-point path-following algorithm', 'dynamic structure', 'block-tridiagonal system', 'Newton step', 'wrap-around permutation', 'parallel computation', 'convex programming', 'decision theory', 'Newton method', 'parallel algorithms', 'predictor-corrector methods']), ('L/sub 2/ model reduction and variance reduction\nWe examine certain variance properties of model reduction. The focus is on\nL/sub 2/ model reduction, but some general results are also presented.\nThese general results can be used to analyze various other model\nreduction schemes. The models we study are finite impulse response\n(FIR) and output error (OE) models. We compare the variance of two\nestimated models. The first one is estimated directly from data and the\nother one is computed by reducing a high order model, by L/sub 2/ model\nreduction. In the FIR case we show that it is never better to estimate\nthe model directly from data, compared to estimating it via L/sub 2/\nmodel reduction of a high order FIR model. For OE models we show that\nthe reduced model has the same variance as the directly estimated one\nif the reduced model class used contains the true system\n', ['L/sub 2/ model reduction', 'variance reduction', 'finite impulse response models', 'FIR models', 'output error models', 'identification', 'identification', 'reduced order systems']), ('"Hidden convexity" of finite-dimensional stationary linear discrete-time\nsystems under conical constraints\nNew properties of finite-dimensional linear discrete-time systems under conical\ncontrol constraints that are similar to the "hidden convexity" of\ncontinuous-time systems are studied\n', ['hidden convexity', 'finite-dimensional stationary linear discrete-time systems', 'conical constraints', 'control constraint', 'controllability', 'discrete time systems', 'linear systems', 'multidimensional systems', 'polynomials', 'vectors']), ('A design to cost system for innovative product development\nPresents a prototype object-oriented and rule-based system for product cost\nmodelling and design for automation at an early design stage. The\ndeveloped system comprises a computer aided design (CAD) solid\nmodelling system, a material selection module, a knowledge-based system\n(KBS), a process optimization module, a design for assembly module, a\ncost estimation module and a user interface. Two manufacturing\nprocesses, namely machining and injection moulding processes, were\nconsidered in the developed system. The main function of the system,\nbesides estimating the product cost, is to generate initial process\nplanning, including the generation and selection of machining\nprocesses, their sequence and their machining parameters, and to\nrecommend the most economical assembly technique for a product and\nprovide design improvement suggestions based on a design feasibility\ntechnique. In addition, a feature-by-feature cost estimation report is\ngenerated using the proposed system to highlight the features of high\nmanufacturing cost. Two case studies were used to validate the\ndeveloped system\n', ['design to cost system', 'innovative product development', 'object-oriented rule-based system', 'product cost modelling', 'design for automation', 'computer aided design solid modelling system', 'material selection module', 'knowledge-based system', 'process optimization module', 'design for assembly module', 'cost estimation module', 'user interface', 'machining', 'injection moulding', 'process planning', 'feature-by-feature cost estimation report', 'fuzzy logic', 'object-oriented programming', 'concurrent engineering', 'CAD', 'costing', 'fuzzy logic', 'knowledge based systems', 'knowledge representation', 'machining', 'moulding', 'object-oriented programming', 'product development', 'solid modelling']), ('Real-time enterprise solutions for discrete manufacturing and consumer goods\nCustomer satisfaction and a focus on core competencies have dominated the\nthinking of a whole host of industries in recent years. However, one\noutcome, the outsourcing of noncore activities, has made the production\nof goods-from order entry to final delivery-more and more complex.\nSuppliers, subsuppliers, producers and customers are therefore busy\nadopting a new, more collaborative approach. This is mainly taking the\nform of order-driven planning and scheduling of production, but it is\nalso being steered by a need to reduce inventories and working capital\nas well as a desire to increase throughput and optimize production\n', ['real-time enterprise solutions', 'discrete manufacturing', 'consumer goods', 'customer satisfaction', 'core competencies', 'order-driven planning', 'production scheduling', 'working capital reduction', 'inventories reduction', 'manufacturing industries', 'production control', 'real-time systems']), ('Generic simulation approach for multi-axis machining. Part 2: model calibration\nand feed rate scheduling\nFor Part 1 see ibid. vol.124 (2002). This is the second part of a two-part\npaper presenting a new methodology for analytically simulating\nmulti-axis machining of complex sculptured surfaces. The first section\nof this paper offers a detailed explanation of the model calibration\nprocedure. A new methodology is presented for accurately determining\nthe cutting force coefficients for multi-axis machining. The force\nmodel presented in Part 1 is reformulated so that the cutting force\ncoefficients account for the effects of feed rate, cutting speed, and a\ncomplex cutting edge design. Experimental results are presented for the\ncalibration procedure. Model verification tests were conducted with\nthese cutting force coefficients. These tests demonstrate that the\npredicted forces are within 5% of experimentally measured forces.\nSimulated results are also shown for predicting dynamic cutting forces\nand static/dynamic tool deflection. The second section of the paper\ndiscusses how the modeling methodology can be applied for feed rate\nscheduling in an industrial application. A case study for process\noptimization of machining an airfoil-like surface is used for\ndemonstration. Based on the predicted instantaneous chip load and/or a\nspecified force constraint, the feed rate scheduling is utilized to\nincrease the metal removal rate. The feed rate scheduling\nimplementation results in a 30% reduction in machining time for the\nairfoil-like surface\n', ['multiple axis machining', 'generic simulation', 'model calibration', 'complex sculptured surfaces', 'cutting force coefficients', 'force model', 'cutting edge design', 'metal removal rate', 'optimization', 'feed rate scheduling', 'calibration', 'digital simulation', 'machining', 'optimisation', 'production engineering computing', 'scheduling']), ('Three-dimensional geometrical optics code for indoor propagation\nThis paper presents a program, GO 3D, for computing the fields of a transmitter\nin an indoor environment using geometrical optics. The program uses an\n"image tree" data structure to construct the images needed to compute\nall the rays carrying fields above a preset "threshold" value, no\nmatter how many reflections are needed. The paper briefly describes the\ninput file required to define wall construction, the floor plan, the\ntransmitter, and the receiver locations. A case study consisting of a\nlong corridor with a small room on one side is used to demonstrate the\nfeatures of the GO 3D program\n', ['three-dimensional geometrical optics', '3D geometrical optics code', 'indoor propagation', 'image tree data structure', 'image construction', 'wall construction', 'floor plan', 'transmitter', 'receiver locations', 'ray tracing', 'data visualisation', 'data visualisation', 'electromagnetic fields', 'image processing', 'indoor radio', 'radiowave propagation', 'ray tracing', 'telecommunication computing', 'tree data structures']), ('Will CPXe save the photofinishing market?\nA consortium of film suppliers and electronics firms has proposed the Common\nPicture Exchange environment. It will let diverse providers cooperate\nvia the Internet to sell digital-photo prints\n', ['CPXe', 'photofinishing market', 'Common Picture Exchange environment', 'Kodak', 'Fujifilm', 'HP', 'Web-services standards', 'desktop publishing', 'image processing', 'photography', 'standards']), ('System embedding. Control with reduced observer\nTwo interrelated problems-design of the reduced observer of plant state\nseparately and together with its control system-were considered from\nthe standpoint of designing the multivariable linear systems from the\ndesired matrix transfer functions. The matrix equations defining the\nentire constructive class of solutions of the posed problems were\nobtained using the system embedding technology. As was demonstrated,\ncontrol based on the reduced observer is capable to provide the desired\nresponse to the control input, as well as the response to the nonzero\ninitial conditions, only for the directly measurable part of the\ncomponents of the state vector. An illustrative example was presented\n', ['system embedding', 'reduced observer control', 'reduced plant state observer design', 'multivariable linear systems', 'matrix transfer functions', 'state vector', 'control system synthesis', 'linear systems', 'multivariable systems', 'observers', 'reduced order systems', 'transfer function matrices']), ('Trends in agent communication language\nAgent technology is an exciting and important new way to create complex\nsoftware systems. Agents blend many of the traditional properties of AI\nprograms - knowledge-level reasoning, flexibility, proactiveness,\ngoal-directedness, and so forth - with insights gained from distributed\nsoftware engineering, machine learning, negotiation and teamwork\ntheory, and the social sciences. An important part of the agent\napproach is the principle that agents (like humans) can function more\neffectively in groups that are characterized by cooperation and\ndivision of labor. Agent programs are designed to autonomously\ncollaborate with each other in order to satisfy both their internal\ngoals and the shared external demands generated by virtue of their\nparticipation in agent societies. This type of collaboration depends on\na sophisticated system of inter-agent communication. The assumption\nthat inter-agent communication is best handled through the explicit use\nof an agent communication language (ACL) underlies each of the articles\nin this special issue. In this introductory article, we will supply a\nbrief background and introduction to the main topics in agent\ncommunication\n', ['agent technology', 'AI programs', 'agent communication language', 'inter-agent communication', 'agent societies', 'KQML', 'semantics', 'conversations', 'distributed software engineering', 'machine learning', 'negotiation', 'teamwork', 'social sciences', 'high level languages', 'multi-agent systems', 'software agents']), ('The Bagsik Oscillator without complex numbers\nWe argue that the analysis of the so-called Bagsik Oscillator, recently\npublished by Piotrowski and Sladkowski (2001), is erroneous due to: (1)\nthe incorrect banking data used and (2) the application of statistical\nmechanism apparatus to processes that are totally deterministic\n', ['Bagsik oscillator', 'noncomplex numbers', 'incorrect banking data', 'statistical mechanism apparatus', 'game theory', 'deterministic processes', 'economics', 'fluctuations', 'game theory', 'nonlinear dynamical systems', 'oscillations', 'statistical mechanics']), ('Scalable techniques from nonparametric statistics for real time robot learning\nLocally weighted learning (LWL) is a class of techniques from nonparametric\nstatistics that provides useful representations and training algorithms\nfor learning about complex phenomena during autonomous adaptive control\nof robotic systems. The paper introduces several LWL algorithms that\nhave been tested successfully in real-time learning of complex robot\ntasks. We discuss two major classes of LWL, memory-based LWL and purely\nincremental LWL that does not need to remember any data explicitly. In\ncontrast to the traditional belief that LWL methods cannot work well in\nhigh-dimensional spaces, we provide new algorithms that have been\ntested on up to 90 dimensional learning problems. The applicability of\nour LWL algorithms is demonstrated in various robot learning examples,\nincluding the learning of devil-sticking, pole-balancing by a humanoid\nrobot arm, and inverse-dynamics learning for a seven and a 30\ndegree-of-freedom robot. In all these examples, the application of our\nstatistical neural networks techniques allowed either faster or more\naccurate acquisition of motor control than classical control\nengineering\n', ['scalable techniques', 'nonparametric statistics', 'real time robot learning', 'locally weighted learning', 'training algorithms', 'complex phenomena', 'autonomous adaptive control', 'memory-based learning', 'purely incremental learning', 'devil-sticking', 'pole-balancing', 'humanoid robot arm', 'inverse-dynamics learning', 'statistical neural networks techniques', 'nonparametric regression', 'learning (artificial intelligence)', 'nonparametric statistics', 'real-time systems', 'robots']), ('A combinatorial, graph-based solution method for a class of continuous-time\noptimal control problems\nThe paper addresses a class of continuous-time, optimal control problems whose\nsolutions are typically characterized by both bang-bang and "singular"\ncontrol regimes. Analytical study and numerical computation of such\nsolutions are very difficult and far from complete when only techniques\nfrom control theory are used. This paper solves optimal control\nproblems by reducing them to the combinatorial search for the shortest\npath in a specially constructed graph. Since the nodes of the graph are\nweighted in a sequence-dependent manner, we extend the classical,\nshortest-path algorithm to our case. The proposed solution method is\ncurrently limited to single-state problems with multiple control\nfunctions. A production planning problem and a train operation problem\nare optimally solved to illustrate the method\n', ['continuous-time optimal control problems', 'combinatorial graph-based solution', 'bang-bang control regimes', 'singular control regimes', 'numerical computation', 'combinatorial search', 'shortest path algorithm', 'single-state problems', 'multiple control functions', 'production planning problem', 'train operation problem', 'weighted graph nodes', 'sequence-dependent manner', 'bang-bang control', 'continuous time systems', 'graph theory', 'optimal control', 'production control', 'railways', 'search problems', 'singular optimal control']), ('L/sub p/ boundedness of (C, 1) means of orthonormal expansions for general\nexponential weights\nLet I be a finite or infinite interval, and let W:I to (0, infinity ). Assume\nthat W/sup 2/ is a weight, so that we may define orthonormal\npolynomials corresponding to W/sup 2/. For f :R to R, let s/sub m/ [f]\ndenote the mth partial sum of the orthonormal expansion of f with\nrespect to these polynomials. We investigate boundedness in weighted\nL/sub p/ spaces of the (C, 1) means 1/n /sub m=1/ Sigma /sup n/s/sub\nm/[f]. The class of weights W/sup 2/ considered includes even and\nnoneven exponential weights\n', ['boundedness', 'orthonormal expansions', 'general exponential weights', 'infinite interval', 'finite interval', 'orthonormal polynomials', 'mth partial sum', 'polynomials']), ('Score tests for zero-inflated Poisson models\nIn many situations count data have a large proportion of zeros and the\nzero-inflated Poisson regression (ZIP) model may be appropriate. A\nsimple score test for zero-inflation, comparing the ZIP model with a\nconstant proportion of excess zeros to a standard Poisson regression\nmodel, was given by van den Broek (1995). We extend this test to the\nmore general situation where the zero probability is allowed to depend\non covariates. The performance of this test is evaluated using a\nsimulation study. To identify potentially important covariates in the\nzero-inflation model a composite test is proposed. The use of the\ngeneral score test and the composite procedure is illustrated on two\nexamples from the literature. The composite score test is found to\nsuggest appropriate models\n', ['count data', 'score tests', 'zero-inflated Poisson regression model', 'zero probability', 'covariates', 'excess zeros', 'simulation', 'composite test', 'data analysis', 'statistical analysis', 'stochastic processes']), ('Dynamic multi-objective heating optimization\nWe develop a multicriteria approach to the problem of space heating under a\ntime varying price of electricity. In our dynamic goal programming\nmodel the goals are ideal temperature intervals and the other criteria\nare the costs and energy consumption. We discuss the modelling\nrequirements in multicriteria problems with a dynamic structure and\npresent a new relaxation method combining the traditional epsilon\n-constraint and goal programming (GP) methods. The multi-objective\nheating optimization (MOHO) application in a spreadsheet environment\nwith numerical examples is described\n', ['space heating', 'dynamic multi-objective heating optimization', 'multicriteria approach', 'time varying electricity price', 'dynamic goal programming model', 'ideal temperature intervals', 'energy consumption', 'modelling requirements', 'dynamic structure', 'relaxation method', 'epsilon -constraint', 'multi-objective heating optimization', 'spreadsheet environment', 'numerical examples', 'dynamic programming', 'operations research', 'power consumption', 'space heating', 'spreadsheet programs']), ('Randomized two-process wait-free test-and-set\nWe present the first explicit, and currently simplest, randomized algorithm for\ntwo-process wait-free test-and-set. It is implemented with two 4-valued\nsingle writer single reader atomic variables. A test-and-set takes at\nmost 11 expected elementary steps, while a reset takes exactly 1\nelementary step. Based on a finite-state analysis, the proofs of\ncorrectness and expected length are compressed into one table\n', ['randomized two-process wait-free test-and-set', 'randomized algorithm', '4-valued single writer single reader atomic variables', 'expected elementary steps', 'finite-state analysis', 'correctness proofs', 'symmetry breaking', 'asynchronous distributed protocols', 'fault-tolerance', 'shared memory', 'wait-free read/write registers', 'distributed algorithms', 'message passing', 'randomised algorithms', 'theorem proving']), ('Experimental feedforward and feedback control of a one-dimensional SMA\ncomposite\nThe control of embedded shape memory alloy (SMA) actuators has recently become\na topic of interest in the field of smart structures. The inherent\ndifficulties associated with SMA actuators has resulted in a variety of\napproaches. Homogenization provides a simplified, yet mathematically\nrigorous, method of determining average stress and strain fields in a\ncomposite. A modified constitutive model is presented based on\nexperimental results demonstrating the inability of most simple\nphenomenological models to capture the effective behavior of SMAs\nduring thermal activation. A feedforward controller is presented for a\nSMA composite based on the homogenization of a modified\nphenomenological model for SMAs in a linear matrix\n', ['thermal activation', 'embedded shape memory alloy', 'SMA', 'smart structures', 'SMA actuators', 'homogenization', 'linear matrix', 'models', 'composite materials', 'feedback', 'feedforward', 'intelligent actuators', 'intelligent control', 'intelligent materials', 'intelligent structures', 'shape memory effects']), ('Multi-hour design of survivable classical IP networks\nMost of Internet intra-domain routing protocols (OSPF, RIP, and IS-IS) are\nbased on shortest path routing. The path length is defined as the sum\nof metrics associated with the path links. These metrics are often\nmanaged by the network administrator. In this context, the design of an\nInternet backbone network consists in dimensioning the network (routers\nand transmission links) and establishing the metric. Many requirements\nhave to be satisfied. First, Internet traffic is not static as\nsignificant variations can be observed during the day. Second, many\nfailures can occur (cable cuts, hardware failures, software failures,\netc.). We present algorithms (meta-heuristics and greedy heuristic) to\ndesign Internet backbone networks, taking into account the multi-hour\nbehaviour of traffic and some survivability requirements. Many\nmulti-hour and protection strategies are studied and numerically\ncompared. Our algorithms can be extended to integrate other quality of\nservice constraints\n', ['multi-hour design', 'survivable classical IP networks', 'Internet intra-domain routing protocols', 'OSPF', 'RIP', 'IS-IS', 'shortest path routing', 'path length', 'path links', 'network administrator', 'Internet backbone network', 'network dimensioning', 'network routers', 'Internet traffic', 'transmission links', 'network failures', 'meta-heuristics algorithm', 'greedy heuristic algorithm', 'survivability requirements', 'network protection', 'quality of service constraints', 'QoS constraints', 'computer network reliability', 'Internet', 'telecommunication network routing', 'telecommunication traffic', 'transport protocols']), ("Behavior of Runge-Kutta discretizations near equilibria of index 2 differential\nalgebraic systems\nWe analyze Runge-Kutta discretizations applied to index 2 differential\nalgebraic equations (DAE's) near equilibria. We compare the geometric\nproperties of the numerical and the exact solutions. It is shown that\nprojected and half-explicit Runge-Kutta methods reproduce the\nqualitative features of the continuous system in the vicinity of an\nequilibrium correctly. The proof combines cut-off and scaling\ntechniques for index 2 differential algebraic equations with some\ninvariant manifold results of Schropp (Geometric properties of\nRunge-Kutta discretizations for index 2 differential algebraic\nequations, Konstanzer Schriften in Mathematik und Informatik 128) and\nclassical results for discretized ordinary differential equations\n", ['Runge-Kutta discretizations', 'index 2 differential algebraic systems', 'equilibria', 'geometric properties', 'half-explicit Runge-Kutta methods', 'continuous system', 'cut-off techniques', 'scaling techniques', 'invariant manifold', 'discretized ordinary differential equations', 'differential equations', 'Runge-Kutta methods']), ('Library services today and tomorrow: lessons from iLumina, a digital library\nfor creating and sharing teaching resources\nThis article is based on the emerging experience associated with a digital\nlibrary of instructional resources, iLumina, in which the contributors\nof resources and the users of those resources are the same-an open\ncommunity of instructors in science, mathematics, engineering, and\ntechnology. Moreover, it is not the resources, most of which will be\ndistributed across the Internet, but metadata about the resources that\nis the focus of the central iLumina repository and its support services\nfor resource contributors and users. The distributed iLumina library is\na community-sharing library for repurposing and adding value to\npotentially useful, mostly non-commercial instructional resources that\nare typically more granular in nature than commercially developed\ncourse materials. The experience of developing iLumina is raising a\nrange of issues that have nothing to do with the place and time\ncharacteristics of the instructional context in which iLumina\ninstructional resources are created or used. The issues instead have\ntheir locus in the democratization of both the professional roles of\nlibrarians and the quality assurance mechanisms associated with\ntraditional peer review\n', ['iLumina', 'digital library', 'teaching resource sharing', 'Internet', 'metadata', 'information resources', 'community-sharing library', 'professional roles', 'academic library', 'librarians', 'quality assurance', 'peer review', 'library automation', 'standards', 'interoperability', 'reusable software', 'distributed systems', 'user issues', 'academic libraries', 'digital libraries', 'educational computing', 'information resources', 'Internet', 'meta data']), ('Variety identification of wheat using mass spectrometry with neural networks\nand the influence of mass spectra processing prior to neural network\nanalysis\nThe performance of matrix-assisted laser desorption/ionisation time-of-flight\nmass spectrometry with neural networks in wheat variety classification\nis further evaluated. Two principal issues were studied: (a) the number\nof varieties that could be classified correctly; and (b) various means\nof preprocessing mass spectrometric data. The number of wheat varieties\ntested was increased from 10 to 30. The main pre-processing method\ninvestigated was based on Gaussian smoothing of the spectra, but other\nmethods based on normalisation procedures and multiplicative scatter\ncorrection of data were also used. With the final method, it was\npossible to classify 30 wheat varieties with 87% correctly classified\nmass spectra and a correlation coefficient of 0.90\n', ['matrix-assisted laser desorption/ionisation time-of-flight mass spectrometry', 'wheat variety classification', 'mass spectrometric data', 'pre-processing- method', 'Gaussian smoothing', 'normalisation procedures', 'multiplicative scatter correction', 'correctly classified mass spectra', 'correlation coefficient', 'variety identification', 'mass spectra processing', 'neural network analysis', 'agriculture', 'biological techniques', 'biology computing', 'botany', 'data analysis', 'Gaussian processes', 'ionisation', 'laser beam applications', 'neural nets', 'spectral analysis', 'spectroscopy computing', 'thermally stimulated desorption', 'time of flight mass spectra']), ('Taylor series based two-dimensional digital differentiators\nA new type of Taylor series based 2-D finite difference approximation is\npresented, and it is shown that the coefficients of these\napproximations are not unique. Explicit formulas are presented for one\nof the possible sets of coefficients for an arbitrary order, by\nextending the previously presented 1-D approximations. These\ncoefficients are implemented as maximally linear 2-D FIR digital\ndifferentiators, and their formulas are modified to narrow the\ninaccuracy regions on the resultant frequency responses, close to the\nNyquist frequencies\n', ['Taylor series', 'two-dimensional digital differentiators', '2D finite difference approximation', '2D FIR digital differentiators', 'Nyquist frequencies', 'FIR digital filters', 'maximally linear digital differentiators', 'frequency response', 'magnitude response', 'approximation theory', 'differentiating circuits', 'finite difference methods', 'FIR filters', 'frequency response', 'series (mathematics)', 'two-dimensional digital filters']), ('Engineering plug-in software components to support collaborative work\nMany software applications require co-operative work support, including\ncollaborative editing, group awareness, versioning, messaging and\nautomated notification and co-ordination agents. Most approaches\nhard-code such facilities into applications, with fixed functionality\nand limited ability to reuse groupware implementations. We describe our\nrecent work in seamlessly adding such capabilities to component-based\napplications via a set of collaborative work-supporting plug-in\nsoftware components. We describe a variety of applications of this\ntechnique, along with descriptions of the novel architecture, user\ninterface adaptation and implementation techniques for the\ncollaborative work-supporting components that we have developed. We\nreport on our experiences to date with this method of supporting\ncollaborative work enhancement of component-based systems, and discuss\nthe advantages of our approach over conventional techniques\n', ['software applications', 'co-operative work support', 'collaborative editing', 'group awareness', 'versioning', 'messaging', 'automated notification', 'plug-in software components', 'groupware', 'collaborative work tools', 'distributed object management', 'groupware', 'software architecture']), ('Proof that the election problem belongs to NF-completeness problems in\nasynchronous distributed systems\nThis paper is about the hardness of the election problem in asynchronous\ndistributed systems in which processes can crash but links are\nreliable. The hardness of the problem is defined with respect to the\ndifficulty to solve it despite failures. It is shown that problems\nencountered in the system are classified as three classes of problems:\nF (fault-tolerant), NF (Not fault-tolerant) and NFC (NF-completeness).\nAmong those, the class NFC are the hardest problems to solve. In this\npaper, we prove that the Election problem is the most difficult problem\nwhich belongs to the class NFC\n', ['election problem', 'NF-completeness problems', 'asynchronous distributed systems', 'distributed computing', 'leader election', 'failure detectors', 'fault-tolerant problems', 'not-fault-tolerant problems', 'computational complexity', 'distributed algorithms', 'fault tolerant computing']), ('Managing system risk\nCompanies are increasingly required to provide assurance that their systems are\nsecure and conform to commercial security standards. Senior business\nmanagers are ultimately responsible for the security of their corporate\nsystems and for the implications in the event of a failure. Businesses\nwill be exposed to unquantified security risks unless they have a\nformal risk management framework in place to enable risks to be\nidentified, evaluated and managed. Failure to assess and manage risks\ncan lead to a business suffering serious financial impacts, commercial\nembarrassment and fines or sanctions from regulators. This is both a\nkey responsibility and opportunity for Management Services\nPractitioners\n', ['risk management framework', 'commercial security standards', 'IT projects', 'risk management', 'security of data']), ("Research challenges and perspectives of the Semantic Web\nAccessing documents and services on today's Web requires human intelligence.\nThe interface to these documents and services is the Web page, written\nin natural language, which humans must understand and act upon. The\npaper discusses the Semantic Web which will augment the current Web\nwith formalized knowledge and data that computers can process. In the\nfuture, some services will mix human-readable and structured data so\nthat both humans and computers can use them. Others will support\nformalized knowledge that only machines will use\n", ['Semantic Web', 'document access', 'Web page', 'natural language', 'formalized knowledge', 'Internet', 'artificial intelligence', 'artificial intelligence', 'information resources', 'Internet']), ('An object-oriented version of SIMLIB (a simple simulation package)\nThis paper introduces an object-oriented version of SIMLIB (an\neasy-to-understand discrete-event simulation package). The\nobject-oriented version is preferable to the original procedural\nlanguage versions of SIMLIB in that it is easier to understand and\nteach simulation from an object point of view. A single-server queue\nsimulation is demonstrated using the object-oriented SIMLIB\n', ['object-oriented version', 'SIMLIB', 'discrete-event simulation', 'teach simulation', 'business data processing', 'computer aided instruction', 'discrete event simulation', 'object-oriented programming']), ("North American carrier survey: simply the best\nNetwork Magazine carried out a North American carrier survey. Thousands of\nnetwork engineers gave information on providers' strengths and\nweaknesses across seven services: private lines, frame relay, ATM,\nVPNs, dedicated Internet access, Ethernet services, and Web hosting.\nRespondents also ranked providers on their ability to perform in up to\neight categories including customer service, reliability, and price.\nUsers rated more than a dozen providers for each survey. Carriers\nneeded to receive at least 30 votes for inclusion in the survey.\nReaders were asked to rate carriers on up to nine categories using a\nscale of 1 (unacceptable) to 5 (excellent). Not all categories are\nequally important. To try and get at these differences, Network\nMagazine asked readers to assign a weight to each category. The big\nwinners were VPNs\n", ['North American carrier survey', 'private lines', 'frame relay', 'ATM', 'VPNs', 'dedicated Internet access', 'Ethernet services', 'Web hosting', 'service providers', 'customer service', 'reliability', 'price', 'asynchronous transfer mode', 'computer network reliability', 'frame relay', 'Internet', 'local area networks']), ('Speaker identification from voice using neural networks\nThe paper provides three different schemes for speaker identification of\npersonnel from their voice using artificial neural networks. The first\nscheme recognizes speakers by employing the classical backpropagation\nalgorithm pre-trained with known voice samples of the persons. The\nsecond scheme provides a framework for classifying the known training\nsamples of the voice features using a hierarchical architecture\nrealized with a self-organizing feature map neural net. The first\nscheme is highly robust as it is capable of identifying the personnel\nfrom their noisy voice samples, but because of its excessive training\ntime it has limited applications for a large voice database. The second\nscheme though not so robust as the former, however, can classify an\nunknown voice sample to its nearest class. The time needed for\nclassification by the first scheme is always unique irrespective of the\nvoice sample. It is proportional to the number of feedforward layers in\nthe network. The time-requirement of the second classification scheme,\nhowever, is not free from the voice features and is proportional to the\nnumber of 2D arrays traversed by the algorithm on the hierarchical\nstructure. The third scheme is highly robust and mis-classification is\nas low as 0.2 per cent. The third scheme combines the composite\nbenefits of a radial basis function neural net and backpropagation\ntrained neural net\n', ['speaker identification', 'artificial neural networks', 'personnel', 'hierarchical architecture', 'self-organizing feature map', 'backpropagation algorithm', 'classification', 'feedforward layers', '2D arrays', 'radial basis function neural net', 'pre-training', 'known voice samples', 'backpropagation', 'pattern classification', 'radial basis function networks', 'self-organising feature maps', 'signal sampling', 'speaker recognition']), ("Extending CTL with actions and real time\nIn this paper, we present the logic ATCTL, which is intended to be used for\nmodel checking models that have been specified in a lightweight version\nof the Unified Modelling Language (UML). Elsewhere, we have defined a\nformal semantics for LUML to describe the models. This paper's goal is\nto give a specification language for properties that fits LUML; LUML\nincludes states, actions and real time. ATCTL extends CTL with\nconcurrent actions and real time. It is based on earlier extensions of\nCTL by R. De Nicola and F. Vaandrager (ACTL) (1990) and R. Alur et aL\n(TCTL) (1993). This makes it easier to adapt existing model checkers to\nATCTL. To show that we can check properties specified in ATCTL in\nmodels specified in LUML, we give a small example using the Kronos\nmodel checker\n", ['actions', 'real time logic', 'logic ATCTL', 'model checking models', 'Unified Modelling Language', 'formal semantics', 'specification language', 'Kronos model checker', 'computation tree logic', 'specification languages', 'temporal logic']), ('A comprehensive chatter prediction model for face turning operation including\ntool wear effect\nPresents a three-dimensional mechanistic frequency domain chatter model for\nface turning processes, that can account for the effects of tool wear\nincluding process damping. New formulations are presented to model the\nvariation in process damping forces along nonlinear tool geometries\nsuch as the nose radius. The underlying dynamic force model simulates\nthe variation in the chip cross-sectional area by accounting for the\ndisplacements in the axial and radial directions. The model can be used\nto determine stability boundaries under various cutting conditions and\ndifferent states of flank wear. Experimental results for different\namounts of wear are provided as a validation for the model\n', ['chatter prediction model', 'face turning operation', 'tool wear effect', 'three-dimensional mechanistic frequency domain chatter model', 'process damping', 'axial directions', 'radial directions', 'stability boundaries', 'flank wear', 'cutting', 'damping', 'machining', 'mechanical engineering computing', 'stability', 'wear']), ('Database technology in digital libraries\nDatabase technology advancements have provided many opportunities for\nlibraries. These advancements can bring the world closer together\nthrough information accessibility. Digital library projects have been\nestablished worldwide to, ultimately, fulfil the needs of end users\nthrough more efficiency and convenience. Resource sharing will continue\nto be the trend for libraries. Changes often create issues which need\nto be addressed. Issues relating to database technology and digital\nlibraries are reviewed. Some of the major challenges in digital\nlibraries and managerial issues are identified as well\n', ['database technology', 'digital libraries', 'information accessibility', 'digital library projects', 'end users', 'resource sharing', 'managerial issues', 'data quality', 'interoperability', 'metadata', 'user interface', 'query processing', 'database management systems', 'digital libraries', 'management', 'query processing']), ('Stability in the numerical solution of the heat equation with nonlocal boundary\nconditions\nThis paper deals with numerical methods for the solution of the heat equation\nwith integral boundary conditions. Finite differences are used for the\ndiscretization in space. The matrices specifying the resulting\nsemidiscrete problem are proved to satisfy a sectorial resolvent\ncondition, uniformly with respect to the discretization parameter.\nUsing this resolvent condition, unconditional stability is proved for\nthe fully discrete numerical process generated by applying A( theta\n)-stable one-step methods to the semidiscrete problem. This stability\nresult is established in the maximum norm; it improves some previous\nresults in the literature in that it is not subject to various\nunnatural restrictions which were imposed on the boundary conditions\nand on the one-step methods\n', ['numerical solution', 'heat equation', 'nonlocal boundary conditions', 'stability', 'integral boundary conditions', 'finite differences', 'space discretization', 'matrices', 'semidiscrete problem', 'sectorial resolvent condition', 'fully discrete numerical process', 'one-step methods', 'maximum norm', 'finite difference methods', 'heat', 'matrix algebra', 'numerical stability', 'physics computing']), ("Efficient cellular automata based versatile multiplier for GF(2/sup m/)\nIn this paper, a low-complexity programmable cellular automata (PCA) based\nversatile modular multiplier in GF(2/sup m/) is presented. The proposed\nversatile multiplier increases flexibility by using the same multiplier\nin different security environments, and it reduces the user's cost.\nMoreover, the multiplier can be easily extended to high order of m for\nmore security, and low-cost serial implementation is feasible in\nrestricted computing environments, such as smart cards and wireless\ndevices\n", ['cellular automata based versatile multiplier', 'low-complexity programmable cellular automata', 'security environments', 'restricted computing environments', 'smart cards', 'wireless devices', 'cellular automata', 'cryptography']), ('Single machine earliness-tardiness scheduling with resource-dependent release\ndates\nThis paper deals with the single machine earliness and tardiness scheduling\nproblem with a common due date and resource-dependent release dates. It\nis assumed that the cost of resource consumption of a job is a\nnon-increasing linear function of the job release date, and this\nfunction is common for all jobs. The objective is to find a schedule\nand job release dates that minimize the total resource consumption, and\nearliness and tardiness penalties. It is shown that the problem is\nNP-hard in the ordinary sense even if the due date is unrestricted (the\nnumber of jobs that can be scheduled before the due date is\nunrestricted). An exact dynamic programming (DP) algorithm for small\nand medium size problems is developed. A heuristic algorithm for\nlarge-scale problems is also proposed and the results of a\ncomputational comparison between heuristic and optimal solutions are\ndiscussed\n', ['single machine earliness-tardiness scheduling', 'resource-dependent release dates', 'common due date', 'job resource consumption cost', 'nonincreasing linear function', 'job release date', 'total resource consumption minimization', 'NP-hard problem', 'exact dynamic programming algorithm', 'medium size problems', 'small size problems', 'heuristic algorithm', 'large-scale problems', 'polynomial time algorithm', 'computational complexity', 'dynamic programming', 'heuristic programming', 'production control']), ('Rats, robots, and rescue\nIn early May, media inquiries started arriving at my office at the Center for\nRobot-Assisted Search and Rescue (www.crasar.org). Because I\'m CRASAR\'s\ndirector, I thought the press was calling to follow up on the recent\nhumanitarian award given to the center\'s founder, John Blitch, for\nsuccessfully using small, backpackable robots at the World Trade Center\ndisaster. Instead, I found they were asking me to comment on the\n"roborats" study in the 2 May 2002 Nature. In this study, rats with\nmedial force brain implants underwent operant conditioning to force\nthem into a form of guided behavior, one aspect of which was thought\nuseful for search and rescue. The article\'s closing comment suggested\nthat a guided rat could serve as both a mobile robot and a biological\nsensor. Although a roboticist by training, I\'m committed to any\ntechnology that will help save lives while reducing the risk to\nrescuers. But rats?\n', ['mobile robot', 'biological sensor', 'guided rat', 'robot-assisted search and rescue', 'emergency services', 'mobile robots']), ('A new approach to the d-MC problem\nMany real-world systems are multi-state systems composed of multi-state\ncomponents in which the reliability can be computed in terms of the\nlower bound points of level d, called d-Mincuts (d-MCs). Such systems\n(electric power, transportation, etc.) may be regarded as flow networks\nwhose arcs have independent, discrete, limited and multi-valued random\ncapacities. In this paper, all MCs are assumed to be known in advance,\nand the authors focused on how to verify each d-MC candidate before\nusing d-MCs to calculate the network reliability. The proposed\nalgorithm is more efficient than existing algorithms. The algorithm\nruns in O(p sigma mn) time, a significant improvement over the previous\nO(p sigma m/sup 2/) time bounds based on max-flow/min-cut, where p and\nor are the number of MCs and d-MC candidates, respectively. It is\nsimple, intuitive and uses no complex data structures. An example is\ngiven to show how all d-MC candidates are found and verified by the\nproposed algorithm. Then the reliability of this example is computed\n', ['d-MC problem', 'multi-state systems', 'multi-state components', 'reliability computation', 'd-Mincuts', 'flow networks', 'failure analysis algorithm', 'time bounds', 'max-flow/min-cut', 'failure analysis', 'reliability theory']), ('Marketing in CSIR libraries and information centres: a study on promotional\nefforts\nThis paper examines the attitudes of librarians towards the promotional aspects\nin several CSIR libraries and information centres of India. The issues\nrelated to promotional activities of these libraries have been\nevaluated to determine the extent to which they are being practised.\nLibrarians hold positive attitudes about promotional aspects of\nlibraries and often practise them without knowing they are practising\nmarketing concepts. Suggestions and strategies for improving the\npromotional activities in libraries and information services are put\nforth so as to meet the information needs and demands of clientele\n', ['CSIR libraries', 'information centres', 'India', 'promotional activities', 'marketing', 'information needs', 'information centres', 'information needs', 'libraries', 'marketing']), ('The California Digital Library and the eScholarship program\nThe eScholarship program was launched in 2000 to foster faculty-led innovation\nin scholarly publishing. An initiative of the University of California\n(UC) and a program of the California Digital Library, the eScholarship\nprogram has stimulated significant interest in its short life. Its\nmodest but visible accomplishments garner praise from many quarters,\nwithin and beyond the University of California. In perhaps the best\nindication of its timeliness and momentum, there are more proposals\nsubmitted to eScholarship today than the CDL can manage. This early\nsuccess is due in part to the sheer power of an idea whose time has\ncome, but also to the unique approach on which CDL was founded and the\neScholarship initiative was first launched\n', ['eScholarship program', 'faculty-led innovation', 'California Digital Library', 'scholarly publishing', 'University of California', 'academic libraries', 'digital libraries', 'electronic publishing']), ('Data allocation on wireless broadcast channels for efficient query processing\nData broadcast is an excellent method for efficient data dissemination in the\nmobile computing environment. The application domain of data broadcast\nwill be widely expanded in the near future, where the client is\nexpected to perform complex queries or transactions on the broadcast\ndata. To reduce the access latency for processing the complex query, it\nis beneficial to place the data accessed in a query close to each other\non the broadcast channel. In this paper, we propose an efficient\nalgorithm to determine the allocation of the data on the broadcast\nchannel such that frequently co-accessed data are not only allocated\nclose to each other, but also in a particular order which optimizes the\nperformance of query processing. Our mechanism is based on the\nwell-known problem named optimal linear ordering. Experiments are\nperformed to justify the benefit of our approach\n', ['database broadcasting', 'query processing', 'access time', 'tuning time', 'broadcast program', 'wireless broadcast channels', 'access latency', 'mobile computing', 'broadcast channels', 'database management systems', 'mobile computing', 'query processing', 'radio links', 'trees (mathematics)']), ('Preintegration lateral inhibition enhances unsupervised learning\nA large and influential class of neural network architectures uses\npostintegration lateral inhibition as a mechanism for competition. We\nargue that these algorithms are computationally deficient in that they\nfail to generate, or learn, appropriate perceptual representations\nunder certain circumstances. An alternative neural network architecture\nis presented here in which nodes compete for the right to receive\ninputs rather than for the right to generate outputs. This form of\ncompetition, implemented through preintegration lateral inhibition,\ndoes provide appropriate coding properties and can be used to learn\nsuch representations efficiently. Furthermore, this architecture is\nconsistent with both neuroanatomical and neuropsychological data. We\nthus argue that preintegration lateral inhibition has computational\nadvantages over conventional neural network architectures while\nremaining equally biologically plausible\n', ['neural network architectures', 'postintegration lateral inhibition', 'competition', 'preintegration lateral inhibition', 'neural network', 'unsupervised learning', 'neural net architecture', 'neural nets', 'unsupervised learning']), ('A shy invariant of graphs\nMoving from a well known result of P.L. Hammer et al. (1982), we introduce a\nnew graph invariant, say lambda (G) referring to any graph G. It is a\nnon-negative integer which is non-zero whenever G contains particular\ninduced odd cycles or, equivalently, admits a particular minimum\nclique-partition. We show that).(G) can be efficiently evaluated and\nthat its determination allows one to reduce the hard problem of\ncomputing a minimum clique-cover of a graph to an identical problem of\nsmaller size and special structure. Furthermore, one has alpha (G)\n<or= theta (G) - lambda (G), where alpha (G) and theta (G)\nrespectively denote the cardinality of a maximum stable set of G and of\na minimum clique-partition of G\n', ['graph invariant', 'induced odd cycles', 'minimum clique-partition', 'minimum clique-cover', 'cardinality', 'maximum stable set', 'computational geometry', 'graph theory']), ('Implementation of universal quantum gates based on nonadiabatic geometric\nphases\nWe propose an experimentally feasible scheme to achieve quantum computation\nbased on nonadiabatic geometric phase shifts, in which a cyclic\ngeometric phase is used to realize a set of universal quantum gates.\nPhysical implementation of this set of gates is designed for Josephson\njunctions and for NMR systems. Interestingly, we find that the\nnonadiabatic phase shift may be independent of the operation time under\nappropriate controllable conditions. A remarkable feature of the\npresent nonadiabatic geometric gates is that there is no intrinsic\nlimitation on the operation time\n', ['quantum computation', 'nonadiabatic geometric phase shifts', 'cyclic geometric phase', 'universal quantum gates', 'Josephson junctions', 'NMR systems', 'nonadiabatic phase shift', 'operation time', 'nonadiabatic geometric gates', 'nuclear magnetic resonance', 'quantum gates', 'superconducting junction devices']), ('Formalization of weighted factors analysis\nWeighted factors analysis (WeFA) has been proposed as a new approach for\nelicitation, representation, and manipulation of knowledge about a\ngiven problem, generally at a high and strategic level. Central to this\nproposal is that a group of experts in the area of the problem can\nidentify a hierarchy of factors with positive or negative influences on\nthe problem outcome. The tangible output of WeFA is a directed weighted\ngraph called a WeFA graph. This is a set of nodes denoting factors that\ncan directly or indirectly influence an overall aim of the graph. The\naim is also represented by a node. Each directed arc is a direct\ninfluence of one factor on another. A chain of directed arcs indicates\nan indirect influence. The influences may be identified as either\npositive or negative. For example, sales and costs are two factors that\ninfluence the aim of profitability in an organization. Sales has a\npositive influence on profitability and costs has a negative influence\non profitability. In addition, the relative significance of each\ninfluence is represented by a weight. We develop Binary WeFA which is a\nvariant of WeFA where the factors in the graph are restricted to being\neither true or false. Imposing this restriction on a WeFA graph allows\nus to be more precise about the meaning of the graph and of reasoning\nin it. Binary WeFA is a new proposal that provides a formal yet\nsufficiently simple language for logic-based argumentation for use by\nbusiness people in decision-support and knowledge management. Whilst\nBinary WeFA is expressively simpler than other logic-based\nargumentation formalisms, it does incorporate a novel formalization of\nthe notion of significance\n', ['weighted factors analysis', 'knowledge elicitation', 'knowledge representation', 'knowledge manipulation', 'significance', 'WeFA graph', 'directed arc', 'profitability', 'organization', 'Binary WeFA', 'reasoning', 'logic-based argumentation', 'decision-support', 'knowledge management', 'directed weighted graph', 'business data processing', 'directed graphs', 'formal logic', 'inference mechanisms', 'knowledge acquisition', 'knowledge representation']), ('A network simplex algorithm with O(n) consecutive degenerate pivots\nWe suggest a pivot rule for the primal simplex algorithm for the minimum cost\nflow problem, known as the network simplex algorithm. Due to\ndegeneracy, cycling may occur in the network simplex algorithm. The\ncycling can be prevented by maintaining strongly feasible bases\nproposed by Cunningham (1976); however, if we do not impose any\nrestrictions on the entering variables, the algorithm can still perform\nan exponentially long sequence of degenerate pivots. This phenomenon is\nknown as stalling. Researchers have suggested several pivot rules with\nthe following bounds on the number of consecutive degenerate pivots: m,\nn/sup 2/, k(k + 1)/2, where n is the number of nodes in the network, m\nis the number of arcs in the network, and k is the number of degenerate\narcs in the basis. (Observe that k <or= n.) In this paper, we\ndescribe an anti-stalling pivot rule that ensures that the network\nsimplex algorithm performs at most k consecutive degenerate pivots.\nThis rule uses a negative cost augmenting cycle to identify a sequence\nof entering variables\n', ['network simplex algorithm', 'degenerate pivots', 'minimum cost flow problem', 'degeneracy', 'cycling', 'stalling', 'anti-stalling pivot rule', 'negative cost augmenting cycle', 'computational complexity', 'minimisation', 'trees (mathematics)']), ('A high-resolution high-frequency monolithic top-shooting microinjector free of\nsatellite drops - part I: concept, design, and model\nIntroduces an innovative microinjector design, featuring a bubble valve, which\nentails superior droplet ejection characteristics and monolithic\nfabrication, which allows handling of a wide range of liquids. This new\nmicroinjector uses asymmetric bubbles to reduce crosstalk, increase\nfrequency response and eliminate satellite droplets. During a firing,\ni.e., droplet ejection, the "virtual valve" closes, by growing a\nthermal bubble in the microchannel, to isolate the microchamber from\nthe liquid supply and neighboring chambers. Between firings, however,\nthe virtual valve opens, by collapsing the bubble, to reduce flow\nrestriction for fast refilling of the microchamber. The use of bubble\nvalves brings about fast and reliable device operation without imposing\nthe significant complication fabrication of physical microvalves would\ncall for. In addition, through a special heater configuration and\nchamber designs, bubbles surrounding the nozzle cut off the tail of the\ndroplets being ejected and completely eliminate satellite droplets. A\nsimple one-dimensional model of the operation of the microinjector is\nused to estimate the bubble formation and liquid refilling\n', ['monolithic top-shooting microinjector', 'bubble valve', 'droplet ejection characteristics', 'asymmetric bubbles', 'crosstalk', 'liquid refilling', 'frequency response', 'satellite droplets', 'virtual valve', 'inkjet printing', 'thermal bubble jet', 'flow restriction', 'chamber designs', 'crosstalk', 'drops', 'frequency response', 'ink jet printers', 'microfluidics']), ('Gearing up for CLS bank\nContinuous-Linked Settlement, a dream of the foreign-exchange community for\nyears, may finally become a reality by the end of 2002\n', ['continuous-linked settlement', 'foreign-exchange', 'foreign exchange trading']), ('A new method of systemological analysis coordinated with the procedure of\nobject-oriented design. II\nFor pt.I. see Vestn. KhGPU, no.81, p.15-18 (2000). The paper presents the\nresults of development of an object-oriented systemological method used\nto design complex systems. A formal system representation, as well as\nan axiomatics of the calculus of systems as functional flow-type\nobjects based on a Node-Function-Object class hierarchy are proposed. A\nformalized NFO/UFO analysis algorithm and CASE tools used to support it\nare considered\n', ['systemological analysis', 'object-oriented design', 'complex systems design', 'formal system representation', 'axiomatics', 'functional flow-type objects', 'formalized NFO/UFO analysis algorithm', 'CASE tools', 'object-oriented programming']), ("The BLISS programming language: a history\nThe BLISS programming language was invented by William A. Wulf and others at\nCarnegie-Mellon University in 1969, originally for the DEC PDP-10.\nBLISS-10 caught the interest of Ronald F. Brender of DEC (Digital\nEquipment Corporation). After several years of collaboration, including\nthe creation of BLISS-11 for the PDP-11, BLISS was adopted as DEC's\nimplementation language for use on its new line of VAX computers in\n1975. DEC developed a completely new generation of BLISSs for the VAX,\nPDP-10 and PDP-11, which became widely used at DEC during the 1970s and\n1980s. With the creation of the Alpha architecture in the early 1990s,\nBLISS was extended again, in both 32- and 64-bit flavors. BLISS support\nfor the Intel IA-32 architecture was introduced in 1995 and IA-64\nsupport is now in progress. BLISS has a number of unusual\ncharacteristics: it is typeless, requires use of an explicit contents\nof operator (written as a period or 'dot'), takes an algorithmic\napproach to data structure definition, has no goto, is an expression\nlanguage, and has an unusually rich compile-time language. This paper\nreviews the evolution and use of BLISS over its three decade lifetime.\nEmphasis is on how the language evolved to facilitate portable\nprogramming while retaining its initial highly machine-specific\ncharacter. Finally, the success of its characteristics are assessed\n", ['BLISS programming language', 'machine-oriented language', 'portable programming', 'system implementation language', 'data structure definition', 'compile-time language', 'high level languages', 'machine oriented languages', 'software portability']), ('Nuclear magnetic resonance molecular photography\nA procedure is described for storing a two-dimensional (2D) pattern consisting\nof 32*32=1024 bits in a spin state of a molecular system and then\nretrieving the stored information as a stack of nuclear magnetic\nresonance spectra. The system used is a nematic liquid crystal, the\nprotons of which act as spin clusters with strong intramolecular\ninteractions. The technique used is a programmable multifrequency\nirradiation with low amplitude. When it is applied to the liquid\ncrystal, a large number of coherent long-lived /sup 1/H response\nsignals can be excited, resulting in a spectrum showing many sharp\npeaks with controllable frequencies and amplitudes. The spectral\nresolution is enhanced by using a second weak pulse with a 90 degrees\nphase shift, so that the 1024 bits of information can be retrieved as a\nset of well-resolved pseudo-2D spectra reproducing the input pattern\n', ['NMR molecular photography', '2D pattern', 'molecular system spin state', 'information storage', 'nematic liquid crystal', 'spin clusters', 'strong intramolecular interactions', 'programmable multifrequency irradiation', 'low amplitude', 'coherent long-lived /sup 1/H response signals', 'spectral resolution', 'second weak pulse', 'pseudo-2D spectra', 'spin echoes', 'Hilbert spaces', 'high-content molecular information processing', 'coupled spins', 'dipole-dipole interactions', 'spin-locking', 'proton spin', 'spin dynamics', '1024 bit', 'dipole coupling', 'liquid crystal devices', 'molecular electronics', 'nematic liquid crystals', 'nuclear magnetic resonance', 'quantum computing', 'spin systems']), ('Testing statistical bounds on entanglement using quantum chaos\nPrevious results indicate that while chaos can lead to substantial entropy\nproduction, thereby maximizing dynamical entanglement, this still falls\nshort of maximality. Random matrix theory modeling of composite quantum\nsystems, investigated recently, entails a universal distribution of the\neigenvalues of the reduced density matrices. We demonstrate that these\ndistributions are realized in quantized chaotic systems by using a\nmodel of two coupled and kicked tops. We derive an explicit statistical\nuniversal bound on entanglement, which is also valid for the case of\nunequal dimensionality of the Hilbert spaces involved, and show that\nthis describes well the bounds observed using composite quantized\nchaotic systems such as coupled tops\n', ['statistical bounds', 'entanglement', 'quantum chaos', 'entropy production', 'maximality', 'random matrix theory', 'composite quantum systems', 'universal distribution', 'reduced density matrices', 'quantized chaotic systems', 'kicked tops', 'Hilbert spaces', 'bound states', 'chaos', 'eigenvalues and eigenfunctions', 'entropy', 'Hilbert spaces', 'matrix algebra', 'quantisation (quantum theory)', 'quantum communication', 'quantum theory']), ('Integrated optical metrology controls post etch CDs\nControl of the transistor gate critical dimension (CD) on the order of a few\nnanometers is a top priority in many advanced IC fabs. Each nanometer\ndeviation from the target gate length translates directly into the\noperational speed of these devices. However, using in-line process\ncontrol by linking the lithography and etch tools can improve CD\nperformance beyond what each individual tool can achieve. The\nintegration of optical CD metrology tools to etch mainframes can result\nin excellent etcher stability and better control of post-etch CDs\n', ['integrated optical metrology', 'transistor gate critical dimension', 'post etch CD control', 'IC fabs', 'target gate length', 'operational speed', 'optical CD metrology tools', 'etch mainframes', 'etcher stability', 'in-line process control', 'lithography tools', 'CD performance', 'photolithography', 'etching', 'integrated circuit measurement', 'photolithography', 'process control', 'size control', 'size measurement']), ('A leaf sequencing algorithm to enlarge treatment field length in IMRT\nWith MLC-based IMRT, the maximum usable field size is often smaller than the\nmaximum field size for conventional treatments. This is due to the\nconstraints of the overtravel distances of MLC leaves and/or jaws.\nUsing a new leaf sequencing algorithm, the usable IMRT field length\n(perpendicular to the MLC motion) can be mostly made equal to the full\nlength of the MLC field without violating the upper jaw overtravel\nlimit. For any given intensity pattern, a criterion was proposed to\nassess whether an intensity pattern can be delivered without violation\nof the jaw position constraints. If the criterion is met, the new\nalgorithm will consider the jaw position constraints during the\nsegmentation for the step and shoot delivery method. The strategy\nemployed by the algorithm is to connect the intensity elements outside\nthe jaw overtravel limits with those inside the jaw overtravel limits.\nSeveral methods were used to establish these connections during\nsegmentation by modifying a previously published algorithm (areal\nalgorithm), including changing the intensity level, alternating the\nleaf-sequencing direction, or limiting the segment field size. The\nalgorithm was tested with 1000 random intensity patterns with\ndimensions of 21*27 cm/sup 2/, 800 intensity patterns with higher\nintensity outside the jaw overtravel limit, and three different types\nof clinical treatment plans that were undeliverable using a\nsegmentation method from a commercial treatment planning system. The\nnew algorithm achieved a success rate of 100% with these test patterns.\nFor the 1000 random patterns, the new algorithm yields a similar\naverage number of segments of 36.9+or-2.9 in comparison to 36.6+or-1.3\nwhen using the areal algorithm. For the 800 patterns with higher\nintensities outside the jaw overtravel limits, the new algorithm\nresults in an increase of 25% in the average number of segments\ncompared to the areal algorithm. However, the areal algorithm fails to\ncreate deliverable segments for 90% of these patterns. Using a single\nisocenter, the new algorithm provides a solution to extend the usable\nIMRT field length from 21 to 27 cm for IMRT on a commercial linear\naccelerator using the step and shoot delivery method\n', ['leaf sequencing algorithm', 'usable intensity modulated radiation therapy field length', 'overtravel distances', 'multileaf-based collimators intensity modulated radiation therapy', 'conformal radiation therapy', 'multileaf collimators jaws', 'multileaf collimators leaves', 'upper jaw overtravel limit', 'intensity pattern', 'jaw position constraints', 'step and shoot delivery method', 'commercial treatment planning system', 'random patterns', 'deliverable segments', 'single isocenter', 'commercial linear accelerator', 'treatment field length', 'intensity elements', 'jaw overtravel limits', 'areal algorithm', 'leaf-sequencing direction', 'segment field size', 'random intensity patterns', 'segmentation method', 'algorithm theory', 'linear accelerators', 'radiation therapy', 'X-ray optics']), ('Product and process innovations in the life cycle of an industry\nFilson (2001) uses industry-level data on firm numbers, price, quantity and\nquality along with an equilibrium model of industry evolution to\nestimate the nature and effects of quality and cost improvements in the\npersonal computer industry and four other new industries. This paper\nstudies the personal computer industry in more detail and shows that\nthe model explains some peculiar patterns that cannot be explained by\nprevious life-cycle models. The model estimates are evaluated using\nhistorical studies of the evolution of the personal computer industry\nand patterns that require further model development are described\n', ['technological change', 'life-cycle models', 'industry dynamics', 'personal computer market', 'microelectronics', 'equilibrium model', 'industry evolution', 'PC industry', 'production cost', 'costing', 'DP industry', 'electronics industry', 'production control', 'technology transfer']), ('The Advanced Encryption Standard - implementation and transition to a new\ncryptographic benchmark\nCryptography is the science of coding information to create unintelligible\nciphers that conceal or hide messages. The process that achieves this\ngoal is commonly referred to as encryption. Although encryption\nprocesses of various forms have been employed for centuries to protect\nthe exchange of messages, the advent of the information age has\nunderscored the importance of strong cryptography as a process to\nsecure data exchanged through electronic means, and has accentuated the\ndemand for products offering these services. This article describes the\nprocess that has led to the development of the latest cryptographic\nbenchmark; the Advanced Encryption Standard (AES). The article briefly\nexamines the requirements set forth for its development, defines how\nthe new standard is implemented, and describes how government,\nbusiness, and industry can transition to AES with minimum impact to\noperations\n', ['Advanced Encryption Standard', 'cryptographic benchmark', 'coding', 'unintelligible ciphers', 'data exchange', 'AES', 'government', 'business', 'industry', 'cryptography', 'software standards']), ('Fast and accurate leaf verification for dynamic multileaf collimation using an\nelectronic portal imaging device\nA prerequisite for accurate dose delivery of IMRT profiles produced with\ndynamic multileaf collimation (DMLC) is highly accurate leaf\npositioning. In our institution, leaf verification for DMLC was\ninitially done with film and ionization chamber. To overcome the\nlimitations of these methods, a fast, accurate and two-dimensional\nmethod for daily leaf verification, using our CCD-camera based\nelectronic portal imaging device (EPID), has been developed. This\nmethod is based on a flat field produced with a 0.5 cm wide sliding gap\nfor each leaf pair. Deviations in gap widths are detected as deviations\nin gray scale value profiles derived from the EPID images, and not by\ndirectly assessing leaf positions in the images. Dedicated software was\ndeveloped to reduce the noise level in the low signal images produced\nwith the narrow gaps. The accuracy of this quality assurance procedure\nwas tested by introducing known leaf position errors. It was shown that\nerrors in leaf gap as small as 0.01-0.02 cm could be detected, which is\ncertainly adequate to guarantee accurate dose delivery of DMLC\ntreatments, even for strongly modulated beam profiles. Using this\nmethod, it was demonstrated that both short and long term\nreproducibility in leaf positioning were within 0.01 cm (1 sigma ) for\nall gantry angles, and that the effect of gravity was negligible\n', ['accurate leaf verification', 'dynamic multileaf collimation', 'electronic portal imaging device', 'accurate dose delivery', 'intensity modulated radiation therapy profiles', 'ionization chamber', 'two-dimensional method', 'CCD-camera based electronic portal imaging device', 'sliding gap', 'leaf pair', 'gap widths', 'gray scale value profiles', 'electronic portal imaging device images', 'noise level', 'signal images', 'leaf position errors', 'modulated beam profiles', 'leaf positioning', 'gantry angles', 'diagnostic radiography', 'dosimetry', 'medical image processing', 'radiation therapy', 'X-ray optics']), ("Pipelined broadcast with enhanced wormhole routers\nThis paper proposes a pipelined broadcast that broadcasts a message of size m\nin O(m+n-1) time in an n-dimensional hypercube. It is based on the\nreplication tree, which is derived from reachable sets. It has greatly\nimproved performance compared to Ho-Kao's (1995) algorithm with the\ntime of O(m[n/log(n+1)]). The communication in the broadcast uses an\nall-port wormhole router with message replication capability. This\npaper includes the algorithm together with performance comparisons to\nprevious schemes in a practical implementation\n", ['enhanced wormhole routers', 'message broadcast', 'n-dimensional hypercube', 'replication tree', 'reachable sets', 'performance', 'communication complexity', 'all-port wormhole router', 'message replication capability', 'intermediate reception', 'pipelined broadcast', 'communication complexity', 'hypercube networks', 'network routing', 'performance evaluation', 'pipeline processing', 'reachability analysis', 'set theory', 'tree data structures']), ('Philadelphia stock exchange taps TimesTen for database technology\nPHLX rolls out Equity Options AutoQuote System to traders as the first\napplication to leverage its enhanced data architecture\n', ['Philadelphia stock exchange', 'TimesTen', 'Equity Options AutoQuote System', 'data architecture', 'stock markets']), ("Electronic books: reports of their death have been exaggerated\nE-books will survive, but not in the consumer market - at least not until\nreading devices become much cheaper and much better in quality (which\nis not likely to happen soon). Library Journal's review of major events\nof the year 2001 noted that two requirements for the success of E-books\nwere development of a sustainable business model and development of\nbetter reading devices. The E-book revolution has therefore become more\nof an evolution. We can look forward to further developments and\nadvances in the future\n", ['electronic books', 'E-books', 'Library Journal', 'electronic publishing', 'library automation']), ('A VMEbus interface for multi-detector trigger and control system\nMUSE (MUltiplicity SElector) is the trigger and control system of CHIMERA, a 4\npi charged particle detector. Initialization of MUSE can be performed\nvia VMEbus. This paper describes the design of VMEbus interface and\nfunctional module in MUSE, and briefly discusses an application of MUSE\n', ['VMEbus interface', 'MUSE', 'CHIMERA', 'trigger system', 'control system', 'computerised control', 'high energy physics instrumentation computing', 'system buses']), ('Data mining business intelligence for competitive advantage\nOrganizations have lately realized that just processing transactions and/or\ninformation faster and more efficiently no longer provides them with a\ncompetitive advantage vis-a-vis their competitors for achieving\nbusiness excellence. Information technology (IT) tools that are\noriented towards knowledge processing can provide the edge that\norganizations need to survive and thrive in the current era of fierce\ncompetition. Enterprises are no longer satisfied with business\ninformation system(s); they require business intelligence system(s).\nThe increasing competitive pressures and the desire to leverage\ninformation technology techniques have led many organizations to\nexplore the benefits of new emerging technology, data warehousing and\ndata mining. The paper discusses data warehouses and data mining tools\nand applications\n', ['business intelligence', 'competitive advantage', 'organizations', 'information technology', 'knowledge processing', 'business information system', 'data warehouses', 'data mining', 'business data processing', 'data mining', 'data warehouses', 'information technology']), ('VSAT technology aids growth\nChoosing to migrate to IP-based applications also means deciding whether\nterrestrial technologies such as frame relay, DSL or "plain old\ntelephone service" (POTS) can provide the scalability, flexibility and\nhigh bandwidth required to support those applications, and whether\nthese technologies can do so affordably. Each option has its tradeoffs.\nAlso, in each case, retailers with nationwide chains have to deal with\nmultiple last-mile service providers for service installation and\nnetwork maintenance. Because of this, many retailers are selecting\ntwo-way satellite networking technology (frequently referred to as\nVSAT) as the technology of choice for always-on, nationwide, high-speed\nconnectivity coupled with end-to-end network ownership and favorable\neconomics. Enterprises are adopting VSAT platforms not only for\nemerging IP and Web-based applications, but also for mission-critical,\nfront-office functions such as credit authorization and point-of-sale\npolling\n', ['VSAT', 'retailers', 'IP-based applications', 'Internet', 'retailing', 'satellite ground stations']), ('A knowledge-navigation system for dimensional metrology\nGeometric dimensioning and tolerancing (GD&T) is a method to specify the\ndimensions and form of a part so that it will meet its design intent.\nGD&T is difficult to master for two main reasons. First, it is based on\ncomplex 3D geometric entities and relationships. Second, the geometry\nis associated with a large, diverse knowledge base of dimensional\nmetrology with many interconnections. This paper describes an approach\nto create a dimensional metrology knowledge base that is organized\naround a set of key concepts and to represent those concepts as virtual\nobjects that can be navigated with interactive, computer visualization\ntechniques to access the associated knowledge. The approach can enable\nseveral applications. First is the application to convey the definition\nand meaning of GD&T over a broad range of tolerance types. Second is\nthe application to provide a visualization of dimensional metrology\nknowledge within a control hierarchy of the inspection process. Third\nis the application to show the coverage of interoperability standards\nto enable industry to make decisions on standards development and\nharmonization efforts. A prototype system has been implemented to\ndemonstrate the principles involved in the approach\n', ['geometric dimensioning', 'tolerancing', 'dimensional metrology', 'visualization', 'knowledge navigation', 'manufacturing training', 'VRML', 'interoperability standards', 'inspection', 'Web', 'computerised instrumentation', 'dimensions', 'inspection', 'knowledge based systems', 'length measurement', 'measurement standards', 'open systems', 'virtual reality languages']), ('Noise and the PSTH response to current transients: II. Integrate-and-fire model\nwith slow recovery and application to motoneuron data\nFor pt.I see ibid., vol.11, no.2 , p.135-151( 2001). A generalized version of\nthe integrate-and-fire model is presented that qualitatively reproduces\nfiring rates and membrane trajectories of motoneurons. The description\nis based on the spike-response model and includes three different time\nconstants: the passive membrane time constant, a recovery time of the\ninput conductance after each spike, and a time constant of the spike\nafterpotential. The effect of stochastic background input on the\nperistimulus time histogram (PSTH) response to spike input is\ncalculated analytically. Model results are compared with the\nexperimental data of Poliakov et al. (1996). The linearized theory\nshows that the PSTH response to an input spike is proportional to a\nfiltered version of the postsynaptic potential generated by the input\nspike. The shape of the filter depends on the background activity. The\nfull nonlinear theory is in close agreement with simulated PSTH data\n', ['PSTH', 'integrate-and-fire model', 'firing rates', 'membrane trajectories', 'spike-response model', 'passive membrane time constant', 'recovery time', 'spike afterpotential', 'motoneuron', 'neural nets']), ('A uniform framework for regulating service access and information release on\nthe Web\nThe widespread use of Internet-based services is increasing the amount of\ninformation (such as user profiles) that clients are required to\ndisclose. This information demand is necessary for regulating access to\nservices, and functionally convenient (e.g., to support service\ncustomization), but it has raised privacy-related concerns which, if\nnot addressed, may affect the users disposition to use network\nservices. At the same time, servers need to regulate service access\nwithout disclosing entirely the details of their access control policy.\nThere is therefore a pressing need for privacy-aware techniques to\nregulate access to services open to the network. We propose an approach\nfor regulating service access and information disclosure on the Web.\nThe approach consists of a uniform formal framework to formulate - and\nreason about - both service access and information disclosure\nconstraints. It also provides a means for parties to communicate their\nrequirements while ensuring that no private information be disclosed\nand that the communicated requirements are correct with respect to the\nconstraints\n', ['service access regulation', 'information release', 'WWW', 'Internet', 'user profiles', 'information demand', 'client server systems', 'access control policy', 'privacy-aware techniques', 'network services', 'information disclosure', 'uniform formal framework', 'reasoning', 'authorisation', 'client-server systems', 'data privacy', 'information resources']), ('Semidefinite programming vs. LP relaxations for polynomial programming\nWe consider the global minimization of a multivariate polynomial on a\nsemi-algebraic set Omega defined with polynomial inequalities. We then\ncompare two hierarchies of relaxations, namely, LP relaxations based on\nproducts of the original constraints, in the spirit of the RLT\nprocedure of Sherali and Adams (1990), and recent semidefinite\nprogramming (SDP) relaxations introduced by the author. The comparison\nis analyzed in light of recent results in real algebraic geometry on\nvarious representations of polynomials, positive on a compact\nsemi-algebraic set\n', ['polynomial programming', 'semidefinite programming relaxations', 'LP relaxations', 'global minimization', 'multivariate polynomial', 'polynomial inequalities', 'real algebraic geometry', 'reformulation linearization technique', 'semi-algebraic set', 'constraint products', 'RLT procedure', 'linear programming', 'linearisation techniques', 'minimisation', 'nonlinear programming', 'polynomials']), ("Nurture the geek in you [accounting on the Internet]\nWhen chartered accountants focus on IT, it's not simply because we think\ntechnology is neat. We keep on top of tech trends and issues because it\nhelps us do our jobs well. We need to know how to best manage and\nimplement the wealth of technology systems within out client base or\nemployer, as well as to determine on an ongoing basis how evolving\ntechnologies might affect business strategies, threats and\nopportunities. One way to stay current with technology is by monitoring\nthe online drumbeat. Imagine the Internet as an endless conversation of\nmillions of chattering voices, each focusing on a multitude of topics\nand issues. It's not surprising that a great deal of the information\nrelates to technology itself, and if you learn how to tune in to the\ndrumbeat, you can keep yourself informed\n", ['chartered accountants', 'Internet', 'information technology', 'Slashdot', 'Techdirt', 'The Register', "Dan Gillmor's Wournal", 'Daypop Top 40', 'RISKS', 'SecurityFocus', 'TechWeb', 'accounting', 'information resources']), ('A scalable and efficient systolic algorithm for the longest common subsequence\nproblem\nA longest common subsequence (LCS) of two strings is a common subsequence of\ntwo strings of maximal length. The LCS problem is that of finding an\nLCS of two given strings and the length of the LCS. This problem has\nbeen the subject of much research because its solution can be applied\nin many areas. In this paper, a scalable and efficient systolic\nalgorithm is presented. For two given strings of length m and n, where\nm>or=n, the algorithm can solve the LCS problem in m+2r-1\n(respectively n+2r-1) time steps with r<n/2 (respectively r<m/2)\nprocessors. Experimental results show that the algorithm can be faster\non multicomputers than all the previous systolic algorithms for the\nsame problem\n', ['systolic algorithm', 'longest common subsequence problem', 'scalable algorithm', 'parallel algorithms', 'data structures', 'parallel algorithms', 'systolic arrays']), ('Computer aided classification of masses in ultrasonic mammography\nFrequency compounding was recently investigated for computer aided\nclassification of masses in ultrasonic B-mode images as benign or\nmalignant. The classification was performed using the normalized\nparameters of the Nakagami distribution at a single region of interest\nat the site of the mass. A combination of normalized Nakagami\nparameters from two different images of a mass was undertaken to\nimprove the performance of classification. Receiver operating\ncharacteristic (ROC) analysis showed that such an approach resulted in\nan area of 0.83 under the ROC curve. The aim of the work described in\nthis paper is to see whether a feature describing the characteristic of\nthe boundary can be extracted and combined with the Nakagami parameter\nto further improve the performance of classification. The combination\nof the features has been performed using a weighted summation. Results\nindicate a 10% improvement in specificity at a sensitivity of 96% after\ncombining the information at the site and at the boundary. Moreover,\nthe technique requires minimal clinical intervention and has a\nperformance that reaches that of the trained radiologist. It is hence\nsuggested that this technique may be utilized in practice to\ncharacterize breast masses\n', ['ultrasonic mammography', 'breast masses', 'computer aided classification', 'frequency compounding', 'ultrasonic B-mode images', 'benign', 'malignant', 'normalized parameters', 'Nakagami distribution', 'single region of interest', 'normalized Nakagami parameters', 'receiver operating characteristic', 'ROC curve', 'weighted summation', 'specificity', 'sensitivity', 'minimal clinical intervention', 'biological organs', 'biomedical ultrasonics', 'image classification', 'mammography', 'medical image processing']), ('Intensity based affine registration including feature similarity for spatial\nnormalization\nThis paper presents a new spatial normalization with affine transformation. The\nquantitative comparison of brain architecture across different subjects\nrequires a common coordinate system. For the analysis of a specific\nbrain area, it is necessary to normalize and compare a region of\ninterest and the global brain. The intensity based registration method\nmatches the global brain well, but a region of interest may not be\nlocally normalized compared to the feature based method. The method in\nthis paper uses feature similarities of local regions as well as\nintensity similarities. The lateral ventricle and central gray nuclei\nof the brain, including the corpus callosum, which is used for features\nin schizophrenia detection, is appropriately normalized. Our method\nreduces the difference of feature areas such as the corpus callosum\n(7.7%, 2.4%) and lateral ventricle (8.2%, 13.5%) compared with mutual\ninformation and Talairach methods\n', ['intensity based affine registration', 'feature similarity', 'spatial normalization', 'affine transformation', 'brain architecture', 'common coordinate system', 'global brain', 'region of interest', 'feature similarities', 'lateral ventricle', 'central gray nuclei', 'corpus callosum', 'schizophrenia detection', 'mutual information method', 'Talairach method', 'brain', 'diseases', 'feature extraction', 'image registration', 'medical image processing', 'stereo image processing']), ('Computational finite-element schemes for optimal control of an elliptic system\nwith conjugation conditions\nNew optimal control problems are considered for distributed systems described\nby elliptic equations with conjugate conditions and a quadratic\nminimized function. Highly accurate computational discretization\nschemes are constructed for the case where a feasible control set u/sub\ndelta / coincides with the full Hilbert space u of controls\n', ['optimal control problems', 'distributed systems', 'elliptic equations', 'conjugate conditions', 'quadratic minimized function', 'computational discretization schemes', 'conjugate gradient methods', 'finite element analysis', 'optimal control']), ('Blitzograms - interactive histograms\nAs computers become ever faster, more and more procedures that were once viewed\nas iterative will continue to become instantaneous. The blitzogram is\nthe application of this trend to histograms, which the author hopes\nwill lead to a better tacit understanding of probability distributions\namong both students and managers. And this is not just an academic\nexercise. Commercial Monte Carlo simulation packages like @RISK and\nCrystal Ball, and my INSIGHT.xla are widely available\n', ['blitzogram', 'histograms', 'probability distributions', 'MBA', 'operations research', 'management science', 'statistics', 'computer graphics', 'educational computing', 'management science', 'statistical analysis']), ('A virtual victory [virtual networks]\nNewly fashionable virtual network operators look all set to clean up in the\ncorporate sector\n', ['virtual network operators', 'corporate sector', 'telecommunication']), ('Run-time data-flow analysis\nParallelizing compilers have made great progress in recent years. However,\nthere still remains a gap between the current ability of parallelizing\ncompilers and their final goals. In order to achieve the maximum\nparallelism, run-time techniques were used in parallelizing compilers\nduring last few years. First, this paper presents a basic run-time\nprivatization method. The definition of run-time dead code is given and\nits side effect is discussed. To eliminate the imprecision caused by\nthe run-time dead code, backward data-flow information must be used.\nProteus Test, which can use backward information in run-time, is then\npresented to exploit more dynamic parallelism. Also, a variation of\nProteus Test, the Advanced Proteus Test, is offered to achieve partial\nparallelism. Proteus Test was implemented on the parallelizing compiler\nAFT. In the end of this paper the program fpppp.f of Spec95fp Benchmark\nis taken as an example, to show the effectiveness of Proteus Test\n', ['run-time data flow analysis', 'parallelizing compilers', 'run-time privatization method', 'run-time dead code', 'backward data-flow information', 'Proteus Test', 'dynamic parallelism', 'data flow analysis', 'parallelising compilers']), ('Contracting in the days of ebusiness\nPutting electronic business on a sound foundation-model theoretically as well\nas technologically-is a central challenge for research as well as\ncommercial development. This paper concentrates on the discovery and\nnegotiation phase of concluding an agreement based on a contract. We\npresent a methodology for moving seamlessly from a many-to-many\nrelationship in the discovery phase to a one-to-one relationship in the\ncontract negotiation phase. Making the content of contracts persistent\nis achieved by reconstructing contract templates by means of mereologic\n(logic of the whole-part relation). Possibly nested sub-structures of\nthe contract template are taken as a basis for negotiation in a\ndialogical way. For the negotiation itself the contract templates are\nextended by implications (logical) and sequences (topical)\n', ['electronic business', 'discovery phase', 'contracting', 'many-to-many relationship', 'one-to-one relationship', 'contract negotiation phase', 'mereologic', 'contract templates', 'nested sub-structure', 'sequences', 'implications', 'contracts', 'electronic commerce']), ('Pattern recognition strategies for molecular surfaces. I. Pattern generation\nusing fuzzy set theory\nA new method for the characterization of molecules based on the model approach\nof molecular surfaces is presented. We use the topographical properties\nof the surface as well as the electrostatic potential, the local\nlipophilicity/hydrophilicity, and the hydrogen bond density on the\nsurface for characterization. The definition and the calculation method\nfor these properties are reviewed. The surface is segmented into\noverlapping patches with similar molecular properties. These patches\ncan be used to represent the characteristic local features of the\nmolecule in a way that is beyond the atomistic resolution but can\nnevertheless be applied for the analysis of partial similarities of\ndifferent molecules as well as for the identification of molecular\ncomplementarity in a very general sense. The patch representation can\nbe used for different applications, which will be demonstrated in\nsubsequent articles\n', ['pattern recognition strategies', 'molecular surfaces', 'pattern generation', 'fuzzy set theory', 'model approach', 'topographical properties', 'electrostatic potential', 'local lipophilicity/hydrophilicity', 'hydrogen bond density', 'segmented surface', 'overlapping patches', 'molecular properties', 'local features', 'atomistic resolution', 'partial similarities', 'molecular complementarity', 'patch representation', 'lipophilicity', 'hydrophilicity', 'fuzzy logic', 'fuzzy set theory', 'hydrogen bonds', 'intermolecular mechanics', 'molecular biophysics', 'pattern classification', 'pattern recognition', 'potential energy functions']), ('Information and information technology\nThis paper reveals the concepts of information and information technology. It\nalso describes the close relationship between information and\ninformation technology. It explains a basic mechanism of different\ndevices of information technology and connotes how they are useful to\nstore, process and retrieve the information. In addition of this, the\npaper shows the present status of information technology and Indian\nuniversities\n', ['information technology', 'information', 'information storage', 'information processing', 'information retrieval', 'Indian universities', 'academic libraries', 'information centres', 'information retrieval', 'information storage', 'library automation']), ('Information architecture: notes toward a new curriculum\nThere are signs that information architecture is coalescing into a field of\nprofessional practice. However, if it is to become a profession, it\nmust develop a means of educating new information architects. Lessons\nfrom other fields suggest that professional education typically evolves\nalong a predictable path, from apprenticeships to trade schools to\ncollege- and university-level education. Information architecture\neducation may develop more quickly to meet the growing demands of the\ninformation society. Several pedagogical approaches employed in other\nfields may be adopted for information architecture education, as long\nas the resulting curricula provide an interdisciplinary approach and\nbalance instruction in technical and design skills with consideration\nof theoretical concepts. Key content areas are information\norganization, graphic. design, computer science, user and usability\nstudies, and communication. Certain logistics must be worked out,\nincluding where information architecture studies should be housed and\nwhat kinds of degrees should be offered and at what levels. The\nsuccessful information architecture curriculum will be flexible and\nadaptable in order to meet the changing needs of students and the\nmarketplace\n', ['professional practice', 'information organization', 'graphic design', 'computer science', 'usability studies', 'information architects', 'professional education', 'pedagogical approaches', 'information architecture education', 'computer science education', 'educational courses', 'information science']), ('Fault-tolerant Hamiltonian laceability of hypercubes\nIt is known that every hypercube Q/sub n/ is a bipartite graph. Assume that\nn>or=2 and F is a subset of edges with |F|<or=n-2. We prove that\nthere exists a Hamiltonian path in Q/sub n/-F between any two vertices\nof different partite sets. Moreover, there exists a path of length\n2/sup n/-2 between any two vertices of the same partite set. Assume\nthat n>or=3 and F is a subset of edges with |F|<or=n-3. We prove\nthat there exists a Hamiltonian path in Q/sub n/-{v}-F between any two\nvertices in the partite set without v. Furthermore, all bounds are\ntight\n', ['fault-tolerant Hamiltonian laceability', 'hypercubes', 'bipartite graph', 'edge subset', 'Hamiltonian path', 'vertices', 'partite sets', 'tight bounds', 'fault tolerant computing', 'graph theory', 'hypercube networks']), ('Dousing terrorist funding: mission possible? [banks]\nThe government is tightening its grip on terrorist money flows. But as the\nbanking industry continues to expand its Patriot Act compliance\nactivities, it is with the realization that a great deal of work\nremains to be done before the American financial system can become\ntruly airtight. Identification instruments, especially drivers\nlicenses, represent a significant weak spot\n', ['banking', 'Patriot Act', 'terrorist funding', 'identification', 'banking', 'computer crime', 'legislation']), ('Simulation of cardiovascular physiology: the diastolic function(s) of the heart\nThe cardiovascular system was simulated by using an equivalent electronic\ncircuit. Four sets of simulations were performed. The basic variables\ninvestigated were cardiac output and stroke volume. They were studied\nas functions (i) of right ventricular capacitance and negative\nintrathoracic pressure; (ii) of left ventricular relaxation and of\nheart rate; and (iii) of left ventricle failure. It seems that a\nsatisfactory simulation of systolic and diastolic functions of the\nheart is possible. Presented simulations improve our understanding of\nthe role of the capacitance of both ventricles and of the diastolic\nrelaxation in cardiovascular physiology\n', ['cardiovascular physiology', 'simulation', 'diastolic function', 'heart', 'equivalent electronic circuit', 'cardiac output', 'stroke volume', 'right ventricular capacitance', 'negative intrathoracic pressure', 'left ventricular relaxation', 'heart rate', 'left ventricle failure', 'systolic functions', 'diastolic relaxation', 'cardiovascular system', 'digital simulation', 'medical computing', 'physiology']), ('When the unexpected happens [disaster planning in banks]\nA business disruption can be as simple as a power failure or as complex as a\nterrorist attack. Regardless, you will need to have a plan to minimize\ninterruptions to both your bank and your customers. Marketers have a\nrole in this readiness process\n', ['disaster planning', 'banks', 'planning', 'recovery', 'public relations', 'emergency management', 'banking', 'disasters', 'planning']), ('Design of PID-type controllers using multiobjective genetic algorithms\nThe design of a PID controller is a multiobjective problem. A plant and a set\nof specifications to be satisfied are given. The designer has to adjust\nthe parameters of the PID controller such that the feedback\ninterconnection of the plant and the controller satisfies the\nspecifications. These specifications are usually competitive and any\nacceptable solution requires a tradeoff among them. An approach for\nadjusting the parameters of a PID controller based on multiobjective\noptimization and genetic algorithms is presented in this paper. The\nMRCD (multiobjective robust control design) genetic algorithm has been\nemployed. The approach can be easily generalized to design\nmultivariable coupled and decentralized PID loops and has been\nsuccessfully validated for a large number of experimental cases\n', ['PID-type controllers', 'multiobjective genetic algorithms', 'feedback interconnection', 'multiobjective robust control design', 'multivariable coupled PID loops', 'decentralized PID loops', 'tuning methods', 'closed loop systems', 'control system synthesis', 'genetic algorithms', 'robust control', 'three-term control', 'transfer function matrices']), ('Combining spatial and scale-space techniques for edge detection to provide a\nspatially adaptive wavelet-based noise filtering algorithm\nNew methods for detecting edges in an image using spatial and scale-space\ndomains are proposed. A priori knowledge about geometrical\ncharacteristics of edges is used to assign a probability factor to the\nchance of any pixel being on an edge. An improved double thresholding\ntechnique is introduced for spatial domain filtering. Probabilities\nthat pixels belong to a given edge are assigned based on pixel\nsimilarity across gradient amplitudes, gradient phases and edge\nconnectivity. The scale-space approach uses dynamic range compression\nto allow wavelet correlation over a wider range of scales. A\nprobabilistic formulation is used to combine the results obtained from\nfiltering in each domain to provide a final edge probability image\nwhich has the advantages of both spatial and scale-space domain\nmethods. Decomposing this edge probability image with the same wavelet\nas the original image permits the generation of adaptive filters that\ncan recognize the characteristics of the edges in all wavelet detail\nand approximation images regardless of scale. These matched filters\npermit significant reduction in image noise without contributing to\nedge distortion. The spatially adaptive wavelet noise-filtering\nalgorithm is qualitatively and quantitatively compared to a frequency\ndomain and two wavelet based noise suppression algorithms using both\nnatural and computer generated noisy images\n', ['spatial techniques', 'scale-space techniques', 'edge detection', 'spatially adaptive wavelet-based noise filtering algorithm', 'a priori knowledge', 'geometrical characteristics', 'probability factor', 'double thresholding technique', 'spatial domain filtering', 'pixel similarity', 'gradient amplitudes', 'gradient phases', 'edge connectivity', 'dynamic range compression', 'wavelet correlation', 'probabilistic formulation', 'final edge probability image', 'adaptive filters', 'approximation images', 'matched filters', 'image noise', 'spatially adaptive wavelet noise-filtering algorithm', 'noise suppression', 'adaptive filters', 'data compression', 'edge detection', 'image enhancement', 'matched filters', 'noise', 'probability', 'wavelet transforms']), ('Automating the compliance and supervision process\nNew technology enables large broker/dealers to supervise and ensure compliance\nacross multiple branches and managers\n', ['compliance', 'supervision', 'brokers', 'risk management', 'risk management', 'securities trading']), ('Decisions, decisions, decisions: a tale of special collections in the small\nacademic library\nA case study of a special collections department in a small academic library\nand how its collections have been acquired and developed over the years\nis described. It looks at the changes that have occurred in the\nacademic environment and what effect, if any, these changes may have\nhad on the department and how it has adapted to them. It raises\nquestions about development and acquisitions policies and procedures\n', ['special collections', 'small academic library', 'case study', 'acquisitions policies', 'out-of-print books', 'University library', 'academic libraries', 'library automation']), ("Using NetCloak to develop server-side Web-based experiments without writing CGI\nprograms\nServer-side experiments use the Web server, rather than the participant's\nbrowser, to handle tasks such as random assignment, eliminating\ninconsistencies with Java and other client-side applications.\nHeretofore, experimenters wishing to create server-side experiments\nhave had to write programs to create common gateway interface (CGI)\nscripts in programming languages such as Perl and C++. NetCloak uses\nsimple, HTML-like commands to create CGIs. We used NetCloak to\nimplement an experiment on probability estimation. Measurements of time\non task and participants' IP addresses assisted quality control.\nWithout prior training, in less than 1 month, we were able to use\nNetCloak to design and create a Web-based experiment and to help\ngraduate students create three Web-based experiments of their own\n", ['NetCloak', 'server-side Web-based experiments', 'CGI programs', 'Web server', 'random assignment', 'Java', 'client-side applications', 'common gateway interface scripts', 'Perl', 'C++ language', 'HTML', 'probability estimation', 'IP addresses', 'quality control', 'graduate students', 'Internet', 'behavioral data', 'psychology', 'educational computing', 'hypermedia markup languages', 'information resources', 'Internet', 'probability', 'psychology']), ('Algorithmic results for ordered median problems\nIn a series of papers a new type of objective function in location theory,\ncalled ordered median function, has been introduced and analyzed. This\nobjective function unifies and generalizes most common objective\nfunctions used in location theory. In this paper we identify finite\ndominating sets for these models and develop polynomial time algorithms\ntogether with a detailed complexity analysis\n', ['algorithmic results', 'ordered median problems', 'objective function', 'location theory', 'ordered median function', 'finite dominating sets', 'polynomial time algorithms', 'detailed complexity analysis', 'computational complexity', 'facility location', 'functions', 'graph theory', 'set theory']), ('Use of SPOT images as a tool for coastal zone management and monitoring of\nenvironmental impacts in the coastal zone\nModern techniques such as remote sensing have been one of the main factors\nleading toward the achievement of serious plans regarding coastal\nmanagement. A multitemporal analysis of land use in certain areas of\nthe Colombian Caribbean Coast is described. It mainly focuses on\nenvironmental impacts caused by anthropogenic activities, such as\ndeforestation of mangroves due to shrimp farming. Selection of\nsensitive areas, percentage of destroyed mangroves, possible endangered\nareas, etc., are some of the results of this analysis. Recommendations\nfor a coastal management plan in the area have also resulted from this\nanalysis. Some other consequences of the deforestation of mangroves in\nthe coastal zone and the construction of shrimp ponds are also\nanalyzed, such as the increase of erosion problems in these areas and\nwater pollution, among others. The increase of erosion in these areas\nhas also changed part of their morphology, which has been studied by\nthe analysis of SPOT images in previous years. A serious concern exists\nabout the future of these areas. For this reason new techniques like\nsatellite images (SPOT) have been applied with good results, leading to\nmore effective control and coastal management in the area. The use of\nSPOT images to study changes of the land use of the area is a useful\ntechnique to determine patterns of human activities and suggest\nsolutions for severe problems in these areas\n', ['coastal zone management', 'SPOT images', 'environmental impact monitoring', 'remote sensing', 'multitemporal analysis', 'land use', 'Colombian Caribbean Coast', 'anthropogenic activities', 'mangrove deforestation', 'shrimp farming', 'endangered areas', 'erosion problems', 'water pollution', 'satellite images', 'human activities', 'supervised classification', 'sedimentation', 'shrimp ponds', 'vectorization', 'vector overlay', 'aquaculture', 'environmental factors', 'erosion', 'image classification', 'vector quantisation', 'vegetation mapping']), ("Diffraction limit for a circular mask with a periodic rectangular apertures\narray\nA mask with periodic apertures imaging system is adopted very widely and plays\na leading role in modern technology for uses such as pinhole cameras,\ncoded imaging systems, optical information processing, etc. because of\nits high resolution, its infinite depth of focus, and its usefulness\nover a broad frequency spectra ranging from visible light to X-rays and\ngamma rays. While the masks with periodic apertures investigated in the\nliterature are limited only to far-field diffraction, they do not take\nthe shift of apertures within the mask into consideration. Therefore\nthe derivation of the far-field diffraction for a single aperture\ncannot be applied to a mask with periodic apertures. The far-field\ndiffraction formula modified for a multiaperture mask has been proposed\nin the past, the analysis remains too complicated to offer some\npractical guidance for mask design. We study a circular mask with\nperiodic rectangular apertures and develop an easier way to interpret\nit. First, the near-field diffraction intensity of a circular aperture\nis calculated by means of Lommel's function. Then the convolution of\nthe circular mask diffraction with periodic rectangular apertures is\nput together, and we can present a simple mathematical tool to analyze\nthe mask properties including the intensity distribution, blurring\naberration, and the criterion of defining the far- or near-field\ndiffraction. This concept can also be expanded to analyze different\ntypes of masks with the arbitrarily shaped apertures\n", ['diffraction limit', 'circular mask', 'periodic rectangular apertures array', 'pinhole cameras', 'coded imaging systems', 'optical information processing', 'high resolution', 'infinite depth of focus', 'broad frequency spectra', 'visible light', 'x rays', 'gamma rays', 'periodic apertures', 'far-field diffraction', 'mask', 'single aperture', 'far-field diffraction formula', 'multiaperture mask', 'periodic rectangular apertures', 'convolution', 'circular mask diffraction', 'near-field diffraction', 'arbitrarily shaped apertures', 'convolution', 'image coding', 'image resolution', 'light diffraction', 'masks', 'optical focusing']), ('Fuzzy modeling based on generalized conjunction operations\nAn approach to fuzzy modeling based on the tuning of parametric conjunction\noperations is proposed. First, some methods for the construction of\nparametric generalized conjunction operations simpler than the known\nparametric classes of conjunctions are considered and discussed.\nSecond, several examples of function approximation by fuzzy models,\nbased on the tuning of the parameters of the new conjunction\noperations, are given and their approximation performances are compared\nwith the approaches based on a tuning of membership functions and other\napproaches proposed in the literature. It is seen that the tuning of\nthe conjunction operations can be used for obtaining fuzzy models with\na sufficiently good performance when the tuning of membership functions\nis not possible or not desirable\n', ['fuzzy modeling', 'generalized conjunction operations', 'function approximation', 'tuning', 'approximation performances', 'membership functions', 't-norm', 'fuzzy inference systems', 'function approximation', 'fuzzy logic', 'fuzzy set theory', 'fuzzy systems', 'modelling', 'tuning']), ('On biorthogonal nonuniform filter banks and tree structures\nThis paper concerns biorthogonal nonuniform filter banks. It is shown that a\ntree structured filter bank is biorthogonal if it is equivalent to a\ntree structured filter bank whose matching constituent levels on the\nanalysis and synthesis sides are themselves biorthogonal pairs. We then\nshow that a stronger statement can be made about dyadic filter banks in\ngeneral: That a dyadic filter bank is biorthogonal if both the analysis\nand synthesis banks can be decomposed into dyadic trees. We further\nshow that these decompositions are stability and FIR preserving. These\nresults, derived for filter banks having filters with rational transfer\nfunctions, thus extend some of the earlier comparable results for\northonormal filter banks\n', ['biorthogonal nonuniform filter banks', 'tree structured filter bank', 'biorthogonal pairs', 'dyadic filter banks', 'dyadic trees', 'stability preserving', 'FIR preserving', 'rational transfer functions', 'filtering theory', 'FIR filters', 'linear phase filters', 'matrix algebra', 'stability', 'wavelet transforms']), ('A large deviations analysis of the transient of a queue with many Markov fluid\ninputs: approximations and fast simulation\nThis article analyzes the transient buffer content distribution of a queue fed\nby a large number of Markov fluid sources. We characterize the\nprobability of overflow at time t, given the current buffer level and\nthe number of sources in the on-state. After scaling buffer and\nbandwidth resources by the number of sources n, we can apply large\ndeviations techniques. The transient overflow probability decays\nexponentially in n. In the case of exponential on/off sources, we\nderive an expression for the decay rate of the rare event probability\nunder consideration. For general Markov fluid sources, we present a\nplausible conjecture. We also provide the "most likely path" from the\ninitial state to overflow (at time t). Knowledge of the decay rate and\nthe most likely path to overflow leads to (i) approximations of the\ntransient overflow probability and (ii) efficient simulation methods of\nthe rare event of buffer overflow. The simulation methods, based on\nimportance sampling, give a huge speed-up compared to straightforward\nsimulations. The approximations are of low computational complexity and\nare accurate, as verified by means of simulation experiments\n', ['large deviations analysis', 'Markov fluid inputs', 'transient buffer content distribution', 'buffer resources', 'bandwidth resources', 'approximations', 'transient overflow probability', 'simulation methods', 'importance sampling', 'computational complexity', 'ATM multiplexers', 'IP routers', 'queuing theory', 'approximation theory', 'asynchronous transfer mode', 'buffer storage', 'computational complexity', 'digital simulation', 'importance sampling', 'Markov processes', 'probability', 'queueing theory', 'telecommunication computing', 'telecommunication network routing', 'transient analysis']), ('On the diophantine equation x/sup 2/+q/sup 2k+1/=y/sup n/\nIn this paper it has been proved that if q is an odd prime, q not=7 (mod 8), n\nis an odd integer >or=5, n is not a multiple of 3 and (h, n)=1,\nwhere h is the class number of the filed Q( square root (-q)), then the\ndiophantine equation x/sup 2/+q/sup 2k+1/=y/sup n/ has exactly two\nfamilies of solutions (q, n, k, x, y)\n', ['diophantine equation', 'odd prime', 'odd integer', 'Lucas sequence', 'primitive divisors', 'number theory']), ('Adjoint-based optimization of steady suction for disturbance control in\nincompressible flows\nThe optimal distribution of steady suction needed to control the growth of\nsingle or multiple disturbances in quasi-three-dimensional\nincompressible boundary layers on a flat plate is investigated. The\nevolution of disturbances is analysed in the framework of the\nparabolized stability equations (PSE). A gradient-based optimization\nprocedure is used and the gradients are evaluated using the adjoint of\nthe parabolized stability equations (APSE) and the adjoint of the\nboundary layer equations (ABLE). The accuracy of the gradient is\nincreased by introducing a stabilization procedure for the PSE. Results\nshow that a suction peak appears in the upstream part of the suction\nregion for optimal control of Tollmien-Schlichting (T-S) waves, steady\nstreamwise streaks in a two-dimensional boundary layer and oblique\nwaves in a quasi-three-dimensional boundary layer subject to an adverse\npressure gradient. The mean flow modifications due to suction are shown\nto have a stabilizing effect similar to that of a favourable pressure\ngradient. It is also shown that the optimal suction distribution for\nthe disturbance of interest reduces the growth rate of other\nperturbations. Results for control of a steady cross-flow mode in a\nthree-dimensional boundary layer subject to a favourable pressure\ngradient show that not even large amounts of suction can completely\nstabilize the disturbance\n', ['adjoint-based optimization', 'steady suction', 'disturbance control', 'incompressible flows', 'quasithree-dimensional incompressible boundary layers', 'flat plate', 'parabolized stability equations', 'gradient-based optimization procedure', 'stabilization procedure', 'Tollmien-Schlichting waves', 'steady streamwise streaks', 'oblique waves', 'adverse pressure gradient', 'mean flow', 'steady cross-flow mode', 'laminar-turbulent transition', 'aerodynamics', 'boundary layer turbulence', 'flow control', 'flow instability', 'laminar to turbulent transitions', 'optimal control', 'waves']), ("Modelling tomographic cone-beam projection data from a polyhedral phantom\nAnalytical phantoms are used to generate projection data for testing\nreconstruction accuracy in computed axial tomography. A circular source\nlocus (equivalent to rotating specimen with a fixed source) provides\ninsufficient data for 'exact' reconstruction in cone-beam transmission\ntomography, thus phantom data are useful for studying the consequent\nerrors and also for investigating alternative scanning loci and\nreconstruction techniques. We present an algorithm that can compute\nphantom cone-beam projection data from a phantom comprising\ngeometrically defined polyhedra. Each polyhedron is defined as a set of\npolygons enclosing a volume of fixed linear attenuation coefficient.\nThe algorithm works by projecting each polygon in turn onto the\nmodelled detector array, which accumulates the product of source to\npolygon intersection distance (for the rays intersecting each detector\nelement), linear attenuation coefficient and sign of projected polygon\narea (indicating whether rays enter or exit the polyhedron at this\nface). The phantom data are rotated according to the projection angle,\nwhilst the source location and detector plane remain fixed. Polyhedra\ncan be of simple geometric form, or complex surfaces derived from 3D\nimages of real specimens. This algorithm is illustrated using a phantom\ncomprising 989 238 polygons, representing an iso-surface generated from\na microtomographic reconstruction of a piece of walrus tusk\n", ['tomographic cone-beam projection data', 'polyhedral phantom', 'reconstruction accuracy', 'computed axial tomography', 'cone-beam transmission tomography', 'alternative scanning loci', 'geometrically defined polyhedra', 'linear attenuation coefficient', 'microtomographic reconstruction', 'walrus tusk', 'reconstruction software accuracy', 'X-ray attenuation', 'cumulative pixel array', 'interpolation', 'geometry file', 'computational geometry', 'computerised tomography', 'image reconstruction', 'interpolation', 'medical image processing', 'surface fitting']), ('Structural invariance of spatial Pythagorean hodographs\nThe structural invariance of the four-polynomial characterization for\nthree-dimensional Pythagorean hodographs introduced by Dietz et al.\n(1993), under arbitrary spatial rotations, is demonstrated. The proof\nrelies on a factored-quaternion representation for Pythagorean\nhodographs in three-dimensional Euclidean space-a particular instance\nof the "PH representation map" proposed by Choi et al. (2002)-and the\nunit quaternion description of spatial rotations. This approach\nfurnishes a remarkably simple derivation for the polynomials u(t),\nupsilon (t), p(t), q(t) that specify the canonical form of a rotated\nPythagorean hodograph, in terms of the original polynomials u(t),\nupsilon (t), p(t), q(t) and the angle theta and axis n of the spatial\nrotation. The preservation of the canonical form of PH space curves\nunder arbitrary spatial rotations is essential to their incorporation\ninto computer-aided design and manufacturing applications, such as the\ncontour machining of free-form surfaces using a ball-end mill and\nrealtime PH curve CNC interpolators\n', ['structural invariance', 'four-polynomial characterization', 'spatial Pythagorean hodographs', '3D Pythagorean hodographs', 'arbitrary spatial rotations', 'factored quaternion representation', '3D Euclidean space', 'PH representation map', 'unit quaternion description', 'spatial rotations', 'CAD/CAM', 'contour machining', 'free-form surfaces', 'ball-end mill', 'real-time PH curve CNC interpolators', 'CAD/CAM', 'computational geometry', 'computerised numerical control', 'interpolation', 'machining']), ('The design and implementation of VAMPIRE\nWe describe VAMPIRE: a high-performance theorem prover for first-order logic.\nAs our description is mostly targeted to the developers of such systems\nand specialists in automated reasoning, it focuses on the design of the\nsystem and some key implementation features. We also analyze the\nperformance of the prover at CASC-JC\n', ['VAMPIRE', 'high-performance theorem prover', 'first-order logic', 'automated reasoning', 'performance evaluation', 'CASC-JC', 'resolution theorem proving', 'formal logic', 'inference mechanisms', 'theorem proving']), ('Tracking nonparameterized object contours in video\nWe propose a new method for contour tracking in video. The inverted distance\ntransform of the edge map is used as an edge indicator function for\ncontour detection. Using the concept of topographical distance, the\nwatershed segmentation can be formulated as a minimization. This new\nviewpoint gives a way to combine the results of the watershed algorithm\non different surfaces. In particular, our algorithm determines the\ncontour as a combination of the current edge map and the contour,\npredicted from the tracking result in the previous frame. We also show\nthat the problem of background clutter can be relaxed by taking the\nobject motion into account. The compensation with object motion allows\nto detect and remove spurious edges in background. The experimental\nresults confirm the expected advantages of the proposed method over the\nexisting approaches\n', ['contour tracking', 'nonparameterized object contours', 'edge indicator function', 'topographical distance', 'watershed segmentation', 'minimization', 'background clutter', 'object motion', 'motion analysis', 'video', 'inverted distance transform', 'edge map', 'motion estimation', 'edge detection', 'edge detection', 'image motion analysis', 'motion compensation', 'motion estimation', 'tracking', 'video signal processing']), ('Stochastic recurrences of Jackpot Keno\nWe describe a mathematical model and simulation study for Jackpot Keno, as\nimplemented by Jupiters Network Gaming (JNG) in the Australian state of\nQueensland, and as controlled by the Queensland Office of Gaming\nRegulation (QOGR) (http://www.qogr.qld.gov.au/keno.shtml). The\nrecurrences for the house net hold are derived and it is seen that\nthese are piecewise linear with a ternary domain split, and further,\nthe split points are stochastic in nature. Since this structure is\nintractable (Brockett and Levine, Statistics & Probability & their\nApplications, CBS College Publishing, 1984), estimation of house net\nhold obtained through an appropriately designed simulator using a\nrandom number generator with desirable properties is described. Since\nthe model and simulation naturally derives hold given payscale, but JNG\nand QOGR require payscale given hold, an inverse problem was required\nto be solved. This required development of a special algorithm, which\nmay be described as a stochastic binary search. Experimental results\nare presented, in which the simulator is used to determine jackpot\npay-scales so as to satisfy legal requirements of approximately 75% of\nnet revenue returned to the players, i.e., 25% net hold for the house\n(JNG). Details of the algorithm used to solve this problem are\npresented, and notwithstanding the stochastic nature of the simulation,\nconvergence to a specified hold for the inverse problem has been\nachieved to within 0.1% in all cases of interest to date\n', ['stochastic recurrences', 'Jackpot Keno', 'mathematical model', 'simulation', 'Jupiters Network Gaming', 'house net hold', 'piecewise linear', 'ternary domain split', 'random number generator', 'inverse problem', 'stochastic binary search', 'probability', 'experimental results', 'legal requirement', 'Chinese lottery game', 'game theory', 'probability', 'random number generation', 'search problems', 'statistical analysis']), ('Parallel and distributed Haskells\nParallel and distributed languages specify computations on multiple processors\nand have a computation language to describe the algorithm, i.e. what to\ncompute, and a coordination language to describe how to organise the\ncomputations across the processors. Haskell has been used as the\ncomputation language for a wide variety of parallel and distributed\nlanguages, and this paper is a comprehensive survey of implemented\nlanguages. It outlines parallel and distributed language concepts and\nclassifies Haskell extensions using them. Similar example programs are\nused to illustrate and contrast the coordination languages, and the\ncomparison is facilitated by the common computation language. A lazy\nlanguage is not an obvious choice for parallel or distributed\ncomputation, and we address the question of why Haskell is a common\nfunctional computation language\n', ['distributed Haskell', 'parallel Haskell', 'distributed languages', 'parallel languages', 'multiple processors', 'coordination language', 'functional programming', 'lazy language', 'functional computation language', 'distributed programming', 'functional languages', 'functional programming', 'parallel languages', 'parallel programming']), ('Direct gear tooth contact analysis for hypoid bevel gears\nA new methodology for tooth contact analysis based on a very general\nmathematical model of the generating process is proposed. Considering\nthe line of action as a first order singularity of a certain operator\nequation we develop first and second order conditions for a pair of\ngenerated gear tooth flanks to be in contact. The constructive approach\nallows the direct computation of the paths of contact as the solution\nof a nonlinear equation system including the exact determination of the\nbounds of the paths of contact. The transmission error as well as\ncurvature properties in the contact points are obtained in a convenient\nway. The resulting contact ellipses approximate the bearing area.\nThrough the use of automatic differentiation all the geometric\nquantities are calculable within the machine accuracy of the computer\n', ['direct gear tooth contact analysis', 'hypoid bevel gears', 'mathematical model', 'generating process', 'first order singularity', 'operator equation', 'second order conditions', 'first order conditions', 'generated gear tooth flanks', 'contact paths', 'nonlinear equation system', 'exact bound determination', 'transmission error', 'curvature properties', 'contact ellipses', 'bearing area', 'automatic differentiation', 'geometric quantities', 'machine accuracy', 'computer', 'differentiation', 'machine bearings', 'mathematical operators', 'mechanical contact', 'mechanical engineering computing', 'nonlinear equations']), ('The ubiquitous provisioning of internet services to portable devices\nAdvances in mobile telecommunications and device miniaturization call for\nproviding both standard and novel location- and context-dependent\nInternet services to mobile clients. Mobile agents are dynamic,\nasynchronous, and autonomous, making the MA programming paradigm\nsuitable for developing novel middleware for mobility-enabled services\n', ['mobile telecommunications', 'device miniaturization', 'Internet services', 'mobile clients', 'mobile agents', 'mobility-enabled services', 'middleware', 'distributed object management', 'Internet', 'mobile computing', 'software agents']), ('Fitting mixed-effects models for repeated ordinal outcomes with the NLMIXED\nprocedure\nThis paper presents an analysis of repeated ordinal outcomes arising from two\npsychological studies. The first case is a repeated measures analysis\nof variance; the second is a mixed-effects regression. in a\nlongitudinal design. In both, the subject-specific variation is modeled\nby including random effects in the linear predictor (inside a link\nfunction) of a generalized linear model. The NLMIXED procedure in SAS\nis used to fit the mixed-effects models for the categorical response\ndata. The presentation emphasizes the parallel between the model.\nspecifications and the SAS statements. The purpose of this paper is to\nfacilitate the use of mixed-effects models in the analysis of repeated\nordinal outcomes\n', ['repeated ordinal outcomes', 'psychological studies', 'repeated measures analysis of variance', 'mixed-effects regression', 'longitudinal design', 'subject-specific variation modeling', 'random effects', 'linear predictor', 'generalized linear model', 'NLMIXED procedure', 'mixed-effects model fitting', 'categorical response data', 'model specifications', 'psychology', 'statistical analysis']), ('A comparative study of some generalized rough approximations\nIn this paper we focus upon a comparison of some generalized rough\napproximations of sets, where the classical indiscernibility relation\nis generalized to any binary reflexive relation. We aim at finding the\nbest of several candidates for generalized rough approximation\nmappings, where both definability of sets by elementary granules of\ninformation as well as the issue of distinction among positive,\nnegative, and border regions of a set are taken into account\n', ['generalized rough approximations', 'classical indiscernibility relation', 'binary reflexive relation', 'generalized rough approximation mappings', 'elementary granules', 'approximation theory', 'computability']), ("Stability analysis of the characteristic polynomials whose coefficients are\npolynomials of interval parameters using monotonicity\nWe analyze the stability of the characteristic polynomials whose coefficients\nare polynomials of interval parameters via monotonicity methods. Our\nstability conditions are based on Frazer-Duncan's theorem and all\nconditions can be checked using only endpoint values of interval\nparameters. These stability conditions are necessary and sufficient\nunder the monotonicity assumptions. When the monotonicity conditions do\nnot hold on the whole parameter region, we present an interval division\nmethod and a transformation algorithm in order to apply the\nmonotonicity conditions. Then, our stability analysis methods can be\napplied to all characteristic polynomials whose coefficients are\npolynomials of interval parameters\n", ['stability analysis', 'characteristic polynomials', 'interval parameters', 'monotonicity', 'Frazer-Duncan theorem', 'endpoint values', 'necessary and sufficient conditions', 'interval division method', 'transformation algorithm', 'polynomials', 'stability']), ('Inhibiting decoherence via ancilla processes\nGeneral conditions are derived for preventing the decoherence of a single\ntwo-state quantum system (qubit) in a thermal bath. The employed\nauxiliary systems required for this purpose are merely assumed to be\nweak for the general condition while various examples such as extra\nqubits and extra classical fields are studied for applications in\nquantum information processing. The general condition is confirmed by\nwell known approaches toward inhibiting decoherence. An approach to\ndecoherence-free quantum memories and quantum operations is presented\nby placing the qubit into the center of a sphere with extra qubits on\nits surface\n', ['decoherence inhibition', 'ancilla processes', 'decoherence', 'single two-state quantum system', 'qubit', 'thermal bath', 'auxiliary systems', 'extra qubits', 'extra classical fields', 'quantum information processing', 'general condition', 'decoherence-free quantum memories', 'quantum operations', 'sphere surface', 'computation theory', 'harmonic oscillators', 'information theory', 'quantum computing', 'quantum theory']), ('Four factors influencing the fair market value of out-of print books. 2\nFot pt.1 see ibid., p.71-8 (2002). Data from the fifty-six titles examined\nqualitatively in the Patterson study are examined quantitatively. In\naddition to the four factors of edition, condition, dust jacket, and\nautograph that were hypothesized to influence the value of a book, four\nother factors for which information was available in the data were\nexamined\n', ['out-of-print books', 'quantitative analysis', 'fair market value', 'pricing', 'economics', 'publisher', 'costing', 'publishing']), ("Meshed atlases for real-time procedural solid texturing\nWe describe an implementation of procedural solid texturing that uses the\ntexture atlas, a one-to-one mapping from an object's surface into its\ntexture space. The method uses the graphics hardware to rasterize the\nsolid texture coordinates as colors directly into the atlas. A\ntexturing procedure is applied per-pixel to the texture map, replacing\neach solid texture coordinate with its corresponding procedural solid\ntexture result. The procedural solid texture is then mapped back onto\nthe object surface using standard texture mapping. The implementation\nrenders procedural solid textures in real time, and the user can design\nthem interactively. The quality of this technique depends greatly on\nthe layout of the texture atlas. A broad survey of texture atlas\nschemes is used to develop a set of general purpose mesh atlases and\ntools for measuring their effectiveness at distributing as many\navailable texture samples as evenly across the surface as possible. The\nmain contribution of this paper is a new multiresolution texture atlas.\nIt distributes all available texture samples in a nearly uniform\ndistribution. This multiresolution texture atlas also supports\nMIP-mapped minification antialiasing and linear magnification filtering\n", ['real-time procedural solid texturing', 'meshed atlases', 'texture atlas', 'one-to-one mapping', 'object surface', 'texture space', 'graphics hardware', 'rasterization', 'solid texture coordinates', 'colors', 'rendering', 'multiresolution texture atlas', 'linear magnification filtering', 'MIP-mapped minification antialiasing', 'image texture', 'real-time systems', 'rendering (computer graphics)']), ('Building a better game through dynamic programming: a Flip analysis\nFlip is a solitaire board game produced by craft woodworkers. We analyze Flip\nand suggest modifications to the rules to make the game more\nmarketable. In addition to being an interesting application of dynamic\nprogramming, this case shows the use of operations research in\nmanagerial decision making\n', ['dynamic programming', 'Flip analysis', 'operations research', 'managerial decision making', 'solitaire board game', 'craft woodworkers', 'dynamic programming', 'games of skill']), ('H/sub 2/ optimization of the three-element type dynamic vibration absorbers\nThe dynamic vibration absorber (DVA) is a passive vibration control device\nwhich is attached to a vibrating body (called a primary system)\nsubjected to exciting force or motion. In this paper, we will discuss\nan optimization problem of the three-element type DVA on the basis of\nthe H/sub 2/ optimization criterion. The objective of the H/sub 2/\noptimization is to reduce the total vibration energy of the system for\noverall frequencies; the total area under the power spectrum response\ncurve is minimized in this criterion. If the system is subjected to\nrandom excitation instead of sinusoidal excitation, then the H/sub 2/\noptimization is probably more desirable than the popular H/sub infinity\n/ optimization. In the past decade there has been increasing interest\nin the three-element type DVA. However, most previous studies on this\ntype of DVA were based on the H/sub infinity / optimization design, and\nno one has been able to find the algebraic solution as of yet. We found\na closed-form exact solution for a special case where the primary\nsystem has no damping. Furthermore, the general case solution including\nthe damped primary system is presented in the form of a numerical\nsolution. The optimum parameters obtained here are compared to those of\nthe conventional Voigt type DVA. They are also compared to other\noptimum parameters based on the H/sub infinity / criterion\n', ['H/sub 2/ optimization', 'passive vibration control', 'power spectrum response', 'Voigt type dynamic vibration absorber', 'three-element type dynamic vibration absorbers', 'optimisation', 'vibration control']), ('Anticipating the further development of cadastral systems\nAlthough the paper recognises the merits of the evolution of cadastral systems\ntowards an increased capability over time, it promotes a radical\nintroduction or overhaul of existing cadastral systems. It encourages\nthe development of a capability to cope with some key drivers of major\nchange. These have been identified as globalisation, the advent of\nfully automated cadastral environments, improved decentralised methods\nof governance and greatly improved service delivery of future cadastral\nsystems to a wide range of users. The paper promotes the registration\nof title supported by government guarantee as an effective means for\nrapidly introducing cadastral systems to facilitate globally\ncompetitive land markets in developing countries. In developing\nautomated environments for cadastral systems, the need to completely\nre-engineer and redesign cadastral systems to meet basic cadastral\nprinciples and responsiveness to individual user needs is promoted. In\nthis environment, highly decentralised cadastral operations and\nadministration combined with light regulatory control are advocated as\na future governance strategy. With regard to the level of services to\nusers, an emphasis on recognising and serving the future needs of users\nis seen as essential. International and national professional and user\norganisations involved in land administration are seen as an important\nvehicle for developing strategies and providing evaluation to guide the\nover-arching development of cadastral systems around the world\n', ['cadastral systems development', 'globalisation', 'automated cadastral environments', 'decentralised governance', 'service delivery', 'government guarantee', 'globally competitive land markets', 'developing countries', 'automated environments', 'cadastral principles', 'user needs', 'highly decentralised cadastral operations', 'light regulatory control', 'future governance strategy', 'future user needs', 'professional organisations', 'user organisations', 'land administration', 'cartography', 'government data processing', 'management of change', 'real estate data processing', 'systems re-engineering', 'town and country planning']), ('Products and polymorphic subtypes\nThis paper is devoted to a comprehensive study of polymorphic subtypes with\nproducts. We first present a sound and complete Hilbert style\naxiomatization of the relation of being a subtype in presence of to , *\ntype constructors and the For all quantifier, and we show that such\naxiornatization is not encodable in the system with to , For all only.\nIn order to give a logical semantics to such a subtyping relation, we\npropose a new form of a sequent which plays a key role in a natural\ndeduction and a Gentzen style calculi. Interestingly enough, the\nsequent must have the form E implies T, where E is a non-commutative,\nnon-empty sequence of typing assumptions and T is a finite binary tree\nof typing judgements, each of them behaving like a pushdown store. We\nstudy basic metamathematical properties of the two logical systems,\nsuch as subject reduction and cut elimination. Some\ndecidability/undecidability issues related to the presented subtyping\nrelation are also explored: as expected, the subtyping over to , *, For\nall is undecidable, being already undecidable for the to , For all\nfragment (as proved in [15]), but for the *, For all fragment it turns\nout to be decidable\n', ['polymorphic subtypes', 'products subtypes', 'Hilbert style axiomatization', 'logical semantics', 'Gentzen style calculi', 'finite binary tree', 'pushdown store', 'metamathernatical properties', 'decidability', 'decidability', 'process algebra', 'semantic networks']), ("Nissan v. Nissan [trademark dispute]\nIs a trademark dispute a case of David v. Goliath or a corporation fending off\na greedy opportunist? This paper discusses the case of Uzi Nissan, who\nis locked in a multimillion-dollar legal battle over whether or not his\nuse of the nissan.com Internet domain name infringes upon Japan's\nNissan Motor Co.'s trademark. At the heart of the matter is the impact\nof the global Internet on trademark law, which traditionally has been\nstrongly influenced by geographic considerations. The paper discusses\nthe background to the case from both sides and the issues involved\n", ['trademark dispute', 'Uzi Nissan', 'nissan.com Internet domain name', 'Nissan Motor Company trademark', 'global Internet', 'trademark law', 'automobile industry', 'industrial property', 'Internet', 'legislation']), ("Dot-Net makes slow progress\nMicrosoft's Windows .Net Enterprise Server Release Candidate I, which was\nreleased at the end of last month, provides an early glimpse of the\nsystem that will eventually replace Windows 200 Advanced Server. The\nsoftware has been improved so that Active Directory is more flexible\nand easier to deploy; and security, scalability and management have\nalso been enhanced\n", ['Windows .Net Enterprise Server', 'security', 'Active Directory', 'scalability', 'computer network management', 'operating systems (computers)']), ("Ethernet networks: getting down to business\nWhile it seems pretty clear that Ethernet has won the battle for the mindshare\nas the network of choice for the factory floor, there's still a war to\nbe won in implementation as cutting-edge manufacturers begin to adopt\nthe technology on a widespread basis\n", ['Ethernet', 'factory floor', 'cutting-edge manufacturers', 'supervisory level', 'factory automation', 'local area networks', 'manufacturing industries']), ('Digital stochastic realization of complex analog controllers\nStochastic logic is based on digital processing of a random pulse stream, where\nthe information is codified as the probability of a high level in a\nfinite sequence. This binary pulse sequence can be digitally processed\nexploiting the similarity between Boolean algebra and statistical\nalgebra. Given a random pulse sequence, any Boolean operation among\nindividual pulses will correspond to an algebraic expression among the\nvariables represented by their respective average pulse rates.\nSubsequently, this pulse stream can be digitally processed to perform\nanalog operations. In this paper, we propose a stochastic approach to\nthe digital implementation of complex controllers using programmable\ndevices as an alternative to traditional digital signal processors. As\nan example, a practical realization of nonlinear dissipative\ncontrollers for a series resonant converter is presented\n', ['digital stochastic realization', 'complex analog controllers', 'stochastic logic', 'random pulse stream', 'finite sequence', 'binary pulse sequence', 'Boolean algebra', 'statistical algebra', 'random pulse sequence', 'Boolean operation', 'average pulse rates', 'pulse stream', 'stochastic approach', 'programmable devices', 'nonlinear dissipative controllers', 'series resonant converter', 'parallel resonant DC-to-DC converters', 'series resonant DC-to-DC converters', 'binary sequences', 'Boolean algebra', 'DC-DC power convertors', 'nonlinear control systems', 'programmable logic arrays', 'resonant power convertors', 'stochastic processes']), ('Keen but confused [workflow & content management]\nIT users find workflow, content and business process management software\nappealing but by no means straightforward to implement. Pat Sweet\nreports on our latest research\n', ['workflow', 'content management', 'business process management software', 'research', 'survey', 'market overview', 'document handling', 'information resources', 'workflow management software']), ("A work journal [librarianship]\nKeeping a work journal can be useful in exploring one's thoughts and feelings\nabout work challenges and work decisions. It can help bring about\ngreater fulfillment in one's work life by facilitating self-renewal,\nchange, the search for new meaning, and job satisfaction. One example\nof a work journal which I kept in 1998 is considered. It touches on\nseveral issues of potential interest to midlife career librarians\nincluding the challenge of technology, returning to work at midlife\nafter raising a family, further education, professional writing, and\njob exchange\n", ['work decisions', 'work challenges', 'job satisfaction', 'self-renewal', 'work journal', 'change', 'midlife career librarians', 'technology', 'further education', 'professional writing', 'job exchange', 'information science', 'library automation', 'management of change', 'personnel', 'technology transfer']), ('Convergence of a finite volume scheme for nonlinear degenerate parabolic\nequations\nOne approximates the entropy weak solution u of a nonlinear parabolic\ndegenerate equation u/sub t/+div(qf(u))- Delta phi (u)=0 by a piecewise\nconstant function u/sub D/ using a discretization D in space and time\nand a finite volume scheme. The convergence of u/sub D/ to u is shown\nas the size of the space and time steps tend to zero. In a first step,\nestimates on u/sub D/ are used to prove the convergence, up to a\nsubsequence, of u/sub D/ to a measure valued entropy solution (called\nhere an entropy process solution). A result of uniqueness of the\nentropy process solution is proved, yielding the strong convergence of\nu/sub D/ to u. Some numerical results on a model equation are shown\n', ['finite volume scheme', 'nonlinear degenerate parabolic equations', 'entropy weak solution', 'piecewise constant function', 'strong convergence', 'numerical results', 'model equation', 'finite volume methods', 'nonlinear equations', 'parabolic equations']), ('Extinction cross sections of realistic raindrops: data-bank established using\nT-matrix method and nonlinear fitting technique\nA new computer program is developed based on the T-matrix method to generate a\nlarge number of total (extinction) cross sections (TCS) values of the\nrealistic raindrops that are deformed due to a balance of the forces\nthat act on a drop failing under gravity, and were described in shape\nby Pruppacher and Pitter (1971). These data for various dimensions of\nthe raindrops (mean effective radius from 0 to 3.25 mm), frequencies\n(10 to 80 GHz), (horizontal and vertical) polarizations, and\ntemperatures (0, 10 and 20 degrees C) are stored to establish a data\nbank. Furthermore, a curve fitting technique, i.e., interpolation of\norder 3, is implemented for the TCS values in the data bank. Therefore,\nthe interpolated TCS results can be obtained readily from the\ninterpolation process with negligible or even null computational time\nand efforts. Error analysis is carried out to show the high accuracy of\nthe present analysis and applicability of the interpolation. At three\noperating frequencies of 15, 21.225, and 38 GHz locally used in\nSingapore, some new TCS values are obtained from the new fast and\nefficient interpolation with a good accuracy\n', ['extinction cross sections', 'realistic raindrops', 'data-bank', 'T-matrix method', 'total cross sections', 'temperature', 'error analysis', 'mean effective radius', 'gravity', 'horizontal polarization', 'vertical polarization', 'interpolation', 'nonlinear curve fitting technique', 'operating frequencies', 'Singapore', 'SHF', 'EHF', 'electromagnetic wave scattering', 'EM wave scattering', 'computer program', '15 GHz', '21.225 GHz', '38 GHz', '10 to 80 GHz', '0 to 3.25 mm', '0 C', '10 C', '20 C', 'curve fitting', 'electromagnetic wave polarisation', 'electromagnetic wave scattering', 'error analysis', 'interpolation', 'matrix algebra', 'microwave propagation', 'millimetre wave propagation', 'rain', 'telecommunication computing', 'tropospheric electromagnetic wave propagation']), ('Simulating fermions on a quantum computer\nThe real-time probabilistic simulation of quantum systems in classical\ncomputers is known to be limited by the so-called dynamical sign\nproblem, a problem leading to exponential complexity. In 1981 Richard\nFeynman raised some provocative questions in connection to the "exact\nimitation" of such systems using a special device named a "quantum\ncomputer". Feynman hesitated about the possibility of imitating fermion\nsystems using such a device. Here we address some of his concerns and,\nin particular, investigate the simulation of fermionic systems. We show\nhow quantum computers avoid the sign problem in some cases by reducing\nthe complexity from exponential to polynomial. Our demonstration is\nbased upon the use of isomorphisms of algebras. We present specific\nquantum algorithms that illustrate the main points of our algebraic\napproach\n', ['quantum computer', 'fermions simulation', 'real-time probabilistic simulation', 'classical computers', 'dynamical sign problem', 'exponential complexity', 'fermion systems', 'sign problem', 'isomorphisms', 'algebras', 'algebra', 'computational complexity', 'fermion systems', 'probability', 'quantum computing']), ('Location of transport nets on a heterogeneous territory\nThe location of transport routes on a heterogeneous territory is studied. The\nnetwork joins a given set of terminal points and a certain number of\nadditional (branch) points. The problem is formulated, properties of\nthe optimal solution for a. tree-like network, and the number of branch\npoints are studied. A stepwise optimization algorithm for a. network\nwith given adjacency matrix based on an algorithm for constructing\nminimal-cost routes is designed\n', ['transport nets', 'heterogeneous territory', 'transport routes', 'terminal points', 'branch points', 'tree-like network', 'stepwise optimization algorithm', 'adjacency matrix', 'matrix algebra', 'optimisation', 'transportation', 'trees (mathematics)']), ('A numerical C/sup 1/-shadowing result for retarded functional differential\nequations\nThis paper gives a numerical C/sup 1/-shadowing between the exact solutions of\na functional differential equation and its numerical approximations.\nThe shadowing result is obtained by comparing exact solutions with\nnumerical approximation which do not share the same initial value.\nBehavior of stable manifolds of functional differential equations under\nnumerics will follow from the shadowing result\n', ['numerical C/sup 1/-shadowing', 'exact solutions', 'numerical approximations', 'stable manifolds', 'retarded functional differential equations', 'approximation theory', 'differential equations', 'functional equations']), ('An integrated optimization model for train crew management\nTrain crew management involves the development of a duty timetable for each of\nthe drivers (crew) to cover a given train timetable in a rail transport\norganization. This duty timetable is spread over a certain period,\nknown as the roster planning horizon. Train crew management may arise\neither from the planning stage, when the total number of crew and crew\ndistributions are to be determined, or from the operating stage when\nthe number of crew at each depot is known as input data. In this paper,\nwe are interested in train crew management in the planning stage. In\nthe literature, train crew management is decomposed into two stages:\ncrew scheduling and crew rostering which are solved sequentially. We\npropose an integrated optimization model to solve both crew scheduling\nand crew rostering. The model enables us to generate either cyclic\nrosters or non-cyclic rosters. Numerical experiments are carried out\nover data sets arising from a practical application\n', ['integrated optimization model', 'train crew management', 'duty timetable', 'rail transport organization', 'roster planning horizon', 'crew scheduling', 'crew rostering', 'cyclic rosters', 'noncyclic rosters', 'integer programming', 'human resource management', 'integer programming', 'operations research', 'railways', 'scheduling']), ('Asymptotic expansions for the zeros of certain special functions\nWe derive asymptotic expansions for the zeros of the cosine-integral Ci(x) and\nthe Struve function H/sub 0/(x), and extend the available formulae for\nthe zeros of Kelvin functions. Numerical evidence is provided to\nillustrate the accuracy of the expansions\n', ['asymptotic expansions', 'zeros', 'cosine-integral', 'Struve function', 'Kelvin functions', 'accuracy', 'function approximation', 'poles and zeros']), ('Telemedicine in the management of a cervical dislocation by a mobile\nneurosurgeon\nNeurosurgical teams, who are normally located in specialist centres, frequently\nuse teleradiology to make a decision about the transfer of a patient to\nthe nearest neurosurgical department. This decision depends on the type\nof pathology, the clinical status of the patient and the prognosis. If\nthe transfer of the patient is not possible, for example because of an\nunstable clinical status, a mobile neurosurgical team may be used. We\nreport a case which was dealt with in a remote French military airborne\nsurgical unit, in the Republic of Chad. The unit, which provides\nhealth-care to the French military personnel stationed there, also\nprovides free medical care for the local population. It conducts about\n100 operations each month. The unit comprises two surgeons (an\northopaedic and a general surgeon), one anaesthetist, two anaesthetic\nnurses, one operating room nurse, two nurses, three paramedics and a\nsecretary. The civilian patient presented with unstable cervical\ntrauma. A mobile neurosurgeon operated on her, and used telemedicine\nbefore, during and after surgery\n', ['cervical dislocation management', 'mobile neurosurgeon', 'teleradiology', 'telemedicine', 'remote French military airborne surgical unit', 'Republic of Chad', 'health care', 'French military personnel', 'civilian patient', 'unstable cervical trauma', 'surgery', 'bone', 'diagnostic radiography', 'neurophysiology', 'orthopaedics', 'radiology', 'surgery', 'telemedicine']), ('Quantum retrodiction in open systems\nQuantum retrodiction involves finding the probabilities for various preparation\nevents given a measurement event. This theory has been studied for some\ntime but mainly as an interesting concept associated with time\nasymmetry in quantum mechanics. Recent interest in quantum\ncommunications and cryptography, however, has provided retrodiction\nwith a potential practical application. For this purpose quantum\nretrodiction in open systems should be more relevant than in closed\nsystems isolated from the environment. In this paper we study\nretrodiction in open systems and develop a general master equation for\nthe backward time evolution of the measured state, which can be used\nfor calculating preparation probabilities. We solve the master\nequation, by way of example, for the driven two-level atom coupled to\nthe electromagnetic field\n', ['quantum retrodiction', 'probabilities', 'preparation events', 'measurement event', 'time asymmetry', 'quantum mechanics', 'quantum communications', 'cryptography', 'retrodictive master equation', 'backward time evolution', 'preparation probabilities', 'driven two level atom-electromagnetic field coupling', 'open systems', 'atom-photon collisions', 'electromagnetic fields', 'master equation', 'open systems', 'probability', 'quantum communication', 'quantum theory']), ("Midlife career choices: how are they different from other career choices?\nIt was 1963 when Candy Start began working in libraries. Libraries seemed to be\na refuge from change, a dependable environment devoted primarily to\npreservation. She was mistaken. Technological changes in every decade\nof her experience have affected how and where she used her MLS. Far\nfrom a static refuge, libraries have proven to be spaceships loaded\nwith precious cargo hurtling into the unknown. The historian in the\nauthor says that perhaps libraries have always been like this. This\npaper looks at a midlife decision point and the choice that this\nlibrarian made to move from a point of lessening productivity and\ninterest to one of increasing challenge and contribution. It is a\npersonal narrative of midlife experience from one librarian's point of\nview. Since writing this article, Candy's career has followed more\nchanges. After selling the WINGS TM system, she has taken her\nexperiences and vision to another library vendor, Gaylord Information\nSystems, where she serves as a senior product strategist\n", ['midlife career choices', 'libraries', 'technological changes', 'productivity', 'employment', 'library automation', 'personnel']), ('Performance comparison between PID and dead-time compensating controllers\nThis paper is intended to answer the question: "When can a simple dead-time\ncompensator be expected to perform better than a PID?". The performance\ncriterion used is the integrated absolute error (IAE). It is compared\nfor PI and PID controllers and a simple dead-time compensator (DTC)\nwhen a step load disturbance is applied at the plant input. Both stable\nand integrating processes are considered. For a fair comparison the\ncontrollers should provide equal robustness in some sense. Here, as a\nmeasure of robustness, the H/sub infinity / norm of the sum of the\nabsolute values of the sensitivity function and the complementary\nsensitivity function is used. Performance of the DTC\'s is given also as\na function of dead-time margin (D/sub M/)\n', ['performance comparison', 'PID controllers', 'dead-time compensating controllers', 'performance criterion', 'integrated absolute error', 'IAE', 'PI controllers', 'dead-time compensator', 'DTC', 'step load disturbance', 'stable processes', 'integrating processes', 'equal robustness', 'complementary sensitivity function', 'dead-time margin', 'absolute value sum H/sub infinity / norm', 'compensation', 'H/sup infinity / control', 'performance index', 'robust control', 'sensitivity', 'three-term control']), ('On conflict-free executions of elementary nets\nDeals with analysis of elementary Petri nets with respect to possibilities of\navoiding conflicts during their executions. There are two main aims of\nthe paper. The first is to find a method of checking if a net is\nconflict-avoidable (i.e., if it possesses a conflict-free fair run).\nThe second is to find a method of rebuilding any net to a totally\nconflict-avoidable net (i.e., a net possessing a conflict-free fair run\nin every one process) with the same behaviour. The main results are the\nfollowing: 1. The proof of decidability, for elementary nets, of the\nproblem of existence of a conflict-avoidable fair process (and an\nalgorithm producing all fair runs). 2. Construction, for an arbitrary\ngiven elementary net, of a totally conflict-avoidable net with the same\nbehaviour. The net, completed this way, has the same behaviour as the\noriginal one. Moreover, it is totally conflict-avoidable, and its\nexecution may be supervised (in order to ensure conflict-freeness) by\nthe reduced case graph built by the algorithm of the former section\n', ['conflict-free executions', 'elementary Petri nets', 'conflict-free fair run', 'totally conflict-avoidable net', 'decidability', 'reduced case graph', 'concurrency theory', 'decidability', 'formal languages', 'grammars', 'Petri nets', 'set theory']), ("Designing a screening experiment for highly reliable products\nWithin a reasonable life-testing time, how to improve the reliability of highly\nreliable products is one of the great challenges. By using a resolution\nIII experiment together with degradation test, Tseng et al. (1995)\npresented a case study of improving the reliability of fluorescent\nlamps. However, in conducting such an experiment, they did not address\nthe problem of how to choose the optimal settings of variables, such as\nsample size, inspection frequency, and termination time for each run,\nwhich are influential to the correct identification of significant\nfactors and the experimental cost. Assuming that the product's\ndegradation paths satisfy Wiener processes, this paper proposes a\nsystematic approach to the aforementioned problem. First, an\nidentification rule is proposed. Next, under the constraints of a\nminimum probability of correct decision and a maximum probability of\nincorrect decision of the proposed identification rule, the optimum\ntest plan can be obtained by minimizing the total experimental cost. An\nexample is provided to illustrate the proposed method\n", ['screening experiment', 'highly reliable products', 'resolution III design', 'degradation tests', 'Wiener process', 'inspection frequency', 'termination time', 'optimal test plan', 'fluorescent lamps', 'minimum probability of correct decision', 'maximum probability of incorrect decision', 'identification rule', 'design of experiments', 'inspection', 'optimisation', 'probability', 'reliability', 'stochastic processes']), ('Experimental design methodology and data analysis technique applied to optimise\nan organic synthesis\nThe study was aimed at maximising the yield of a Michaelis-Becker dibromoalkane\nmonophosphorylation reaction. In order to save time and money, we first\napplied a full factorial experimental design to search for the optimum\nconditions while performing a small number of experiments. We then used\nthe principal component analysis (PCA) technique to evidence two\nuncontrolled factors. Lastly, a special experimental design that took\ninto account all the influential factors allowed us to determine the\nmaximum-yield experimental conditions. This study also evidenced the\ncomplementary nature of experimental design methodology and data\nanalysis techniques\n', ['Michaelis-Becker dibromoalkane monophosphorylation reaction', 'full factorial experimental design', 'optimum conditions', 'data analysis technique', 'organic synthesis', 'principal component analysis', 'uncontrolled factors', 'maximum-yield experimental conditions', 'chemical reactions', 'chemistry computing', 'data analysis', 'design of experiments', 'organic compounds', 'principal component analysis']), ('Efficient computation of local geometric moments\nLocal moments have attracted attention as local features in applications such\nas edge detection and texture segmentation. The main reason for this is\nthat they are inherently integral-based features, so that their use\nreduces the effect of uncorrelated noise. The computation of local\nmoments, when viewed as a neighborhood operation, can be interpreted as\na convolution of the image with a set of masks. Nevertheless, moments\ncomputed inside overlapping windows are not independent and convolution\ndoes not take this fact into account. By introducing a matrix\nformulation and the concept of accumulation moments, this paper\npresents an algorithm which is computationally much more efficient than\nconvolving and yet as simple\n', ['local geometric moments computation', 'local features', 'edge detection', 'texture segmentation', 'integral-based features', 'neighborhood operation', 'image convolution', 'overlapping windows', 'matrix formulation', 'accumulation moments', 'computationally efficient algorithm', 'image analysis', 'convolution', 'edge detection', 'image processing', 'image segmentation', 'image texture', 'matrix algebra']), ('Prediction of tool and chip temperature in continuous and interrupted machining\nA numerical model based on the finite difference method is presented to predict\ntool and chip temperature fields in continuous machining and time\nvarying milling processes. Continuous or steady state machining\noperations like orthogonal cutting are studied by modeling the heat\ntransfer between the tool and chip at the tool-rake face contact zone.\nThe shear energy created in the primary zone, the friction energy\nproduced at the rake face-chip contact zone and the heat balance\nbetween the moving chip and stationary tool are considered. The\ntemperature distribution is solved using the finite difference method.\nLater, the model is extended to milling where the cutting is\ninterrupted and the chip thickness varies with time. The proposed model\ncombines the steady-state temperature prediction in continuous\nmachining with transient temperature evaluation in interrupted cutting\noperations where the chip and the process change in a discontinuous\nmanner. The mathematical models and simulation results are in\nsatisfactory agreement with experimental temperature measurements\nreported in the literature\n', ['tool temperature prediction', 'chip temperature prediction', 'continuous machining', 'interrupted machining', 'numerical model', 'finite difference method', 'time varying milling processes', 'orthogonal cutting', 'heat transfer', 'tool-rake face contact zone', 'shear energy', 'primary zone', 'friction energy', 'temperature distribution', 'first-order dynamic system', 'thermal properties', 'cutting', 'finite difference methods', 'heat transfer', 'machine tools', 'machining', 'mechanical engineering computing', 'wear']), ('Fractional motion control: application to an XY cutting table\nIn path tracking design, the dynamic of actuators must be taken into account in\norder to reduce overshoots appearing for small displacements. A new\napproach to path tracking using fractional differentiation is proposed\nwith its application on a XY cutting table. It permits the generation\nof optimal movement reference-input leading to a minimum path\ncompletion time, taking into account both maximum velocity,\nacceleration and torque and the bandwidth of the closed-loop system.\nFractional differentiation is used here through a Davidson-Cole filter.\nA methodology aiming at improving the accuracy especially on\ncheckpoints is presented. The reference-input obtained is compared with\nspline function. Both are applied to an XY cutting table model and\nactuator outputs compared\n', ['fractional motion control', 'XY cutting table', 'path tracking design', 'actuators', 'fractional differentiation', 'minimum path completion time', 'closed-loop system', 'Davidson-Cole filter', 'spline function', 'optimization', 'calculus', 'cutting', 'motion control', 'optimisation', 'polynomial approximation', 'position control', 'splines (mathematics)', 'tracking filters']), ("Integrated process control using an in situ sensor for etch\nThe migration to tighter geometries and more complex process sequence\nintegration schemes requires having the ability to compensate for\nupstream deviations from target specifications. Doing so ensures\nthat-downstream process sequences operate on work-in-progress that is\nwell within control. Because point-of-use visibility of\nwork-in-progress quality has become of paramount concern in the\nindustry's drive to reduce scrap and improve yield, controlling trench\ndepth has assumed greater importance. An integrated, interferometric\nbased, rate monitor for etch-to-depth and spacer etch applications has\nbeen developed for controlling this parameter. This article\ndemonstrates that the integrated rate monitor, using polarization and\ndigital signal processing, enhances control etch-to-depth processes and\ncan also be implemented as a predictive endpoint in a wafer\nmanufacturing environment for dual damascene trench etch and spacer\netch applications\n", ['interferometric in situ etch sensor', 'integrated process control', 'polarization', 'digital signal processing', 'wafer manufacturing environment', 'process predictive endpoint', 'dual damascene trench etch', 'spacer etch applications', 'IC geometry', 'complex process sequence integration schemes', 'upstream deviation compensation', 'target specifications', 'downstream process sequences', 'point-of-use visibility', 'work-in-progress quality', 'scrap reduction', 'yield improvement', 'trench depth control', 'interferometry', 'integrated etch rate monitor', 'etching', 'inspection', 'integrated circuit interconnections', 'integrated circuit measurement', 'integrated circuit metallisation', 'integrated circuit yield', 'light interferometry', 'light polarisation', 'optical information processing', 'process control', 'process monitoring', 'quality control']), ('Electronic signatures - much ado?\nWhilst the market may be having a crisis of confidence regarding the prospects\nfor e-commerce, the EU and the Government continue apace to develop the\nlegal framework. Most recently, this has resulted in the Electronic\nSignatures Regulations 2002. These Regulations were made on 13 February\n2002 and came into force on 8 March 2002. The Regulations implement the\nEuropean Electronic Signatures Directive (1999/93/EC). Critics may say\nthat the Regulations were implemented too late (they were due to have\nbeen implemented by 19 July 2001), with too short a consultation period\n(25 January 2002 to 12 February 2002) and with an unconvincing case as\nto what they add to English law (as to which, read on). The author\nexplains the latest development on e-signatures and the significance of\nCertification Service Providers (CSPs)\n', ['e-commerce', 'legal framework', 'Electronic Signatures Regulations 2002', 'European Electronic Signatures Directive', 'electronic commerce', 'legislation', 'standards']), ('Stock market dynamics\nWe elucidate on several empirical statistical observations of stock market\nreturns. Moreover, we find that these properties are recurrent and are\nalso present in invariant measures of low-dimensional dynamical\nsystems. Thus, we propose that the returns are modeled by the first\nPoincare return time of a low-dimensional chaotic trajectory. This\nmodeling, which captures the recurrent properties of the return\nfluctuations, is able to predict well the evolution of the observed\nstatistical quantities. In addition, it explains the reason for which\nstocks present simultaneously dynamical properties and high\nuncertainties. In our analysis, we use data from the S&P 500 index and\nthe Brazilian stock Telebras\n', ['stock market returns', 'empirical statistical observations', 'invariant measures', 'low-dimensional dynamical systems', 'first Poincare return time', 'low-dimensional chaotic trajectory', 'statistical quantities', 'Brazilian stock', 'econophysics', 'chaos', 'economic cybernetics', 'stock markets']), ("CherylAnn Silberer: all about process [accounting technologist]\nSilberer's company, CompLete, is making a specialty of workflow process\nanalysis\n", ['CompLete', 'workflow process analysis', 'accounting technologist', 'accounting', 'workflow management software']), ('Optimal estimation of a finite sample of a discrete chaotic process\nThe synthesis of optimal algorithms for estimating discrete chaotic processes\nspecified by a finite sample is considered; various possible approaches\nare discussed. Expressions determining the potential accuracy in\nestimating a single value of the chaotic process are derived. An\nexample of the application of the general equations obtained is given\n', ['optimal estimation', 'finite sample', 'discrete chaotic process', 'optimal algorithm synthesis', 'space-time filtering', 'chaos', 'filtering theory', 'optimisation', 'signal sampling']), ('Summarization beyond sentence extraction: A probabilistic approach to sentence\ncompression\nWhen humans produce summaries of documents, they do not simply extract\nsentences and concatenate them. Rather, they create new sentences that\nare grammatical, that cohere with one another, and that capture the\nmost salient pieces of information in the original document. Given that\nlarge collections of text/abstract pairs are available online, it is\nnow possible to envision algorithms that are trained to mimic this\nprocess. In this paper, we focus on sentence compression, a simpler\nversion of this larger challenge. We aim to achieve two goals\nsimultaneously: our compressions should be grammatical, and they should\nretain the most important pieces of information. These two goals can\nconflict. We devise both a noisy-channel and a decision-tree approach\nto the problem, and we evaluate results against manual compressions and\na simple baseline\n', ['sentence compression', 'grammatical', 'noisy-channel', 'decision-tree', 'document summarization', 'abstracting', 'computational linguistics', 'document handling']), ("On emotion and bounded rationality: reply to Hanoch\nThe author refers to the comment made by Hanoch (see ibid. vol.49 (2000)) on\nhis model of bounded rationality and the role of the Yerkes-Dodson law\nand emotional arousal in it. The author points out that Hanoch's\ncomment, however, conspicuously fails to challenge - much less\ncontradict - the central hypothesis of his paper. In addition, several\nof Hanoch's criticisms are based on a wrong characterization of the\npositions\n", ['emotion', 'bounded rationality', 'Yerkes-Dodson law', 'decision-making', 'psychology', 'psychology']), ('Temp IT chief rallies troops [Mori]\nThe appointment of a highly qualified interim IT manager enabled market\nresearch company Mori to rapidly restructure its IT department. Now the\nresulting improvements are allowing it to support an increasing role\nfor technology in the assimilation and analysis of market research\n', ['market research company', 'Mori', 'interim IT manager', 'DP management', 'marketing']), ('Combining constraint programming and linear programming on an example of bus\ndriver scheduling\nProvides details of a successful application where the column generation\nalgorithm was used to combine constraint programming and linear\nprogramming. In the past, constraint programming and linear programming\nwere considered to be two competing technologies that solved similar\ntypes of problems. Both these technologies had their strengths and\nweaknesses. The paper shows that the two technologies can be combined\ntogether to extract the strengths of both these technologies. Details\nof a real-world application to optimize bus driver duties are given.\nThis system was developed by ILOG for a major software house in Japan\nusing ILOG-Solver and ILOG-CPLEX, constraint programming and linear\nprogramming C/C++ libraries\n', ['constraint programming', 'linear programming', 'bus driver scheduling', 'column generation algorithm', 'ILOG', 'ILOG-Solver', 'ILOG-CPLEX', 'C/C++ libraries', 'constraint handling', 'constraint theory', 'human resource management', 'integer programming', 'linear programming', 'matrix algebra', 'scheduling', 'transportation']), ('Multiple model adaptive estimation with filter spawning\nMultiple model adaptive estimation (MMAE) with filter spawning is used to\ndetect and estimate partial actuator failures on the VISTA F-16. The\ntruth model is a full six-degree-of-freedom simulation provided by\nCalspan and General Dynamics. The design models are chosen as 13-state\nlinearized models, including first order actuator models. Actuator\nfailures are incorporated into the truth model and design model\nassuming a "failure to free stream." Filter spawning is used to include\nadditional filters with partial actuator failure hypotheses into the\nMMAE bank. The spawned filters are based on varying degrees of partial\nfailures (in terms of effectiveness) associated with the\ncomplete-actuaton-failure hypothesis with the highest conditional\nprobability of correctness at the current time. Thus, a blended\nestimate of the failure effectiveness is found using the filters\'\nestimates based upon a no-failure hypothesis, a complete actuator\nfailure hypothesis, and the spawned filters\' partial-failure\nhypotheses. This yields substantial precision in effectiveness\nestimation, compared with what is possible without spawning additional\nfilters, making partial failure adaptation a viable methodology\n', ['multiple model adaptive estimation', 'filter spawning', 'partial actuator failures', 'VISTA F-16', 'truth model', 'six-degree-of-freedom simulation', 'Calspan', 'in-flight simulator', 'test aircraft', 'flight control systems', 'General Dynamics', 'linearized models', 'MMAE', 'partial failures', 'conditional probability', 'no-failure hypothesis', 'actuators', 'adaptive estimation', 'aerospace simulation', 'aircraft control', 'military aircraft', 'probability']), ('Mathematical fundamentals of constructing fuzzy Bayesian inference techniques\nProblems and an associated technique for developing a Bayesian approach to\ndecision-making in the case of fuzzy data are presented. The concept of\nfuzzy and pseudofuzzy quantities is introduced and main operations with\npseudofuzzy quantities are considered. The basic relationships and the\nprincipal concepts of the Bayesian decision procedure based on the\nmodus-ponens rule are proposed. Some problems concerned with the\npractical realization of the fuzzy Bayesian method are considered\n', ['mathematical fundamentals', 'fuzzy Bayesian inference techniques', 'decision making', 'pseudofuzzy quantities', 'modus-ponens rule', 'Bayes methods', 'decision theory', 'fuzzy logic', 'inference mechanisms']), ('Feldkamp-type image reconstruction from equiangular data\nThe cone-beam approach for image reconstruction attracts increasing attention\nin various applications, especially medical imaging. Previously, the\ntraditional practical cone-beam reconstruction method, the Feldkamp\nalgorithm, was generalized into the case of spiral/helical scanning\nloci with equispatial cone-beam projection data. In this paper, we\nformulated the generalized Feldkamp algorithm in the case of\nequiangular cone-beam projection data, and performed numerical\nsimulation to evaluate the image quality. Because medical\nmulti-slice/cone-beam CT scanners typically use equiangular projection\ndata, our new formula may be useful in this area as a framework for\nfurther refinement and a benchmark for comparison\n', ['Feldkamp-type image reconstruction', 'equiangular data', 'cone-beam approach', 'medical imaging', 'practical cone-beam reconstruction method', 'spiral/helical scanning loci', 'equispatial cone-beam projection data', 'generalized Feldkamp algorithm', 'equiangular cone-beam projection data', 'numerical simulation', 'image quality', 'medical multi-slice/cone-beam CT scanners', 'computerised tomography', 'image reconstruction', 'medical image processing']), ("Evolution of the high-end computing market in the USA\nThis paper focuses on the technological change in the high-end computing\nmarket. The discussion combines historical analysis with strategic\nanalysis to provide a framework to analyse a key component of the\ncomputer industry. This analysis begins from the perspective of\ngovernment research and development spending; then examines the\nconfusion around the evolution of the high-end computing market in the\ncontext of standard theories of technology strategy and new product\ninnovation. Rather than the high-end market being 'dead', one should\nview the market as changing due to increased capability and competition\nfrom the low-end personal computer market. The high-end market is also\nresponding to new product innovation from the introduction of new\nparallel computing architectures. In the conclusion, key leverage\npoints in the market are identified and the trends in high-end\ncomputing are highlighted with implications\n", ['high-end computing market evolution', 'USA', 'historical analysis', 'strategic analysis', 'computer industry', 'government research', 'development spending', 'technology strategy', 'new product innovation', 'competition', 'low-end personal computer market', 'parallel computing architectures', 'supercomputing', 'DP industry', 'parallel processing']), ('The role and future of subject classification: the exploitation of resources\nIt is imperative that the library information systems (LIS) profession and LIS\neducators appreciate fully the contribution that classification makes\nto the discipline and that it is no longer seen as the domain of the\nacademic, isolated theorist, but becomes an integral part of our\nunderstanding of the contribution that the LIS community can make to\nsociety as a whole - as well as to particular areas such as legal\ninformation\n', ['subject classification', 'library information systems', 'LIS', 'information resources', 'legal information', 'classification', 'information resources', 'law administration', 'library automation']), ('A new high resolution color flow system using an eigendecomposition-based\nadaptive filter for clutter rejection\nWe present a new signal processing strategy for high frequency color flow\nmapping in moving tissue environments. A new application of an\neigendecomposition-based clutter rejection filter is presented with\nmodifications to deal with high blood-to-clutter ratios (BCR).\nAdditionally, a new method for correcting blood velocity estimates with\nan estimated tissue motion profile is detailed. The performance of the\nclutter filter and velocity estimation strategies is quantified using a\nnew swept-scan signal model. In vivo color flow images are presented to\nillustrate the potential of the system for mapping blood flow in the\nmicrocirculation with external tissue motion\n', ['high resolution colour flow system', 'eigendecomposition-based adaptive filter', 'clutter rejection filter', 'signal processing strategy', 'high frequency color flow mapping', 'HF colour flow mapping', 'moving tissue environments', 'high blood-to-clutter ratios', 'blood velocity estimates correction', 'estimated tissue motion profile', 'swept-scan signal model', 'in vivo color flow images', 'blood flow mapping', 'microcirculation', 'echoes', 'clutter suppression performance', 'adaptive filters', 'biomedical ultrasonics', 'blood flow measurement', 'clutter', 'colour', 'eigenvalues and eigenfunctions', 'image resolution', 'medical image processing', 'ultrasonic imaging']), ('Lifting factorization of discrete W transform\nA general method is proposed to factor the type-IV discrete W transform\n(DWT-IV) into lifting steps and additions. Then, based on the\nrelationships among various types of DWTs, four types of DWTs are\nfactored into lifting steps and additions. After approximating the\nlifting matrices, we get four types of new integer DWTs (IntDWT-I,\nIntDWT-II, IntDWT-III, and IntDWT-IV) which are floating-point\nmultiplication free. Integer-to-integer transforms (II-DWT), which\napproximate to DWT, are also proposed. Fast algorithms are given for\nthe new transforms and their computational complexities are analyzed\n', ['lifting factorization', 'discrete wavelet transform', 'DWT', 'lifting matrices', 'integer transforms', 'computational complexity', 'data compression', 'feature extraction', 'multiframe detection', 'filter bank', 'lossless coding schemes', 'mobile devices', 'integer arithmetic', 'mobile computing', 'channel bank filters', 'data compression', 'discrete wavelet transforms', 'matrix algebra', 'mobile computing', 'transform coding']), ('WAM!Net: private pipes for electronic media\n"We are the digital version of FedEx. We offer storage and intelligent\nworkflow." The United States military - especially during war time - is\npretty careful about the way it handles its workflow and\ncommunications. Before a company is awarded a government contract, the\ncompany and its technology are screened and verified. If the technology\nor its creators aren\'t trustworthy and secure, chances are they aren\'t\ngetting by Uncle Sam. Record companies and publishing houses tend to\nfeel the same way. After all, security is just as important to a record\nexecutive as it is to a Navy commander. WAM!Net, a Wide-Area Media\nnetwork (hence, the name) passes muster with both. The company, which\nemploys about 320 employees around the world, has 15000 customers\nincluding the US Navy and a host of record labels, publishing\ncompanies, healthcare providers, and advertising agencies, all of whom\nuse its network as a way to transport, store, and receive data. "We are\nthe digital version of FedEx. We offer storage and intelligent\nworkflow," says Murad Velani, executive vice president of sales and\nmarketing for WAM!Net. "We started out as purely transport and we\'ve\nbecome a digital platform."\n', ['United States military', 'Wide-Area Media network', 'U.S. Navy', 'record labels', 'publishing companies', 'healthcare providers', 'advertising agencies', 'intelligent workflow', 'WAM!Net', 'content creators', 'electronic media', 'high-speed private network', 'ATM technology', 'content information', 'publishing information', 'client-server format', 'ASP format', 'digital platform', 'electronic data interchange', 'electronic publishing', 'security of data', 'wide area networks']), ('7 key tests in choosing your Web site firm\nMost legal firms now have a Web site and are starting to evaluate the return on\ntheir investment. The paper looks at factors involved when choosing a\nfirm to help set up or improve a Web site. (1) Look for a company that\ncombines technical skills and business experience. (2) Look for a\ncompany that offers excellent customer service. (3) Check that the Web\nsite firm is committed to developing and proactively updating the Web\nsite. (4) Make sure the firm has a proven track record and a good\nportfolio. (5) Look for a company with both a breadth as well as depth\nof skills. (6) Make sure the firm can deliver work on target, in budget\nand to specification. (7) Ensure that you will enjoy working and feel\ncomfortable with the Web site firm staff\n', ['Web site', 'customer service', 'proactive updating', 'legal firms', 'return on investment', 'technical skills', 'business experience', 'information resources', 'law administration']), ('An approximation to the F distribution using the chi-square distribution\nFor the cumulative distribution function (c.d.f.) of the F distribution, F(x;\nk, n), with associated degrees of freedom, k and n, a shrinking factor\napproximation (SFA), G( lambda kx; k), is proposed for large n and any\nfixed k, where G(x; k) is the chi-square c.d.f. with degrees of\nfreedom, k, and lambda = lambda (kx; n) is the shrinking factor.\nNumerical analysis indicates that for n/k >or= 3, approximation\naccuracy of the SFA is to the fourth decimal place for most small\nvalues of k. This is a substantial improvement on the accuracy that is\nachievable using the normal, ordinary chi-square, and Scheffe-Tukey\napproximations. In addition, it is shown that the theoretical\napproximation error of the SFA, |F(x; k,n)-G( lambda kx; k)|, is\nO(1/n/sup 2/) uniformly over x\n', ['F distribution', 'cumulative distribution function', 'degrees of freedom', 'shrinking factor approximation', 'chi-square distribution', 'numerical analysis', 'approximation theory', 'probability', 'statistical analysis']), ('An analytical model for a composite adaptive rectangular structure using the\nHeaviside function\nThe objective of this article is to describe a mathematical model, based on the\nHeaviside function and on the delta -Dirac distribution, for a\ncomposite adaptive rectangular structure with embedded and/or bonded\npiezoelectric actuators and sensors. In the adopted structure model,\nthe laminae are made up a configuration of rectangular nonpiezoelectric\nand piezoelectric patches. The laminae do not all have the same area\nnor do they present the same configuration, such that there are points\nwhere there is no material. The equations of motion and the boundary\nconditions, which describe the electromechanical coupling, are based on\nthe Mindlin displacement field, on the linear theory of\npiezoelectricity, and on the Hamilton principle\n', ['composite adaptive rectangular structure', 'mathematical model', 'Heaviside function', 'delta-Dirac distribution', 'embedded actuators', 'embedded sensors', 'bonded actuators', 'bonded sensors', 'piezoelectric actuators', 'piezoelectric sensors', 'piezoelectric patches', 'nonpiezoelectric patches', 'equations of motion', 'boundary conditions', 'electromechanical coupling', 'Mindlin displacement field', 'Hamilton principle', 'closed-form solution', 'Lagrangian functions', 'linear piezoelectricity', 'constitutive relations', 'virtual kinetic energy', 'rectangular composite plate', 'finite-element method', 'boundary-value problems', 'composite materials', 'dynamic response', 'electric potential', 'finite element analysis', 'intelligent actuators', 'intelligent sensors', 'intelligent structures', 'piezoelectric actuators', 'piezoelectric transducers', 'piezoelectricity']), ('The limits of shape constancy: point-to-point mapping of perspective\nprojections of flat figures\nThe present experiments investigate point-to-point mapping of perspective\ntransformations of 2D outline figures under diverse viewing conditions:\nbinocular free viewing, monocular perspective with 2D cues masked by an\noptic tunnel, and stereoptic viewing through an optic tunnel. The first\nexperiment involved upright figures, and served to determine baseline\npoint-to-point mapping accuracy, which was found to be very good. Three\nshapes were used: square, circle and irregularly round. The main\nexperiment, with slanted figures, involved only two shapes-square and\nirregularly shaped-showed at several slant degrees. Despite the\naccumulated evidence for shape constancy when the outline of\nperspective projections is considered, metric perception of the inner\nstructure of such projections was quite limited. Systematic distortions\nwere found, especially with more extreme slants, and attributed to the\njoint effect of several factors: anchors, 3D information, and slant\nunderestimation. Contradictory flatness cues did not detract from\nperformance, while stereoptic information improved it\n', ['shape constancy', 'point-to-point mapping', 'flat figure perspective projections', 'experiments', '2D outline figures', 'diverse viewing conditions', 'binocular free viewing', 'monocular perspective', '2D cues', 'optic tunnel', 'stereoptic viewing', '3D shape perception', 'human factors', '3D information displays', 'anchors', '3D information', 'slant underestimation', 'human factors', 'three-dimensional displays', 'user interfaces', 'visual perception']), ('Active vibration control of composite sandwich beams with piezoelectric\nextension-bending and shear actuators\nWe have used quasi-static equations of piezoelectricity to derive a finite\nelement formulation capable of modelling two different kinds of\npiezoelastically induced actuation in an adaptive composite sandwich\nbeam. This formulation is made to couple certain piezoelectric\nconstants to a transverse electric field to develop extension-bending\nactuation and shear-induced actuation. As an illustration, we present a\nsandwich model of three sublaminates: face/core/face. We develop a\ncontrol scheme based on the linear quadratic regulator/independent\nmodal space control (LQR/IMSC) method and use this to estimate the\nactive stiffness and the active damping introduced by shear and\nextension-bending actuators. To assess the performance of each type of\nactuator, a dynamic response study is carried out in the modal domain.\nWe observe that the shear actuator is more efficient in actively\ncontrolling the vibration than the extension-bending actuator for the\nsame control effort\n', ['quasi-static equations', 'piezoelectricity', 'finite element formulation', 'piezoelastically', 'adaptive composite sandwich beam', 'piezoelectric constants', 'transverse electric field', 'extension-bending actuation', 'finite element procedure', 'shear-induced actuation', 'sandwich model', 'sublaminates', 'linear quadratic regulator', 'modal space control', 'active stiffness', 'active damping', 'shear actuators', 'extension-bending actuators', 'dynamic response', 'modal domain', 'damping', 'dynamic response', 'elastic constants', 'finite element analysis', 'intelligent actuators', 'laminates', 'piezoelectric actuators', 'piezoelectricity', 'vibration control']), ('Linear complexity of polyphase power residue sequences\nThe well known family of binary Legendre or quadratic residue sequences can be\ngeneralised to the multiple-valued case by employing a polyphase\nrepresentation. These p-phase sequences, with p prime, also have prime\nlength L, and can be constructed from the index sequence of length L\nor, equivalently, from the cosets of pth power residues and\nnon-residues modulo-L. The linear complexity of these polyphase\nsequences is derived and shown to fall into four classes depending on\nthe value assigned to b/sub 0/, the initial digit of the sequence, and\non whether p belongs to the set of pth power residues or not. The\ncharacteristic polynomials of the linear feedback shift registers that\ngenerate these sequences are also derived\n', ['linear complexity', 'polyphase power residue sequences', 'binary Legendre sequences', 'quadratic residue sequences', 'multiple-valued case', 'p-phase sequences', 'polynomials', 'linear feedback shift registers', 'cryptographic applications', 'key stream ciphers', 'binary sequences', 'binary sequences', 'computational complexity', 'cryptography', 'polynomials']), ('If the RedBoot fits [open-source ROM monitor]\nMany embedded developers today use a ROM- or flash-resident software program\nthat provides functionality such as loading and running application\nsoftware, scripting, read/write access to processor registers, and\nmemory dumps. A ROM monitor, as it is often called, can be a useful and\nfar less expensive debugging tool than an in-circuit emulator. This\narticle describes the RedBoot ROM monitor. It takes a look at the\nfeatures offered by the RedBoot ROM monitor and sees how it can be\nconfigured. It also walks through the steps of rebuilding and\ninstalling a new RedBoot image on a target platform. Finally, it looks\nat future enhancements that are coming in new releases and how to get\nsupport and additional information when using RedBoot. Although RedBoot\nuses software modules from the eCos real-time operating system (RTOS)\nand is often used in systems running embedded Linux, it is completely\nindependent of both operating systems. RedBoot can be used with any\noperating system or RTOS, or even without one\n', ['RedBoot', 'open-source ROM monitor', 'embedded systems', 'flash-resident software program', 'scripting', 'processor register access', 'memory dumps', 'debugging tool', 'bootstrapping', 'eCos', 'real-time operating system', 'embedded Linux', 'computer bootstrapping', 'embedded systems', 'operating systems (computers)', 'program debugging', 'public domain software', 'read-only storage', 'storage management', 'system monitoring']), ('An intelligent tutoring system for a power plant simulator\nIn this paper, an intelligent tutoring system (ITS) is proposed for a power\nplant simulator. With a well designed ITS, the need for an instructor\nis minimized and the operator may readily and efficiently take, in\nreal-time, the control of simulator with appropriate messages he(she)\ngets from the tutoring system. Using SIMULINK and based on object\noriented programming (OOP) and C programming language, a fossil-fuelled\npower plant simulator with an ITS is proposed. Promising results are\ndemonstrated for a typical power plant\n', ['fossil-fuelled power plant simulator', 'intelligent tutoring system', 'SIMULINK', 'object oriented programming', 'control simulation', 'C programming language', 'CAI', 'computer based training', 'control engineering education', 'intelligent tutoring systems', 'power engineering education', 'power station control', 'thermal power stations']), ('The necessity of real-time-fact and fiction in digital reference systems\nCurrent discussions and trends in digital reference have emphasized the use of\nreal-time digital reference services. Recent articles have questioned\nboth the utility and use of asynchronous services such as e-mail. This\narticle uses data from the AskERIC digital reference service to\ndemonstrate that asynchronous services are not only useful and used,\nbut may have greater utility than real-time systems\n', ['real-time digital reference services', 'asynchronous services', 'e-mail', 'AskERIC', 'personalized Internet-based service', 'digital library', 'digital libraries', 'electronic mail', 'information services', 'real-time systems']), ('Stability and L/sub 2/ gain properties of LPV systems\nStability and L/sub 2/ gain properties of linear parameter-varying systems are\nobtained under assumed bounds on either the maximum or average value of\nthe parameter rate\n', ['stability', 'L/sub 2/ gain properties', 'linear parameter-varying systems', 'parameter rate', 'Gromwall-Bellman inequality', 'gain scheduled control', 'asymptotic stability', 'linear systems']), ("Numerical simulation of information recovery in quantum computers\nDecoherence is the main problem to be solved before quantum computers can be\nbuilt. To control decoherence, it is possible to use error correction\nmethods, but these methods are themselves noisy quantum computation\nprocesses. In this work, we study the ability of Steane's and Shor's\nfault-tolerant recovering methods, as well as a modification of\nSteane's ancilla network, to correct errors in qubits. We test a way to\nmeasure correctly ancilla's fidelity for these methods, and state the\npossibility of carrying out an effective error correction through a\nnoisy quantum channel, even using noisy error correction methods\n", ['numerical simulation', 'information recovery', 'quantum computers', 'decoherence control', 'error correction methods', 'noisy quantum computation processes', 'fault-tolerant recovering methods', 'ancilla network', 'ancilla fidelity', 'qubits', 'noisy error correction methods', 'noisy quantum channel', 'error correction', 'fault tolerant computing', 'information theory', 'quantum computing']), ('Assessment of the macrocyclic effect for the complexation of crown-ethers with\nalkali cations using the substructural molecular fragments method\nThe Substructural Molecular Fragments method (Solov\'ev, V. P.; Varnek, A. A.;\nWipff, G. J. Chem. Inf. Comput. Sci. 2000, 40, 847-858) was applied to\nassess stability constants (logK) of the complexes of crown-ethers,\npolyethers, and glymes with Na/sup +/, K/sup +/, and Cs/sup +/ in\nmethanol. One hundred forty-seven computational models including\ndifferent fragment sets coupled with linear or nonlinear fitting\nequations were applied for the data sets containing 69 (Na/sup +/), 123\n(K/sup +/), and 31 (Cs/sup +/) compounds. To account for the\n"macrocyclic effect" for crown-ethers, an additional "cyclicity"\ndescriptor was used. "Predicted" stability constants both for\nmacrocyclic compounds and for their open-chain analogues are in good\nagreement with the experimental data reported earlier and with those\nstudied experimentally in this work. The macrocyclic effect as a\nfunction of cation and ligand is quantitatively estimated for all\nstudied crown-ethers\n', ['substructural molecular fragments method', 'stability constants', 'complexation', 'crown-ethers', 'alkali cations', 'macrocyclic effect', 'computational models', 'different fragment sets', 'nonlinear fitting equations', 'linear fitting equations', 'cyclicity descriptor', 'open-chain analogues', 'data mining', 'structure-property tool', 'molecular graph decomposition', 'quantitative structure-properties relationship', 'augmented atom', 'TRAIL program', 'statistical parameters', 'thermodynamic parameters', 'association', 'chemical equilibrium', 'chemistry computing', 'graph theory', 'organic compounds', 'solvent effects']), ('An effective feedback control mechanism for DiffServ architecture\nAs a scalable QoS (Quality of Service) architecture, Diffserv (Differentiated\nService) mainly consists of two components: traffic conditioning at the\nedge of the Diffserv domain and simple packet forwarding inside the\nDiffServ domain. DiffServ has many advantages such as flexibility,\nscalability and simplicity. But when providing AF (Assured Forwarding)\nservices, DiffServ has some problems such as unfairness among\naggregated flows or among micro-flows belonging to an aggregated flow.\nIn this paper, a feedback mechanism for AF aggregated flows is proposed\nto solve this problem. Simulation results show that this mechanism does\nimprove the performance of DiffServ. First, it can improve the fairness\namong aggregated flows and make DiffServ more friendly toward TCP\n(Transmission Control Protocol) flows. Second, it can decrease the\nbuffer requirements at the congested router and thus obtain lower delay\nand packet loss rate. Third, it also keeps almost the same link utility\nas in normal DiffServ. Finally, it is simple and easy to be implemented\n', ['QoS', 'Diffserv', 'traffic conditioning', 'packet forwarding', 'AF', 'feedback mechanism', 'fairness', 'TCP', 'feedback control', 'QoS architecture', 'feedback', 'packet switching', 'quality of service', 'transport protocols']), ('Stochastic systems with a random jump in phase trajectory: stability of their\nmotions\nThe probabilistic stability of the perturbed motion of a system with parameters\nunder the action of a general Markov process is studied. The phase\nvector is assumed to experience random jumps when the structure the\nsystem suffers random jumps. Such a situation is encountered, for\nexample, in the motion of a solid with random jumps in its mass. The\nmean-square stability of random-structure linear systems and stability.\nof nonlinear systems in the first approximation are studied. The\napplied approach is helpful in studying the asymptotic probabilistic\nstability and mean-square exponential stability of stochastic systems\nthrough the stability of the respective deterministic systems\n', ['stochastic systems', 'random jump', 'phase trajectory', 'general Markov process', 'asymptotic probabilistic stability', 'mean-square exponential stability', 'asymptotic stability', 'differential equations', 'probability', 'stochastic systems']), ('Technology decisions 2002\nThe paper looks at the critical hardware, software, and services choices\nmanufacturers are making as they begin to emerge from the recession and\nposition themselves for the future\n', ['manufacturing industries', 'information technology', 'management of change', 'customer relationship management', 'services choices', 'enterprise resource planning', 'information technology', 'management of change', 'manufacturing data processing', 'manufacturing resources planning', 'technological forecasting']), ('Orthogonality of the Jacobi polynomials with negative integer parameters\nIt is well known that the Jacobi polynomials P/sub n//sup ( alpha , beta )/(x)\nare orthogonal with respect to a quasi-definite linear functional\nwhenever alpha , beta , and alpha + beta + 1 are not negative integer\nnumbers. Recently, Sobolev orthogonality for these polynomials has been\nobtained for alpha a negative integer and beta not a negative integer\nand also for the case alpha = beta negative integer numbers. In this\npaper, we give a Sobolev orthogonality for the Jacobi polynomials in\nthe remainder cases\n', ['orthogonality', 'quasi-definite linear functional', 'Sobolev orthogonality', 'Jacobi polynomials', 'negative integer parameters', 'Jacobian matrices', 'polynomials']), ('Approach to adaptive neural net-based H/sub infinity / control design\nAn approach is investigated for the adaptive neural net-based H/sub infinity /\ncontrol design of a class of nonlinear uncertain systems. In the\nproposed framework, two multilayer feedforward neural networks are\nconstructed as an alternative to approximate the nonlinear system. The\nneural networks are piecewisely interpolated to generate a linear\ndifferential inclusion model by which a linear state feedback H/sub\ninfinity / control law can be applied. An adaptive weight adjustment\nmechanism for the multilayer feedforward neural networks is developed\nto ensure H/sub infinity / regulation performance. It is shown that\nfinding the control gain matrices can be transformed into a standard\nlinear matrix inequality problem and solved via a developed recurrent\nneural network\n', ['adaptive neural net-based H/sub infinity / control design', 'nonlinear uncertain systems', 'multilayer feedforward neural networks', 'piecewise interpolation', 'linear differential inclusion model', 'linear state feedback', 'control gain matrices', 'linear matrix inequality problem', 'recurrent neural network', 'LMI', 'adaptive control', 'control system synthesis', 'feedforward neural nets', 'H/sup infinity / control', 'linear systems', 'matrix algebra', 'multilayer perceptrons', 'neurocontrollers', 'nonlinear control systems', 'state feedback', 'uncertain systems']), ('Modeling undesirable factors in efficiency evaluation\nData envelopment analysis (DEA) measures the relative efficiency of decision\nmaking units (DMUs) with multiple performance factors which are grouped\ninto outputs and inputs. Once the efficient frontier is determined,\ninefficient DMUs can improve their performance to reach the efficient\nfrontier by either increasing their current output levels or decreasing\ntheir current input levels. However, both desirable (good) and\nundesirable (bad) factors may be present. For example, if inefficiency\nexists in production processes where final products are manufactured\nwith a production of wastes and pollutants, the outputs of wastes and\npollutants are undesirable and should be reduced to improve the\nperformance. Using the classification invariance property, we show that\nthe standard DEA model can be used to improve the performance via\nincreasing the desirable outputs and decreasing the undesirable\noutputs. The method can also be applied to situations when some inputs\nneed to be increased to improve the performance. The linearity and\nconvexity of DEA are preserved through our proposal\n', ['data envelopment analysis', 'decision making units', 'multiple performance factors', 'efficient frontier', 'current output levels', 'current input levels', 'production processes', 'final product manufacture', 'wastes', 'pollutants', 'classification invariance property', 'desirable outputs', 'undesirable outputs', 'linear programming', 'efficiency evaluation', 'undesirable factor modeling', 'data envelopment analysis', 'distributed decision making', 'linear programming', 'production']), ('An overview of modems\nThis paper describes cursory glance of different types of modems classified for\napplication, range, line type, operating mode, synchronizing mode,\nmodulation, etc., highly useful for all engineering students of\ncommunication, electrical, computer science and information technology\nstudents. This paper also describes the standards and protocols used\nand the future trend\n', ['modems', 'line type', 'operating mode', 'synchronizing mode', 'modulation', 'engineering students', 'communication students', 'electrical students', 'computer science students', 'information technology students', 'standards', 'protocols', 'modems', 'modulation', 'protocols', 'telecommunication standards']), ('Perspectives on academic vs. industry environments for women in computer\nscience\nThe authors were tenure track faculty members at the Colorado School of Mines\nand later moved into senior positions at software companies. Both are\npart of two-career couples as well, and both have two children. In this\narticle, they discuss their impressions and share anecdotes regarding\nthe differing experiences of women and families in these two\nenvironments\n', ['industry environment', 'academic environments', 'women', 'computer science', 'faculty members', 'software companies', 'career', 'children', 'gender gap', 'computer science education', 'DP industry', 'gender issues', 'social aspects of automation']), ('Improving the predicting power of partial order based QSARs through linear\nextensions\nPartial order theory (POT) is an attractive and operationally simple method\nthat allows ordering of compounds, based on selected structural and/or\nelectronic descriptors (modeled order), or based on their end points,\ne.g., solubility (experimental order). If the modeled order resembles\nthe experimental order, compounds that are not experimentally\ninvestigated can be assigned a position in the model that eventually\nmight lead to a prediction of an end-point value. However, in the\napplication of POT in quantitative structure-activity relationship\nmodeling, only the compounds directly comparable to the noninvestigated\ncompounds are applied. To explore the possibilities of improving the\nmethodology, the theory is extended by application of the so-called\nlinear extensions of the model order. The study show that partial\nordering combined with linear extensions appears as a promising tool\nproviding probability distribution curves in the range of possible\nend-point values for compounds not being experimentally investigated\n', ['quantitative structure-activity relationships', 'partial order theory', 'predicting power improvement', 'linear extensions', 'structural descriptors', 'electronic descriptors', 'modeled order', 'end points', 'graphical representation', 'combinatorial rule', 'most probable linear order', 'partially ordered set', 'Hasse diagram', 'solubilities', 'organic compounds', 'chemistry computing', 'combinatorial mathematics', 'organic compounds', 'physical chemistry', 'probability', 'set theory', 'solubility']), ('Entangling atoms in bad cavities\nWe propose a method to produce entangled spin squeezed states of a large number\nof atoms inside an optical cavity. By illuminating the atoms with\nbichromatic light, the coupling to the cavity induces pairwise exchange\nof excitations which entangles the atoms. Unlike most proposals for\nentangling atoms by cavity QED, our proposal does not require the\nstrong coupling regime g/sup 2// kappa Gamma >>1, where g is the\natom cavity coupling strength, kappa is the cavity decay rate, and\nGamma is the decay rate of the atoms. In this work the important\nparameter is Ng/sup 2// kappa Gamma , where N is the number of atoms,\nand our proposal permits the production of entanglement in bad cavities\nas long as they contain a large number of atoms\n', ['atom entanglement', 'entangled spin squeezed states', 'optical cavity', 'bichromatic light illumination', 'coupling', 'pairwise exchange', 'excitations', 'cavity QED', 'strong coupling regime', 'cavity decay rate', 'atom cavity coupling strength', 'bad cavities', 'atom-photon collisions', 'information theory', 'optical squeezing', 'quantum optics', 'quantum theory']), ('On the contractivity of implicit-explicit linear multistep methods\nThis paper is concerned with the class of implicit-explicit linear multistep\nmethods for the numerical solution of initial value problems for\nordinary differential equations which are composed of stiff and\nnonstiff parts. We study the contractivity of such methods, with regard\nto linear autonomous systems of ordinary differential equations and a\n(scaled) Euclidean norm. In addition, we derive a strong stability\nresult based on the stability regions of these methods\n', ['implicit-explicit linear multistep methods', 'contractivity', 'numerical solution', 'initial value problems', 'ordinary differential equations', 'linear autonomous systems', 'Euclidean norm', 'stability result', 'differential equations', 'initial value problems', 'iterative methods', 'matrix algebra', 'stability']), ('Feedforward maximum power point tracking of PV systems using fuzzy controller\nA feedforward maximum power (MP) point tracking scheme is developed for the\ninterleaved dual boost (IDB) converter fed photovoltaic (PV) system\nusing fuzzy controller. The tracking algorithm changes the duty ratio\nof the converter such that the solar cell array (SCA) voltage equals\nthe voltage corresponding to the MP point at that solar insolation.\nThis is done by the feedforward loop, which generates an error signal\nby comparing the instantaneous array voltage and reference voltage. The\nreference voltage for the feedforward loop, corresponding to the MP\npoint, is obtained by an off-line trained neural network. Experimental\ndata is used for off-line training of the neural network, which employs\nback-propagation algorithm. The proposed fuzzy feedforward peak power\ntracking effectiveness is demonstrated through the simulation and\nexperimental results, and compared with the conventional proportional\nplus integral (PI) controller based system. Finally, a comparative\nstudy of interleaved boost and conventional boost converter for the PV\napplications is given and their suitability is discussed\n', ['feedforward maximum power point tracking', 'PV systems', 'fuzzy controller', 'interleaved dual boost converter feed', 'photovoltaic system', 'tracking algorithm', 'duty ratio', 'solar cell array voltage', 'solar insolation', 'feedforward loop', 'error signal', 'instantaneous array voltage', 'reference voltage', 'off-line trained neural network', 'back-propagation algorithm', 'fuzzy feedforward peak power tracking effectiveness', 'backpropagation', 'feedforward', 'fuzzy control', 'neurocontrollers', 'photovoltaic power systems', 'solar cell arrays', 'tracking']), ("19in monitors [CRT survey]\nUpgrade your monitor from as little as Pounds 135. With displays on test and\nranging up to Pounds 400, whether you're after the last word in quality\nor simply looking for again, this Labs holds the answer. Looks at ADI\nMicroScan M900, CTX PR960F, Eizo FlexScan T766, Hansol 920D,\nHansol920P, Hitachi CM715ET, Hitachi CM721FET, liyama Vision Master Pro\n454, LG Flatron 915FT Plus, Mitsubishi Diamond Pro 920, NEC MultiSync\nFE950+, Philips 109S40, Samsung SyncMaster 959NF, Sony Multiscan\nCPD-G420, and ViewSonic G90f\n", ['19in monitors', 'CRT survey', 'ADI MicroScan M900', 'CTX PR960F', 'Eizo FlexScan T766', 'Hansol 920D', 'Hansol920P', 'Hitachi CM715ET', 'Hitachi CM721FET', 'liyama Vision Master Pro 454', 'LG Flatron 915FT Plus', 'Mitsubishi Diamond Pro 920', 'NEC MultiSync FE950', 'Philips 109S40', 'Samsung SyncMaster 959NF', 'Sony Multiscan CPD-G420', 'ViewSonic G90f', '19 in', "buyer's guides", 'cathode-ray tube displays']), ('Improving the frequency stability of microwave oscillators by utilizing the\ndual-mode sapphire-loaded cavity resonator\nThe design and experimental testing of a novel control circuit to stabilize the\ntemperature of a sapphire-loaded cavity whispering gallery\nresonator-oscillator and improve its medium-term frequency stability is\npresented. Finite-element software was used to predict frequencies and\nquality factors of WGE/sub 7,0,0/ and the WGH/sub 9,0,0/ modes near 9\nGHz, and separated in frequency by approximately 80 MHz. Calculations\nshow that the novel temperature control circuits from the difference\nfrequency can result in a frequency stability of better than one part\nin 10/sup 13/ at 270 K. Also, we present details on the best way to\ncouple orthogonally to two modes of similar frequency but different\npolarization\n', ['microwave oscillators', 'temperature control circuit', 'dual-mode sapphire-loaded cavity resonator', 'frequency standard', 'frequency stability', 'temperature stabilisation', 'whispering gallery resonator-oscillator', 'finite-element analysis', 'whispering gallery modes', 'difference frequency', 'high-quality factor', '9 GHz', '270 K', 'dielectric resonator oscillators', 'finite element analysis', 'frequency stability', 'frequency standards', 'microwave oscillators', 'sapphire', 'temperature control']), ('Two-step integral imaging for orthoscopic three-dimensional imaging with\nimproved viewing resolution\nWe present a two-step integral imaging system to obtain 3-D orthoscopic real\nimages. By adopting a nonstationary micro-optics technique, we\ndemonstrate experimentally the potential usefulness of two-step\nintegral imaging\n', ['two-step integral imaging', 'resolution improved viewing', 'two-step integral imaging system', '3-D orthoscopic real images', 'nonstationary micro-optics technique', '3-D image reconstruction', 'liquid crystal light valve', 'display device', 'LCLV', 'pickup lenslet array', 'CCD image sensors', 'image reconstruction', 'image resolution', 'light valves', 'liquid crystal displays', 'microlenses', 'optical arrays']), ('Uniform hyperbolic polynomial B-spline curves\nThis paper presents a new kind of uniform splines, called hyperbolic polynomial\nB-splines, generated over the space Omega =span{sinh t, cosh t, t/sup\nk-3/, t/sup k-3/, t/sup k-4/, ..., t 1} in which k is an arbitrary\ninteger larger than or equal to 3. Hyperbolic polynomial B-splines\nshare most of the properties of B-splines in polynomial space. We give\nsubdivision formulae for this new kind of curve and then prove that\nthey have variation diminishing properties and the control polygons of\nthe subdivisions converge. Hyperbolic polynomial B-splines can handle\nfreeform curves as well as remarkable curves such as the hyperbola and\nthe catenary. The generation of tensor product surfaces using these new\nsplines is straightforward. Examples of such tensor product surfaces:\nthe saddle surface, the catenary cylinder, and a certain kind of ruled\nsurface are given\n', ['uniform hyperbolic polynomial B-spline curves', 'arbitrary integer', 'subdivision formulae', 'control polygons', 'subdivisions', 'freeform curves', 'hyperbola', 'catenary', 'tensor product surface generation', 'saddle surface', 'catenary cylinder', 'ruled surface', 'computational geometry', 'polynomials', 'splines (mathematics)']), ('Nonlockability in multirings and hypercubes at serial transmission of data\nblocks\nFor the multiring and hypercube, a method of conflictless realization of an\narbitrary permutation of "large" data items that can be divided into\nmany "smaller" data blocks was considered, and its high efficiency was\ndemonstrated\n', ['nonlockability', 'multirings', 'hypercubes', 'data block serial transmission', 'multiprocessor computer systems', 'data communication', 'multiprocessor interconnection networks']), ('Extended depth-of-focus imaging of chlorophyll fluorescence from intact leaves\nImaging dynamic changes in chlorophyll a fluorescence provides a valuable means\nwith which to examine localised changes in photosynthetic function.\nMicroscope-based systems provide excellent spatial resolution which\nallows the response of individual cells to be measured. However, such\nsystems have a restricted depth of focus and, as leaves are inherently\nuneven, only a small proportion of each image at any given focal plane\nis in focus. In this report we describe the development of algorithms,\nspecifically adapted for imaging chlorophyll fluorescence and\nphotosynthetic function in living plant cells, which allow\nextended-focus images to be reconstructed from images taken in\ndifferent focal planes. We describe how these procedures can be used to\nreconstruct images of chlorophyll fluorescence and calculated\nphotosynthetic parameters, as well as producing a map of leaf topology.\nThe robustness of this procedure is demonstrated using leaves from a\nnumber of different plant species\n', ['chlorophyll fluorescence', 'intact leaves', 'extended depth-of-focus imaging', 'leaf topology map', 'plant species', 'calculated photosynthetic parameters', 'individual cells response', 'microscope-based systems', 'charge-coupled device', 'maximum fluorescence yield', 'minimum fluorescence yield', 'variable fluorescence', 'numerical aperture', 'primary quinone acceptor', 'spatial resolution', 'algorithms development', 'extended-focus images reconstruction', 'biophysical research technique', 'biological techniques', 'biology computing', 'fluorescence', 'image reconstruction', 'image resolution', 'optical focusing', 'optical microscopy', 'organic compounds', 'photosynthesis']), ('Fabrication of polymeric microlens of hemispherical shape using micromolding\nPolymeric microlenses play an important role in reducing the size, weight, and\ncost of optical data storage and optical communication systems. We\nfabricate polymeric microlenses using the microcompression molding\nprocess. The design and fabrication procedures for mold insertion is\nsimplified using silicon instead of metal. PMMA powder is used as the\nmolding material. Governed by process parameters such as temperature\nand pressure histories, the micromolding process is controlled to\nminimize various defects that develop during the molding process. The\nradius of curvature and magnification ratio of fabricated microlens are\nmeasured as 150 mu m and over 3.0, respectively\n', ['polymeric microlens fabrication', 'micromolding', 'hemispherical shape microlens', 'size', 'weight', 'cost', 'optical data storage', 'optical communication systems', 'polymeric microlenses', 'microcompression molding process', 'fabrication procedures', 'design procedures', 'mold insertion', 'silicon', 'PMMA powder', 'molding material', 'process parameters', 'temperature', 'pressure', 'micromolding process', 'magnification ratio', '300 micron', 'microlenses', 'micromachining', 'moulding', 'optical design techniques', 'optical fabrication', 'optical polymers', 'optical storage']), ('New paradigms for interactive 3D volume segmentation\nWe present a new virtual reality-based interaction metaphor for semi-automatic\nsegmentation of medical 3D volume data. The mouse-based, manual\ninitialization of deformable surfaces in 3D represents a major\nbottleneck in interactive segmentation. In our multi-modal system we\nenhance this process with additional sensory feedback. A 3D haptic\ndevice is used to extract the centreline of a tubular structure. Based\non the obtained path a cylinder with varying diameter is generated,\nwhich in turn is used as the initial guess for a deformable surface\n', ['interactive 3D volume segmentation', 'virtual reality', 'interaction metaphor', 'medical image segmentation', 'mouse', 'deformable surfaces', 'interactive segmentation', 'multi-modal system', 'sensory feedback', '3D haptic device', 'tubular structure', 'varying diameter cylinder', 'deformable surface', 'haptic interaction', 'haptic interfaces', 'image segmentation', 'medical image processing', 'virtual reality']), ('Multi-output regression using a locally regularised orthogonal least-squares\nalgorithm\nThe paper considers data modelling using multi-output regression models. A\nlocally regularised orthogonal least-squares (LROLS) algorithm is\nproposed for constructing sparse multi-output regression models that\ngeneralise well. By associating each regressor in the regression model\nwith an individual regularisation parameter, the ability of the\nmulti-output orthogonal least-squares (OLS) model selection to produce\na parsimonious model with a good generalisation performance is greatly\nenhanced\n', ['multi-output regression models', 'locally regularised orthogonal least-squares algorithm', 'data modelling', 'sparse multi-output regression models', 'parsimonious model', 'nonlinear system modelling', 'LROLS algorithm', 'least squares approximations', 'modelling', 'nonlinear systems', 'statistical analysis', 'time series']), ('Dynamical transition to periodic motions of a recurrent bus induced by nonstops\nWe study the dynamical behavior of a recurrent bus on a circular route with\nmany bus stops when the recurrent bus passes some bus stops without\nstopping. The recurrent time (one period) is described in terms of a\nnonlinear map. It is shown that the recurrent bus exhibits the complex\nperiodic behaviors. The dynamical transitions to periodic motions occur\nby increasing nonstops. The periodic motions depend on the property of\nan attractor of the nonlinear map. The period n of the attractor varies\nsensitively with the number of nonstops\n', ['dynamical transition', 'periodic motions', 'recurrent bus', 'nonstops', 'circular route', 'recurrent time', 'nonlinear map', 'complex periodic behaviors', 'attractor', 'nonlinear dynamical systems', 'phase transformations', 'road traffic']), ("Buying into the relationship [business software]\nChoosing the right software to improve business processes can have a huge\nimpact on a company's efficiency and profitability. While it is\nsometimes hard to get beyond vendor hype about software features and\nfunctionality and know what to realistically expect, it is even more\ndifficult to determine if the vendor is the right vendor to partner\nwith. Thus picking the right software is important, but companies have\nto realize that what they are really buying into is a relationship with\nthe vendor\n", ['business software', 'vendor relationship', 'management', 'functionality', 'software evaluation', 'business data processing', 'DP management', 'software selection']), ('Current waveform control of a high-power-factor rectifier circuit for harmonic\nsuppression of voltage and current in a distribution system\nThis paper presents the input current waveform control of the rectifier circuit\nwhich realizes simultaneously the high input power factor and the\nharmonics suppression of the receiving-end voltage and the source\ncurrent under the distorted receiving-end voltage. The proposed input\ncurrent waveform includes the harmonic components which are in phase\nwith the receiving-end voltage harmonics. The control parameter in the\nproposed waveform is designed by examining the characteristics of both\nthe harmonic suppression effect in the distribution system and the\ninput power factor of the rectifier circuit. The effectiveness of the\nproposed current waveform has been confirmed experimentally\n', ['input current waveform control', 'high input power factor', 'receiving-end voltage', 'source current', 'distorted receiving-end voltage', 'input current waveform', 'receiving-end voltage harmonics', 'high-power-factor rectifier circuit', 'harmonic voltage suppression', 'harmonic current suppression', 'distribution system', '200 V', '60 Hz', '8 kVA', '2 kW', 'electric current control', 'harmonics suppression', 'power distribution control', 'power factor', 'power system harmonics', 'rectifying circuits']), ('A knowledge management framework for the support of decision making in\nhumanitarian assistance/disaster relief\nThe major challenge in current humanitarian assistance/disaster relief (HA/DR)\nefforts is that diverse information and knowledge are widely\ndistributed and owned by different organizations. These resources are\nnot efficiently organized and utilized during HA/DR operations. We\npresent a knowledge management framework that integrates multiple\ninformation technologies to collect, analyze, and manage information\nand knowledge for supporting decision making in HA/DR. The framework\nwill help identify the information needs, be aware of a disaster\nsituation, and provide decision-makers with useful relief\nrecommendations based on past experience. A comprehensive, consistent\nand authoritative knowledge base within the framework will facilitate\nknowledge sharing and reuse. This framework can also be applied to\nother similar real-time decision-making environments, such as crisis\nmanagement and emergency medical assistance\n', ['knowledge management framework', 'decision support system', 'humanitarian assistance', 'disaster relief', 'organizations', 'information technology', 'information needs', 'knowledge sharing', 'knowledge reuse', 'real-time decision-making environments', 'crisis management', 'emergency medical assistance', 'case-based reasoning', 'case-based reasoning', 'decision support systems', 'disasters', 'emergency services', 'information needs', 'knowledge based systems', 'public administration', 'real-time systems']), ("Windows XP fast user switching\nThe Windows NT family of operating systems has always supported the concept of\nmultiple user accounts, but they've taken the concept a step further\nwith Windows XP's Fast User Switching feature. Fast User Switching is a\nnew feature of Windows XP that allows multiple users to log on to the\nsame machine and quickly switch between the logged on accounts. Fast\nUser Switching is implemented using some of the built-in capabilities\nof Terminal Services. Terminal Server has been around for a while but\nis much more feature rich and integrated in Windows XP. A machine with\nthe terminal services (Remote Desktop) client can log on to and run\napplications on a remote machine running the terminal server\n", ['Windows XP Fast User Switching', 'multiple user logon access', 'operating systems', 'multiple user accounts', 'Terminal Services', 'Terminal Server', 'Remote Desktop', 'network operating systems']), ('Extrapolation in Lie groups with approximated BCH-formula\nWe present an extrapolation algorithm for the integration of differential\nequations in Lie groups which is a suitable generalization of the\nwell-known GBS-algorithm for ODEs. Sufficiently accurate approximations\nto the BCH-formula are required to reach a given order. We give such\napproximations with a minimized number of commutators\n', ['Lie groups', 'approximated BCH-formula', 'differential equations', 'GBS-algorithm', 'geometric integration', 'extrapolation methods', 'differential equations', 'extrapolation', 'integration', 'Lie groups']), ('Semantic data broadcast for a mobile environment based on dynamic and adaptive\nchunking\nDatabase broadcast is an effective and scalable approach to disseminate\ninformation of high affinity to a large collection of mobile clients. A\ncommon problem of existing broadcast approaches is the lack of\nknowledge for a client to determine if all data items satisfying its\nquery could be obtained from the broadcast. We therefore propose a\nsemantic-based broadcast approach. A semantic descriptor is attached to\neach broadcast unit, called a data chunk. This semantic descriptor\nallows a client to determine if a query can be answered entirely based\non broadcast items and, if needed, identify the precise definition of\nthe remaining items in the form of a "supplementary" query. Data chunks\ncan be of static or dynamic sizes and organized hierarchically. Their\nboundary can be determined on-the-fly, adaptive to the nature of client\nqueries. We investigate different ways of organizing the data chunks\nover a broadcast channel to improve access performance. We introduce\nthe data affinity index metric, which more accurately reflects\nclient-perceived performance. A simulation model is built to evaluate\nour semantic-based broadcast schemes\n', ['mobile databases', 'mobile computing', 'semantic data broadcast', 'mobile clients', 'semantic descriptor', 'data chunking', 'query processing', 'data affinity index', 'adaptive chunking', 'answerability', 'client-server systems', 'data handling', 'database management systems', 'mobile computing', 'query processing']), ('Responding to market trends with predictive segmentation [health care]\nTechnology and technological advances have always been a part of healthcare,\nbut often it\'s advances in treatment machinery and materials that get\nthe attention. However, technology gains also occur behind the scenes\nin operations. One of the less glamorous but powerful technological\nadvances available today is predictive segmentation, a phrase that\nmeans "a new way to assess and view individuals in the market based on\ntheir health status and health needs." Sophisticated databases, data\nmining, neural networks and statistical capabilities have enabled the\ndevelopment of predictive segmentation techniques. These predictive\nmodels for healthcare can identify who is likely to need certain\nservices and who is likely to become ill. They are a significant\ndeparture from various geographical and attitudinal segmentation\nmethods that healthcare strategists have used in the past to gain a\nbetter understanding of their customers\n', ['market trends', 'healthcare', 'predictive segmentation', 'data mining', 'neural networks', 'data mining', 'health care']), ("Encouraging women in computer science\nAt a cost to both their own opportunities and society's ability to produce\npeople with much-needed technical skills, women continue to be\nunderrepresented in computer science degree programs at both the\nundergraduate and graduate level. Although some of the barriers that\nwomen face have their foundations in cultural expectations established\nwell before the college level, we believe that departments can take\neffective steps to increase recruitment and retention of women\nstudents. This paper describes several strategies we have adopted at\nStanford over the past decade\n", ['technical skills', 'computer science degree programs', 'undergraduate level', 'graduate level', 'cultural expectations', 'women student recruitment', 'women student retention', 'computer science education', 'employment', 'gender issues']), ('A solvable queueing network model for railway networks and its validation and\napplications for the Netherlands\nThe performance of new railway networks cannot be measured or simulated, as no\ndetailed train schedules are available. Railway infrastructure and\ncapacities are to be determined long before the actual traffic is\nknown. This paper therefore proposes a solvable queueing network model\nto compute performance measures of interest without requiring train\nschedules (timetables). Closed form expressions for mean delays are\nobtained. New network designs, traffic scenarios, and capacity\nexpansions can so be evaluated. A comparison with real delay data for\nthe Netherlands supports the practical value of the model. A special\nDutch cargo-line application is included\n', ['railway networks', 'solvable queueing network model', 'Netherlands', 'railway infrastructure', 'railway capacities', 'performance measures', 'closed form expressions', 'mean delays', 'network designs', 'traffic scenarios', 'capacity expansions', 'Dutch cargo-line application', 'delay estimation', 'queueing theory', 'railways']), ("Computer program to generate operant schedules\nA computer program for programming schedules of reinforcement is described.\nStudents can use the program to experience schedules of reinforcement\nthat are typically used with nonhuman subjects. Accumulative recording\nof a student's response can be shown on the screen and/or printed with\nthe computer's printer. The program can also be used to program operant\nschedules for animal subjects. The program was tested with human\nsubjects experiencing fixed ratio, variable ratio, fixed interval, and\nvariable interval schedules. Performance for human subjects on a given\nschedule was similar to performance for nonhuman subjects on the same\nschedule\n", ['operant schedule generation', 'computer program', 'reinforcement schedule programming', 'nonhuman subjects', 'cumulative student response recording', 'animal subjects', 'fixed ratio schedules', 'variable ratio schedules', 'fixed interval schedules', 'variable interval schedules', 'human subjects', 'behavioural sciences computing']), ('Application of multiprocessor systems for computation of jets\nThe article describes the implementation of methods for numerical solution of\ngas-dynamic problems on a wide class of multiprocessor systems,\nconventionally characterized as "cluster" systems. A standard\ndata-transfer interface - the so-called message passing interface - is\nused for parallelization of application algorithms among processors.\nSimulation of jets escaping into a low-pressure region is chosen as a\ncomputational example\n', ['multiprocessor systems', 'computation of jets', 'gas-dynamic problems', 'cluster systems', 'data-transfer interface', 'message passing interface', 'low-pressure region', 'computational fluid dynamics', 'mathematics computing', 'message passing', 'multiprocessing systems']), ('An optimal control algorithm based on reachability set approximation and\nlinearization\nThe terminal functional of a general control system is refined by studying an\nanalogous problem for a variational system and regularization. A\nsequential refinement method is designed by combining the local\napproximation of the reachability set and reduction. The corresponding\nalgorithm has relaxation properties. An illustrative example is given\n', ['determinate systems', 'optimal control algorithm', 'reachability set approximation', 'linearization', 'terminal functional', 'variational system', 'regularization', 'sequential refinement method', 'local approximation', 'relaxation properties', 'functional equations', 'matrix algebra', 'optimal control', 'reachability analysis', 'set theory', 'variational techniques']), ('Text-independent speaker verification using utterance level scoring and\ncovariance modeling\nThis paper describes a computationally simple method to perform text\nindependent speaker verification using second order statistics. The\nsuggested method, called utterance level scoring (ULS), allows one to\nobtain a normalized score using a single pass through the frames of the\ntested utterance. The utterance sample covariance is first calculated\nand then compared to the speaker covariance using a distortion measure.\nSubsequently, a distortion measure between the utterance covariance and\nthe sample covariance of data taken from different speakers is used to\nnormalize the score. Experimental results from the 2000 NIST speaker\nrecognition evaluation are presented for ULS, used with different\ndistortion measures, and for a Gaussian mixture model (GMM) system. The\nresults indicate that ULS as a viable alternative to GMM whenever the\ncomputational complexity and verification accuracy needs to be traded\n', ['text-independent speaker verification', 'utterance level scoring', 'covariance modeling', 'computationally simple method', 'second order statistics', 'normalized score', 'sample covariance', 'speaker covariance', 'distortion measure', 'NIST speaker recognition evaluation', 'distortion measures', 'Gaussian mixture model', 'GMM', 'computational complexity', 'verification accuracy', 'computational complexity', 'covariance analysis', 'Gaussian distribution', 'speaker recognition', 'statistical analysis']), ('Improved approximation of Max-Cut on graphs of bounded degree\nLet alpha approximately=0.87856 denote the best approximation ratio currently\nknown for the Max-Cut problem on general graphs. We consider a\nsemidefinite relaxation of the Max-Cut problem, round it using the\nrandom hyperplane rounding technique of M.X. Goemans and D.P.\nWilliamson (1995), and then add a local improvement step. We show that\nfor graphs of degree at most Delta , our algorithm achieves an\napproximation ratio of at least alpha + epsilon , where epsilon >0\nis a constant that depends only on Delta .. Using computer assisted\nanalysis, we show that for graphs of maximal degree 3 our algorithm\nobtains an approximation ratio of at least 0.921, and for 3-regular\ngraphs the approximation ratio is at least 0.924. We note that for the\nsemidefinite relaxation of Max-Cut used by Goemans and Williamson the\nintegrality gap is at least 1/0.885, even for 2-regular graphs\n', ['Max-Cut approximation', 'semidefinite relaxation', 'approximation ratio', 'computer assisted analysis', '2-regular graphs', 'bounded degree graph', 'best approximation ratio', 'computational geometry', 'graph theory', 'probability']), ('Arranging solid balls to represent a graph\nBy solid balls, we mean a set of balls in R/sup 3/ no two of which can\npenetrate each other. Every finite graph G can be represented by\narranging solid balls in the following way: Put red balls in R/sup 3/,\none for each vertex of G, and connect two red balls by a chain when\nthey correspond to a pair of adjacent vertices of G, where a chain\nmeans a finite sequence of blue solid balls in which each consecutive\nballs are tangent. (We may omit the chain if the two red balls are\nalready tangent.) The ball number b(G) of G is the minimum number of\nballs (red and blue) necessary to represent G. If we put the balls and\nchains on a table so that all balls sit on the table, then the minimum\nnumber of balls for G is denoted by bT(G). Among other things, we prove\nthat b(K/sub 6/) = 8, b(K/sub 7/) = 13 and b/sub T/(K/sub 5/) = 8,b/sub\nT/(K/sub 6/) = 14. We also prove that c/sub 1/n/sup 3/ < b(K/sub n/)\n< c/sub 2/n/sup 3/ log n, c/sub 3/n/sup 4//log n < b/sub T/(K/sub\nn/) < c/sub 4/n/sup 4/\n', ['solid balls', 'finite graph', 'adjacent vertices', 'finite sequence', 'graph representation', 'computational geometry', 'graph theory']), ('From powder to perfect parts\nGKN Sinter Metals has increased productivity and quality by automating the\npowder metal lines that produce its transmission parts\n', ['GKN Sinter Metals', 'powder metal lines', 'automating', 'conveyors', 'gentle transfer units', 'robotic systems', 'metallurgical industries']), ('Computer program for calculating the p-value in testing process capability\nindex C/sub pmk/\nMany process capability indices, including C/sub p/, C/sub pk/, and C/sub pm/,\nhave been proposed to provide numerical measures on the process\npotential and performance. Combining the advantages of these indices,\nPearn et al. (1992) introduced a new capability index called C/sub\npmk/, which has been shown to be a useful capability index for\nprocesses with two-sided specification limits. In this paper, the\nauthors implement the theory of a testing hypothesis using the natural\nestimator of C/sub pmk/, and provide an efficient Maple computer\nprogram to calculate the p-values. They also provide tables of the\ncritical values for some commonly used capability requirements. Based\non the test, they develop a simple step-by-step procedure for in-plant\napplications. The practitioners can use the proposed procedure to\ndetermine whether their process meets the preset capability\nrequirement, and make reliable decisions\n', ['testing process capability index', 'computer program', 'process potential', 'process performance', 'testing hypothesis', 'natural estimator', 'Maple', 'in-plant applications', 'preset capability requirement', 'reliable decisions', 'p-value calculation', 'engineering computing', 'process control', 'quality control', 'software packages', 'testing']), ('Accelerating filtering techniques for numeric CSPs\nSearch algorithms for solving Numeric CSPs (Constraint Satisfaction Problems)\nmake an extensive use of filtering techniques. In this paper we show\nhow those filtering techniques can be accelerated by discovering and\nexploiting some regularities during the filtering process. Two kinds of\nregularities are discussed, cyclic phenomena in the propagation queue\nand numeric regularities of the domains of the variables. We also\npresent in this paper an attempt to unify numeric CSPs solving methods\nfrom two distinct communities, that of CSP in artificial intelligence,\nand that of interval analysis\n', ['search algorithms', 'Numeric CSPs', 'Constraint Satisfaction Problems', 'filtering techniques', 'CSPs-solving', 'artificial intelligence', 'interval analysis', 'extrapolation methods', 'propagation', 'pruning', 'constraint handling', 'constraint theory', 'nonlinear equations']), ('Block truncation image bit plane coding\nBlock truncation coding (BTC) is a successful image compression technique due\nto its simple and fast computational burden. The bit rate is fixed to\n2.0 bits/pixel, whose performance is moderate in terms of compression\nratio compared to other compression schemes such as discrete cosine\ntransform (DCT), vector quantization (VQ), wavelet transform coding\n(WTC), etc. Two kinds of overheads are required for BTC coding: bit\nplane and quantization values, respectively. A new technique is\npresented to reduce the bit plane overhead. Conventional bit plane\noverhead is 1.0 bits/pixel; we decrease it to 0.734 bits/pixel while\nmaintaining the same decoded quality as absolute moment BTC (AMBTC)\ndoes for the "Lena" image. Compared to other published bit plane coding\nstrategies, the proposed method outperforms all of the existing methods\n', ['image bit plane coding', 'block truncation coding', 'image compression technique', 'bit rate', 'performance', 'compression ratio', 'bit plane overhead', 'decoded quality', 'absolute moment BTC', 'AMBTC', 'quantization values', 'Lena image', 'block codes', 'data compression', 'image coding', 'quantisation (signal)']), ('Deterministic single-photon source for distributed quantum networking\nA sequence of single photons is emitted on demand from a single three-level\natom strongly coupled to a high-finesse optical cavity. The photons are\ngenerated by an adiabatically driven stimulated Raman transition\nbetween two atomic ground states, with the vacuum field of the cavity\nstimulating one branch of the transition, and laser pulses\ndeterministically driving the other branch. This process is unitary and\ntherefore intrinsically reversible, which is essential for quantum\ncommunication and networking, and the photons should be appropriate for\nall-optical quantum information processing\n', ['deterministic single-photon source', 'distributed quantum networking', 'single three-level atom', 'high-finesse optical cavity', 'adiabatically driven stimulated Raman transition', 'vacuum field', 'quantum communication', 'all-optical quantum information processing', 'quantum communication', 'quantum computing', 'quantum optics', 'stimulated Raman scattering']), ('Doubly invariant equilibria of linear discrete-time games\nThe notion of doubly invariant (DI) equilibrium is introduced. The concept\nextends controlled and robustly controlled invariance notions to the\ncontext of two-person dynamic games. Each player tries to keep the\nstate in a region of state space independently of the actions of the\nrival player. The paper gives existence conditions, criteria and\nalgorithms for the determination of DI equilibria of linear dynamic\ngames in discrete time. Two examples illustrate the results. The first\none is in the area of fault-tolerant controller synthesis. The second\nis an application to macroeconomics\n', ['doubly invariant equilibria', 'linear discrete-time games', 'robustly controlled invariance', 'two-person dynamic games', 'state space', 'existence conditions', 'fault-tolerant controller synthesis', 'macroeconomics', 'control system synthesis', 'discrete time systems', 'economic cybernetics', 'fault tolerance', 'game theory', 'invariance', 'linear systems', 'state-space methods']), ('A new result on the global convergence of Hopfield neural networks\nIn this work, we discuss Hopfield neural networks, investigating their global\nstability. Some sufficient conditions for a class of Hopfield neural\nnetworks to be globally stable and globally exponentially stable are\ngiven\n', ['Hopfield neural networks', 'global stability', 'sufficient conditions', 'globally exponentially stable networks', 'convergence', 'Hopfield neural nets', 'stability']), ('Novel approach to super-resolution pits readout\nWe proposed a novel method to realize the readout of super-resolution pits by\nusing a super-resolution reflective film to replace the reflective\nlayer of the conventional ROM. At the same time, by using Sb as the\nsuper-resolution reflective layer and SiN as a dielectric layer, the\nsuper-resolution pits with diameters of 380 nm were read out by a setup\nwhose laser wavelength is 632.8 nm and numerical aperture is 0.40. In\naddition, the influence of the Sb thin film thickness on the readout\nsignal was investigated, the results showed that the optimum Sb thin\nfilm thickness is 28 to 30 nm, and the maximum CNR is 38 to 40 dB\n', ['super-resolution pits readout', 'super-resolution reflective film', 'Sb super-resolution reflective layer', 'SiN dielectric layer', 'numerical aperture', 'Sb thin film thickness', 'readout signal', 'maximum CNR', '380 nm', '632.8 nm', '28 to 30 nm', 'Sb-SiN', 'antimony', 'optical disc storage', 'optical films', 'optical resolving power', 'silicon compounds']), ('Grey-box model identification via evolutionary computing\nThis paper presents an evolutionary grey-box model identification methodology\nthat makes the best use of a priori knowledge on a clear-box model with\na global structural representation of the physical system under study,\nwhilst incorporating accurate blackbox models for immeasurable and\nlocal nonlinearities of a practical system. The evolutionary technique\nis applied to building dominant structural identification with local\nparametric tuning without the need of a differentiable performance\nindex in the presence of noisy data. It is shown that the evolutionary\ntechnique provides an excellent fitting performance and is capable of\naccommodating multiple objectives such as to examine the relationships\nbetween model complexity and fitting accuracy during the model building\nprocess. Validation results show that the proposed method offers\nrobust, uncluttered and accurate models for two practical systems. It\nis expected that this type of grey-box models will accommodate many\npractical engineering systems for a better modelling accuracy\n', ['system identification', 'grey-box models', 'evolutionary algorithms', 'genetic evolution', 'multiobjective optimisation', 'hydraulic system', 'nonlinear system', 'genetic algorithms', 'hydraulic systems', 'identification', 'nonlinear systems']), ('Universal approximation by hierarchical fuzzy system with constraints on the\nfuzzy rule\nThis paper presents a special hierarchical fuzzy system where the outputs of\nthe previous layer are not used in the IF-parts, but used only in the\nTHEN-parts of the fuzzy rules of the current layer. The proposed scheme\ncan be shown to be a universal approximator to any continuous function\non a compact set if complete fuzzy sets are used in the IF-parts of the\nfuzzy rules with singleton fuzzifier and center average defuzzifier.\nFrom the simulation of ball and beam control system, it is demonstrated\nthat the proposed scheme approximates with good accuracy the model\nnonlinear controller with fewer fuzzy rules than the centralized fuzzy\nsystem and its control performance is comparable to that of the\nnonlinear controller\n', ['hierarchical fuzzy system', 'fuzzy rules', 'universal approximator', 'continuous function', 'ball and beam control system', 'hierarchical fuzzy logic', 'Stone-Weierstrass theorem', 'constraint handling', 'fuzzy logic', 'fuzzy systems']), ('Multicell converters: active control and observation of flying-capacitor\nvoltages\nThe multicell converters introduced more than ten years ago make it possible to\ndistribute the voltage constraints among series-connected switches and\nto improve the output waveforms (increased number of levels and\napparent frequency). The balance of the constraints requires an\nappropriate distribution of the flying voltages. This paper presents\nsome solutions for the active control of the voltages across the flying\ncapacitors in the presence of rapid variation of the input voltage. The\nlatter part of this paper is dedicated to the observation of these\nvoltages using an original modeling of the converter\n', ['multicell converters', 'active control', 'flying-capacitor voltages', 'Kalman filtering', 'multilevel systems', 'nonlinear systems', 'power electronics', 'power systems harmonics', 'series-connected switches', 'output waveforms improvement', 'input voltage', 'capacitors', 'Kalman filters', 'power convertors', 'power system harmonics', 'switching circuits', 'voltage control']), ("Computing stationary Nash equilibria of undiscounted single-controller\nstochastic games\nGiven a two-person, nonzero-sum stochastic game where the second player\ncontrols the transitions, we formulate a linear complementarity problem\nLCP(q, M) whose solution gives a Nash equilibrium pair of stationary\nstrategies under the limiting average payoff criterion. The matrix M\nconstructed is of the copositive class so that Lemke's algorithm will\nprocess it. We will also do the same for a special class of N-person\nstochastic games called polymatrix stochastic games\n", ['undiscounted single-controller stochastic games', 'stationary Nash equilibria', 'two-person nonzero-sum stochastic game', 'linear complementarity problem', 'stationary strategies', 'limiting average payoff criterion', 'copositive class matrix', 'Lemke algorithm', 'N-person stochastic games', 'polymatrix stochastic games', 'complementarity', 'linear programming', 'matrix algebra', 'stochastic games']), ('Conformal-mapping design tools for coaxial couplers with complex cross section\nNumerical conformal mapping is exploited as a simple, accurate, and efficient\ntool for the analysis and design of coaxial waveguides and couplers of\ncomplex cross section. An implementation based on the\nSchwarz-Christoffel Toolbox, a public-domain MATLAB package, is applied\nto slotted coaxial cables and to symmetrical coaxial couplers, with\ncircular or polygonal inner conductors and external shields. The effect\nof metallic diaphragms of arbitrary thickness, partially separating the\ninner conductors, is also easily taken into account. The proposed\ntechnique is validated against the results of the finite-element\nmethod, showing excellent agreement at a fraction of the computational\ncost, and is also extended to the case of nonsymmetrical couplers,\nproviding the designer with important additional degrees of freedom\n', ['conformal mapping design tools', 'coaxial couplers', 'complex cross section', 'coaxial waveguides', 'Schwarz-Christoffel Toolbox', 'public-domain MATLAB package', 'slotted coaxial cables', 'symmetrical couplers', 'circular inner conductors', 'polygonal inner conductors', 'external shields', 'metallic diaphragms', 'nonsymmetrical couplers', 'numerical conformal transformations', 'CAD', 'capacitance', 'coaxial cables', 'coaxial waveguides', 'directional couplers', 'electric impedance', 'electrical engineering computing', 'numerical analysis', 'waveguide couplers']), ('Car-caravan snaking. 2 Active caravan braking\nFor part 1, see ibid., p.707-22. Founded on the review and results of Part 1,\nPart 2 contains a description of the virtual design of an active\nbraking system for caravans or other types of trailer, to suppress\nsnaking vibrations, while being simple from a practical viewpoint. The\ndesign process and the design itself are explained. The performance is\nexamined by simulations and it is concluded that the system is\neffective, robust and realizable with modest and available components\n', ['car-caravan snaking', 'active caravan braking', 'virtual design', 'trailer', 'snaking vibrations suppression', 'dynamics', 'automobiles', 'braking', 'dynamics', 'motion control', 'vibration control']), ('An intelligent system combining different resource-bounded reasoning techniques\nIn this paper, PRIMES (Progressive Reasoning and Intelligent multiple MEthods\nSystem), a new architecture for resource-bounded reasoning that\ncombines a form of progressive reasoning and the so-called multiple\nmethods approach is presented. Each time-critical reasoning unit is\ndesigned in such a way that it delivers an approximate result in time\nwhenever an overload or a failure prevents the system from producing\nthe most accurate result. Indeed, reasoning units use approximate\nprocessing based on two salient features. First, an incremental\nprocessing unit constructs an approximate solution quickly and then\nrefines it incrementally. Second, a multiple methods approach proposes\ndifferent alternatives to solve the problem, each of them being\nselected according to the available resources. In allowing several\nresource-bounded reasoning paradigms to be combined, we hope to extend\ntheir actual scope to cover more real-world application domains\n', ['resource-bounded reasoning techniques', 'intelligent multiple methods system', 'time-critical reasoning unit', 'approximate processing', 'complex systems', 'real-time performance', 'PRIMES', 'progressive reasoning', 'inference mechanisms', 'knowledge based systems', 'real-time systems']), ("Development and evaluation of a case-based reasoning classifier for prediction\nof breast biopsy outcome with BI-RADS/sup TM/ lexicon\nApproximately 70-85% of breast biopsies are performed on benign lesions. To\nreduce this high number of biopsies performed on benign lesions, a\ncase-based reasoning (CBR) classifier was developed to predict biopsy\nresults from BI-RADS/sup TM/ findings. We used 1433 (931 benign)\nbiopsy-proven mammographic cases. CBR similarity was defined using\neither the Hamming or Euclidean distance measure over case features.\nTen features represented each case: calcification distribution,\ncalcification morphology, calcification number, mass margin, mass\nshape, mass density, mass size, associated findings, special cases, and\nage. Performance was evaluated using Round Robin sampling, Receiver\nOperating Characteristic (ROC) analysis, and bootstrap. To determine\nthe most influential features for the CBR, an exhaustive feature search\nwas performed over all possible feature combinations (1022) and\nsimilarity thresholds. Influential features were defined as the most\nfrequently occurring features in the feature subsets with the highest\npartial ROC areas (/sub 0.90/AUC). For CBR with Hamming distance, the\nmost influential features were found to be mass margin, calcification\nmorphology, age, calcification distribution, calcification number, and\nmass shape, resulting in an /sub 0.90/AUC of 0.33. At 95% sensitivity,\nthe Hamming CBR would spare from biopsy 34% of the benign lesions. At\n98% sensitivity, the Hamming CBR would spare 27% benign lesions. For\nthe CBR with Euclidean distance, the most influential feature subset\nconsisted of mass margin, calcification morphology, age, mass density,\nand associated findings, resulting in /sub 0.90/AUC of 0.37. At 95%\nsensitivity, the Euclidean CBR would spare from biopsy 41% benign\nlesions. At 98% sensitivity, the Euclidean CBR would spare 27% benign\nlesions. The profile of cases spared by both distance measures at 98%\nsensitivity indicates that the CBR is a potentially useful diagnostic\ntool for the classification of mammographic lesions, by recommending\nshort-term follow-up for likely benign lesions that is in agreement\nwith final biopsy results and mammographer's intuition\n", ['case-based reasoning classifier', 'breast biopsy outcome', 'BI-RADS lexicon', 'benign lesions', 'biopsy-proven mammographic cases', 'CBR similarity', 'Hamming distance measure', 'Euclidean distance measure', 'calcification distribution', 'calcification morphology', 'calcification number', 'mass margin', 'mass shape', 'mass density', 'mass size', 'associated findings', 'special cases', 'age', 'Round Robin sampling', 'Receiver Operating Characteristic analysis', 'bootstrap', 'feature combinations', 'similarity thresholds', 'feature subsets', 'highest partial ROC areas', 'influential features', 'diagnostic tool', 'mammographic lesion classification', 'short-term follow-up', 'biological organs', 'cancer', 'case-based reasoning', 'feature extraction', 'image classification', 'mammography', 'medical expert systems', 'medical image processing', 'sensitivity analysis']), ("Optimize/sup IT/ robot condition monitoring tool\nAs robots have gained more and more 'humanlike' capability, users have looked\nincreasingly to their builders for ways to measure the critical\nvariables-the robotic equivalent of a physical check-up-in order to\nmonitor their condition and schedule maintenance more effectively. This\nis all the more essential considering the tremendous pressure there is\nto improve productivity in today's global markets. Developed for ABB\nrobots with an S4-family controller and based on the company's broad\nprocess know-how, Optimize/sup IT/ robot condition monitoring offers\nmaintenance routines with embedded checklists that give a clear\nindication of a robot's operating condition. It performs semi-automatic\nmeasurements that support engineers during trouble-shooting and enable\naction to be taken to prevent unplanned stops. By comparing these\nmeasurements with reference data, negative trends can be detected early\nand potential breakdowns predicted. Armed with all these features,\nOptimize/sup IT/ robot condition monitoring provides the ideal basis\nfor reliability-centered maintenance (RCM) for robots\n", ['Optimize/sup IT/ robot condition monitoring tool', 'maintenance scheduling', 'condition monitoring', 'ABB robots', 'S4-family controller', 'semi-automatic measurements', 'reliability-centered maintenance', 'computerised monitoring', 'condition monitoring', 'industrial robots', 'maintenance engineering', 'reliability', 'scheduling']), ('On the expected value of the minimum assignment\nThe minimum k-assignment of an m*n matrix X is the minimum sum of k entries of\nX, no two of which belong to the same row or column. Coppersmith and\nSorkin conjectured that if X is generated by choosing each entry\nindependently from the exponential distribution with mean 1, then the\nexpected value of its minimum k-assignment is given by an explicit\nformula, which has been proven only in a few cases. In this paper we\ndescribe our efforts to prove the Coppersmith-Sorkin conjecture by\nconsidering the more general situation where the entries x/sub ij/ of X\nare chosen independently from different distributions. In particular,\nwe require that x/sub ij/ be chosen from the exponential distribution\nwith mean 1/r/sub i/c/sub j/. We conjecture an explicit formula for the\nexpected value of the minimum k-assignment of such X and give evidence\nfor this formula\n', ['minimum k-assignment', 'm * n matrix', 'exponential distribution', 'rational function', 'bipartite graph', 'algorithm theory', 'graph theory', 'matrix algebra']), ("Help-desk support is key to wireless success [finance]\nA well thought out help desk can make or break an institution's mobile play.\nSchwab, Ameritrade and RBC are taking their support function seriously\n", ['finance', 'help desk', 'Schwab', 'Ameritrade', 'RBC', 'wireless', 'investment', 'technical support services', 'wireless LAN']), ('Experimental investigation of active vibration control using neural networks\nand piezoelectric actuators\nThe use of neural networks for identification and control of smart structures\nis investigated experimentally. Piezoelectric actuators are employed to\nsuppress the vibrations of a cantilevered plate subject to impulse,\nsine wave and band-limited white noise disturbances. The neural\nnetworks used are multilayer perceptrons trained with error\nbackpropagation. Validation studies show that the identifier predicts\nthe system dynamics accurately. The controller is trained adaptively\nwith the help of the neural identifier. Experimental results\ndemonstrate excellent closed-loop performance and robustness of the\nneurocontroller\n', ['active vibration control', 'neural networks', 'piezoelectric actuators', 'identification', 'control', 'smart structures', 'cantilevered plate', 'white noise disturbances', 'multilayer perceptrons', 'error backpropagation', 'closed-loop performance', 'robustness', 'neurocontroller', 'vibration suppression', 'accelerometers', 'backpropagation', 'closed loop systems', 'intelligent control', 'intelligent structures', 'multilayer perceptrons', 'piezoelectric actuators', 'vibration control', 'white noise']), ('Migrating to public librarianship: depart on time to ensure a smooth flight\nCareer change can be a difficult, time-consuming, and anxiety-laden process for\nanyone contemplating this important decision. The challenges faced by\nlibrarians considering the move from academic to public librarianship\ncan be equally and significantly demanding. To most outsiders, at least\non the surface, it may appear to be a quick and easy transition to\nmake, but some professional librarians recognize the distinct\ndifferences between these areas of librarianship. Although the\nubiquitous nature of technology has brought the various work\nresponsibilities of academic and public librarians closer together\nduring the last decade, there remain key differences in job-related\nduties and the work environments. These dissimilarities pose meaningful\nhurdles to leap for academic librarians wishing to migrate to the\npublic sector. The paper considers the variations between academic and\npublic librarianship\n', ['public librarianship', 'career change', 'academic library', 'public library', 'professional librarians', 'library technology', 'work responsibilities', 'job-related duties', 'work environments', 'academic libraries', 'employment', 'human resource management', 'information science', 'personnel', 'professional aspects', 'public libraries']), ('Problems with my PDA\nTom Berry has lost his PDA, and now he has an even better understanding of the\nrisks and benefits of working on the move\n', ['PDA', 'mobile technology', 'risks', 'benefits', 'mobile computing', 'notebook computers']), ('Mid-market accounting systems\nWelcome to our fourth annual survey of accounting systems and enterprise\nresource planning (ERP) systems. Last September, we concentrated on\nfinancial and distribution systems for medium-sized businesses (mid\nmarket) and included 22 products in our charts. This year, we extended\nthe products to include manufacturing and added 34 products to the list\n', ['mid-market accounting systems', 'survey', 'enterprise resource planning', 'manufacturing', 'accounting', "buyer's guides"]), ('Property testers for dense Constraint Satisfaction programs on finite domains\nMany NP-hard languages can be "decided" in subexponential time if the\ndefinition of "decide" is relaxed only slightly. Rubinfeld and Sudan\nintroduced the notion of property testers, probabilistic algorithms\nthat can decide, with high probability, if a function has a certain\nproperty or if it is far from any function having this property.\nGoldreich, Goldwasser, and Ron constructed property testers with\nconstant query complexity for dense instances of a large class of graph\nproblems. Since many graph problems can be viewed as special cases of\nthe Constraint Satisfaction Problem on Boolean domains, it is natural\nto try to construct property testers for more general cases of the\nConstraint Satisfaction Problem. In this paper, we give explicit\nconstructions of property testers using a constant number of queries\nfor dense instances of Constraint Satisfaction Problems where the\nconstraints have constant arity and the variables assume values in some\ndomain of finite size\n', ['NP-hard languages', 'property testers', 'probabilistic algorithms', 'constant query complexity', 'constraint satisfaction', 'dense instances', 'randomized sampling', 'subexponential time', 'graph problems', 'Constraint Satisfaction Problem', 'computational complexity', 'constraint theory', 'graph theory']), ("Bandwidth vs. gains design of H/sub infinity / tracking controllers for\ncurrent-fed induction motors\nDescribes a systematic procedure for designing speed and rotor flux norm\ntracking H/sub infinity /. controllers with unknown load torque\ndisturbances for current-fed induction motors. A new effective design\ntool is developed to allow selection of the control gains so as to\nadjust the disturbances' rejection capability of the controllers in the\nface of the bandwidth requirements of the closed-loop system.\nApplication of the proposed design procedure is demonstrated in a case\nstudy, and the results of numerical simulations illustrate the\nsatisfactory performance achievable even in presence of rotor\nresistance uncertainty\n", ['H/sub infinity / tracking controllers', 'current-fed induction motors', 'speed controllers', 'rotor flux norm controllers', 'unknown load torque disturbances', 'design tool', 'disturbances rejection capability', 'bandwidth requirements', 'closed-loop system', 'feedback linearization', 'observers', 'closed loop systems', 'control system synthesis', 'H/sup infinity / control', 'induction motors', 'machine control', 'observers', 'velocity control']), ('Expert advice - how can my organisation take advantage of reverse auctions\nwithout jeopardising existing supplier relationships?\nIn a recent survey, AMR Research found that companies that use reverse auctions\nto negotiate prices with suppliers typically achieve savings of between\n10% and 15% on direct goods and between 20% and 25% on indirect goods,\nand can slash sourcing cycle times from months to weeks. Suppliers,\nhowever, are less enthusiastic. They believe that these savings are\nachieved only by stripping the human element out of negotiations and\nevaluating bids on price alone, which drives down their profit margins.\nAs a result, reverse auctions carry the risk of jeopardising long-term\nand trusted relationships. Suppliers that have not been involved in a\nreverse auction before typically fear the bidding event itself -\narguably the most theatrical and, therefore, most hyped-up part of the\nprocess. Although it may only last one hour, weeks of preparation go\ninto setting up a successful bidding event\n', ['reverse auctions', 'supplier relationships', 'preparation', 'Request For Quotation', 'electronic commerce', 'purchasing']), ("Medical image computing at the Institute of Mathematics and Computer Science in\nMedicine, University Hospital Hamburg-Eppendorf\nThe author reviews the history of medical image computing at his institute,\nsummarizes the achievements, sketches some of the difficulties\nencountered, and draws conclusions that might be of interest especially\nto people new to the field. The origin and history section provides a\nchronology of this work, emphasizing the milestones reached during the\npast three decades. In accordance with the author's group's focus on\nimaging, the paper is accompanied by many pictures, some of which, he\nthinks, are of historical value\n", ['Institute of Mathematics and Computer Science in Medicine', 'University Hospital Hamburg-Eppendorf', 'medical image computing history', 'historical value', 'difficulties encountered', 'medical diagnostic imaging', 'work chronology', 'history', 'medical image processing', 'reviews']), ('Waiting-time distribution of a discrete-time multiserver queue with correlated\narrivals and deterministic service times: D-MAP/D/k system\nWe derive the waiting-time distribution of a discrete-time multiserver queue\nwith correlated arrivals and deterministic (or constant) service times.\nWe show that the procedure for obtaining the waiting-time distribution\nof a multiserver queue is reduced to that of a single-server queue. We\npresent a complete solution to the waiting-time distribution of\nD-MAP/D/k queue together with some computational results\n', ['waiting-time distribution', 'discrete-time multiserver queue', 'correlated arrivals', 'deterministic service times', 'D-MAP/D/k system', 'Markovian arrival process', 'asynchronous transfer mode', 'discrete time systems', 'Markov processes', 'matrix algebra', 'probability', 'queueing theory']), ('Evolving receptive-field controllers for mobile robots\nThe use of evolutionary methods to generate controllers for real-world\nautonomous agents has attracted attention. Most of the pertinent\nresearch has employed genetic algorithms or variations thereof.\nResearch has applied an alternative evolutionary method, evolution\nstrategies, to the generation of simple Braitenberg vehicles. This\napplication accelerates the development of such controllers by more\nthan an order of magnitude (a few hours compared to more than two\ndays). Motivated by this useful speedup, the paper investigates the\nevolution of more complex architectures, receptive-field controllers,\nthat can employ nonlinear interactions and, therefore, can yield more\ncomplex behavior. It is interesting to note that the evolution strategy\nyields the same efficacy in terms of function evaluations, even though\nthe second class of controllers requires up to 10 times more parameters\nthan the simple Braitenberg architecture. In addition to the speedup,\nthere is an important theoretical reason for preferring an evolution\nstrategy over a genetic algorithm for this problem, namely the presence\nof epistasis\n', ['receptive-field controllers', 'mobile robots', 'evolutionary methods', 'real-world autonomous agents', 'evolution strategies', 'simple Braitenberg vehicles', 'nonlinear interactions', 'complex behavior', 'radial basis functions', 'scalability', 'evolutionary computation', 'mobile robots', 'neurocontrollers']), ('Laser-based internal profile measurement system\nAn automatic laser-based system to measure the internal profiles of various\nstructures has been developed. The system uses a point laser source\nthrough a rotating optical device fixed on to a laser measurement\nmeter. A notebook computer with custom software is used to control the\nlaser meter and rotating device to estimate the scanned profile shape\nand to determine the resulting cross-section area. The information\nprovided by this system is essential to construction industry,\nincluding window and door builders; the glass, panel, board, and floor\ntile manufacturers; carpet venders; and building contractors for cost\nestimation and production control. As a result, the lead time for\ndelivering the customized windowpanes, woodwork, floor tiles, and\nceilings can be reduced. Applications of this system for measuring the\nshapes of window frames and floor plans are described and demonstrated.\nThe measurement accuracy is evaluated and analyzed. Results have\nindicated that the measurement accuracy can be achieved within 4% of\nthe measurement distance, for typical window designs and floor patterns\nrequired by major window manufacturers. Recommendations to improve the\nsystem are also included\n', ['laser-based internal profile measurement system', 'internal profiles', 'point laser source', 'rotating optical device', 'laser meter', 'rotating device', 'floor tile manufacturers', 'carpet venders', 'building contractors', 'cost estimation', 'production control', 'customized windowpanes', 'window frames', 'civil engineering computing', 'computerised instrumentation', 'measurement by laser beam']), ('An algorithm combining neural networks with fundamental parameters\nAn algorithm combining neural networks with the fundamental parameters\nequations (NNFP) is proposed for making corrections for non-linear\nmatrix effects in x-ray fluorescence analysis. In the algorithm, neural\nnetworks were applied to relate the concentrations of components to\nboth the measured intensities and the relative theoretical intensities\ncalculated by the fundamental parameter equations. The NNFP algorithm\nis compared with the classical theoretical correction models, including\nthe fundamental parameters approach, the Lachance-Traill model, a\nhyperbolic function model and the COLA algorithm. For an alloy system\nwith 15 measured elements, in most cases, the prediction errors of the\nNNFP algorithm are lower than those of the fundamental parameters\napproach, the Lachance-Traill model, the hyperbolic function model and\nthe COLA algorithm separately. If there are the serious matrix effects,\nsuch as matrix effects among Cr, Fe and Ni, the NNFP algorithm\ngenerally decreased predictive errors as compared with the classical\nmodels, except for the case of Cr by the fundamental parameters\napproach. The main reason why the NNFP algorithm has generally a better\npredictive ability than the classical theoretical correction models\nmight be that neural networks can better calibrate the non-linear\nmatrix effects in a complex multivariate system\n', ['algorithm', 'neural networks', 'fundamental parameters', 'fundamental parameters equations', 'nonlinear matrix effects', 'x-ray fluorescence analysis', 'intensities', 'NNFP algorithm', 'theoretical correction models', 'Lachance-Traill model', 'hyperbolic function model', 'COLA algorithm', 'alloy system', 'Cr', 'Fe', 'Ni', 'complex multivariate system', 'chromium alloys', 'iron alloys', 'neural nets', 'nickel alloys', 'spectroscopy computing', 'X-ray fluorescence analysis']), ('Routing security in wireless ad hoc networks\nA mobile ad hoc network consists of a collection of wireless mobile nodes that\nare capable of communicating with each other without the use of a\nnetwork infrastructure or any centralized administration. MANET is an\nemerging research area with practical applications. However, wireless\nMANET is particularly vulnerable due to its fundamental\ncharacteristics, such as open medium, dynamic topology, distributed\ncooperation, and constrained capability. Routing plays an important\nrole in the security of the entire network. In general, routing\nsecurity in wireless MANETs appears to be a problem that is not trivial\nto solve. In this article we study the routing security issues of\nMANETs, and analyze in detail one type of attack-the "black hole"\nproblem-that can easily be employed against the MANETs. We also propose\na solution for the black hole problem for ad hoc on-demand distance\nvector routing protocol\n', ['routing security', 'wireless ad hoc networks', 'mobile ad hoc network', 'wireless mobile nodes', 'wireless MANET', 'open medium', 'dynamic topology', 'distributed cooperation', 'on-demand distance vector routing protocol', 'satellite transmission', 'home wireless personal area networks', 'land mobile radio', 'radio networks', 'security of data', 'telecommunication network routing', 'telecommunication security', 'transport protocols']), ("Pane relief. Robotic solutions for car windshield assembly\nJust looking through a car's windshield doesn't give us much reason to wonder\nabout how it's made. The idea that special manufacturing expertise\nmight be required can hardly occur to anyone, but that's exactly what\nis needed to ensure crystal-clear visibility, not to mention a perfect\nfit every time one is pressed into place on a car production line.\nComprising two thin glass sheets joined by a vinyl interlayer,\nwindshields are assembled-usually manually-to very precise product and\nenvironmental specifications. To make sure this is done as perfectly as\npossible, the industry invests heavily in the equipment used for their\nfabrication. ABB has now developed a robot-based Compact Assembling\nSystem for the automatic assembly of laminated windshields that speeds\nup production and increases cost efficiency\n", ['car windshield assembly robots', 'manufacturing expertise', 'car production line', 'Compact Assembling System', 'laminated windshields assembly automation', 'production', 'cost efficiency', 'ABB', 'assembling', 'automobile industry', 'industrial robots']), ("BT voices its support for IP\nBTexact's chief technology officer, Mick Reeve, gives his views on the future\nfor voice over DSL services and virtual private networks, and defends\nthe slow rollout of public access WLANs\n", ['BTexact', 'voice over DSL', 'virtual private networks', 'public access WLANs', 'Internet telephony', 'local area networks', 'wide area networks']), ('Implementing equals for mixed-type comparison\nThe idea of comparing objects of different types is not entirely off base, in\nparticular for classes from the same class hierarchy. After all,\nobjects from the same class hierarchy (and by class hierarchy we mean\nall classes derived from a common superclass other than Object) have\nsomething in common, namely at least the superclass part. As we\ndemonstrated in a previous paper (2002), providing a correct\nimplementation of a mixed-type comparison is a non-trivial task. In\nthis article, we will show one way of implementing a mixed-type\ncomparison of objects from the same class hierarchy that meets the\nrequirements of the equals contract\n', ['Java', 'equals contract', 'transitivity requirement', 'mixed-type comparison', 'superclass', 'Java', 'object-oriented programming']), ('Robust fuzzy controlled photovoltaic power inverter with Taguchi method\nThis paper presents design and implementation of a robust fuzzy controlled\nphotovoltaic (PV) power inverter with Taguchi tuned scaling factors. To\nachieve fast transient response, small steady-state error and system\nrobustness, a robust fuzzy controller is adopted, in which its input\nand output scaling factors are determined efficiently by using the\nTaguchi-tuning algorithm. The proposed system can operate in different\nmodes, grid-connection mode and stand-alone mode, and can accommodate\nwide load variations. Simulation results and hardware measurements\nobtained from a prototype with a microcontroller (Intel 80196KC) are\npresented to verify the theoretical discussions, and its adaptivity,\nrobustness and feasibility\n', ['robust fuzzy controlled photovoltaic power inverter', 'Taguchi method', 'tuned scaling factors', 'transient response', 'steady-state error', 'system robustness', 'output scaling factors', 'grid-connection mode', 'stand-alone mode', 'load variations', 'microcontroller', 'adaptivity', 'feasibility', 'fuzzy control', 'invertors', 'microcontrollers', 'photovoltaic power systems', 'robust control', 'Taguchi methods', 'transient response']), ("A formal model of computing with words\nClassical automata are formal models of computing with values. Fuzzy automata\nare generalizations of classical automata where the knowledge about the\nsystem's next state is vague or uncertain. It is worth noting that like\nclassical automata, fuzzy automata can only process strings of input\nsymbols. Therefore, such fuzzy automata are still (abstract) devices\nfor computing with values, although a certain vagueness or uncertainty\nare involved in the process of computation. We introduce a new kind of\nfuzzy automata whose inputs are instead strings of fuzzy subsets of the\ninput alphabet. These new fuzzy automata may serve as formal models of\ncomputing with words. We establish an extension principle from\ncomputing with values to computing with words. This principle indicates\nthat computing with words can be implemented with computing with values\nwith the price of a big amount of extra computations\n", ['formal model', 'computing with words', 'fuzzy automata', 'fuzzy subsets', 'input alphabet', 'extension principle', 'pushdown automata', 'finite automata', 'formal languages', 'fuzzy logic']), ("Portal payback\nThe benefits of deploying a corporate portal are well-documented: access to\napplications and content is centralised, so users do not spend hours\nsearching for information; the management of disparate applications is\nalso centralised, and by allowing users to access 'self-service'\napplications in areas such as human resources and procurement,\norganisations spend less time on manual processing tasks. But how far\ncan prospective customers rely on the ROI figures presented to them by\nportal technology vendors? In particular, how reliable are the 'ROI\ncalculators' these vendors supply on their web sites?\n", ['corporate portal', 'return on investment', 'ROI calculator', 'web sites', 'metrics', 'DP management', 'intranets']), ('Generalized confidence sets for a statistically indeterminate random vector\nA problem is considered for the construction of confidence sets for a random\nvector, the information on distribution parameters of which is\nincomplete. To obtain exact estimates and a detailed analysis of the\nproblem, the notion is introduced of a generalized confidence set for a\nstatistically indeterminate random vector. Properties of generalized\nconfidence sets are studied. It is shown that the standard method of\nestimation, which relies on the unification of confidence sets, leads\nin many cases to wider confidence estimates. For a normally distributed\nrandom vector with an inaccurately known mean value, generalized\nconfidence sets are built tip and the dependence of sizes of a\ngeneralized confidence set on the forms and parameters of a set of\npossible mean values is examined\n', ['generalized confidence sets', 'statistically indeterminate random vector', 'distribution parameters', 'normally distributed random vector', 'normal distribution', 'set theory', 'state estimation', 'stochastic systems', 'vectors']), ('Sliding-mode control scheme for a class of continuous chemical reactors\nThe synthesis of a robust control law for regulation control of a class of\nrelative-degree-one nonlinear systems is presented. The control design\nis based on a sliding-mode uncertainty estimator, developed under a\nframework of algebraic-differential concepts. The closed-loop stability\nfor the underlying closed-loop system is achieved via averaging\ntechniques. Robustness of the proposed control scheme is proved in the\nface of noise measurements, model uncertainties and sustained\ndisturbances. The performance of the proposed control law is\nillustrated with numerical simulations, comparing the proposed\ncontroller with a well tuned PI controller\n', ['sliding-mode control scheme', 'continuous chemical reactors', 'robust control law synthesis', 'relative-degree-one nonlinear systems', 'sliding-mode uncertainty estimator', 'algebraic-differential concepts', 'closed-loop stability', 'closed-loop system', 'averaging techniques', 'noise measurements', 'model uncertainties', 'sustained disturbances', 'chemical technology', 'closed loop systems', 'control system synthesis', 'noise', 'nonlinear systems', 'robust control', 'two-term control', 'uncertain systems', 'variable structure systems']), ('Conditions for the local manipulation of Gaussian states\nWe present a general necessary and sufficient criterion for the possibility of\na state transformation from one mixed Gaussian state to another of a\nbipartite continuous-variable system with two modes. The class of\noperations that will be considered is the set of local Gaussian\ncompletely positive trace-preserving maps\n', ['local manipulation', 'Gaussian states', 'state transformation', 'bipartite continuous-variable system', 'trace-preserving maps', 'quantum information theory', 'bound states', 'covariance matrices', 'Gaussian processes', 'information theory', 'quantum communication', 'quantum theory', 'transforms']), ('Minimizing weighted number of early and tardy jobs with a common due window\ninvolving location penalty\nStudies a single machine scheduling problem to minimize the weighted number of\nearly and tardy jobs with a common due window. There are n\nnon-preemptive and simultaneously available jobs. Each job will incur\nan early (tardy) penalty if it is early (tardy) with respect to the\ncommon due window under a given schedule. The window size is a given\nparameter but the window location is a decision variable. The objective\nof the problem is to find a schedule that minimizes the weighted number\nof early and tardy jobs and the location penalty. We show that the\nproblem is NP-complete in the ordinary sense and develop a dynamic\nprogramming based pseudo-polynomial algorithm. We conduct computational\nexperiments, the results of which show that the performance of the\ndynamic algorithm is very good in terms of memory requirement and CPU\ntime. We also provide polynomial time algorithms for two special cases\n', ['early jobs', 'tardy jobs', 'common due window', 'single machine scheduling problem', 'decision variable', 'location penalty', 'NP-complete problem', 'dynamic programming', 'pseudo-polynomial algorithm', 'computational complexity', 'dynamic programming', 'minimisation', 'production control']), ('Becoming a chief librarian: an analysis of transition stages in academic\nlibrary leadership\nThe author explores how the four-part model of transition cycles identified by\nNicholson and West (1988) applies to becoming a chief librarian of an\nacademic library. The four stages: preparation, encounter, adjustment,\nand stabilization, are considered from the micro-, mezzo-, and\nmacrolevels of the organization, as well as for their psychological and\nsocial impact on the new job incumbent. An instrument for assessment of\ntransitional success which could be administered in the adjustment or\nstabilization stage is considered\n', ['chief librarian', 'transition stages', 'academic library leadership', 'organization', 'psychological impact', 'social impact', 'job', 'transition cycles model', 'academic libraries', 'employment', 'human resource management', 'information science', 'personnel', 'professional aspects']), ('Design PID controllers for desired time-domain or frequency-domain response\nPractical requirements on the design of control systems, especially process\ncontrol systems, are usually specified in terms of time-domain\nresponse, such as overshoot and rise time, or frequency-domain\nresponse, such as resonance peak and stability margin. Although\nnumerous methods have been developed for the design of the\nproportional-integral-derivative (PID) controller, little work has been\ndone in relation to the quantitative time-domain and frequency-domain\nresponses. In this paper, we study the following problem: Given a\nnominal stable process with time delay, we design a suboptimal PID\ncontroller to achieve the required time-domain response or\nfrequency-domain response for the nominal system or the uncertain\nsystem. An H/sub infinity / PID controller is developed based on\noptimal control theory and the parameters are derived analytically. Its\nproperties are investigated and compared with that of two developed\nsuboptimal controllers: an H/sub 2/ PID controller and a Maclaurin PID\ncontroller\n', ['time-domain response', 'frequency-domain response', 'process control systems', 'overshoot', 'rise time', 'resonance peak', 'stability margin', 'proportional-integral derivative controller', 'nominal stable process', 'suboptimal controller', 'H/sub infinity / PID controller', 'H/sub 2/ PID controller', 'optimal control', 'Maclaurin PID controller', 'control system synthesis', 'frequency-domain synthesis', 'H/sup infinity / control', 'stability', 'suboptimal control', 'three-term control', 'time-domain synthesis']), ('The average-case identifiability and controllability of large scale systems\nNeeds for increased product quality, reduced pollution, and reduced energy and\nmaterial consumption are driving enhanced process integration. This\nincreases the number of manipulated and measured variables required by\nthe control system to achieve its objectives. This paper addresses the\nquestion of whether processes tend to become increasingly more\ndifficult to identify and control as the process dimension increases.\nTools and results of multivariable statistics are used to show that,\nunder a variety of assumed distributions on the elements, square\nprocesses of higher dimension tend to be more difficult to identify and\ncontrol, whereas the expected controllability and identifiability of\nnonsquare processes depends on the relative numbers of measured and\nmanipulated variables. These results suggest that the procedure of\nsimplifying the control problem so that only a square process is\nconsidered is a poor practice for large scale systems\n', ['large scale systems', 'average-case identifiability', 'average-case controllability', 'enhanced process integration', 'process control', 'multivariable statistics', 'high dimension square processes', 'nonsquare processes', 'measured variables', 'manipulated variables', 'process identification', 'Monte Carlo simulations', 'chemical engineering', 'chemical technology', 'controllability', 'higher order statistics', 'identification', 'large-scale systems', 'Monte Carlo methods', 'multivariable control systems', 'process control', 'transfer functions']), ('Robust self-tuning PID controller for nonlinear systems\nIn this paper, we propose a robust self-tuning PID controller suitable for\nnonlinear systems. The control system employs a preload relay (P_Relay)\nin series with a PID controller. The P_Relay ensures a high gain to\nyield a robust performance. However, it also incurs a chattering\nphenomenon. In this paper, instead of viewing the chattering as an\nundesirable yet inevitable feature, we use it as a naturally occurring\nsignal for tuning and re-tuning the PID controller as the operating\nregime digresses. No other explicit input signal is required. Once the\nPID controller is tuned for a particular operating point, the relay may\nbe disabled and chattering ceases correspondingly. However, it is\ninvoked when there is a change in setpoint to another operating regime.\nIn this way, the approach is also applicable to time-varying systems as\nthe PID tuning can be continuous, based on the latest set of chattering\ncharacteristics. Analysis is provided on the stability properties of\nthe control scheme. Simulation results for the level control of fluid\nin a spherical tank using the scheme are also presented\n', ['nonlinear systems', 'robust self-tuning PID controller', 'preload relay', 'robust performance', 'chattering phenomenon', 'naturally occurring signal', 'controller tuning', 'controller re-tuning', 'relay disabling', 'operating regime', 'time-varying systems', 'continuous tuning', 'stability properties', 'simulation results', 'fluid level control', 'spherical tank', 'adaptive control', 'control system analysis', 'level control', 'nonlinear control systems', 'relay control', 'robust control', 'self-adjusting systems', 'stability', 'three-term control', 'time-varying systems']), ("New investors get steal of a deal [Global Crossing]\nHutchison Telecommunications and Singapore Technologies take control of Global\nCrossing for a lot less money than they originally offered. The deal\nleaves the bankrupt carrier intact, but doesn't put it in the clear\njust yet\n", ['Hutchison Telecommunications', 'Singapore Technologies', 'Global Crossing', 'bankrupt', 'telecommunication']), ('A transmission line fault-location system using the wavelet transform\nThis paper describes the locating system of line-to-ground faults on a power\ntransmission line by using a wavelet transform. The possibility of the\nlocation with the surge generated by a fault has been theoretically\nproposed. In order to make the method practicable, the authors realize\nvery fast processors. They design the wavelet transform and location\nchips, and construct a very fast fault location system by processing\nthe measured data in parallel. This system is realized by a computer\nwith three FPGA processor boards on a PCI bus. The processors are\ncontrolled by UNIX and the system has a graphical user interface with\nan X window system\n', ['power transmission line fault-location system', 'wavelet transform', 'computer simulation', 'line-to-ground faults', 'fault surge generation', 'FPGA processor boards', 'PCI bus', 'UNIX', 'graphic user interface', 'X window system', 'fault location', 'graphical user interfaces', 'power system analysis computing', 'power transmission faults', 'power transmission lines', 'transmission line theory', 'transmission network calculations', 'wavelet transforms']), ('Capturing niche markets with copper\nFor "last-mile access" in niche applications, twisted copper pair may be the\ncable of best option to gain access and deliver desired services. The\narticle discusses how operators can use network edge devices to serve\nnew customers. Niche market segments represent a significant\nopportunity for cable TV delivery of television and high-speed Internet\nsignals. But the existing telecommunications infrastructure in those\ndevelopments frequently presents unique challenges for the service\nprovider to overcome\n', ['last-mile access', 'twisted copper pair', 'network edge devices', 'copper cables', 'niche markets', 'cable television', 'Internet', 'subscriber loops', 'twisted pair cables']), ('Calculation of the probability of survival of an insurance company with\nallowance for the rate of return for a Poisson stream of premiums\nThe probability of survival of an insurance company with the working capital is\ncalculated for a Poisson stream of premiums\n', ['survival probability', 'insurance company', 'return rate', 'Poisson premium stream', 'probability density function', 'economics', 'insurance', 'probability', 'random processes', 'statistical mechanics']), ('Multi-agent collaboration for B2B workflow monitoring\nBusiness-to-business (B2B) application environments are exceedingly dynamic and\ncompetitive. This dynamism is manifested in the form of changing\nprocess requirements and time constraints. However, current workflow\nmanagement technologies have difficulties trying to solve problems,\nsuch as: how to deal with the dynamic nature of B2B commerce processes,\nhow to manage the distributed knowledge and recourses, and how to\nreduce the transaction risk. In this paper, a collaborative multi-agent\nsystem is proposed. Multiple intelligent agents in our system can work\ntogether not only to identify the workflow problems, but also to solve\nsuch problems, by applying business rules, such as re-organizing the\nprocurement and the transaction processes, and making necessary\nworkflow process changes\n', ['multi-agent collaboration', 'B2B workflow monitoring', 'Internet', 'business-to-business applications', 'changing process requirements', 'time constraints', 'workflow management', 'electronic commerce', 'transaction risk', 'business rules', 'electronic commerce', 'Internet', 'multi-agent systems', 'transaction processing', 'workflow management software']), ('Technology on social issues of videoconferencing on the Internet: a survey\nConstant advances in audio/video compression, the development of the multicast\nprotocol as well as fast improvement in computing devices (e.g. higher\nspeed, larger memory) have set forth the opportunity to have resource\ndemanding videoconferencing (VC) sessions on the Internet. Multicast is\nsupported by the multicast backbone (Mbone), which is a special portion\nof the Internet where this protocol is being deployed. Mbone VC tools\nare steadily emerging and the user population is growing fast. VC is a\nfascinating application that has the potential to greatly impact the\nway we remotely communicate and work. Yet, the adoption of VC is not as\nfast as one could have predicted. Hence, it is important to examine the\nfactors that affect a widespread adoption of VC. This paper examines\nthe enabling technology and the social issues. It discusses the\nachievements and identifies the future challenges. It suggests an\nintegration of many emerging multimedia tools into VC in order to\nenhance its versatility for more effectiveness\n', ['videoconferencing', 'Internet', 'multicast protocol', 'multicast backbone', 'Mbone', 'multimedia', 'social issues', 'data compression', 'Internet', 'multimedia communication', 'protocols', 'social aspects of automation', 'teleconferencing']), ('Adaptable dialog boxes for cross-platform programming\nThe author presents a framework for building dialog boxes that adapt to the\nlook and feel of their platform. This method also helps with a few\nrelated problems: specifying cross-platform resources and handling\ndialog size changes due to localization. He uses a combination of XML,\nautomatic layout, and run-time dialog creation to give you most of the\nbenefits of platform-specific resources, without the associated pain.\nSource code with an implementation of the layout engine for Mac OS 9.1\n("Carbon"), Mac OS X, and Microsoft Windows can be downloaded from the\nCUJ website at <www.cuj.com/code>. You can use this code as is,\nor as a starting point for your own more complete implementation\n', ['dialog boxes', 'cross-platform resources', 'dialog size changes', 'localization', 'XML', 'automatic layout', 'run-time dialog creation', 'platform-specific resources', 'Mac OS 9.1', 'Mac OS X', 'Microsoft Windows', 'cross-platform programming', 'adaptable dialog boxes', 'C++ language', 'graphical user interfaces', 'hypermedia markup languages', 'software portability']), ('An optimization based approach to the train operator scheduling problem at\nSingapore MRT\nSingapore Mass Rapid Transit (SMRT) operates two train lines with 83 kilometers\nof track and 48 stations. A total of 77 trains are in operation during\npeak hours and 41 during off-peak hours. We report on an optimization\nbased approach to develop a computerized train-operator scheduling\nsystem that has been implemented at SMRT. The approach involves a\nbipartite matching algorithm for the generation of night duties and a\ntabu search algorithm for the generation of day duties. The system\nautomates the train-operator scheduling process at SMRT and produces\nfavorable schedules in comparison with the manual process. It is also\nable to handle the multiple objectives inherent in the crew scheduling\nsystem. While trying to minimize the system wide crew-related costs,\nthe system is also able to address concern with respect to the number\nof split duties\n', ['optimization based approach', 'Singapore Mass Rapid Transit', 'computerized train-operator scheduling system', 'bipartite matching algorithm', 'night duties', 'tabu search algorithm', 'day duties', 'crew scheduling system', 'rapid transit systems', 'scheduling', 'search problems']), ('Edge-colorings with no large polychromatic stars\nGiven a graph G and a positive integer r, let f/sub r/(G) denote the largest\nnumber of colors that can be used in a coloring of E(G) such that each\nvertex is incident to at most r colors. For all positive integers n and\nr, we determine f/sub r/(K/sub n,n/) exactly and f/sub r/(K/sub n/)\nwithin 1. In doing so, we disprove a conjecture by Y. Manoussakis et\nal. (1996)\n', ['edge colorings', 'polychromatic stars', 'positive integer', 'positive integers', 'graph colouring']), ('The incredible shrinking pipeline\nWe look at the harsh facts concerning the percentage of degrees awarded in CS\nto women. We study the trend of degrees awarded in CS since 1980, and\ncompare the trend in CS to other science and engineering disciplines.\nWe consider the relationship between the percentage of degrees awarded\nto women by a CS department and the college the CS department is\nwithin. We find that CS departments in engineering colleges graduate,\non average, proportionately fewer women than CS departments in\nnon-engineering colleges. We request that the community respond to the\nfacts and speculations presented in this article\n', ['pipeline shrinkage problem', 'women', 'computer science degrees', 'science', 'engineering', 'computer science education', 'gender issues']), ("Succession in standardization: grafting XML onto SGML\nSuccession in standardization is often a problem. The advantages of\nimprovements must be weighed against those of compatibility. If\ncompatibility considerations dominate, a grafting process takes place.\nAccording to our taxonomy of succession, there are three types of\noutcomes. A Type I succession, where grafting is successful, entails\ncompatibility between successors, technical paradigm compliance and\ncontinuity in the standards trajectory. In this paper, we examine\nissues of succession and focus on the Extensible Markup Language (XML).\nIt was to be grafted on the Standard Generalized Markup Language\n(SGML), a stable standard since 1988. However, XML was a profile, a\nsubset and an extension of SGML (1988). Adaptation of SGML was needed\n(SGML 1999) to forge full (downward) compatibility with XML (1998). We\ndescribe the grafting efforts and analyze their outcomes. Our\nconclusion is that although SGML was a technical exemplar for XML\ndevelopers, full compatibility was not achieved. The widespread use of\nHyperText Mark-up Language (HTML) exemplified the desirability of\nsimplicity in XML, standardization. This and HTML's user market largely\nexplain the discontinuity in SGML-XML succession\n", ['XML', 'SGML', 'standardization', 'grafting process', 'Type I succession', 'Extensible Markup Language', 'Standard Generalized Markup Language', 'hypermedia markup languages', 'page description languages', 'standardisation']), ('Applying genetic algorithms to solve the fuzzy optimal profit problem\nThis study investigated the application of genetic algorithms in solving a\nfuzzy optimization problem that arises in business and economics. In\nthis problem, a fuzzy price is determined using a linear or a quadratic\nfuzzy demand function as well as a linear cost function. The objective\nis to find the optimal fuzzy profit, which is derived from the fuzzy\nprice and fuzzy cost. Traditional methods for solving this problem are\n(1) the extension principle, and (2) using interval arithmetic and\nalpha -cuts. However, we argue that traditional methods for solving\nthis problem are too restrictive to produce an optimal solution, and\nthat an alternative approach is possibly needed. We use genetic\nalgorithms to obtain an approximate solution for this fuzzy optimal\nprofit problem without using membership functions. We not only give\nempirical examples to show the effectiveness of this approach, but also\ngive theoretical proofs to validate correctness of the algorithm. We\nconclude that genetic algorithms can produce good approximate solutions\nwhen applied to solve fuzzy optimization problems\n', ['genetic algorithms', 'fuzzy optimization problem', 'fuzzy optimal profit problem', 'economics', 'business', 'fuzzy price', 'quadratic fuzzy demand function', 'linear fuzzy demand function', 'linear cost function', 'approximate solution', 'theoretical proofs', 'algorithm correctness validation', 'economic cybernetics', 'fuzzy logic', 'fuzzy set theory', 'genetic algorithms', 'program verification']), ('The effects of technology on midcareer librarians\nThis article investigates technology competency requirements in the library\nprofession. Using the position advertisements in American Libraries in\nfive-year increments over a twenty-year period (1970-1990), the article\nexamines and evaluates the advertised qualifications of positions and\nattempts to see if midcareer librarians-especially those who have\nachieved their degree prior to the change in MLS curriculum that\ncurrently emphasizes technology-are "effective" librarians in the\npresent and future job market\n', ['midcareer librarians', 'technology competency requirements', 'library profession', 'job market', 'employment', 'library automation', 'personnel']), ('Explicit matrix representation for NURBS curves and surfaces\nThe matrix forms for curves and surfaces were largely promoted in CAD/CAM. In\nthis paper we have presented two matrix representation formulations for\narbitrary degree NURBS curves and surfaces explicitly other than\nrecursively. The two approaches are derived from the computation of\ndivided difference and the Marsden identity respectively. The explicit\ncoefficient matrix of B-spline with equally spaced knot and Bezier\ncurves and surfaces can be obtained by these formulae. The coefficient\nformulae and the coefficient matrix formulae developed in this paper\nexpress non-uniform B-spline functions of arbitrary degree in explicit\npolynomial and matrix forms.. They are useful for the evaluation and\nthe conversion of NURBS curves and surfaces, in CAD/CAM systems\n', ['explicit matrix representation', 'NURBS curves', 'NURBS surfaces', 'CAD/CAM', 'matrix representation formulations', 'divided difference', 'Marsden identity', 'explicit coefficient matrix', 'B-spline', 'equally spaced knot', 'Bezier curves', 'Bezier surfaces', 'coefficient formulae', 'coefficient matrix formulae', 'nonuniform B-spline functions', 'explicit polynomial forms', 'explicit matrix forms', 'CAD/CAM', 'computational geometry', 'matrix algebra', 'splines (mathematics)']), ('Lung metastasis detection and visualization on CT images: a knowledge-based\nmethod\nA solution to the problem of lung metastasis detection on computed tomography\n(CT) scans of the thorax is presented. A knowledge-based top-down\napproach for image interpretation is used. The method is inspired by\nthe manner in which a radiologist and radiotherapist interpret CT\nimages before radiotherapy is planned. A two-dimensional followed by a\nthree-dimensional analysis is performed. The algorithm first detects\nthe thorax contour, the lungs and the ribs, which further help the\ndetection of metastases. Thus, two types of tumors are detected:\nnodules and metastases located at the lung extremities. A method to\nvisualize the anatomical structures segmented is also presented. The\nsystem was tested on 20 patients (988 total images) from the Oncology\nDepartment of La Chaux-de-Fonds Hospital and the results show that the\nmethod is reliable as a computer-aided diagnostic tool for clinical\npurpose in an oncology department\n', ['lung metastasis detection', 'data visualization', 'CT images', 'computed tomography', 'knowledge-based top-down approach', 'two-dimensional analysis', 'three-dimensional analysis', 'computer-aided diagnostic tool', 'oncology', 'medical imaging', 'knowledge representation', 'thorax', 'image interpretation', 'cancer', 'computerised tomography', 'data visualisation', 'image recognition', 'knowledge representation', 'lung', 'medical expert systems', 'medical image processing', 'tumours']), ('A quantum full adder for a scalable nuclear spin quantum computer\nWe demonstrate a strategy for implementation a quantum full adder in a spin\nchain quantum computer. As an example, we simulate a quantum full adder\nin a chain containing 201 spins. Our simulations also demonstrate how\none can minimize errors generated by non-resonant effects\n', ['quantum full adder', 'scalable nuclear spin quantum computer', 'nonresonant effects', 'error minimization', 'adders', 'error compensation', 'minimisation', 'nuclear spin', 'quantum computing', 'quantum gates']), ('Use of web technologies in construction project management: what are the\ncritical success/failure factors?\nA concept of how the World Wide Web (WWW) and its associated technologies can\nbe used to manage construction projects has been recognized by\npractitioners in the construction industry for quite sometime. This\nconcept is often referred to as a Web-Based Project Management System\n(WPMS). It promises, to enhance construction project documentation and\ncontrol, and to revolutionize the way construction project teams\nprocess and transmit project information. WPMS is an electronic\nproject-management system conducted through the Internet. The system\nprovides a centralized, commonly accessible, reliable means of\ntransmitting and storing project information. Project information is\nstored on the server and a standard Web browser is used as the gateway\nto exchange this information, eliminating geographic and hardware\nplatforms boundary\n', ['Web-Based Project Management System', 'construction industry', 'project documentation', 'project control', 'success', 'implementation', 'Web browser', 'construction industry', 'Internet', 'project management']), ('Analysis and efficient implementation of a linguistic fuzzy c-means\nThe paper is concerned with a linguistic fuzzy c-means (FCM) algorithm with\nvectors of fuzzy numbers as inputs. This algorithm is based on the\nextension principle and the decomposition theorem. It turns out that\nusing the extension principle to extend the capability of the standard\nmembership update equation to deal with a linguistic vector has a huge\ncomputational complexity. In order to cope with this problem, an\nefficient method based on fuzzy arithmetic and optimization has been\ndeveloped and analyzed. We also carefully examine and prove that the\nalgorithm behaves in a way similar to the FCM in the degenerate\nlinguistic case. Synthetic data sets and the iris data set have been\nused to illustrate the behavior of this linguistic version of the FCM\n', ['linguistic fuzzy c-means algorithm', 'fuzzy numbers', 'extension principle', 'decomposition theorem', 'computational complexity', 'fuzzy arithmetic', 'optimization', 'linguistic vectors', 'computational complexity', 'fuzzy logic', 'fuzzy set theory', 'pattern recognition', 'vectors']), ('Enterprise in focus at NetSec 2002\nNetSec 2002 took place in San Francisco, amid industry reflection on the\nbalance to be struck between combatting cyber-terrorism and\nsafeguarding civil liberties post-9.11. The author reports on the\npunditry and the pedagogy at the CSI event, focusing on security in the\nenterprise\n', ['NetSec 2002', 'CSI', 'enterprise security', 'business data processing', 'computer network management', 'security of data']), ('Evicting orang utans from the office [electronic storage of legal files]\nHaving espoused the principle of the paperless office some time ago, we decided\nto apply it to our stored files. First we consulted the Law Society\nrules governing storage of files on electronic media. The next step was\nfor us to draw up a protocol for scanning the files. The benefits of\nthe exercise have been significant. The area previously used for\nstorage has been freed for other use. Files are now available online,\ninstantaneously. When we have needed to send out files to the client or\nfollowing a change of solicitor, we have been able to do so almost\nimmediately, by E-mail, retaining a copy for our future reference. The\nfiles are protected from loss or deterioration, back-up copies having\nbeen taken which are stored off site. The complete stored file archive\ncan be put in your pocket (in CD-ROM format) or on a laptop,\nfacilitating remote working\n', ['paperless office', 'legal files', 'electronic storage', 'Law Society rules', 'file scanning', 'CD-ROM', 'file archive', 'CD-ROMs', 'law administration', 'records management', 'storage management']), ('Local satellite\nConsumer based mobile satellite phone services went from boom to burn up in\ntwelve months despite original forecasts predicting 10 million to 40\nmillion users by 2005. Julian Bright wonders what prospects the\ntechnology has now and if going regional might be one answer\n', ['mobile satellite phone services', 'mobile communication', 'satellite communication', 'telephony']), ('Some properties of Hadamard matrices coming from dihedral groups\nH. Kimura (1996) introduced a method to construct Hadamard matrices of degree\n8n + 4 from the dihedral group of order 2n. In this paper we study some\nproperties of this construction\n', ['Hadamard matrices', 'dihedral groups', 'Hadamard matrices']), ("A comparison of computational color constancy algorithms. I: Methodology and\nexperiments with synthesized data\nWe introduce a context for testing computational color constancy, specify our\napproach to the implementation of a number of the leading algorithms,\nand report the results of three experiments using synthesized data.\nExperiments using synthesized data are important because the ground\ntruth is known, possible confounds due to camera characterization and\npre-processing are absent, and various factors affecting color\nconstancy can be efficiently investigated because they can be\nmanipulated individually and precisely. The algorithms chosen for close\nstudy include two gray world methods, a limiting case of a version of\nthe Retinex method, a number of variants of Forsyth's (1990)\ngamut-mapping method, Cardei et al.'s (2000) neural net method, and\nFinlayson et al.'s color by correlation method (Finlayson et al. 1997,\n2001; Hubel and Finlayson 2000) . We investigate the ability of these\nalgorithms to make estimates of three different color constancy\nquantities: the chromaticity of the scene illuminant, the overall\nmagnitude of that illuminant, and a corrected, illumination invariant,\nimage. We consider algorithm performance as a function of the number of\nsurfaces in scenes generated from reflectance spectra, the relative\neffect on the algorithms of added specularities, and the effect of\nsubsequent clipping of the data. All data is available on-line at\nhttp://www.cs.sfu.ca/~color/data, and implementations for most of the\nalgorithms are also available (http://www.cs.sfu.ca/~color/code)\n", ['computational color constancy algorithms', 'synthesized data', 'gray world methods', 'Retinex method', 'gamut-mapping method', 'neural net method', 'color by correlation method', 'chromaticity', 'scene illuminant', 'illumination invariant image', 'algorithm performance', 'reflectance spectra', 'specularities', 'clipping', 'correlation methods', 'image colour analysis', 'neural nets']), ("Electronic data exchange for real estate\nWith HM Land Registry's consultation now underway, no one denies that the\nproperty industry is facing a period of unprecedented change. PISCES\n(Property Information Systems Common Exchange) is a property-focused\nelectronic data exchange standard. The standard is a set of definitions\nand rules to facilitate electronic transfer of data between key\nbusiness areas and between different types of software packages that\nare used regularly by the property industry. It is not itself a piece\nof software but an enabling technology that allows software providers\nto prepare solutions within their own packages to transfer data between\ndatabases. This provides the attractive prospect of seamless transfer\nof data within and between systems and organisations\n", ['HM Land Registry', 'property industry', 'PISCES', 'Property Information Systems Common Exchange', 'electronic data exchange', 'standard', 'software packages', 'databases', 'seamless transfer', 'electronic data interchange', 'information systems', 'real estate data processing', 'standards']), ("Look who's talking [voice recognition]\nVoice recognition could be the answer to the problem of financial fraud, but in\nthe world of biometric technology, money talks\n", ['voice recognition', 'financial fraud', 'biometric', 'cost', 'banking', 'biometrics (access control)', 'fraud', 'speaker recognition']), ('Defining electronic librarianship: a content analysis of job advertisements\nAdvances in technology create dramatic changes within libraries. The complex\nissues surrounding this new electronic, end-user environment have major\nramifications and require expert knowledge. Electronic services\nlibrarians and electronic resources librarians are two specialized\ntitles that have recently emerged within the field of librarianship to\nfill this niche. Job advertisements listed in American Libraries from\nJanuary 1989 to December 1998 were examined to identify\nresponsibilities, qualifications, organizational and salary information\nrelating to the newly emerging role of electronic librarian\n', ['electronic librarianship', 'content analysis', 'job advertisements', 'electronic end-user environment', 'electronic resources librarians', 'electronic services librarians', 'American Libraries', 'responsibilities', 'qualifications', 'organizational information', 'salary information', 'employment', 'information science', 'library automation', 'personnel', 'salaries']), ('Efficient combinational verification using overlapping local BDDs and a hash\ntable\nWe propose a novel methodology that combines local BDDs (binary decision\ndiagrams) with a hash table for very efficient verification of\ncombinational circuits. The main purpose of this technique is to remove\nthe considerable overhead associated with case-by-case verification of\ninternal node pairs in typical internal correspondence based\nverification methods. Two heuristics based on the number of structural\nlevels of circuitry looked at and the total number of nodes in the BDD\nmanager are used to control the BDD sizes and introduce new cutsets\nbased on already found equivalent nodes. We verify the ISCAS85\nbenchmark circuits and demonstrate significant speedup over existing\nmethods. We also verify several hard industrial circuits and show our\nsuperiority in extracting internal equivalences\n', ['combinational verification', 'overlapping local BDDs', 'hash table', 'combinational circuit verification', 'case-by-case verification', 'internal node pairs', 'internal correspondence based verification', 'heuristics', 'structural levels', 'BDD manager', 'BDD sizes', 'cutsets', 'ISCAS85 benchmark circuits', 'hard industrial circuits', 'internal equivalences', 'formal verification', 'internal correspondence-based verification', 'binary decision diagrams', 'binary decision diagrams', 'circuit testing', 'combinational circuits', 'formal verification', 'logic CAD', 'logic testing']), ('Robustness of trajectories with finite time extent\nThe problem of estimating perturbation bounds of finite trajectories is\nconsidered. The trajectory is assumed to be generated by a linear\nsystem with uncertainty characterized in terms of integral quadratic\nconstraints. It is shown that such perturbation bounds can be obtained\nas the solution to a nonconvex quadratic optimization problem, which\ncan be addressed using Lagrange relaxation. The result can be used in\nrobustness analysis of hybrid systems and switched dynamical systems\n', ['trajectories robustness', 'finite time extent', 'perturbation bounds', 'linear system', 'uncertainty', 'integral quadratic constraints', 'nonconvex quadratic optimization problem', 'Lagrange relaxation', 'robustness analysis', 'hybrid systems', 'switched dynamical systems', 'invariance', 'linear systems', 'optimal control', 'optimisation', 'robust control', 'uncertain systems']), ('A min-max theorem on feedback vertex sets\nWe establish a necessary and sufficient condition for the linear system {x : Hx\n>or= e, x >or= 0} associated with a bipartite tournament to be\ntotally dual integral, where H is the cycle-vertex incidence matrix and\ne is the all-one vector. The consequence is a min-max relation on\npacking and covering cycles, together with strongly polynomial time\nalgorithms for the feedback vertex set problem and the cycle packing\nproblem on the corresponding bipartite tournaments. In addition, we\nshow that the feedback vertex set problem on general bipartite\ntournaments is NP-complete and approximable within 3.5 based on the\nmin-max theorem\n', ['feedback vertex sets', 'min-max theorem', 'necessary sufficient condition', 'linear system', 'bipartite tournament', 'totally dual integral system', 'cycle-vertex incidence matrix', 'all-one vector', 'covering cycles', 'strongly polynomial time algorithms', 'cycle packing problem', 'feedback vertex set problem', 'NP-complete problem', 'graphs', 'combinatorial optimization problems', 'linear programming duality theory', 'bin packing', 'duality (mathematics)', 'feedback', 'graph theory', 'linear programming', 'linear systems', 'minimax techniques', 'polynomial approximation']), ('A summary of methods applied to tool condition monitoring in drilling\nPresents a summary of the monitoring methods, signal analysis and diagnostic\ntechniques for tool wear and failure monitoring in drilling that have\nbeen tested and reported in the literature. The paper covers only\nindirect monitoring methods such as force, vibration and current\nmeasurements. Signal analysis techniques cover all the methods that\nhave been used with indirect measurements including e.g. statistical\nparameters and Fast Fourier and Wavelet Transform. Only a limited\nnumber of automatic diagnostic tools have been developed for diagnosis\nof the condition of the tool in drilling. All of these rather diverse\napproaches that have been available are covered in this study. Only in\na few of the papers have attempts been made to compare the chosen\napproach with other methods. Many of the papers only present one\napproach and unfortunately quite often the test material of the study\nis limited especially in what comes to the cutting process parameter\nvariation and also workpiece material\n', ['tool condition monitoring', 'drilling', 'monitoring methods', 'signal analysis', 'diagnostic techniques', 'tool wear', 'failure monitoring', 'indirect monitoring methods', 'force measurements', 'vibration measurements', 'current measurements', 'statistical parameters', 'fast Fourier transform', 'wavelet transform', 'automatic diagnostic tools', 'acoustic emission', 'condition monitoring', 'diagnostic expert systems', 'electric current measurement', 'fast Fourier transforms', 'force measurement', 'machine tools', 'machining', 'neural nets', 'process monitoring', 'signal processing', 'statistical analysis', 'vibration measurement', 'wavelet transforms', 'wear']), ('The role of B2B engines in B2B integration architectures\nSemantic B2B integration architectures must enable enterprises to communicate\nstandards-based B2B events like purchase orders with any potential\ntrading partner. This requires not only back end application\nintegration capabilities to integrate with e.g. enterprise resource\nplanning (ERP) systems as the company-internal source and destination\nof B2B events, but also a capability to implement every necessary B2B\nprotocol like electronic data interchange (EDI), RosettaNet as well as\nmore generic capabilities like Web services (WS). This paper shows the\nplacement and functionality of B2B engines in semantic B2B integration\narchitectures that implement a generic framework for modeling and\nexecuting any B2B protocol. A detailed discussion shows how a B2B\nengine can provide the necessary abstractions to implement any\nstandard-based B2B protocol or any trading partner specific\nspecialization\n', ['B2B engines', 'semantic B2B integration architectures', 'standards-based B2B event communication', 'purchase orders', 'trading partner', 'ERP systems', 'EDI', 'RosettaNet', 'Web services', 'modeling', 'business communication', 'electronic commerce', 'electronic data interchange', 'Internet', 'protocols']), ('Teaching management science with spreadsheets: From decision models to decision\nsupport\nThe 1990s were a decade of enormous change for management science (MS)\neducators. While the outlook at the beginning of the decade was\nsomewhat bleak, the renaissance in MS education brought about by the\nuse of spreadsheets as the primary delivery vehicle for quantitative\nmodeling techniques has resulted in a much brighter future. This paper\ntakes inventory of the current state of MS education and suggests some\npromising new directions in the area of decision support systems for MS\neducators to consider for the future\n', ['management science', 'MS education', 'spreadsheets', 'quantitative modeling', 'decision support systems', 'business data processing', 'computer science education', 'decision support systems', 'management science', 'spreadsheet programs']), ('Adaptive array antenna based on radial basis function network as multiuser\ndetection for WCDMA\nAn adaptive array antenna is proposed based on the radial basis function (RBF)\nnetwork as a multiuser detector for a WCDMA system. The proposed system\ncalculates the optimal combining weight coefficients using sample\nmatrix inversion with a common correlation matrix algorithm and obtains\nthe channel response vector using the RBF output signal\n', ['adaptive array antenna', 'radial basis function network', 'multiuser detection', 'W-CDMA', 'RBF network', 'optimal combining weight coefficients', 'sample matrix inversion', 'correlation matrix algorithm', 'channel response vector', 'Wideband code division multiple access', 'adaptive antenna arrays', 'adaptive signal detection', 'code division multiple access', 'multiuser channels', 'radial basis function networks']), ('Tools for the analysis of dose optimization. I. Effect-volume histogram\nWith the advent of dose optimization algorithms, predominantly for\nintensity-modulated radiotherapy (IMRT), computer software has\nprogressed beyond the point of being merely a tool at the hands of an\nexpert and has become an active, independent mediator of the dosimetric\nconflicts between treatment goals and risks. To understand and control\nthe internal decision finding as well as to provide means to influence\nit, a tool for the analysis of the dose distribution is presented which\nreveals the decision-making process performed by the algorithm. The\ninternal trade-offs between partial volumes receiving high or low doses\nare driven by functions which attribute a weight to each volume\nelement. The statistics of the distribution of these weights is cast\ninto an effect-volume histogram (EVH) in analogy to dose-volume\nhistograms. The analysis of the EVH reveals which traits of the optimum\ndose distribution result from the defined objectives, and which are a\nrandom consequence of under- or misspecification of treatment goals.\nThe EVH can further assist in the process of finding suitable\nobjectives and balancing conflicting objectives. If biologically\ninspired objectives are used, the EVH shows the distribution of local\ndose effect relative to the prescribed level\n', ['dose optimization algorithms', 'effect-volume histogram', 'intensity-modulated radiotherapy', 'dosimetric conflicts', 'treatment goals', 'treatment risks', 'computer software', 'decision-making process', 'partial volumes', 'high doses', 'low doses', 'volume element weights', 'treatment planning', 'objective function', 'insufficient target coverage', 'exponential law', 'cell survival', 'one-sided quadratic penalties', 'quadratic overdose penalty', 'decision theory', 'dosimetry', 'graphs', 'medical computing', 'optimisation', 'radiation therapy', 'software tools']), ("Interaction and presence in the clinical relationship: virtual reality (VR) as\ncommunicative medium between patient and therapist\nThe great potential offered by virtual reality (VR) to clinical psychologists\nderives prevalently from the central role, in psychotherapy, occupied\nby the imagination and by memory. These two elements, which are\nfundamental in our life, present absolute and relative limits to the\nindividual potential. Using VR as an advanced imaginal system, an\nexperience that is able to reduce the gap existing between imagination\nand reality, it is possible to transcend these limits. In this sense,\nVR can improve the efficacy of a psychological therapy for its\ncapability of reducing the distinction between the computer's reality\nand the conventional reality. Two are the core characteristics of this\nsynthetic imaginal experience: the perceptual illusion of nonmediation\nand the possibility of building and sharing a common ground. In this\nsense, experiencing presence in a clinical virtual environment (VE),\nsuch as a shared virtual hospital, requires more than reproduction of\nthe physical features of external reality. It requires the creation and\nsharing of the cultural web that makes meaningful, and therefore\nvisible, both people and objects populating the environment. The paper\noutlines a framework for supporting the development and tuning of\nclinically oriented VR systems\n", ['virtual reality', 'patient-therapist communication', 'clinical psychology', 'psychotherapy', 'imagination', 'memory', 'presence', 'psychological therapy', 'clinical virtual environment', 'shared virtual hospital', 'medical computing', 'patient treatment', 'psychology', 'user interfaces', 'virtual reality']), ('Network-centric systems\nThe author describes a graduate-level course that addresses cutting-edge issues\nin network-centric systems while following a more traditional graduate\nseminar format\n', ['network-centric systems', 'graduate level course', 'computer networks', 'computer science education', 'educational courses']), ("A digital-driving system for smart vehicles\nIn the wake of the computer and information technology revolutions, vehicles\nare undergoing dramatic changes in their capabilities and how they\ninteract with drivers. Although some vehicles can decide to either\ngenerate warnings for the human driver or control the vehicle\nautonomously, they must usually make these decisions in real time with\nonly incomplete information. So, human drivers must still maintain\ncontrol over the vehicle. I sketch a digital driving behavior model. By\nsimulating and analyzing driver behavior during different maneuvers\nsuch as lane changing, lane following, and traffic avoidance,\nresearchers participating in the Beijing Institute of Technology's\ndigital-driving project will be able to examine the possible\ncorrelations or causal relations between the smart vehicle, IVISs, the\nintelligent road-traffic-information network, and the driver. We aim to\nsuccessfully demonstrate that a digital-driving system can provide a\ndirection for developing human-centered smart vehicles\n", ['digital driving system', 'human-centered smart vehicles', 'in-vehicle information systems', 'intelligence', 'intelligent driver-vehicle interface', 'ecological driver-vehicle interface', 'vehicle control', 'interactive communication', 'intelligent road traffic information network', 'intelligent transportation systems', 'maneuvers', 'traffic avoidance', 'lane following', 'lane changing', 'automated highways', 'driver information systems', 'intelligent control', 'road vehicles', 'user interfaces']), ('More than the money [software project]\nExperiences creating budgets for large software projects have taught\nmanufacturers that it is not about the money - it is about what one\nreally needs. Before a company can begin to build a budget for a\nsoftware. project, it has to have a good understanding of what business\nissues need to be addressed and what the business objectives are. This\nstep is critical because it defines the business goals, outlines the\nmetrics for success, sets the scope for the project, and defines the\ncriteria for selecting the right software\n', ['software projects', 'manufacturing industry', 'budgeting', 'management', 'software requirements', 'budgeting', 'manufacturing data processing', 'project management', 'software development management']), ('Noise-constrained hyperspectral data compression\nStorage and transmission requirements for hyperspectral data sets are\nsignificant. To reduce hardware costs, well-designed compression\ntechniques are needed to preserve information content while maximizing\ncompression ratios. Lossless compression techniques maintain data\nintegrity, but yield small compression ratios. We present a slightly\nlossy compression algorithm that uses the noise statistics of the data\nto preserve information content while maximizing compression ratios.\nThe adaptive principal components analysis (APCA) algorithm uses noise\nstatistics to determine the number of significant principal components\nand selects only those that are required to represent each pixel to\nwithin the noise level. We demonstrate the effectiveness of these\nmethods with airborne visible/infrared spectrometer (AVIRIS),\nhyperspectral digital imagery collection experiment (HYDICE),\nhyperspectral mapper (HYMAP), and Hyperion datasets\n', ['noise-constrained hyperspectral data compression', 'storage requirements', 'transmission requirements', 'hyperspectral data sets', 'hardware costs', 'information content', 'compression ratios', 'lossless compression techniques', 'data integrity', 'slightly lossy compression algorithm', 'noise statistics', 'adaptive principal components analysis algorithm', 'noise level', 'airborne visible/infrared spectrometer hyperspectral digital imagery collection experiment', 'AVIRIS HYDICE', 'hyperspectral mapper', 'HYMAP', 'Hyperion datasets', 'Gaussian statistics', 'adaptive signal processing', 'data compression', 'Gaussian noise', 'image coding', 'infrared spectroscopy', 'principal component analysis', 'remote sensing', 'spectral analysis', 'visible spectroscopy']), ('Computer-mediated communication and remote management: integration or\nisolation?\nThe use of intranets and e-mails to communicate with remote staff is increasing\nrapidly within organizations. For many companies this is viewed as a\nspeedy and cost-effective way of keeping in contact with staff and\nensuring their continuing commitment to company goals. This article\nhighlights the problems experienced by staff when managers use\nintranets and e-mails in an inappropriate fashion for these purposes.\nIssues of remoteness and isolation are discussed, along with the\nreports of frustration and disidentification experienced. However, it\nwill be shown that when used appropriately, communication using these\ntechnologies can facilitate shared understanding and help remote staff\nto view their company as alive and exciting. Theoretical aspects are\nhighlighted and the implications of these findings are discussed\n', ['computer-mediated communication', 'remote management', 'intranets', 'e-mails', 'remote staff', 'organizations', 'companies', 'cost-effective', 'managers', 'remoteness', 'electronic mail', 'home working', 'human factors', 'intranets', 'personnel', 'psychology', 'social aspects of automation']), ('Quasi-Newton algorithm for adaptive minor component extraction\nAn adaptive quasi-Newton algorithm is first developed to extract a single minor\ncomponent corresponding to the smallest eigenvalue of a stationary\nsample covariance matrix. A deflation technique instead of the commonly\nused inflation method is then applied to extract the higher-order minor\ncomponents. The algorithm enjoys the advantage of having a simpler\ncomputational complexity and a highly modular and parallel structure\nfor efficient implementation. Simulation results are given to\ndemonstrate the effectiveness of the proposed algorithm for extracting\nmultiple minor components adaptively\n', ['quasi-Newton algorithm', 'adaptive minor component extraction', 'eigenvalue', 'stationary sample covariance matrix', 'deflation technique', 'higher-order minor components', 'computational complexity', 'modular structure', 'parallel structure', 'simulation results', 'adaptive estimation', 'DOA estimation', 'ROOT-MUSIC estimator', 'adaptive estimation', 'adaptive signal processing', 'computational complexity', 'covariance matrices', 'direction-of-arrival estimation', 'eigenvalues and eigenfunctions', 'Newton method']), ("Challenges and trends in discrete manufacturing\nOver 50 years ago, the 100,000 workers at Ford's Rouge automobile factory\nturned out 1200 cars per day. Nowadays, Ford's plant on that same site\nstill produces 800 cars each day but with just 3000 workers. Similar\nstories abound in the manufacturing industries; technology revolution\nand evolution; a shift from vertical integration, better business and\nproduction practices and improved industrial relations-all have changed\nmanufacturing beyond recognition. So what are the current challenges\nand trends in manufacturing? Certainly, the relentless advance of\ntechnology will continue, as will user pressure for more customized\ndesign or improved environmental friendliness. Some trends are already\nwith us and more, as yet indiscernible, will come. But one major,\nfundamental shift now resounding throughout industry is the way in\nwhich information involving every single aspect of the manufacturing\nprocess is being integrated into one seamless system\n", ['discrete manufacturing', 'challenges', 'automobile factory', 'technology revolution', 'technology evolution', 'business practices', 'production practices', 'industrial relations', 'trends', 'seamless manufacturing process', 'automobile industry', 'manufacturing industries', 'process control']), ('Influence of the process design on the control strategy: application in\nelectropneumatic field\nThis article proposes an example of electropneumatic system where the\narchitecture of the process is modified with respect to both the\nspecifications for position and velocity tracking and a criterion\nconcerning the energy consumption. Experimental results are compared\nand analyzed using an industrial bench test. For this, a complete model\nof the system is presented, and two kinds of nonlinear control laws are\ndeveloped, a monovariable and multivariable type based on the flatness\ntheory\n', ['electropneumatic systems', 'positioning systems', 'position control', 'monovariable control', 'multivariable control', 'velocity control', 'tracking', 'energy consumption', 'nonlinear control', 'flatness theory', 'electropneumatic control equipment', 'machine tools', 'nonlinear control systems', 'position control', 'tracking', 'velocity control']), ('Fuzzy logic controlled shunt active power filter for power quality improvement\nThe simulation and experimental study of a fuzzy logic controlled, three-phase\nshunt active power filter to improve power quality by compensating\nharmonics and reactive power required by a nonlinear load is presented.\nThe advantage of fuzzy control is that it is based on a linguistic\ndescription and does not require a mathematical model of the system.\nThe fuzzy control scheme is realised on an inexpensive dedicated\nmicro-controller (INTEL 8031) based system. The compensation process is\nbased on sensing line currents only, an approach different from\nconventional methods, which require harmonics or reactive volt-ampere\nrequirement of the load. The performance of the fuzzy logic controller\nis compared with a conventional PI controller. The dynamic behavior of\nthe fuzzy controller is found to be better than the conventional PI\ncontroller. PWM pattern generation is based on carrierless hysteresis\nbased current control to obtain the switching signals. Various\nsimulation and experimental results are presented under steady state\nand transient conditions\n', ['three-phase shunt active power filter', 'harmonics compensation', 'reactive power compensation', 'nonlinear load', 'micro-controller', 'PWM pattern generation', 'carrierless hysteresis based current control', 'switching signals', 'power quality improvement', 'fuzzy logic control simulation', 'control performance', 'active filters', 'compensation', 'control system analysis', 'fuzzy control', 'microcontrollers', 'power harmonic filters', 'power supply quality', 'power system control', 'power system harmonics', 'reactive power control']), ('Prediction and compensation of dynamic errors for coordinate measuring machines\nCoordinate measuring machines (CMMs) are already widely utilized as measuring\ntools in the modem manufacturing industry. Rapidly approaching now is\nthe trend for next-generation CMMs. However, the increases in measuring\nvelocity of CMM applications are limited by dynamic errors that occur\nin CMMs. In this paper a systematic approach for modeling the dynamic\nerrors of a touch-trigger probe CMM is developed through theoretical\nanalysis and experimental study. An overall analysis of the dynamic\nerrors of CMMs is conducted, with weak components of the CMM identified\nby a laser interferometer. The probing process, as conducted with a\ntouch-trigger probe, is analyzed. The dynamic errors are measured,\nmodeled, and predicted using neural networks. The results indicate\nthat, using this mode, it is possible to compensate for the dynamic\nerrors of CMMs\n', ['coordinate measuring machines', 'dynamic errors', 'inertial forces', 'touch-trigger probe', 'laser interferometer', 'neural networks', 'manufacturing industry', 'compensation', 'automatic test equipment', 'error compensation', 'light interferometers', 'machine tools', 'neural nets', 'probes']), ('Electrical facility construction work for information network structuring by\nthe use of sewage conduits\nTo confront the advent of the advanced information society, there has been a\npressing demand for the adjustment of the communications infrastructure\nand the structuring of the information network by utilizing the sewage\nconduits. The City of Tokyo is promoting a project by the name of the\nsewer optical fiber teleway (SOFT) network plan. According to this\nplan, the total distance of the optical fiber network laid in the sewer\nconduits is scheduled to reach about 470 km by the end of March 2000.\nAt the final stage, this distance will reach 800 km as a whole. We\ncompleted the construction work for the information control facilities\nscattered in 11 places inclusive of the Treatment Site S, with the\nintention to adjust and extend the information transmission network\nlaid through the above-mentioned optical fiber network, to be used\nexclusively by the Bureau of Sewerage. This construction work is\ndescribed in the paper\n', ['electrical facility construction work', 'information network structuring', 'sewage conduits', 'communications infrastructure', 'Tokyo', 'sewer optical fiber teleway network plan', 'information control facilities', 'Treatment Site S', 'information transmission network', 'Bureau of Sewerage', 'asynchronous transmission mode switches', 'ATM switches', 'information networks', 'optical cables', 'optical fibre networks', 'water treatment']), ("Lob's theorem as a limitation on mechanism\nWe argue that Lob's Theorem implies a limitation on mechanism. Specifically, we\nargue, via an application of a generalized version of Lob's Theorem,\nthat any particular device known by an observer to be mechanical cannot\nbe used as an epistemic authority (of a particular type) by that\nobserver: either the belief-set of such an authority is not\nmechanizable or, if it is, there is no identifiable formal system of\nwhich the observer can know (or truly believe) it to be the\ntheorem-set. This gives, we believe, an important and hitherto\nunnoticed connection between mechanism and the use of authorities by\nhuman-like epistemic agents\n", ['Lob Theorem', 'limitation on mechanism', 'epistemic authority', 'belief-set', 'formal system', 'theorem-set', 'human-like epistemic agents', 'belief maintenance', 'cognitive systems', 'inference mechanisms']), ('Correlation of intuitionistic fuzzy sets by centroid method\nIn this paper, we propose a method to calculate the correlation coefficient of\nintuitionistic fuzzy sets by means of "centroid". This value obtained\nfrom our formula tell us not only the strength of relationship between\nthe intuitionistic fuzzy sets, but also whether the intuitionistic\nfuzzy sets are positively or negatively related. This approach looks\nbetter than previous methods which only evaluate the strength of the\nrelation. Furthermore, we extend the "centroid" method to\ninterval-valued intuitionistic fuzzy sets. The value of the correlation\ncoefficient between interval-valued intuitionistic fuzzy sets lies in\nthe interval [-1, 1], as computed from our formula\n', ['correlation coefficient', 'intuitionistic fuzzy sets', 'centroid method', 'interval-valued intuitionistic fuzzy sets', 'fuzzy set theory']), ("The AT89C51/52 flash memory programmers\nWhen faced with a plethora of applications to design, it's essential to have a\nversatile microcontroller in hand. The author describes the AT89C51/52\nmicrocontrollers. To get you started, he'll describe his inexpensive\nmicrocontroller programmer\n", ['AT89C51/52', 'flash memory programmers', 'microcontrollers', 'device programmer', 'microcontroller programmer', 'microcontrollers', 'programming']), ("Implementing: it's all about processes\nLooks at how the key to successful technology deployment can be found in a set\nof four basic disciplines\n", ['technology deployment', 'implementation', 'incremental targets', 'third-party integration', 'vendor-supplied hardware integration services', 'vendor-supplied software integration services', 'manufacturers', 'management', 'manufacturing industries']), ("Multiple comparison methods for means\nMultiple comparison methods (MCMs) are used to investigate differences between\npairs of population means or, more generally, between subsets of\npopulation means using sample data. Although several such methods are\ncommonly available in statistical software packages, users may be\npoorly informed about the appropriate method(s) to use and/or the\ncorrect way to interpret the results. This paper classifies the MCMs\nand presents the important methods for each class. Both simulated and\nreal data are used to compare the methods, and emphasis is placed on a\ncorrect application and interpretation. We include suggestions for\nchoosing the best method. Mathematica programs developed by the authors\nare used to compare MCMs. By taking the advantage of Mathematica's\nnotebook structure, all interested student can use these programs to\nexplore the subject more deeply\n", ['multiple comparison procedures', 'population means', 'error rate', 'single-step procedures', 'step-down procedures', 'sales management', 'pack-age design', 'marketing', 'sales management', 'sampling methods']), ("Using Internet search engines to estimate word frequency\nThe present research investigated Internet search engines as a rapid,\ncost-effective alternative for estimating word frequencies. Frequency\nestimates for 382 words were obtained and compared across four methods:\n(1) Internet search engines, (2) the Kucera and Francis (1967) analysis\nof a traditional linguistic corpus, (3) the CELEX English linguistic\ndatabase (Baayen et al., 1995), and (4) participant ratings of\nfamiliarity. The results showed that Internet search engines produced\nfrequency estimates that were highly consistent with those reported by\nKucera and Francis and those calculated from CELEX, highly consistent\nacross search engines, and very reliable over a 6 month period of time.\nAdditional results suggested that Internet search engines are an\nexcellent option when traditional word frequency analyses do not\ncontain the necessary data (e.g., estimates for forenames and slang).\nIn contrast, participants' familiarity judgments did not correspond\nwell with the more objective estimates of word frequency. Researchers\nare advised to use search engines with large databases (e.g.,\nAltaVista) to ensure the greatest representativeness of the frequency\nestimates\n", ['Internet search engines', 'word frequency estimation', 'linguistic corpus', 'CELEX English linguistic database', 'participant familiarity ratings', 'large databases', 'Internet', 'linguistics', 'psychology', 'search engines']), ('A GRASP heuristic for the mixed Chinese postman problem\nArc routing problems (ARPs) consist of finding a traversal on a graph\nsatisfying some conditions related to the links of the graph. In the\nChinese postman problem (CPP) the aim is to find a minimum cost tour\n(closed walk) traversing all the links of the graph at least once. Both\nthe Undirected CPP, where all the links are edges that can be traversed\nin both ways, and the Directed CPP, where all the links are arcs that\nmust be traversed in a specified way, are known to be polynomially\nsolvable. However, if we deal with a mixed graph (having edges and\narcs), the problem turns out to be NP-hard. In this paper, we present a\nheuristic algorithm for this problem, the so-called Mixed CPP (MCPP),\nbased on greedy randomized adaptive search procedure (GRASP)\ntechniques. The algorithm has been tested and compared with other known\nand recent methods from the literature on a wide collection of randomly\ngenerated instances, with up to 200 nodes and 600 links, producing\nencouraging computational results. As far as we know, this is the best\nheuristic algorithm for the MCPP, with respect to solution quality,\npublished up to now\n', ['mixed Chinese postman problem', 'GRASP heuristic', 'arc routing problems', 'graph traversal', 'minimum cost tour', 'closed walk', 'NP-hard problem', 'heuristic algorithm', 'greedy randomized adaptive search procedure', 'optimization problems', 'metaheuristics', 'graph theory', 'heuristic programming', 'operations research', 'optimisation', 'search problems']), ("Use of natural language processing to translate clinical information from a\ndatabase of 889,921 chest radiographic reports\nThe aim was to evaluate translation of chest radiographic reports using natural\nlanguage processing and to compare the findings with those in the\nliterature. A natural language processor coded 10 years of narrative\nchest radiographic reports from an urban academic medical center.\nCoding for 150 reports was compared with manual coding. Frequencies and\ncooccurrences of 24 clinical conditions (diseases, abnormalities, and\nclinical states) were estimated. The ratio of right to left lung mass,\nassociation of pleural effusion with other conditions, and frequency of\nbullet and stab wounds were compared with independent observations. The\nsensitivity and specificity of the system's pneumothorax coding were\ncompared with those of manual financial coding. Internal and external\nvalidation in this study confirmed the accuracy of natural language\nprocessing for translating chest radiographic narrative reports into a\nlarge database of information\n", ['natural language processing', 'chest radiographic report database', 'clinical information translation', 'urban academic medical center', 'clinical condition frequency', 'clinical condition cooccurrence', 'right to left lung mass ratio', 'pleural effusion', 'bullet wounds', 'stab wounds', 'pneumothorax coding', 'diagnostic radiography', 'lung', 'medical computing', 'medical information systems', 'natural languages']), ('Banking on SMA funds [separately managed accounts]\nFrom investment management to technology to back-office services, outsourcers\nare elbowing their way into the SMA business. Small banks are paying\nattention-and hoping to reap the rewards\n', ['separately managed accounts', 'investment management', 'technology', 'back-office services', 'outsourcers', 'small banks', 'banking', 'investment', 'outsourcing']), ('Error-probability analysis of MIL-STD-1773 optical fiber data buses\nWe have analyzed the error probabilities of MIL-STD-1773 optical fiber data\nbuses with three modulation schemes, namely, original Manchester II\nbi-phase coding, PTMBC, and EMBC-BSF. Using these derived expressions\nof error probabilities, we can also compare the receiver sensitivities\nof such optical fiber data buses\n', ['error probabilities', 'optical fiber data buses', 'modulation schemes', 'Manchester bi-phase coding', 'receiver sensitivities', 'codes', 'error statistics', 'military systems', 'optical fibre networks', 'optical modulation', 'optical receivers', 'sensitivity', 'system buses']), ('Kontiki. Shortcuts for content\'s trip to the edge\nWhen electronic files get zapped from one location to another, you probably\naren\'t thinking about the physical distance they must travel-or how\nthat distance might affect the time it takes to get there. But if you\nwork for CDN company Kontiki, this is just about all you think about.\nChampioning a P2P-like "bandwidth harvesting" technology, Kontiki has\nfigured out how to not only quickly distribute content to the "edge"\nbut to utilize a combination of centralized servers and a network of\nenduser machines to collect, or "harvest," underutilized bandwidth and\nmake redundant file requests more efficient\n', ['electronic files', 'Kontiki', 'P2P-like bandwidth harvesting technology', 'centralized servers', 'enduser machines', 'redundant file requests', 'underutilized bandwidth', 'electronic data interchange', 'Internet']), ('New thinking on rendering\nLooks at how graphics hardware solves a range of rendering problems\n', ['rendering', 'graphics hardware', 'programmability', 'Gourand-shaded image', 'color values', 'add-on boards', 'CAD', 'computer graphic equipment', 'rendering (computer graphics)']), ('A novel robot hand with embedded shape memory alloy actuators\nDescribes the development of an active robot hand, which allows smooth and\nlifelike motions for anthropomorphic grasping and fine manipulations.\nAn active robot finger 10 mm in outer diameter with a shape memory\nalloy (SMA) wire actuator embedded in the finger with a constant\ndistance from the geometric centre of the finger was designed and\nfabricated. The practical specifications of the SMA wire and the\nflexible rod were determined on the basis of a series of formulae. The\nactive finger consists of two bending parts, the SMA actuators and a\nconnecting part. The mechanical properties of the bending part are\ninvestigated. The control system on the basis of resistance feedback is\nalso presented. Finally, a robot hand with three fingers was designed\nand the grasping experiment was carried out to demonstrate its\nperformance\n', ['embedded shape memory alloy actuators', 'lifelike motions', 'anthropomorphic grasping', 'fine manipulations', 'active finger', 'resistance feedback', 'flexible rod', 'active robot hand', 'bending', 'intelligent actuators', 'manipulators', 'position control', 'shape memory effects']), ('Project scheduling under time dependent costs-a branch and bound algorithm\nIn a given project network, execution of each activity in normal duration\nrequires utilization of certain resources. If faster execution of an\nactivity is desired then additional resources at extra cost would be\nrequired. Given a project network, the cost structure for each activity\nand a planning horizon, the project compression problem is concerned\nwith the determination of optimal schedule of performing each activity\nwhile satisfying given restrictions and minimizing the total cost of\nproject execution. The paper considers the project compression problem\nwith time dependent cost structure for each activity. The planning\nhorizon is divided into several regular time intervals over which the\ncost structure of an activity may vary. But the cost structure of the\nactivities remains the same within a time interval. The objective is to\nfind an optimal project schedule minimizing the total project cost. We\npresent a mathematical model for this problem, develop some heuristics\nand an exact branch and bound algorithm. Using simulated problems we\nprovide an insight into the computational performances of heuristics\nand the branch and bound algorithm\n', ['project scheduling', 'time dependent costs', 'branch and bound algorithm', 'project network', 'planning horizon', 'project compression problem', 'optimal schedule', 'heuristics', 'decision theory', 'integer programming', 'minimisation', 'nonlinear programming', 'project management', 'scheduling', 'tree searching']), ('Conditions for decentralized integral controllability\nThe term decentralized integral controllability (DIC) pertains to the existence\nof stable decentralized controllers with integral action that have\nclosed-loop properties such as stable independent detuning. It is\nespecially useful to select control structures systematically at the\nearly stage of control system design because the only information\nneeded for DIC is the steady-state process gain matrix. Here, a\nnecessary and sufficient condition conjectured in the literature is\nproved. The real structured singular value which can exploit realness\nof the controller gain is used to describe computable conditions for\nDIC. The primary usage of DIC is to eliminate unworkable pairings. For\nthis, two other simple necessary conditions are proposed. Examples are\ngiven to illustrate the effectiveness of the proposed conditions for\nDIC\n', ['decentralized integral controllability', 'necessary sufficient conditions', 'stable decentralized controllers', 'integral action', 'closed-loop properties', 'stable independent detuning', 'control system design', 'systematic control structure selection', 'steady-state process gain matrix', 'real structured singular value', 'controller gain realness', 'unworkable pairing elimination', 'Schur complement', 'closed loop systems', 'control system synthesis', 'controllability', 'decentralised control', 'matrix algebra', 'root loci']), ('Sigma -admissible families over linear orders\nAdmissible sets of the form HYP(M), where M is a recursively saturated system,\nare treated. We provide descriptions of subsets M, which are Sigma /sub\n*/-sets in HYP(M), and of families of subsets M, which form Sigma\n-regular families in HYP(M), in terms of the concept of being\nfundamental couched in the article. Fundamental subsets and families\nare characterized for models of dense linear orderings\n', ['Sigma -admissible families', 'linear orders', 'HYP(M)', 'recursively saturated system', 'fundamental subsets', 'dense linear orderings', 'formal logic', 'recursive functions']), ("Developing a hardware and programming curriculum for middle school girls\nTechbridge provides experiences and resources that would teach girls technology\nskills as well as excite their curiosity and build their confidence.\nFunded by the National Science Foundation and sponsored by Chabot Space\nand Science Center in Oakland, California, Techbridge is a three-year\nprogram that serves approximately 200 girls annually. Techbridge is\nhosted at 8 middle and high schools in Oakland and at the California\nSchool for the Blind in Fremont, California generally as an\nafter-school program meeting once a week. Techbridge comes at a\ncritical time in girls' development when girls have many important\ndecisions to make regarding classes and careers, but often lack the\nconfidence and guidance to make the best choices. Techbridge helps\ngirls plan for the next steps to high school and college with its role\nmodels and guidance. Techbridge also provides training and resources\nfor teachers, counselors, and families\n", ['middle school girls', 'hardware and programming curriculum', 'Techbridge', 'technology skills teaching', 'computer science education', 'gender issues', 'programming']), ("Programmatic efforts encouraging women to enter the information technology\nworkforce\nFor over a decade the National Science Foundation (NSF) has been supporting\nprojects designed to improve opportunities for women in computing. From\nan initial emphasis on increasing the number of women in graduate\nschool studying computer science and engineering, NSF's current\nemphasis has broadened to include research studies examining the\nunderlying reasons why women are underrepresented in the information\ntechnology (IT) workforce. This paper describes the recent history of\nNSF's activities in this area and the subsequent emergence of a\nresearch portfolio addressing the underrepresentation issue\n", ['National Science Foundation', 'women', 'computing', 'graduate school', 'engineering', 'IT workforce', 'history', 'computer science education', 'DP industry', 'employment', 'gender issues', 'personnel']), ('Estimation of 3-D left ventricular deformation from medical images using\nbiomechanical models\nThe quantitative estimation of regional cardiac deformation from\nthree-dimensional (3-D) image sequences has important clinical\nimplications for the assessment of viability in the heart wall. We\npresent here a generic methodology for estimating soft tissue\ndeformation which integrates image-derived information with\nbiomechanical models, and apply it to the problem of cardiac\ndeformation estimation. The method is image modality independent. The\nimages are segmented interactively and then initial correspondence is\nestablished using a shape-tracking approach. A dense motion field is\nthen estimated using a transversely isotropic, linear-elastic model,\nwhich accounts for the muscle fiber directions in the left ventricle.\nThe dense motion field is in turn used to calculate the deformation of\nthe heart wall in terms of strain in cardiac specific directions. The\nstrains obtained using this approach in open-chest dogs before and\nafter coronary occlusion, exhibit a high correlation with strains\nproduced in the same animals using implanted markers. Further, they\nshow good agreement with previously published results in the\nliterature. This proposed method provides quantitative regional 3-D\nestimates of heart deformation\n', ['3-D left ventricular deformation estimation', 'medical diagnostic imaging', 'biomechanical models', 'regional cardiac deformation', 'quantitative estimation', 'transversely isotropic linear-elastic model', 'cardiac specific directions', 'open-chest dogs', 'muscle fiber directions', 'generic methodology', 'interactively segmented images', '3-D image sequences', 'nonrigid motion estimation', 'magnetic resonance imaging', 'left ventricular motion estimation', 'biomedical MRI', 'cardiology', 'image segmentation', 'image sequences', 'medical image processing', 'motion estimation', 'physiological models']), ('Lattice Boltzmann schemes for quantum applications\nWe review the basic ideas behind the quantum lattice Boltzmann equation (LBE),\nand present a few thoughts on the possible use of such an equation for\nsimulating quantum many-body problems on both (parallel) electronic and\nquantum computers\n', ['lattice Boltzmann schemes', 'quantum applications', 'quantum many-body problems', 'quantum computers', 'parallel computing', 'lattice theory', 'many-body problems', 'parallel algorithms', 'physics computing', 'quantum computing']), ('Guidelines, the Internet, and personal health: insights from the Canadian\nHEALNet experience\nThe objectives are to summarize the insights gained in collaborative research\nin a Canadian Network of Centres of Excellence, devoted to the\npromotion of evidence-based practice, and to relate this experience to\nInternet support of health promotion and consumer health informatics. A\nsubjective review of insights is undertaken. Work directed the\ndevelopment of systems incorporating guidelines, care maps, etc., for\nuse by professionals met with limited acceptance. Evidence-based tools\nfor health care consumers are a desirable complement but require\nradically different content and delivery modes. In addition to\nevidence-based material offered by professionals, a wide array of\nInternet-based products and services provided by consumers for\nconsumers emerged and proved a beneficial complement. The\nconsumer-driven products and services provided via the Internet are a\npotentially important and beneficial complement of traditional health\nservices. They affect the health consumer-provider roles and require\nchanges in healthcare practices\n', ['collaborative research', 'Canadian Network of Centres of Excellence', 'evidence-based practice', 'Internet support', 'health promotion', 'consumer health informatics', 'personal health', 'health consumer-provider roles', 'health care', 'Internet', 'medical computing', 'medical information systems', 'medicine']), ('Two-scale curved element method for elliptic problems with small periodic\ncoefficients\nThis paper is concerned with the second order elliptic problems with small\nperiodic coefficients on a bounded domain with a curved boundary. A\ntwo-scale curved element method which couples linear elements and\nisoparametric elements is proposed. The error estimate is obtained over\nthe given smooth domain. Furthermore an additive Schwarz method is\nprovided for the isoparametric element method\n', ['two-scale curved element method', 'elliptic problems', 'small periodic coefficients', 'second order elliptic problems', 'bounded domain', 'curved boundary', 'linear elements', 'isoparametric elements', 'error estimate', 'additive Schwarz method', 'isoparametric element method', 'computational geometry', 'error analysis', 'interpolation']), ('Laguerre pseudospectral method for nonlinear partial differential equations\nThe Laguerre Gauss-Radau interpolation is investigated. Some approximation\nresults are obtained. As an example, the Laguerre pseudospectral scheme\nis constructed for the BBM equation. The stability and the convergence\nof proposed scheme are proved. The numerical results show the high\naccuracy of this approach\n', ['Laguerre pseudospectral method', 'nonlinear partial differential equations', 'Laguerre Gauss-Radau interpolation', 'approximation results', 'BBM equation', 'stability', 'numerical results', 'nonlinear differential equations', 'interpolation', 'nonlinear differential equations', 'polynomials', 'stability']), ('Closed loop finite-element modeling of active constrained layer damping in the\ntime domain analysis\nA three-dimensional finite-element closed-loop model has been developed to\npredict the effects of active-passive damping on a vibrating structure.\nThe Golla-Hughes-McTavish method is employed to capture the\nviscoelastic material behavior in a time domain analysis. The\nparametric study includes the different control gains as well as\ngeometric parameters related to the active constrained layer damping\n(ACLD) treatment. Comparisons are made among several ACLD models, the\npassive constrained model and the active damping model. The results\nobtained here reiterate that ACLD is somewhat better for vibration\nsuppression than either the purely passive or the active system and\nprovides higher structural damping with less control gain when compared\nto the purely active system. Since the ACLD performance can be reduced\nby the viscoelastic layer, the design of the ACLD model must be given a\ncareful consideration in order to optimize the effect of passive\ndamping\n', ['three-dimensional finite-element closed-loop model', 'Golla-Hughes-McTavish method', 'viscoelastic material', 'time domain analysis', 'active constrained layer damping', 'ACLD models', 'passive constrained model', 'active damping model', 'passive damping', 'vibration suppression', 'structural damping', 'viscoelastic layer', 'closed loop systems', 'control system analysis', 'damping', 'finite element analysis', 'optimisation', 'time-domain analysis', 'vibration control']), ('The top cycle and uncovered solutions for weak tournaments\nWe study axiomatic properties of the top cycle and uncovered solutions for weak\ntournaments. Subsequently, we establish its connection with the\nrational choice theory\n', ['top cycle', 'uncovered solutions', 'weak tournaments', 'axiomatic properties', 'rational choice theory', 'decision theory', 'game theory']), ('Parallel implicit predictor corrector methods\nThe performance of parallel codes for the solution of initial value problems is\nusually strongly sensitive to the dimension of the continuous problem.\nThis is due to the overhead related to the exchange of information\namong the processors and motivates the problem of minimizing the amount\nof communications. According to this principle, we define the so called\nParallel Implicit Predictor Corrector Methods and in this class we\nderive A-stable, L-stable and numerically zero-stable formulas. The\nlatter property refers to the zero-stability condition of a given\nformula when roundoff errors are introduced in its coefficients due to\ntheir representation in finite precision arithmetic. Some numerical\nexperiment show the potentiality of this approach\n', ['parallel implicit predictor corrector methods', 'initial value problems', 'numerically zero-stable formulas', 'zero-stability condition', 'roundoff errors', 'finite precision arithmetic', 'absolute stability', 'initial value problems', 'matrix algebra', 'predictor-corrector methods', 'roundoff errors']), ('A novel control logic for fast valving operations\nThis letter proposes new control logic for operating parallel valves in fast\nvalving schemes in order to improve the transient stability performance\nof power systems. A fast valving scheme using parallel valves overcomes\nmany of the limitations of the conventional scheme. The proposed\ncontrol logic for operating these valves has been applied to a typical\nsingle machine infinite bus system. Single as well as multiple stroke\noperations for controlling the turbine power output have been studied\nwith the new control sequences. Encouraging results have been shown\nover the conventional schemes of fast valving\n', ['control logic', 'parallel valves operation', 'transient stability performance', 'single machine infinite bus system', 'multiple stroke operations', 'single stroke operations', 'turbine power output control', 'transient stability', 'power generation control', 'power system transient stability', 'turbines', 'valves']), ('Computer processing of data on mental impairments during the acute period of\nconcussion\nThe article presents results of computer processing of experimental information\nobtained from patients during the acute period of concussion. A number\nof computational procedures are described\n', ['computer processing', 'mental impairments', 'acute period of concussion', 'computational procedures', 'medical administrative data processing']), ('The road ahead [supply chains]\nExecutive supply chain managers, says David Metcalfe of Forrester Research,\nneed the skills and precision of Mongolian archers on horseback. They\nmust be able to hit their target, in this case customer demand, while\nmoving at great speed. But what is wrong with the supply chains\ncompanies have in place already? According to Metcalfe, current\nmanufacturing models are too inflexible. A recent survey conducted by\nForrester Research supports this claim. It found that 42% of\nrespondents could not transfer production from one plant to another in\nthe event of a glitch in the supply chain. A further 32% said it would\nbe possible, but extremely costly\n', ['supply chains', 'Forrester Research', 'survey', 'manufacturing', 'business networks', 'manufacturing industries', 'production control', 'purchasing', 'stock control']), ('Mathematical aspects of computer-aided share trading\nWe consider problems of statistical analysis of share prices and propose\nprobabilistic characteristics to describe the price series. We discuss\nthree methods of mathematical modelling of price series with given\nprobabilistic characteristics\n', ['computer-aided share trading', 'statistical analysis', 'share price', 'probabilistic characteristics', 'price series', 'mathematical modelling', 'financial data processing', 'probability', 'series (mathematics)', 'statistical analysis', 'stock markets']), ("The disconnect continues [digital content providers]\nThe relationships between the people who buy digital content and those who sell\nit are probably more acrimonious than ever before, says Dick Curtis, a\ndirector and lead analyst for the research firm Outsell Inc., where he\ncovers econtent contract and negotiation strategies. Several buyers\nagree with his observation. They cite aggressive sales tactics, an\nunwillingness to deliver content in formats buyers need, a reluctance\nto provide licensing terms that take into account the structure of\ntoday's corporations, and inadequate service and support as a few of\nthe factors underlying the acrimony. Still, many buyers remain\noptimistic that compromises can be reached on some of these issues. But\nfirst, they say, sellers must truly understand the econtent needs of\ntoday's enterprises\n", ['digital content', 'econtent contract', 'econtent negotiation', 'sales tactics', 'econtent buyers', 'news databases', 'Web site', 'contracts', 'information industry', 'information resources', 'information services', 'Internet']), ('Fuzzy systems with overlapping Gaussian concepts: Approximation properties in\nSobolev norms\nIn this paper the approximating capabilities of fuzzy systems with overlapping\nGaussian concepts are considered. The target function is assumed to be\nsampled either on a regular gird or according to a uniform probability\ndensity. By exploiting a connection with Radial Basis Functions\napproximators, a new method for the computation of the system\ncoefficients is provided, showing that it guarantees uniform\napproximation of the derivatives of the target function\n', ['fuzzy systems', 'overlapping Gaussian concepts', 'radial basis functions', 'learning', 'fuzzy system models', 'reproducing kernel Hilbert spaces', 'fuzzy systems', 'Hilbert spaces', 'radial basis function networks']), ("Organizational design, information transfer, and the acquisition of\nrent-producing resources\nWithin the resource-based view of the firm, a dynamic story has emerged in\nwhich the knowledge accumulated over the history of a firm and embedded\nin organizational routines and structures influences the firm's ability\nto recognize the value of new resources and capabilities. This paper\nexplores the possibility of firms to select organizational designs that\nincrease the likelihood that they will recognize and value\nrent-producing resources and capabilities. A computational model is\ndeveloped to study the tension between an organization's desire to\nexplore its environment for new capabilities and the organization's\nneed to exploit existing capabilities. Support is provided for the\nproposition that integration, both externally and internally, is an\nimportant source of dynamic capability. The model provides greater\ninsight into the tradeoffs between these two forms of integration and\nsuggests when one form may be preferred over another. In particular,\nevidence is provided that in uncertain environments, the ability to\nexplore possible alternatives is critical while in more certain\nenvironments, the ability to transfer information internally is\nparamount\n", ['organizational design', 'information transfer', 'rent-producing resources', 'computational model', 'uncertain environments', 'probability', 'certain environments', 'social networks', 'business strategy', 'investments', 'commerce', 'economic cybernetics', 'investment', 'probability']), ('Verification of ideological classifications-a statistical approach\nThe paper presents a statistical method of verifying ideological\nclassifications of votes. Parliamentary votes, preclassified by an\nexpert (on a chosen subset), are verified at an assumed significance\nlevel by seeking the most likely match with the actual vote results.\nClassifications that do not meet the requirements defined are rejected.\nThe results obtained can be applied in the ideological dimensioning\nalgorithms, enabling ideological identification of dimensions obtained\n', ['ideological classifications', 'statistical approach', 'parliamentary votes', 'significance level', 'ideological dimensioning algorithms', 'ideological space', 'bootstrap', 'estimation theory', 'politics', 'probability', 'set theory']), ('The role of CAUL (Council of Australian Libraries) in consortial purchasing\nThe Council of Australian University Librarians, constituted in 1965 for the\npurposes of cooperative action and the sharing of information, assumed\nthe role of consortial purchasing agent in 1996 on behalf of its\nmembers and associate organisations in Australia and New Zealand. This\nrole continues to grow in tandem with the burgeoning of electronic\npublication and the acceptance of publishers of the advantages of\ndealing with consortia. The needs of the Australian university\ncommunity overlap significantly with consortia in North America and\nEurope, but important differences are highlighted\n', ['Council of Australian University Librarians', 'cooperative action', 'information sharing', 'consortial purchasing', 'Australia', 'New Zealand', 'electronic publication', 'North America', 'Europe', 'academic libraries', 'electronic publishing', 'library automation', 'purchasing', 'research libraries']), ('Definition of a similarity measure between cases based on auto/cross-fuzzy\nthesauri\nA similarity measure between cases is needed in order to evaluate the degree of\nsimilarity when using past similar cases in order to resolve current\nproblems. In similar case retrieval, multiple indices are set up in\norder to characterize the queries and individual cases, then terms are\ngiven as values to each. The similarity measure between cases commonly\nused is defined using the rate at which the values provided from the\ncorresponding indices match. In practice, however, values cannot be\nexpected to be mutually exclusive. As a result, a natural expansion of\nthis approach is to have relationships in which mutually similar\nmeanings are reflected in the similarity measure between cases. In this\npaper the authors consider an auto-fuzzy thesaurus which gives the\nrelationship for values between corresponding indices and a cross-fuzzy\nthesaurus which gives the relationship for values between mutually\ndistinct indices, then defines a similarity measure between cases which\nconsiders the relationship of index values based on these thesauri.\nThis definition satisfies the characteristics required for the\noperation of case-based retrieval even when one value is not\nnecessarily given in the index. Finally, using a test similar case\nretrieval system, the authors perform a comparative analysis of the\nproposed similarity measure between cases and a conventional approach\n', ['case similarity measure', 'relationship indices', 'corresponding indices', 'mutually distinct indices', 'case-based retrieval', 'decision making support system', 'problem solving', 'similar case retrieval', 'auto-fuzzy thesaurus', 'cross-fuzzy thesaurus', 'case-based reasoning', 'decision support systems', 'fuzzy logic', 'indexing', 'problem solving', 'thesauri']), ('Handles and exception safety, Part 1. A simple handle class\nEvery C++ program that uses inheritance must manage memory somehow. The most\nobvious way to do so is directly, but programmers who create\ncomplicated data structures often have trouble figuring out what parts\nof those data structures are safe to delete when. The classical method\nof dealing with such complexity is to hide it in a class. Such classes\nare typically called handles; the idea is to attach a handle object to\nanother object that contains the actual data. The simplest form of a\nhandle, which we have discussed in this article, is one in which each\nhandle object corresponds to a single object from the inheritance\nhierarchy. Such handles are straightforward to use and to implement and\ntend to be intrinsically exception safe in almost all respects. The one\nexception hazard in such a class is typically the assignment operator.\nAssignment operators often test for self-assignment to avoid aliasing\nproblems. As Herb Sutter has observed (2000), programs that need such\ntests are almost always exception unsafe. By rewriting the assignment\noperator, we ensure that we do not do anything irrevocable until the\npossibility of throwing an exception has passed. This strategy ensures\nthat if an exception occurs while our assignment operator is executing,\nwe do not corrupt the rest of our system\n', ['handles', 'assignment operator', 'self-assignment', 'aliasing problems', 'exception', 'C++ program', 'inheritance hierarchy', 'C++ language', 'exception handling', 'inheritance', 'object-oriented programming', '    storage management']), ('Three-dimensional spiral MR imaging: application to renal multiphase\ncontrast-enhanced angiography\nA fast MR pulse sequence with spiral in-plane readout and conventional 3D\npartition encoding was developed for multiphase contrast-enhanced\nmagnetic resonance angiography (CE-MRA) of the renal vasculature.\nCompared to a standard multiphase 3D CE-MRA with FLASH readout, an\nisotropic in-plane spatial resolution of 1.4*1.4 mm/sup 2/ over 2.0*1.4\nmm/sup 2/ could be achieved with a temporal resolution of 6 sec. The\ntheoretical gain of spatial resolution by using the spiral pulse\nsequence and the performance in the presence of turbulent flow was\nevaluated in phantom measurements. Multiphase 3D CE-MRA of the renal\narteries was performed in five healthy volunteers using both\ntechniques. A deblurring technique was used to correct the spiral raw\ndata. Thereby, the off-resonance frequencies were determined by\nminimizing the imaginary part of the data in image space. The chosen\ncorrection algorithm was able to reduce image blurring substantially in\nall MRA phases. The image quality of the spiral CE-MRA pulse sequence\nwas comparable to that of the FLASH CE-MRA with increased spatial\nresolution and a 25% reduced contrast-to-noise ratio. Additionally,\nartifacts specific to spiral MRI could be observed which had no impact\non the assessment of the renal arteries\n', ['3D spiral MRI', 'renal multiphase contrast-enhanced angiography', 'flow artifacts', 'deblurring', 'fast pulse sequence', 'spiral in-plane readout', '3D partition encoding', 'renal vasculature', 'off-resonance frequencies', 'image quality', 'reduced contrast-to-noise ratio', 'image reconstruction', 'FLASH sequence', 'spatial resolution', 'biomedical MRI', 'blood vessels', 'image resolution', 'image restoration', 'kidney', 'medical image processing']), ('Quadratic Gauss sums over finite commutative rings\nThis article explicitly determines the quadratic Gauss sum over finite\ncommutative rings\n', ['quadratic Gauss sum', 'finite commutative rings', 'number theory']), ("International customers, suppliers, and document delivery in a fee-based\ninformation service\nThe Purdue University Libraries library fee-based information service, the\nTechnical Information Service (TIS), works with both international\ncustomers and international suppliers to meet its customers' needs for\ndifficult and esoteric document requests. Successful completion of\nthese orders requires the ability to verify fragmentary citations;\nascertain documents' availability; obtain pricing information;\ncalculate inclusive cost quotes; meet customers' deadlines; accept\ninternational payments; and ship across borders. While international\norders make tip a small percent of the total workload, these\nchallenging and rewarding orders meet customers' needs and offer\ncontinuous improvement opportunities to the staff\n", ['Purdue University Libraries fee-based information service', 'Technical Information Service', 'international suppliers', 'international customers', 'document requests', 'document delivery', 'fragmentary citation verification', 'document availability', 'pricing information', 'inclusive cost quotes', 'customer deadline meeting', 'international payments', 'continuous staff improvement', 'academic libraries', 'costing', 'document delivery', 'interlibrary loan']), ("Holding on [workflow & content management]\nMarc Fresko of Cornwell Management Consultants says 'think ahead' when\ndeveloping your electronic records management policy\n", ['Cornwell Management Consultants', 'electronic records management policy', 'records management']), ('Boolean operators and the naive end-user: moving to AND\nSince so few end-users make use of Boolean searching, it is obvious that any\neffective solution needs to take this reality into account. The most\nimportant aspect of a technical solution should be that it does not\nrequire any effort on the part of users. What is clearly needed is for\nsearch engine designers and programmers to take account of the\ninformation-seeking behavior of Internet users. Users must be able to\nenter a series of words at random and have those words automatically\ntreated as a carefully constructed Boolean AND search statement\n', ['Boolean operators', 'AND operator', 'Boolean searching', 'search engine design', 'information-seeking behavior', 'Internet', 'Boolean algebra', 'information resources', 'query formulation', 'search engines']), ('Much ado about nothing: Win32.Perrun\nJPEG files do not contain any executable code and it is impossible to infect\nsuch files. The author takes a look at the details surrounding the\nWin32.Perrun virus and make clear exactly what it does. The main virus\nfeature is its ability to affect JPEG image files (compressed graphic\nimages) and to spread via affected JPEG files. The virus affects, or\nmodifies, or alters JPEG files but does not "infect" them\n', ['Win32.Perrun', 'JPEG files', 'virus', 'compressed graphic images', 'computer graphics', 'computer viruses', 'data compression', 'file organisation']), ("A framework for rapid local area modeling for construction automation\nRapid 3D positioning and modeling in construction can be used to more\neffectively plan, visualize, and communicate operations before\nexecution. It can also help to optimize equipment operations,\nsignificantly improve safety, and enhance a remote operator's spatial\nperception of the workspace. A new framework for rapid local area\nsensing and 3D modeling for better planning and control of construction\nequipment operation is described and demonstrated. By combining\nhuman-assisted graphical workspace modeling with pre-stored\nComputer-Aided Design (CAD) models and simple sensors (such as\nsingle-axis laser rangefinders and remote video cameras), modeling time\ncan be significantly reduced while potentially increasing modeling\naccuracy\n", ['rapid local area modeling', 'construction automation', 'rapid 3D positioning', 'equipment operations', 'spatial perception', 'rapid local area sensing', '3D modeling', 'human-assisted graphical workspace modeling', 'pre-stored Computer-Aided Design models', 'single-axis laser rangefinders', 'remote video cameras', 'construction industry', 'industrial manipulators', 'telerobotics']), ('Quantum computation for physical modeling\nOne of the most famous American physicists of the twentieth century, Richard\nFeynman, in 1982 was the first to propose using a quantum mechanical\ncomputing device to efficiently simulate quantum mechanical many-body\ndynamics, a task that is exponentially complex in the number of\nparticles treated and is completely intractable by any classical\ncomputing means for large systems of many particles. In the two decades\nfollowing his work, remarkable progress has been made both\ntheoretically and experimentally in the new field of quantum\ncomputation\n', ['quantum computation', 'physical modeling', 'quantum mechanical computing', 'quantum mechanical many-body dynamics', 'many-body problems', 'quantum computing', 'quantum theory']), ('Hordes: a multicast based protocol for anonymity\nWith widespread acceptance of the Internet as a public medium for communication\nand information retrieval, there has been rising concern that the\npersonal privacy of users can be eroded by cooperating network\nentities. A technical solution to maintaining privacy is to provide\nanonymity. We present a protocol for initiator anonymity called Hordes,\nwhich uses forwarding mechanisms similar to those used in previous\nprotocols for sending data, but is the first protocol to make use of\nmulticast routing to anonymously receive data. We show this results in\nshorter transmission latencies and requires less work of the protocol\nparticipants, in terms of the messages processed. We also present a\ncomparison of the security and anonymity of Hordes with previous\nprotocols, using the first quantitative definition of anonymity and\nunlinkability. Our analysis shows that Hordes provides anonymity in a\ndegree similar to that of Crowds and Onion Routing, but also that\nHordes has numerous performance advantages\n', ['Hordes', 'protocol', 'Internet', 'personal privacy', 'cooperating network entities', 'initiator anonymity', 'forwarding mechanisms', 'multicast routing', 'transmission latencies', 'unlinkability', 'Crowds', 'Onion Routing', 'performance', 'data privacy', 'Internet', 'multicast communication', 'protocols', 'telecommunication network routing']), ('Uncertainty bounds and their use in the design of interval type-2 fuzzy logic\nsystems\nWe derive inner- and outer-bound sets for the type-reduced set of an interval\ntype-2 fuzzy logic system (FLS), based on a new mathematical\ninterpretation of the Karnik-Mendel iterative procedure for computing\nthe type-reduced set. The bound sets can not only provide estimates\nabout the uncertainty contained in the output of an interval type-2\nFLS, but can also be used to design an interval type-2 FLS. We\ndemonstrate, by means of a simulation experiment, that the resulting\nsystem can operate without type-reduction and can achieve similar\nperformance to one that uses type-reduction. Therefore, our new design\nmethod, based on the bound sets, can relieve the computation burden of\nan interval type-2 FLS during its operation, which makes an interval\ntype-2 FLS useful for real-time applications\n', ['uncertainty bounds', 'interval type-2 fuzzy logic systems', 'inner-bound sets', 'outer-bound sets', 'type-reduced set', 'Karnik-Mendel iterative procedure', 'real-time applications', 'time-series forecasting', 'forecasting theory', 'fuzzy logic', 'fuzzy set theory', 'fuzzy systems', 'iterative methods', 'probability', 'time series', 'uncertain systems']), ('A method of determining a sequence of the best solutions to the problems of\noptimization on finite sets and the problem of network reconstruction\nA method of determining a sequence of the best solutions to the problems of\noptimization on finite sets was proposed. Its complexity was estimated\nby a polynomial of the dimension of problem input, given number of\nsequence terms, and complexity of completing the design of the original\nextremal problem. The technique developed was applied to the typical\nproblem of network reconstruction with the aim of increasing its\nthroughput under restricted reconstruction costs\n', ['best solutions', 'optimization', 'finite sets', 'network reconstruction', 'complexity', 'computational complexity', 'graph theory', 'optimisation', 'set theory']), ('Equilibrium swelling and kinetics of pH-responsive hydrogels: models,\nexperiments, and simulations\nThe widespread application of ionic hydrogels in a number of applications like\ncontrol of microfluidic flow, development of muscle-like actuators,\nfiltration/separation and drug delivery makes it important to properly\nunderstand these materials. Understanding hydrogel properties is also\nimportant from the standpoint of their similarity to many biological\ntissues. Typically, gel size is sensitive to outer solution pH and salt\nconcentration. In this paper, we develop models to predict the\nswelling/deswelling of hydrogels in buffered pH solutions. An\nequilibrium model has been developed to predict the degree of swelling\nof the hydrogel at a given pH and salt concentration in the solution. A\nkinetic model has been developed to predict the rate of swelling of the\nhydrogel when the solution pH is changed. Experiments are performed to\ncharacterize the mechanical properties of the hydrogel in different pH\nsolutions. The degree of swelling as well as the rate of swelling of\nthe hydrogel are also studied through experiments. The simulations are\ncompared with experimental results and the models are found to predict\nthe swelling/deswelling processes accurately\n', ['pH-responsive hydrogels', 'ionic hydrogels', 'microfluidic flow', 'muscle-like actuators', 'filtration/separation', 'drug delivery', 'gel size', 'swelling/deswelling', 'buffered pH solutions', 'equilibrium model', 'mechanical properties', 'drug delivery systems', 'filtration', 'gels', 'microactuators', 'microfluidics', 'separation', 'swelling']), ('Multicriterion optimization of composite laminates for maximum failure margins\nwith an interactive descent algorithm\nAn interactive multicriterion optimization method for composite laminates\nsubjected to multiple loading conditions is introduced. Laminate\nmargins to initial failure (first ply failure, FPF) with respect of the\napplied loading conditions are treated as criteria. The original\nproblem is reduced to a, bicriterion problem by introducing parameters\nto combine criteria in a linear manner. The problem is solved by using\nan interactive descent algorithm. Both the conditions required for a\ndiscrete procedure to converge towards a Pareto optimum and numerical\nexamples are given\n', ['interactive multicriterion optimization', 'composite laminates', 'maximum failure margins', 'multiple loading conditions', 'interactive descent algorithm', 'first ply failure', 'bicriterion problem', 'discrete procedure', 'convergence', 'Pareto optimum', 'convergence of numerical methods', 'laminates', 'optimisation', 'structural engineering computing']), ('Who wants to see a $million error?\nInspired by the popular television show "Who Wants to Be a Millionaire?", this\ncase discusses the monetary decisions contestants face on a game\nconsisting of 15 increasingly difficult multiple choice questions.\nSince the game continues as long as a contestant answers correctly,\nthis case, at its core, is one of sequential decision analysis,\namenable to analysis via stochastic dynamic programming. The case is\nalso suitable for a course dealing with single decision analysis,\nallowing for discussion of utility theory and Bayesian probability\nrevision. In developing a story line for the case, the author has\nsprinkled in much background material on probability and statistics.\nThis material is placed in a historical context, illuminating some of\nthe influential scholars involved in the development of these subjects\nas well as the birth of operations research and the management sciences\n', ['operations research', 'game theory', 'decision analysis', 'stochastic dynamic programming', 'educational course', 'statistics', 'probabilistic models', 'decision theory', 'dynamic programming', 'educational courses', 'operations research', 'probability', 'statistical analysis']), ('Medicine in the 21 st century: global problems, global solutions\nThe objectives are to discuss application areas of information, technology in\nmedicine and health care on the occasion of the opening of the Private\nUniversitat fur Medizinische Informatik and Technik Tirol/University\nfor Health Informatics and Technology Tyrol (LIMIT) at Innsbruck,\nTyrol, Austria. Important application areas of information technology\nin medicine and health are appropriate individual access to medical\nknowledge, new engineering developments such as new radiant imaging\nmethods and the implantable pacemaker/defibrillator devices,\nmathematical modeling for understanding the workings of the human body,\nthe computer-based patient record, as well as new knowledge in\nmolecular biology, human genetics, and biotechnology. Challenges and\nresponsibilities for medical informatics research include medical data\nprivacy and intellectual property rights inherent in the content of the\ninformation systems\n', ['health care', 'medicine', 'information technology', 'individual medical knowledge access', 'engineering developments', 'radiant imaging methods', 'implantable pacemaker devices', 'implantable defibrillator devices', 'mathematical modeling', 'human body', 'computer-based patient record', 'molecular biology', 'human genetics', 'biotechnology', 'medical informatics research', 'medical data privacy', 'intellectual property rights', 'information systems', 'biomedical engineering', 'biotechnology', 'data privacy', 'genetics', 'health care', 'industrial property', 'medical computing', 'medical information systems', 'medicine', 'molecular biophysics', 'pacemakers', 'records management']), ("Virtual reality treatment of flying phobia\nFlying phobia (FP) might become a very incapacitating and disturbing problem in\na person's social, working, and private areas. Psychological\ninterventions based on exposure therapy have proved to be effective,\nbut given the particular nature of this disorder they bear important\nlimitations. Exposure therapy for FP might be excessively costly in\nterms of time, money, and efforts. Virtual reality (VR) overcomes these\ndifficulties as different significant environments might be created,\nwhere the patient can interact with what he or she fears while in a\ntotally safe and protected environment, the therapist's consulting\nroom. This paper intends, on one hand, to show the different scenarios\ndesigned by our team for the VR treatment of FP, and on the other, to\npresent the first results supporting the effectiveness of this new tool\nfor the treatment of FP in a multiple baseline study\n", ['medical virtual reality', 'psychology', 'flying phobia', 'patient treatment', 'psychological interventions', 'anxiety disorders', 'virtual exposure', 'exposure therapy', 'medical computing', 'patient treatment', 'psychology', 'user interfaces', 'virtual reality']), ("Networking in the palm of your hand [PDA buyer's guide]\nAs PDAs move beyond the personal space and into the enterprise, you need to get\na firm grip on the options available for your users. What operating\nsystem do you choose? What features do you and your company need? How\nwill these devices fit into the existing corporate infrastructure? What\nabout developer support?\n", ['PDAs', 'operating system', 'corporate infrastructure', 'developer support', "buyer's guide", "buyer's guides", 'computer network management', 'notebook computers']), ('Modularity in technology and organization\nThe paper is an attempt to raid both the literature on modular design and the\nliterature on property rights to create the outlines of a modularity\ntheory of the firm. Such a theory will look at firms, and other\norganizations, in terms of the partitioning of rights-understood as\nprotected spheres of authority-among cooperating parties. It will\nassert that organizations reflect nonmodular structures, that is,\nstructures in which decision rights, rights of alienation, and residual\nclaims to income do not all reside in the same hands\n', ['modularity', 'technology', 'organization', 'property rights', 'partitioning of rights', 'authority', 'cooperating parties', 'nonmodular structures', 'decision rights', 'rights of alienation', 'transaction costs', 'corporate modelling', 'industrial property']), ('Self-reproduction in three-dimensional reversible cellular space\nDue to inevitable power dissipation, it is said that nano-scaled computing\ndevices should perform their computing processes in a reversible\nmanner. This will be a large problem in constructing three-dimensional\nnano-scaled functional objects. Reversible cellular automata (RCA) are\nused for modeling physical phenomena such as power dissipation, by\nstudying the dissipation of garbage signals. We construct a\nthree-dimensional self-inspective self-reproducing reversible cellular\nautomaton by extending the two-dimensional version SR/sub 8/. It can\nself-reproduce various patterns in three-dimensional reversible\ncellular space without dissipating garbage signals\n', ['self-reproduction', 'nano-scaled computing devices', 'power dissipation', '3D self-inspective self-reproducing cellular automata', 'reversible cellular automata', 'artificial life', 'three-dimensional reversible cellular space', 'artificial life', 'cellular automata', 'self-reproducing automata']), ('Ten suggestions for a gender-equitable CS classroom\nThough considerable attention has been paid to the creation of a nurturing\nenvironment for women in the field of computer science, proposed\nsolutions have primarily focused on activities outside of the\nclassroom. This paper presents a list of suggestions for modifications\nto both the pedagogy and content of CS courses designed to make the CS\nclassroom environment more inviting for women students\n', ['gender-equitable classroom', 'CS classroom environment', 'computer science', 'nurturing environment', 'pedagogy', 'CS course content', 'women students', 'computer science education', 'gender issues', 'teaching']), ('An improved fuzzy MCDM model based on ideal and anti-ideal concepts\nLiang presented (1999) a fuzzy multiple criteria decision making (MCDM) method\nbased on the concepts of ideal and anti-ideal points. Despite its\nmerits, Liang method has the following limitations: (i) the objective\ncriteria are converted into dimensionless indices and the subjective\ncriteria are not converted, which may prevent compatibility for these\ncriteria, (ii) the formulas for converting objective criteria are not\nreliable, and (iii) an unreliable ranking method, i.e. maximizing set\nand minimizing set, is applied to rank the fuzzy numbers. This paper\napplies the Hsu and Chen method and suggests a fuzzy number ranking\nmethod to propose an improved fuzzy MCDM model based on ideal and\nanti-ideal concepts to overcome the shortcomings of the Liang method.\nNumerical examples demonstrate the effectiveness and feasibility of the\nproposed ranking method and the improved model, respectively\n', ['fuzzy MCDM model', 'ideal concepts', 'anti-ideal concepts', 'dimensionless indices', 'fuzzy number ranking', 'multicriterion decision-making', 'decision theory', 'fuzzy set theory']), ('Resolution of a current-mode algorithmic analog-to-digital converter\nErrors limiting the resolution of current-mode algorithmic analog-to-digital\nconverters are mainly related to current mirror operation. While\nsystematic errors can be minimized by proper circuit techniques, random\nsources are unavoidable. In this paper a statistical analysis of the\nresolution of a typical converter is carried out taking into account\nprocess tolerances. To support the analysis, a 4-bit ADC, realized in a\n0.35- mu m CMOS technology, was exhaustively simulated. Results were\nfound to be in excellent agreement with theoretical derivations\n', ['current-mode ADC', 'algorithmic ADC', 'analog-to-digital converters', 'resolution', 'A/D converters', 'circuit techniques', 'statistical analysis', 'CMOS technology', 'error analysis', 'tolerance analysis', 'circuit analysis', '0.35 micron', '4 bit', 'analogue-digital conversion', 'CMOS integrated circuits', 'current-mode circuits', 'error analysis', 'network analysis', 'statistical analysis', 'tolerance analysis']), ("Swamped by data [storage]\nWhile the cost of storage has plummeted, the demand continued to climb and\nthere are plenty of players out there offering solutions to a company's\nburgeoning storage needs\n", ['cost of storage', 'IT personnel', 'resource management', 'disk capacity management', 'disk optimisation', 'file system automation', 'storage virtualisation', 'storage area networks', 'network attached storage', 'DP management', 'storage management']), ('Improved analysis for the nonlinear performance of CMOS current mirrors with\ndevice mismatch\nThe nonlinear performance of the simple and complementary MOSFET current\nmirrors are analyzed. Closed-form expressions are obtained for the\nharmonic and intermodulation components resulting from a\nmultisinusoidal input current. These expressions can be used for\npredicting the limiting values of the input current under prespecified\nconditions of threshold-voltage mismatches and/or transconductance\nmismatches. The case of a single input sinusoid is discussed in detail\nand the results are compared with SPICE simulations\n', ['nonlinear performance', 'CMOS current mirrors', 'device mismatch', 'complementary MOSFET current mirrors', 'closed-form expressions', 'harmonic components', 'intermodulation components', 'multisinusoidal input current', 'input current', 'threshold-voltage mismatch', 'SPICE simulations', 'simulation results', 'transconductance mismatch', 'circuit simulation', 'CMOS integrated circuits', 'current mirrors', 'harmonic distortion', 'intermodulation', 'MOSFET circuits', 'nonlinear network analysis']), ('Taiwan power company phases into AM/FM\nTo face the challenges and impact of the inevitable trend toward privatization\nand deregulation, the Taiwan Power Co. (TPC) devised short- and\nlong-term strategic computerization development plans. These\ndevelopment efforts created a master plan that included building an\nAutomated Mapping and Facilities Management (AM/ FM) system for Taipei\nCity District Office (TCDO). This project included a pilot project\nfollowed by evaluation before the roll out to the complete service\nterritory of TCDO. The pilot project took three years to install,\ncommission and-via the evaluation process-reach the conclusion that\nAM/FM was technologically feasible\n', ['Taiwan Power Company', 'AM/FM', 'Taipei City District Office', 'Automated Mapping and Facilities Management', 'pilot project', 'complete service territory', 'deregulation', 'privatization', 'electricity supply industry', 'geographic information systems', 'management']), ("Activity and location recognition using wearable sensors\nUsing measured acceleration and angular velocity data gathered through\ninexpensive, wearable sensors, this dead-reckoning method can determine\na user's location, detect transitions between preselected locations,\nand recognize and classify sitting, standing, and walking behaviors.\nExperiments demonstrate the proposed method's effectiveness\n", ['measured acceleration', 'angular velocity', 'wearable sensors', 'dead-reckoning method', "user's location", 'preselected locations', 'transitions', 'sitting', 'standing', 'walking', 'mobile computing', 'position measurement', 'sensors']), ('TPTP, CASC and the development of a semantically guided theorem prover\nThe first-order theorem prover SCOTT has been through a series of versions over\nsome ten years. The successive provers, while retaining the same\nunderlying technology, have used radically different algorithms and\nshown wide differences of behaviour. The development process has\ndepended heavily on experiments with problems from the TPTP library and\nhas been sharpened by participation in CASC each year since 1997. We\noutline some of the difficulties inherent in designing and refining a\ntheorem prover as complex as SCOTT, and explain our experimental\nmethodology. While SCOTT is not one of the systems which have been\nhighly optimised for CASC, it does help to illustrate the influence of\nboth CASC and the TPTP library on contemporary theorem proving research\n', ['TPTP library', 'Semantically Constrained Otter', 'proof searches', 'CASC', 'semantically guided theorem prover', 'first-order theorem prover', 'SCOTT', 'experimental methodology', 'formal logic', 'inference mechanisms', 'search problems', 'theorem proving']), ('Exact frequency-domain reconstruction for thermoacoustic tomography. II.\nCylindrical geometry\nFor pt. I see ibid., vol. 21, no. 7, p. 823-8 (2002). Microwave-induced\nthermoacoustic tomography (TAT) in a cylindrical configuration is\ndeveloped to image biological tissue. Thermoacoustic signals are\nacquired by scanning a flat ultrasonic transducer. Using a new\nexpansion of a spherical wave in cylindrical coordinates, we apply the\nFourier and Hankel transforms to TAT and obtain an exact\nfrequency-domain reconstruction method. The effect of discrete spatial\nsampling on image quality is analyzed. An aliasing-proof reconstruction\nmethod is proposed. Numerical and experimental results are included\n', ['medical diagnostic imaging', 'frequency-domain reconstruction', 'flat ultrasonic transducer', 'thermoacoustic tomography', 'cylindrical geometry', 'discrete spatial sampling effect', 'ultrasound imaging', 'spherical wave expansion', 'aliasing-proof reconstruction method', 'Hankel transform', 'acoustic tomography', 'biomedical ultrasonics', 'Fourier transforms', 'image reconstruction', 'medical image processing', 'microwave imaging', 'thermoacoustics']), ('Towards a NMR implementation of a quantum lattice gas algorithm\nRecent theoretical results suggest that an array of quantum information\nprocessors communicating via classical channels can be used to solve\nfluid dynamics problems. Quantum lattice-gas algorithms (QLGA) running\non such architectures have been shown to solve the diffusion equation\nand the nonlinear Burgers equations. In this report, we describe\nprogress towards an ensemble nuclear magnetic resonance (NMR)\nimplementation of a QLGA that solves the diffusion equation. The\nmethods rely on NMR techniques to encode an initial mass density into\nan ensemble of two-qubit quantum information processors. Using standard\npulse techniques, the mass density can then manipulated and evolved\nthrough the steps of the algorithm. We provide the experimental results\nof our first attempt to realize the NMR implementation. The results\nqualitatively follow the ideal simulation, but the observed\nimplementation errors highlight the need for improved control\n', ['NMR implementation', 'quantum lattice gas algorithm', 'quantum information processors', 'fluid dynamics problems', 'diffusion equation', 'nonlinear Burgers equations', 'nuclear magnetic resonance', 'two-qubit quantum information.processors', 'computational fluid dynamics', 'diffusion', 'lattice gas', 'nuclear magnetic resonance', 'quantum computing']), ('Universal simulation of Hamiltonian dynamics for quantum systems with\nfinite-dimensional state spaces\nWhat interactions are sufficient to simulate arbitrary quantum dynamics in a\ncomposite quantum system? Dodd et al. [Phys. Rev. A 65, 040301(R)\n(2002)] provided a partial solution to this problem in the form of an\nefficient algorithm to simulate any desired two-body Hamiltonian\nevolution using any fixed two-body entangling N-qubit Hamiltonian, and\nlocal unitaries. We extend this result to the case where the component\nsystems are qudits, that is, have D dimensions. As a consequence we\nexplain how universal quantum computation can be performed with any\nfixed two-body entangling N-qudit Hamiltonian, and local unitaries\n', ['universal simulation', 'Hamiltonian dynamics', 'quantum systems', 'finite- dimensional state spaces', 'quantum dynamics', 'composite quantum system', 'two-body Hamiltonian evolution', 'fixed two-body entangling N-qubit Hamiltonian', 'local unitaries', 'D-dimensional component systems', 'universal quantum computation', 'fixed two-body entangling N-qudit Hamiltonian', 'digital simulation', 'dynamics', 'quantum computing']), ("Personal cards for on-line purchases\nBuying presents over the Web has advantages for a busy person: lots of choices,\n24-hour accessibility, quick delivery, and you don't even have to wrap\nthe gift. But many people like to select a card or write a personal\nnote to go with their presents, and the options for doing that have\nbeen limited. Two companies have seen this limitation as an\nopportunity: 4YourSoul.com and CardintheBox.com\n", ['4YourSoul.com', 'CardintheBox.com', 'personalized printing', 'personal cards', 'online purchases', 'electronic commerce', 'printing', 'retailing']), ('A case for end system multicast\nThe conventional wisdom has been that Internet protocol (IP) is the natural\nprotocol layer for implementing multicast related functionality.\nHowever, more than a decade after its initial proposal, IP multicast is\nstill plagued with concerns pertaining to scalability, network\nmanagement, deployment, and support for higher layer functionality such\nas error, flow, and congestion control. We explore an alternative\narchitecture that we term end system multicast, where end systems\nimplement all multicast related functionality including membership\nmanagement and packet replication. This shifting of multicast support\nfrom routers to end systems has the potential to address most problems\nassociated with IP multicast. However, the key concern is the\nperformance penalty associated with such a model. In particular, end\nsystem multicast introduces duplicate packets on physical links and\nincurs larger end-to-end delays than IP multicast. We study these\nperformance concerns in the context of the Narada protocol. In Narada,\nend systems self-organize into an overlay structure using a fully\ndistributed protocol. Further, end systems attempt to optimize the\nefficiency of the overlay by adapting to network dynamics and by\nconsidering application level performance. We present details of Narada\nand evaluate it using both simulation and Internet experiments. Our\nresults indicate that the performance penalties are low both from the\napplication and the network perspectives. We believe the potential\nbenefits of transferring multicast functionality from end systems to\nrouters significantly outweigh the performance penalty incurred\n', ['end system multicast', 'Internet protocol', 'protocol layer', 'IP multicast', 'network management', 'network scalability', 'higher layer functionality', 'congestion control', 'membership management', 'packet replication', 'network routers', 'end-to-end delays', 'Narada protocol', 'overlay structure', 'distributed protocol', 'network dynamics', 'application level performance', 'simulation', 'Internet experiments', 'performance penalties', 'self-organizing protocol', 'computer network management', 'delays', 'Internet', 'multicast communication', 'performance evaluation', 'self-adjusting systems', 'telecommunication congestion control', 'telecommunication network routing', 'transport protocols']), ('Accessible streaming content\nMake sure your Web site is offering quality service to all your users. The\narticle provides some tips and tactics for making your streaming media\naccessible. Accessibility of streaming content for people with\ndisabilities is often not part of the spec for multimedia projects, but\nit certainly affects your quality of service. Most of the resources\navailable on Web accessibility deal with HTML. Fortunately, rich media\nand streaming content developers have a growing number of experts to\nturn to for information and assistance. The essentials of providing\naccessible streaming content are simple: blind and visually impaired\npeople need audio to discern important visual detail and interface\nelements, while deaf and hard-of-hearing people need text to access\nsound effects and dialog. Actually implementing these principles is\nquite a challenge, though. Now due to a relatively new law in the US,\nknown as Section 508, dealing with accessibility issues is becoming an\nessential part of publishing on the Web\n', ['Web site', 'quality service', 'streaming media', 'content providers', 'United States', 'accessible streaming content', 'disabled users', 'multimedia projects', 'Web accessibility', 'HTML', 'streaming content developers', 'visually impaired people', 'blind people', 'visual detail', 'interface elements', 'deaf people', 'hard-of-hearing people', 'sound effects', 'Section 508', 'accessibility issues', 'Web publishing', 'electronic publishing', 'handicapped aids', 'hypermedia markup languages', 'information resources', 'multimedia communication', 'user interfaces']), ('A linear time special case for MC games\nMC games are infinite duration two-player games played on graphs. Deciding the\nwinner in MC games is equivalent to the the modal mu-calculus model\nchecking. In this article we provide a linear time algorithm for a\nclass of MC games. We show that, if all cycles in each strongly\nconnected component of the game graph have at least one common vertex,\nthe winner can be found in linear time. Our results hold also for\nparity games, which are equivalent to MC games\n', ['linear time special case', 'MC games', 'two-player games', 'modal mu-calculus model checking', 'linear time algorithm', 'game theory']), ('Quantum limit on computational time and speed\nWe investigate if physical laws can impose limits on computational time and\nspeed of a quantum computer built from elementary particles. We show\nthat the product of the speed and the running time of a quantum\ncomputer is limited by the type of fundamental interactions present\ninside the system. This will help us to decide as to what type of\ninteraction should be allowed in building quantum computers in\nachieving the desired speed\n', ['quantum limit', 'computational time', 'computational speed', 'quantum computer', 'fundamental interactions', 'bound states', 'information theory', 'quantum computing', 'quantum statistical mechanics']), ('Horizontal waypoint guidance design using optimal control\nA horizontal waypoint guidance algorithm is proposed by applying line-following\nguidance to waypoint line segments in sequence. The line-following\nguidance is designed using an LQR (linear quadratic regulator). Then,\nthe optimal waypoint changing points are derived by minimizing the\naccelerations required for changing the waypoint line segments. Also\nderived is a sufficient condition for the stability bound of ground\nspeed changes based on the Lyapunov stability theorem. Simulation\nresults show that the proposed algorithm can effectively guide a\nvehicle along the sequence of waypoint line segments\n', ['horizontal waypoint guidance algorithm', 'line-following guidance', 'waypoint line segments', 'LQR', 'linear quadratic regulator', 'optimal waypoint changing points', 'stability bound', 'ground speed changes', 'Lyapunov stability theorem', 'unmanned flying vehicle', 'threat avoidance', 'terrain masking', 'attack directions', 'target location arrival time', 'aircraft control', 'linear quadratic control', 'Lyapunov methods', 'military aircraft', 'missile guidance', 'stability']), ('Development of a health guidance support system for lifestyle improvement\nThe objective is to provide automated advice for lifestyle adjustment based on\nan assessment of the results of a questionnaire and medical examination\nor health checkup data. A system was developed that gathers data based\non questions regarding weight gain, exercise, smoking, sleep, eating\nhabits, salt intake, animal fat intake, snacks, alcohol, and oral\nhygiene, body mass index, resting blood pressure, fasting blood sugar,\ntotal cholesterol, triglycerides, uric acid and liver function tests.\nBased on the relationships between the lifestyle data and the health\ncheckup data, a health assessment sheet was generated for persons being\nallocated to a multiple-risk factor syndrome group. Health assessment\nand useful advice for lifestyle improvement were automatically\nextracted with the system, toward the high risk group for life style\nrelated diseases. The system is operational. In comparison with\nconventional, limited advice methods, we developed a practical system\nthat defined the necessity for lifestyle improvement more clearly, and\nmade giving advice easier\n', ['health guidance support system', 'lifestyle improvement', 'questionnaire', 'medical examination', 'health checkup data', 'weight gain', 'smoking', 'exercise', 'sleep', 'eating habits', 'salt intake', 'animal fat intake', 'snacks', 'alcohol', 'oral hygiene', 'body mass index', 'resting blood pressure', 'fasting blood sugar', 'total cholesterol', 'triglycerides', 'uric acid', 'liver function tests', 'diseases', 'health care', 'medical computing']), ('Perfusion quantification using Gaussian process deconvolution\nThe quantification of perfusion using dynamic susceptibility contrast MRI\n(DSC-MRI) requires deconvolution to obtain the residual impulse\nresponse function (IRF). In this work, a method using the Gaussian\nprocess for deconvolution (GPD) is proposed. The fact that the IRF is\nsmooth is incorporated as a constraint in the method. The GPD method,\nwhich automatically estimates the noise level in each voxel, has the\nadvantage that model parameters are optimized automatically. The GPD is\ncompared to singular value decomposition (SVD) using a common threshold\nfor the singular values, and to SVD using a threshold optimized\naccording to the noise level in each voxel. The comparison is carried\nout using artificial data as well as data from healthy volunteers. It\nis shown that GPD is comparable to SVD with a variable optimized\nthreshold when determining the maximum of the IRF, which is directly\nrelated to the perfusion. GPD provides a better estimate of the entire\nIRF. As the signal-to-noise ratio (SNR) increases or the time\nresolution of the measurements increases, GPD is shown to be superior\nto SVD. This is also found for large distribution volumes\n', ['perfusion quantification', 'Gaussian process deconvolution', 'dynamic susceptibility contrast MRI', 'residual impulse response function', 'optimized model parameters', 'singular value decomposition', 'noise level', 'capillary blood flow', 'mean transit time', 'optimized joint Gaussian distribution', 'correlation length', 'likelihood function', 'biomedical MRI', 'deconvolution', 'Gaussian distribution', 'haemodynamics', 'haemorheology', 'medical image processing', 'singular value decomposition']), ('Chaos theory as a framework for studying information systems\nThis paper introduces chaos theory as a means of studying information systems.\nIt argues that chaos theory, combined with new techniques for\ndiscovering patterns in complex quantitative and qualitative evidence,\noffers a potentially more substantive approach to understand the nature\nof information systems in a variety of contexts. The paper introduces\nchaos theory concepts by way of an illustrative research design\n', ['chaos theory', 'information systems', 'pattern discovery', 'complex quantitative evidence', 'qualitative evidence', 'chaos', 'information systems', 'pattern recognition']), ("The free lunch is over: online content subscriptions on the rise\nHigh need, rather than high use, may be what really determines a user's\nwillingness to pay. Retooling and targeting content may be a sharper\nstrategy than trying to re-educate users that it is time to pay up for\nmaterial that has been free. Waiting for a paradigm shift in general\nuser attitudes about paying for online content Could be a fool's errand\n", ['online content subscriptions', 'content retooling', 'content targeting', 'pay-to-play business models', 'information resources', 'Internet']), ('Improving supply-chain performance by sharing advance demand information\nIn this paper, we analyze how sharing advance demand information (ADI) can\nimprove supply-chain performance. We consider two types of ADI,\naggregated ADI (A-ADI) and detailed ADI (D-ADI). With A-ADI, customers\nshare with manufacturers information about whether they will place an\norder for some product in the next time period, but do not share\ninformation about which product they will order and which of several\npotential manufacturers will receive the order. With D-ADI, customers\nadditionally share information about which product they will order, but\nwhich manufacturer will receive the order remains uncertain. We develop\nand solve mathematical models of supply chains where ADI is shared. We\nderive exact expressions and closed-form approximations for expected\ncosts, expected base-stock levels, and variations of the production\nquantities. We show that both the manufacturer and the customers\nbenefit from sharing ADI, but that sharing ADI increases the bullwhip\neffect. We also show that under certain conditions it is optimal to\ncollect ADI from either none or all of the customers. We study two\nsupply chains in detail: a supply chain with an arbitrary number of\nproducts that have identical demand rates, and a supply chain with two\nproducts that have arbitrary demand rates. For these two supply chains,\nwe analyze how the values of A-ADI and D-ADI depend on the\ncharacteristics of the supply chain and on the quality of the shared\ninformation, and we identify conditions under which sharing A-ADI and\nD-ADI can significantly reduce cost. Our results can be used by\ndecision makers to analyze the cost savings that can be achieved by\nsharing ADI and help them to determine if sharing ADI is beneficial for\ntheir supply chains\n', ['supply-chain performance improvement', 'advance demand information', 'aggregated ADI', 'detailed ADI', 'information sharing', 'manufacturing', 'mathematical models', 'closed-form approximations', 'expected costs', 'expected base-stock levels', 'production quantity variations', 'bullwhip effect', 'arbitrary product number', 'identical demand rates', 'arbitrary demand rates', 'shared information quality', 'decision makers', 'cost savings', 'forecasting', 'decision theory', 'forecasting theory', 'management science', 'manufacture', 'stock control']), ('Speech enhancement using a mixture-maximum model\nWe present a spectral domain, speech enhancement algorithm. The new algorithm\nis based on a mixture model for the short time spectrum of the clean\nspeech signal, and on a maximum assumption in the production of the\nnoisy speech spectrum. In the past this model was used in the context\nof noise robust speech recognition. In this paper we show that this\nmodel is also effective for improving the quality of speech signals\ncorrupted by additive noise. The computational requirements of the\nalgorithm can be significantly reduced, essentially without paying\nperformance penalties, by incorporating a dual codebook scheme with\ntied variances. Experiments, using recorded speech signals and actual\nnoise sources, show that in spite of its low computational\nrequirements, the algorithm shows improved performance compared to\nalternative speech enhancement algorithms\n', ['mixture-maximum model', 'spectral domain', 'speech enhancement algorithm', 'mixture model', 'short time spectrum', 'clean speech signal', 'noisy speech spectrum', 'noise robust speech recognition', 'speech signal quality', 'additive noise', 'performance penalties', 'dual codebook', 'tied variances', 'recorded speech signals', 'noise sources', 'low computational requirements', 'Gaussian mixture model', 'MIXMAX model', 'speech intelligibility', 'computational complexity', 'Gaussian processes', 'noise', 'spectral analysis', 'speech coding', 'speech enhancement', 'speech intelligibility', 'speech recognition']), ('A comparison of different decision algorithms used in volumetric storm cells\nclassification\nDecision algorithms useful in classifying meteorological volumetric radar data\nare discussed. Such data come from the radar decision support system\n(RDSS) database of Environment Canada and concern summer storms created\nin this country. Some research groups used the data completed by RDSS\nfor verifying the utility of chosen methods in volumetric storm cells\nclassification. The paper consists of a review of experiments that were\nmade on the data from RDSS database of Environment Canada and presents\nthe quality of particular classifiers. The classification accuracy\ncoefficient is used to express the quality. For five research groups\nthat led their experiments in a similar way it was possible to compare\nreceived outputs. Experiments showed that the support vector machine\n(SVM) method and rough set algorithms which use object oriented reducts\nfor rule generation to classify volumetric storm data perform better\nthan other classifiers\n', ['decision algorithms', 'volumetric storm cells classification', 'meteorological volumetric radar data', 'radar decision support system', 'summer storms', 'classification accuracy', 'support vector machine', 'rough set algorithms', 'object oriented reducts', 'data mining', 'decision support systems', 'meteorological radar', 'pattern recognition']), ("Convergence of Toland's critical points for sequences of DC functions and\napplication to the resolution of semilinear elliptic problems\nWe prove that if a sequence (f/sub n/)/sub n/ of DC functions (difference of\ntwo convex functions) converges to a DC function f in some appropriate\nway and if u/sub n/ is a critical point of f/sub n/, in the sense\ndescribed by Toland (1978, 1979), and is such that (u/sub n/)/sub n/\nconverges to u, then u is a critical point of f, still in Toland's\nsense. We also build a new algorithm which searches for this critical\npoint u and then apply it in order to compute the solution of a\nsemilinear elliptic equation\n", ['critical point convergence', 'DC function sequences', 'semilinear elliptic problems', 'convex function difference', 'semilinear elliptic equation', 'convergence', 'elliptic equations', 'sequences']), ('A maximum-likelihood surface estimator for dense range data\nDescribes how to estimate 3D surface models from dense sets of noisy range data\ntaken from different points of view, i.e., multiple range maps. The\nproposed method uses a sensor model to develop an expression for the\nlikelihood of a 3D surface, conditional on a set of noisy range\nmeasurements. Optimizing this likelihood with respect to the model\nparameters provides an unbiased and efficient estimator. The proposed\nnumerical algorithms make this estimation computationally practical for\na wide variety of circumstances. The results from this method compare\nfavorably with state-of-the-art approaches that rely on the\nclosest-point or perpendicular distance metric, a convenient heuristic\nthat produces biased solutions and fails completely when surfaces are\nnot sufficiently smooth, as in the case of complex scenes or noisy\nrange measurements. Empirical results on both simulated and real ladar\ndata demonstrate the effectiveness of the proposed method for several\ndifferent types of problems. Furthermore, the proposed method offers a\ngeneral framework that can accommodate extensions to include surface\npriors, more sophisticated noise models, and other sensing modalities,\nsuch as sonar or synthetic aperture radar\n', ['maximum-likelihood surface estimator', 'dense range data', '3D surface models', 'noisy range data', 'sensor model', 'unbiased estimator', 'heuristic', 'biased solutions', 'complex scenes', 'noisy range measurements', 'simulated ladar data', 'real ladar data', 'sonar', 'synthetic aperture radar', 'surface reconstruction', 'surface fitting', 'optimal estimation', 'parameter estimation', 'Bayesian estimation', 'registration', 'calibration', 'calibration', 'computer vision', 'distance measurement', 'image registration', 'maximum likelihood estimation', 'optical radar', 'surface fitting']), ('One structure for fractional delay filter with small number of multipliers\nA wide-bandwidth, high-resolution fractional delay filter (FDF) structure with\na small number of multipliers per output sample and a short coefficient\ncomputing time is presented. The proposal is based on the use of a\nfrequency FDF design method up to only half of the Nyquist frequency,\nin a multirate structure\n', ['fractional delay filter', 'multipliers', 'wide-bandwidth filter', 'high-resolution filter', 'short coefficient computing time', 'multirate structure', 'finite impulse response filter', 'FIR filter', 'frequency domain design method', 'digital arithmetic', 'digital filters', 'FIR filters', 'frequency-domain synthesis', 'multiplying circuits']), ('Application foundations [application servers]\nThe changing role of application servers means choosing the right platform has\nbecome a complex challenge\n', ['application servers', 'Microsoft .Net', 'transaction processing', 'security', 'availability', 'load balancing', 'Java 2 Enterprise Edition', 'application program interfaces']), ('Hardware and software platform for real-time processing and visualization of\nechographic radiofrequency signals\nIn this paper the architecture of a hardware and software platform, for\nultrasonic investigation is presented. The platform, used in\nconjunction with an analog front-end hardware for driving the\nultrasonic transducers of any commercial echograph, having the\nradiofrequency echo signal access, make it possible to dispose of a\npowerful echographic system for experimenting any processing technique,\nalso in a clinical environment in which real-time operation mode is an\nessential prerequisite. The platform transforms any echograph into a\ntest-system for evaluating the diagnostic effectiveness of new\ninvestigation techniques. A particular user interface was designed in\norder to allow a real-time and simultaneous visualization of the\nresults produced in the different stages of the chosen processing\nprocedure. This is aimed at obtaining a better optimization of the\nprocessing algorithm. The most important platform aspect, which also\nconstitutes the basic differentiation with respect to similar systems,\nis the direct processing of the radiofrequency echo signal, which is\nessential for a complete analysis of the particular ultrasound-media\ninteraction phenomenon. The platform completely integrates the\narchitecture of a personal computer (PC) giving rise to several\nbenefits, such as the quick technological evolution in the PC field and\nan extreme degree of programmability for different applications. The PC\nalso constitutes the user interface, as a flexible and intuitive\nvisualization support, and performs some software signal processing, by\ncustom algorithms and commercial libraries. The realized close synergy\nbetween hardware and software allows the acquisition and real-time\nprocessing of the echographic radiofrequency (RF) signal with fast data\nrepresentation\n', ['echographic radiofrequency signal', 'real-time processing', 'data visualization', 'hardware platform', 'software platform', 'ultrasonic imaging', 'clinical diagnosis', 'user interface', 'personal computer', 'acoustic signal processing', 'biomedical ultrasonics', 'data visualisation', 'medical signal processing', 'real-time systems']), ('On bivariate dependence and the convex order\nWe investigate the interplay between variability (in the sense of the convex\norder) and dependence in a bivariate framework, extending some previous\nresults in this area. We exploit the fact that discrete uniform\ndistributions are dense in the space of probability measures in the\ntopology of weak convergence to prove our central result. We also\nobtain a partial result in the general multivariate case. Our findings\ncan be interpreted in terms of the impact of component variability on\nthe mean life of correlated serial and parallel systems\n', ['bivariate dependence', 'convex order', 'discrete uniform distributions', 'probability measures', 'topology', 'weak convergence', 'component variability', 'mean life', 'serial systems', 'parallel systems', 'bivariate probability distributions', 'convergence', 'probability', 'reliability theory', 'topology']), ("Scribe: a large-scale and decentralized application-level multicast\ninfrastructure\nThis paper presents Scribe, a scalable application-level multicast\ninfrastructure. Scribe supports large numbers of groups, with a\npotentially large number of members per group. Scribe is built on top\nof Pastry, a generic peer-to-peer object location and routing substrate\noverlayed on the Internet, and leverages Pastry's reliability,\nself-organization, and locality properties. Pastry is used to create\nand manage groups and to build efficient multicast trees for the\ndissemination of messages to each group. Scribe provides best-effort\nreliability guarantees, and we outline how an application can extend\nScribe to provide stronger reliability. Simulation results, based on a\nrealistic network topology model, show that Scribe scales across a wide\nrange of groups and group sizes. Also, it balances the load on the\nnodes while achieving acceptable delay and link stress when compared\nwith Internet protocol multicast\n", ['Scribe', 'decentralized application-level multicast infrastructure', 'scalable application-level multicast infrastructure', 'Pastry', 'generic peer-to-peer object location', 'generic routing substrate', 'Internet', 'self-organization', 'discrete event simulator', 'locality properties', 'best-effort reliability guarantees', 'simulation results', 'network topology model', 'group size', 'network nodes', 'delay', 'link stress', 'Internet protocol multicast', 'computer network reliability', 'discrete event simulation', 'Internet', 'large-scale systems', 'multicast communication', 'performance evaluation', 'self-adjusting systems', 'telecommunication network routing', 'transport protocols']), ('Novel ZE-isomerism descriptors derived from molecular topology and their\napplication to QSAR analysis\nWe introduce several series of novel ZE-isomerism descriptors derived directly\nfrom two-dimensional molecular topology. These descriptors make use of\na quantity named ZE-isomerism correction, which is added to the vertex\ndegrees of atoms connected by double bonds in Z and E configurations.\nThis approach is similar to the one described previously for\ntopological chirality descriptors (Golbraikh, A., et al. J. Chem. Inf.\nComput. Sci. 2001, 41, 147-158). The ZE-isomerism descriptors include\nmodified molecular connectivity indices, overall Zagreb indices,\nextended connectivity, overall connectivity, and topological charge\nindices. They can be either real or complex numbers. Mathematical\nproperties of different subgroups of ZE-isomerism descriptors are\ndiscussed. These descriptors circumvent the inability of conventional\ntopological indices to distinguish between Z and E isomers. The\napplicability of ZE-isomerism descriptors to QSAR analysis is\ndemonstrated in the studies of a series of 131 anticancer agents\ninhibiting tubulin polymerization\n', ['ZE-isomerism descriptors', 'two-dimensional molecular topology', 'QSAR analysis', 'quantitative structure-activity relationship', 'ZE-isomerism correction', 'vertex degrees', 'double bond connected atoms', 'modified molecular connectivity indices', 'overall Zagreb indices', 'extended connectivity', 'overall connectivity', 'topological charge indices', 'complex numbers', 'anticancer agents', 'tubulin polymerization', 'descriptor pharmacophore', 'chemical databases', 'molecular graphs', 'computer-assisted drug design', 'toxicities', 'combinatorial chemical libraries', 'biochemistry', 'bonds (chemical)', 'chemistry computing', 'chirality', 'combinatorial mathematics', 'isomerism', 'molecular biophysics', 'topology', 'vocabulary']), ('The analysis and control of longitudinal vibrations from wave viewpoint\nThe analysis and control of longitudinal vibrations in a rod from feedback wave\nviewpoint are synthesized. Both collocated and noncollocated feedback\nwave control strategies are explored. The control design is based on\nthe local properties of wave transmission and reflection in the\nvicinity of the control force applied area, hence there is no complex\nclosed form solution involved. The controller is designed to achieve\nvarious goals, such as absorbing the incoming vibration energy,\ncreating a vibration free zone and eliminating standing waves in the\nstructure. The findings appear to be very useful in practice due to the\nsimplicity in the implementation of the controllers\n', ['longitudinal vibration control', 'feedback waves', 'noncollocated feedback wave control', 'collocated feedback wave control', 'control design', 'wave transmission', 'wave reflection', 'control force', 'complex closed form solution', 'standing waves', 'vibration energy', 'vibration free zone', 'acoustic noise', 'acoustic wave reflection', 'acoustic wave transmission', 'force feedback', 'vibration control', 'vibrations']), ('Detection and estimation of abrupt changes in the variability of a process\nDetection of change-points in normal means is a well-studied problem. The\nparallel problem of detecting changes in variance has had less\nattention. The form of the generalized likelihood ratio test statistic\nhas long been known, but its null distribution resisted exact analysis.\nIn this paper, we formulate the change-point problem for a sequence of\nchi-square random variables. We describe a procedure that is exact for\nthe distribution of the likelihood ratio statistic for all even degrees\nof freedom, and gives upper and lower bounds for odd (and also for\nnon-integer) degrees of freedom. Both the liberal and conservative\nbounds for chi /sub 1//sup 2/ degrees of freedom are shown through\nsimulation to be reasonably tight. The important problem of testing for\nchange in the normal variance of individual observations corresponds to\nthe chi /sub 1//sup 2/ case. The non-null case is also covered, and\nconfidence intervals for the true change point are derived. The\nmethodology is illustrated with an application to quality control in a\ndeep level gold mine. Other applications include ambulatory monitoring\nof medical data and econometrics\n', ['abrupt change detection', 'abrupt change estimation', 'process variability', 'sequence', 'chi-square random variables', 'generalized likelihood ratio test statistic', 'distribution', 'even degrees of freedom', 'upper bounds', 'lower bounds', 'odd degrees of freedom', 'noninteger degrees of freedom', 'liberal bounds', 'conservative bounds', 'simulation', 'individual observations', 'non null case', 'confidence intervals', 'quality control', 'deep level gold mine', 'ambulatory monitoring', 'medical data', 'econometrics', 'data analysis', 'maximum likelihood estimation', 'quality control', 'sequences', 'statistical process control']), ('Assessment of prehospital chest pain using telecardiology\nTwo hundred general practitioners were equipped with a portable\nelectrocardiograph which could transmit a 12-lead electrocardiogram\n(ECG) via a telephone line. A cardiologist was available 24 h a day for\nan interactive teleconsultation. In a 13 month period there were 5073\ncalls to the telecardiology service and 952 subjects with chest pain\nwere identified. The telecardiology service allowed the general\npractitioners to manage 700 cases (74%) themselves; further diagnostic\ntests were requested for 162 patients (17%) and 83 patients (9%) were\nsent to the hospital emergency department. In the last group a\ncardiological diagnosis was confirmed in 60 patients and refuted in 23.\nSeven patients in whom the telecardiology service failed to detect a\ncardiac problem were hospitalized in the subsequent 48 h. The\ntelecardiology service showed a sensitivity of 97.4%, a specificity of\n89.5% and a diagnostic accuracy of 86.9% for chest pain. Telemedicine\ncould be a useful tool in the diagnosis of chest pain in primary care\n', ['prehospital chest pain assessment', 'telecardiology', 'general practitioners', 'portable electrocardiograph', 'electrocardiogram transmission', 'telephone line', 'interactive teleconsultation', 'patients', 'diagnostic tests', 'hospital emergency department', 'sensitivity', 'specificity', 'diagnostic accuracy', 'primary care', '13 month', 'electrocardiography', 'medical diagnostic computing', 'medical signal processing', 'patient diagnosis', 'telemedicine']), ('Multiecho segmented EPI with z-shimmed background gradient compensation\n(MESBAC) pulse sequence for fMRI\nA MultiEcho Segmented EPI with z-shimmed BAckground gradient Compensation\n(MESBAC) pulse sequence is proposed and validated for functional MRI\n(fMRI) study in regions suffering from severe susceptibility artifacts.\nThis sequence provides an effective tradeoff between spatial and\ntemporal resolution and reduces image distortion and signal dropout.\nThe blood oxygenation level-dependent (BOLD)-weighted fMRI signal can\nbe reliably obtained in the region of the orbitofrontal cortex (OFC).\nTo overcome physiological motion artifacts during prolonged\nmultisegment EPI acquisition, two sets of navigator echoes were\nacquired in both the readout and phase-encoding directions. Ghost\nartifacts generally produced by single-shot EPI acquisition were\neliminated by separately placing the even and odd echoes in different\nk-space trajectories. Unlike most z-shim methods that focus on\nincreasing temporal resolution for event-related functional brain\nmapping, the MESBAC sequence simultaneously addresses problems of image\ndistortion and signal dropout while maintaining sufficient temporal\nresolution. The MESBAC sequence will be particularly useful for\npharmacological and affective fMRI studies in brain regions such as the\nOFC, nucleus accumbens, amygdala, para-hippocampus, etc\n', ['fMRI', 'multiecho segmented EPI', 'z-shimmed background gradient compensation', 'gradient compensation pulse sequence', 'severe susceptibility artifacts', 'BOLD-weighted signal', 'orbitofrontal cortex', 'navigator echoes', 'ghost artifacts', 'event-related functional brain mapping', 'signal dropout', 'spatial resolution', 'temporal resolution', 'image distortion', 'biomedical MRI', 'brain', 'image resolution', 'image segmentation', 'medical image processing', 'motion compensation']), ('A comparison theorem for the iterative method with the preconditioner (I +\nS/sub max/)\nA.D. Gunawardena et al. (1991) have reported the modified Gauss-Seidel method\nwith a preconditioner (I + S). In this article, we propose to use a\npreconditioner (I + S/sub max/) instead of (I + S). Here, S/sub max/ is\nconstructed by only the largest element at each row of the upper\ntriangular part of A. By using the lemma established by M. Neumann and\nR.J. Plemmons (1987), we get the comparison theorem for the proposed\nmethod. Simple numerical examples are also given\n', ['iterative method', 'preconditioner', 'modified Gauss-Seidel method', 'comparison theorem', 'conjugate gradient methods', 'iterative methods']), ('Disposable mobiles\nAfter many delays, the reusable, recyclable, disposable mobile phone is finally\ngoing on sale in the US. But with a business model largely dependent on\nniche markets, Elizabeth Biddlecombe asks if these simplified handsets\nwill be good enough to survive a brutal market\n', ['disposable mobile phone', 'simplified handsets', 'reusable', 'recyclable', 'cellular radio', 'telephone sets']), ('Design and analysis of optimal material distribution policies in flexible\nmanufacturing systems using a single AGV\nModern automated manufacturing processes employ automated guided vehicles\n(AGVs) for material handling, which serve several machine centres (MC)\nin a factory. Optimal scheduling of AGVs can significantly help to\nincrease the efficiency of the manufacturing process by minimizing the\nidle time of MCs waiting for the raw materials. We analyse the\nrequirements for an optimal schedule and then provide a mathematical\nframework for an efficient schedule of material delivery by an AGV. A\nmathematical model is developed and then a strategy for optimal\nmaterial distribution of the available raw material to the MCs is\nderived. With this model, the optimal number of MCs to be utilized is\nalso determined. Finally, the material delivery schedule employing\nmultiple journeys to the MCs by the AGV is carried out. Through\nrigorous analysis and simulation experiments, we show that such a\ndelivery strategy will optimize the overall performance\n', ['optimal material distribution policies', 'flexible manufacturing systems', 'AGV', 'automated guided vehicle', 'material handling', 'machine centres', 'waiting time', 'manufacturing lead time', 'optimal scheduling', 'idle time minimization', 'material delivery', 'automatic guided vehicles', 'flexible manufacturing systems', 'flow graphs', 'materials handling', 'optimisation', 'production control']), ('Adaptive tracking controller design for robotic systems using Gaussian wavelet\nnetworks\nAn adaptive tracking control design for robotic systems using Gaussian wavelet\nnetworks is proposed. A Gaussian wavelet network with accurate\napproximation capability is employed to approximate the unknown\ndynamics of robotic systems by using an adaptive learning algorithm\nthat can learn the parameters of the dilation and translation of\nGaussian wavelet functions. Depending on the finite number of wavelet\nbasis functions which result in inevitable approximation errors, a\nrobust control law is provided to guarantee the stability of the\nclosed-loop robotic system that can be proved by Lyapunov theory.\nFinally, the effectiveness of the Gaussian wavelet network-based\ncontrol approach is illustrated through comparative simulations on a\nsix-link robot manipulator\n', ['adaptive tracking controller design', 'robotic systems', 'Gaussian wavelet networks', 'accurate approximation capability', 'unknown dynamics', 'adaptive learning algorithm', 'approximation errors', 'robust control law', 'closed-loop system', 'Lyapunov theory', 'six-link robot manipulator', 'adaptive control', 'closed loop systems', 'control system synthesis', 'Lyapunov methods', 'manipulator dynamics', 'neurocontrollers', 'robust control', 'tracking']), ('Optimal bandwidth utilization of all-optical ring with a converter of degree 4\nIn many models of all-optical routing, a set of communication paths in a\nnetwork is given, and a wavelength is to be assigned to each path so\nthat paths sharing an edge receive different wavelengths. The goal is\nto assign as few wavelengths as possible, in order to use the optical\nbandwidth efficiently. If a node of a network contains a wavelength\nconverter, any path that passes through this node may change its\nwavelength. Having converters at some of the nodes can reduce the\nnumber of wavelengths required for routing. This paper presents a\nwavelength converter with degree 4 and gives a routing algorithm which\nshows that any routing with load L can be realized with L wavelengths\nwhen a node of an all-optical ring hosts such a wavelength converter.\nIt is also proved that 4 is the minimum degree of the converter to\nreach the full utilization of the available wavelengths if only one\nnode of an all-optical ring hosts a converter\n', ['all-optical routing', 'communication paths', 'all-optical network', 'wavelength assignment', 'wavelength translation', 'wavelength converter', 'all-optical ring', 'optical fibre networks', 'telecommunication network routing', 'wavelength division multiplexing']), ('Multiple criteria decision making without optimization\nWe present a development intended to make interactive decision making schemes\naccessible for a wider spectrum of decision makers. To this aim we\npropose to eliminate the need to solve optimization problems at\nsuccessive iterations of interactive decision processes. We show that\nthe need for optimization can be eliminated by the ability of\nestablishing sufficiently tight bounds on criteria values for efficient\ndecisions prior to explicit identification of such decisions. We\npresent a technique, fully operational and numerically simple, for\nestablishing such bounds. Bounds are dynamic, i.e., they become\nstronger with the growing number of decisions explicitly identified.\nThey are also parametric with respect to weighting coefficients. We\nalso point out how this technique can enhance the existing interactive\ndecision making methods\n', ['multiple criteria decision making', 'interactive decision making schemes', 'decision makers', 'criteria values bounds', 'efficient decisions', 'computational complexity', 'convergence', 'decision theory']), ("An application of fuzzy linear regression to the information technology in\nTurkey\nFuzzy set theory deals with the vagueness of human thought. A major\ncontribution of fuzzy set theory is its capability of representing\nvague knowledge. Fuzzy set theory is very practical when sufficient and\nreliable data isn't available. Information technology (IT) is the\nacquisition, processing, storage and dissemination of information in\nall its forms (auditory, pictorial, textual and numerical) through a\ncombination of computers, telecommunication, networks and electronic\ndevices. IT includes matters concerned with the furtherance of computer\nscience and technology, design, development, installation and\nimplementation of information systems and applications. In the paper,\nassuming that there are n independent variables and the regression\nfunction is linear, the possible levels of information technology (the\nsale levels of computer equipment) in Turkey are forecasted by using\nfuzzy linear regression. The independent variables assumed are the\nimport level and the export level of computer equipment\n", ['fuzzy linear regression', 'information technology', 'Turkey', 'vague knowledge representation', 'IT', 'computers', 'telecommunication', 'electronic devices', 'computer science', 'computer technology', 'information systems', 'regression function', 'computer equipment export level', 'fuzzy set theory', 'information technology', 'statistical analysis', 'technological forecasting']), ('International library consortia: positive starts, promising futures\nLibrary consortia have grown substantially over the past ten years, both within\nNorth America and globally. As this resurgent consortial movement has\nbegun to mature, and as publishers and vendors have begun to adapt to\nconsortial purchasing models, consortia have expanded their agendas for\naction. The movement to globalize consortia is traced (including the\ndevelopment and current work of the International Coalition of Library\nConsortia-ICOLC). A methodology is explored to classify library\nconsortia by articulating the key factors that affect and distinguish\nconsortia as organizations within three major areas: strategic,\ntactical, and practical (or managerial) concerns. Common consortial\nvalues are examined, and a list of known international library\nconsortia is presented\n', ['consortial purchasing models', 'international library consortia', 'libraries', 'purchasing']), ('The effect of a male-oriented computer gaming culture on careers in the\ncomputer industry\nIf careers in the computer industry were viewed, it would be evident that there\nis a conspicuous gender gap between the number of male and female\nemployees. The same gap can be observed at the college level where\nmales are dominating females as to those who pursue and obtain a degree\nin computer science. The question that this research paper intends to\nshow is: why are males so dominant when it comes to computer related\nmatters? The author has traced this question back to the computer game.\nComputer games are a fun medium and provide the means for an individual\nto become computer literate through the engagement of spatial learning\nand cognitive processing abilities. Since such games are marketed\nalmost exclusively to males, females have a distinct disadvantage.\nMales are more computer literate through the playing of computer games,\nand are provided with an easy lead-in to more advanced utilization of\ncomputers such as programming. Females tend to be turned off due to the\nmale stereotypes and marketing associated with games and thus begins\nthe gender gap\n', ['careers', 'computer industry', 'gender gap', 'computer science degree', 'computer games', 'computer literacy', 'female employees', 'spatial learning', 'cognitive processing', 'male stereotypes', 'marketing', 'computer games', 'computer literacy', 'DP industry', 'gender issues', 'personnel']), ('Two issues in setting call centre staffing levels\nMotivated by a problem facing the Police Communication Centre in Auckland, New\nZealand, we consider the setting of staffing levels in a call centre\nwith priority customers. The choice of staffing level over any\nparticular time period (e.g., Monday from 8 am-9 am) relies on accurate\narrival rate information. The usual method for identifying the arrival\nrate based on historical data can, in some cases, lead to considerable\nerrors in performance estimates for a given staffing level. We explain\nwhy, identify three potential causes of the difficulty, and describe a\nmethod for detecting and addressing such a problem\n', ['call centre staffing levels', 'police communication centre', 'Auckland', 'New Zealand', 'priority customers', 'arrival rate information', 'performance estimates', 'forecast error', 'nonstationarity', 'conditional Poisson process', 'call centres', 'exponential distribution', 'human resource management', 'operations research', 'police', 'queueing theory', 'stochastic processes']), ('The network society as seen from Italy\nItaly was behind the European average in Internet development for many years,\nbut a new trend, which has brought considerable change, emerged at the\nend of 1998 and showed its effects in 2000 and the following years. Now\nItaly is one of the top ten countries worldwide in Internet hostcount\nand the fourth largest in Europe. The density of Internet activity in\nItaly in proportion to the population is still below the average in the\nEuropean Union, but is growing faster than Germany, the UK and France,\nand faster than the worldwide or European average. From the point of\nview of media control there are several problems. Italy has democratic\ninstitutions and freedom of speech, but there is an alarming\nconcentration in the control of mainstream media (especially\nbroadcast). There are no officially declared restrictions in the use of\nthe Internet, but several legal and regulatory decisions reveal a\ndesire to limit freedom of opinion and dialogue and/or gain centralized\ncontrol of the Net\n', ['network society', 'Italy', 'European average', 'Internet development', 'Internet hostcount', 'Europe', 'Internet activity', 'European Union', 'Germany', 'UK', 'France', 'worldwide average', 'media control', 'democratic institutions', 'freedom of speech', 'mainstream media', 'broadcast media', 'legal decisions', 'regulatory decisions', 'centralized control', 'government policies', 'Internet', 'politics', 'social aspects of automation']), ('A six-degree-of-freedom precision motion stage\nThis article presents the design and performance evaluation of a\nsix-degree-of-freedom piezoelectrically actuated fine motion stage that\nwill be used for three dimensional error compensation of a long-range\ntranslation mechanism. Development of a single element, piezoelectric\nlinear displacement actuator capable of translations of 1.67 mu m with\n900 V potential across the electrodes and under a 27.4 N axial load and\n0.5 mm lateral distortion is presented. Finite element methods have\nbeen developed and used to evaluate resonant frequencies of the stage\nplatform and the complete assembly with and without a platform payload.\nIn general, an error of approximately 10.0% between the finite element\nresults and the experimentally measured values were observed. The\ncomplete fine motion stage provided approximately +or-0.93 mu m of\ntranslation and +or-38.0 mu rad of rotation in all three planes of\nmotion using an excitation range of 1000 V. An impulse response\nindicating a fundamental mode resonance at 162 Hz was measured with a\n0.650 kg payload rigidly mounted to the top of the stage\n', ['six-degree-of-freedom precision motion stage', 'performance evaluation', 'design', 'piezoelectrically actuated fine motion stage', '3D error compensation', 'long-range translation mechanism', 'single element piezoelectric linear displacement actuator', 'finite element methods', 'resonant frequency', 'stage platform', 'platform payload', 'impulse response', 'fundamental mode resonance', '1.67 micron', '900 V', '1000 V', '0.93 to -0.93 micron', '162 Hz', '650.0 gm', 'error compensation', 'finite element analysis', 'microactuators', 'micropositioning', 'piezoelectric actuators']), ("A computational model of learned avoidance behavior in a one-way avoidance\nexperiment\nWe present a computational model of learned avoidance behavior in a one-way\navoidance experiment. Our model employs the reinforcement learning\nparadigm and a temporal-difference algorithm to implement both\nclassically conditioned and instrumentally conditioned components. The\nrole of the classically conditioned component is to develop an\nexpectation of future benefit that is a function of the learning\nsystem's state and action. Competition among the instrumentally\nconditioned components determines the overt behavior generated by the\nlearning system. Our model displays, in simulation, the reduced latency\nof the avoidance behavior during learning with continuing trials and\nthe resistance to extinction of the avoidance response. These results\nare consistent with experimentally observed animal behavior. Our model\nextends the traditional two-process learning mechanism of Mowrer (1947)\nby explicitly defining the mechanisms of proprioceptive feedback, an\ninternal clock, and generalization over the action space\n", ['computational model', 'learned avoidance behavior', 'one-way avoidance experiment', 'reinforcement learning', 'temporal-difference algorithm', 'classically conditioned components', 'instrumentally conditioned components', 'reduced latency', 'animal behavior', 'traditional two-process learning mechanism', 'proprioceptive feedback', 'internal clock', 'behavioural sciences', 'cognitive systems', 'learning (artificial intelligence)']), ('Underground poetry, collecting poetry, and the librarian\nA powerful encounter with underground poetry and its important role in poetry,\nliterature, and culture is discussed. The acquisitions difficulties\nencountered in the unique publishing world of underground poetry are\nintroduced. Strategies for acquiring underground poetry for library\ncollections are proposed, including total immersion and local focus,\nwith accompanying action\n', ['librarian', 'underground poetry', 'publishing', 'library collections', 'out-of-print books', 'special collections', 'literature', 'culture', 'library automation', 'literature', 'publishing']), ('Simple minds [health care IT]\nA few things done properly, and soon, is the short-term strategy for the UK NHS\nIT programme. Can it deliver this time?\n', ['UK NHS IT programme', 'health care', 'strategy', 'health care']), ("The Tattletale technique\nPractical experience has taught many Java developers one thing: critical\nresources (mutexes, database connections, transactions, file handles,\netc.) require timely and systematic release. Unfortunately, Java's\ngarbage collector is not up to that job. According to the Java Language\nSpecification, there are no guarantees when a garbage collector will\nrun, when it will collect an object, or when it will finalize an object\n- if ever. Even more unfortunately, Java's counterpart to the C++\ndestructor (the finally block) is both tedious and error-prone,\nrequiring developers to constantly remember and duplicate\nresource-releasing code. Consequently, even good Java developers can\nforget to release critical resources. There is a light at the end of\nthe tunnel. Java may make it easier to leak critical resources, but it\nalso provides the necessary mechanisms to easily track them down. The\nTattletale technique is a simple method for designing new classes and\nretrofitting existing classes to quickly and easily detect the\noffending code responsible for leaking resources\n", ['Java', 'critical resources', 'mutexes', 'database connections', 'transactions', 'file handles', 'garbage collector', 'resource-releasing code', 'Tattletale technique', 'resources leaking', 'Java', 'program diagnostics', 'storage management']), ('Knowledge management-capturing the skills of key performers in the power\nindustry\nThe growing pressure to reduce the cost of electrical power in recent years has\nresulted in an enormous "brain-drain" within the power industry. A\nnovel approach has been developed by Eskom to capture these skills\nbefore they are lost and to incorporate these into a computer-based\nprogramme called "knowledge management"\n', ['power industry', 'key performers', 'knowledge management', 'skills capture', 'brain-drain', 'Eskom', 'computer-based programme', 'South Africa', 'personnel management', 'electricity supply industry', 'human resource management', 'knowledge engineering']), ("Ultrafast compound imaging for 2-D motion vector estimation: application to\ntransient elastography\nThis paper describes a new technique for two-dimensional (2-D) imaging of the\nmotion vector at a very high frame rate with ultrasound. Its potential\nis experimentally demonstrated for transient elastography. But, beyond\nthis application, it also could be promising for color flow and\nreflectivity imaging. To date, only axial displacements induced in\nhuman tissues by low-frequency vibrators were measured during transient\nelastography. The proposed technique allows us to follow both axial and\nlateral displacements during the shear wave propagation and thus should\nimprove Young's modulus image reconstruction. The process is a\ncombination of several ideas well-known in ultrasonic imaging:\nultra-fast imaging, multisynthetic aperture beamforming, 1-D speckle\ntracking, and compound imaging. Classical beamforming in the transmit\nmode is replaced here by a single plane wave insonification increasing\nthe frame rate by at least a factor of 128. The beamforming is achieved\nonly in the receive mode on two independent subapertures. Comparison of\nsuccessive frames by a classical 1-D speckle tracking algorithm allows\nestimation of displacements along two different directions linked to\nthe subapertures beams. The variance of the estimates is finally\nimproved by tilting the emitting plane wave at each insonification,\nthus allowing reception of successive decorrelated speckle patterns\n", ['ultrafast compound imaging', '2D motion vector estimation', 'two-dimensional imaging', 'transient elastography', '2D imaging', 'high frame rate', 'ultrasound', 'colour flow imaging', 'reflectivity imaging', 'human tissues', 'axial displacements', 'lateral displacements', 'shear wave propagation', "Young's modulus image reconstruction", 'ultrasonic imaging', 'multisynthetic aperture beamforming', '1D speckle tracking algorithm', 'decorrelated speckle patterns', 'single plane wave insonification', 'array signal processing', 'biomedical ultrasonics', 'medical image processing', 'motion estimation', 'speckle', 'tracking', 'ultrasonic imaging', "Young's modulus"]), ('A dynamic checkpoint scheduling scheme for fault tolerant distributed computing\nsystems\nThe selection of the optimal checkpointing interval has been a very critical\nissue in implementing checkpointing-recovery schemes for fault tolerant\ndistributed systems. This paper presents a new scheme that allows a\nprocess to select the proper checkpointing interval dynamically. A\nprocess in the system evaluates the cost of checkpointing and possible\nrollback for each checkpointing interval and selects the proper time\ninterval for the next checkpointing. Unlike the other schemes, the\noverhead incurred by both the checkpointing and rollback activities are\nconsidered for the cost evaluation, and the current communication\npattern is reflected in the selection of the checkpointing interval.\nMoreover, the proposed scheme requires no extra message communication\nfor the checkpointing interval selection and can easily be incorporated\ninto the existing checkpointing coordination schemes\n', ['dynamic checkpoint scheduling scheme', 'fault tolerant computing', 'distributed computing systems', 'optimal checkpointing interval', 'rollback recovery', 'cost evaluation', 'communication pattern', 'distributed processing', 'fault tolerant computing', 'processor scheduling', 'system recovery']), ('John McCarthy: father of AI\nIf John McCarthy, the father of AI, were to coin a new phrase for "artificial\nintelligence" today, he would probably use "computational\nintelligence." McCarthy is not just the father of AI, he is also the\ninventor of the Lisp (list processing) language. The author considers\nMcCarthy\'s conception of Lisp and discusses McCarthy\'s recent research\nthat involves elaboration tolerance, creativity by machines, free will\nof machines, and some improved ways of doing situation calculus\n', ['John McCarthy', 'father of AI', 'artificial intelligence', 'computational intelligence', 'Lisp', 'list processing', 'elaboration tolerance', 'creativity', 'free will', 'situation calculus', 'artificial intelligence', 'LISP']), ('Wave propagation related to high-speed train. A scaled boundary FE-approach for\nunbounded domains\nAnalysis of wave propagation in solid materials under moving loads is a topic\nof great interest in railway engineering. The objective of the paper is\nthree-dimensional modelling of high-speed train related ground\nvibrations; in particular the question of how to account for the\nunbounded media is addressed. For efficient and accurate modelling of\nrailway structural components taking the unbounded media into account,\na hybrid method based on a combination of the conventional finite\nelement method and scaled boundary finite element method is\nestablished. In the paper, element matrices and solution procedures for\nthe scaled boundary finite element method (SBFEM) are derived. A\nnon-linear finite element iteration scheme using Lagrange multipliers\nand coupling between the unbounded domain and the finite element domain\nare also discussed. Two numerical examples including one example\ndemonstrating the dynamical response of a railroad section are\npresented to demonstrate the performance of the proposed method\n', ['wave propagation', 'solid materials', 'railway engineering', '3D modelling', 'high-speed train related ground vibrations', 'unbounded media', 'modelling', 'railway structural components', 'scaled boundary finite element method', 'element matrices', 'solution procedures', 'nonlinear finite element iteration scheme', 'Lagrange multipliers', 'dynamical response', 'railroad section', 'boundary-elements methods', 'finite element analysis', 'iterative methods', 'Jacobian matrices', 'railways', 'structural engineering computing', 'vibrations', 'wave propagation']), ('A wizard idea [Internet in finance]\nNew technology is set to become an ever-more important area of work for\nbrokers. Lawrie Holmes looks at how the Internet is driving change and\nopportunity\n', ['brokers', 'Internet', 'finance', 'electronic commerce', 'finance', 'Internet', 'investment']), ('Topology-adaptive modeling of objects using surface evolutions based on 3D\nmathematical morphology\nLevel set methods were proposed mainly by mathematicians for constructing a\nmodel of a 3D object of arbitrary topology. However, those methods are\ncomputationally inefficient due to repeated distance transformations\nand increased dimensions. In the paper, we propose a new method of\nmodeling fast objects of arbitrary topology by using a surface\nevolution approach based on mathematical morphology. Given sensor data\ncovering the whole object surface, the method begins with an initial\napproximation of the object by evolving a closed surface into a model\ntopologically equivalent to the real object. The refined approximation\nis then performed using energy minimization. The method has been\napplied in several experiments using range data, and the results are\nreported in the paper\n', ['level set methods', 'topology-adaptive modeling', 'surface evolutions', '3D mathematical morphology', '3D object', 'arbitrary topology', 'repeated distance transformations', 'initial approximation', 'refined approximation', 'energy minimization', 'range data', 'pseudo curvature flow', 'approximation theory', 'curvature measurement', 'distance measurement', 'image processing', 'mathematical morphology', 'minimisation', 'set theory', 'topology']), ('A re-examination of probability matching and rational choice\nIn a typical probability learning task participants are presented with a\nrepeated choice between two response alternatives, one of which has a\nhigher payoff probability than the other. Rational choice theory\nrequires that participants should eventually allocate all their\nresponses to the high-payoff alternative, but previous research has\nfound that people fail to maximize their payoffs. Instead, it is\ncommonly observed that people match their response probabilities to the\npayoff probabilities. We report three experiments on this choice\nanomaly using a simple probability learning task in which participants\nwere provided with (i) large financial incentives, (ii) meaningful and\nregular feedback, and (iii) extensive training. In each experiment\nlarge proportions of participants adopted the optimal response strategy\nand all three of the factors mentioned above contributed to this. The\nresults are supportive of rational choice theory\n', ['probability learning task', 'rational choice theory', 'probability matching', 'payoff probability', 'response probabilities', 'choice anomaly', 'large financial incentives', 'meaningful regular feedback', 'extensive training', 'optimal response strategy', 'feedback', 'rationality', 'decision theory', 'feedback', 'probability', 'psychology']), ('Technology in distance education: a global perspective to alternative delivery\nmechanisms\nTechnology is providing a positive impact on delivery mechanisms employed in\ndistance education at the university level. Some institutions are\nincorporating distance education as a way to extend the classroom.\nOther institutions are investigating new delivery mechanisms, which\nsupport a revised perspective on education. These latter institutions\nare revising their processes for interacting with students, and taking\na more "learner centered" approach to the delivery of education. This\narticle discusses the impact of technology on the delivery mechanisms\nemployed in distance education. A framework is proposed here, which\npresents a description of alternative modes of generic delivery\nmechanisms. It is suggested that those institutions, which adopt a\ndelivery mechanism employing an asynchronous mode, can gain the most\nbenefit from technology. This approach seems to represent the only\ntruly innovative use of technology in distance education. The approach\ncreates a student-oriented environment while maintaining high levels of\ninteraction, both of which are factors that contribute to student\nsatisfaction with their overall educational experience\n', ['distance education', 'educational technology', 'university education', 'learner centered approach', 'student satisfaction', 'global perspective', 'asynchronous mode', 'distance learning', 'educational technology']), ('Software Technology: looking for quality accountants\nSoftware Technology wants to turn 23 years of reselling experience in the legal\nbusiness into an asset in the accounting market\n', ['Software Technology', 'reselling', 'accounting market', 'accounting']), ("Low to mid-speed copiers [buyer's guide]\nThe low to mid-speed copier market is being transformed by the almost universal\nadoption of digital solutions. The days of the analogue copier are\nnumbered as the remaining vendors plan to withdraw from this sector by\n2005. Reflecting the growing market for digital, vendors are reducing\nprices, making a digital solution much more affordable. The battle for\nthe copier market is intense, and the popularity of the multifunctional\ndevice is going to transform the office equipment market. As total cost\nof ownership becomes increasingly important and as budgets are\nsqueezed, the most cost-effective solutions are those that will survive\nthis shake-down\n", ['low to mid-speed copier market', 'total cost of ownership', "buyer's guides", 'photocopying']), ('Stabilization of positive systems with first integrals\nPositive systems possessing first integrals are considered. These systems\nfrequently occur in applications. The paper is devoted to two\nstabilization problems. The first is concerned with the design of\nfeedbacks to stabilize a given level set. Secondly, it is shown that\nthe same feedback allows us to globally stabilize an equilibrium point\nif it is asymptotically stable with respect to initial conditions in\nits level set. Two examples are provided and the results are compared\nwith those in the literature\n', ['positive systems', 'first integrals', 'feedbacks design', 'global stabilization', 'equilibrium point', 'asymptotic stability', 'chemical reactors', 'asymptotic stability', 'chemical technology', 'closed loop systems', 'control system synthesis', 'feedback', 'integral equations', 'set theory']), ('Tool and process improvements from MFC control system technology\nA new approach to MFC calibration links the physical parameters of nitrogen to\nthe physical characteristics of various process gases. This precludes\nthe conventional need for surrogate gases. What results is a\nphysics-based tuning algorithm and enhanced digital control system that\nenables rearranging and gas change of digital MFCs. The end result\nshould be better process control through more accurate gas flow. The\nnew method also decreases the number of MFC spare parts required to\nback up a fab\n', ['MFC control system technology', 'calibration', 'process gas', 'surrogate gas', 'tuning algorithm', 'digital control', 'process control', 'gas flow', 'semiconductor fab', 'tool technology', 'mass flow controller', 'N/sub 2/', 'calibration', 'controllers', 'digital control', 'flow control', 'process control', 'tuning']), ('Accelerated simulation of the steady-state availability of non-Markovian\nsystems\nA general accelerated simulation method for evaluation of the steady-state\navailability of non-Markovian systems is proposed. It is applied to the\ninvestigation of a class of systems with repair. Numerical examples are\ngiven\n', ['accelerated simulation', 'steady-state availability', 'non-Markovian systems', 'general accelerated simulation method', 'numerical examples', 'fault trees', 'probability', 'system recovery']), ('A column generation approach to delivery planning over time with inhomogeneous\nservice providers and service interval constraints\nWe consider a problem of delivery planning over multiple time periods.\nDeliveries must be made to customers having nominated demand in each\ntime period. Demand must be met in each time period by use of some\ncombination of inhomogeneous service providers. Each service provider\nhas a different delivery capacity, different cost of delivery to each\ncustomer, a different utilisation requirement, and different rules\ngoverning the spread of deliveries in time. The problem is to plan\ndeliveries so as to minimise overall costs, subject to demand being met\nand service rules obeyed. A natural integer programming model was found\nto be intractable, except on problems with loose demand constraints,\nwith gaps between best lower bound and best feasible solution of up to\n35.1%, with an average of 15.4% over the test data set. In all but the\nproblem with loosest demand constraints, Cplex 6.5 applied to this\nformulation failed to find the optimal solution before running out of\nmemory. However a column generation approach improved the lower bound\nby between 0.6% and 21.9%, with an average of 9.9%, and in all cases\nfound the optimal solution at the root node, without requiring\nbranching\n', ['column generation approach', 'delivery planning over time', 'inhomogeneous service providers', 'service interval constraints', 'delivery capacity', 'lower bound', 'transportation', 'goods distribution', 'integer programming', 'linear programming']), ('Self-organized critical traffic in parallel computer networks\nIn a recent paper, we analysed the dynamics of traffic flow in a simple, square\nlattice architecture. It was shown that a phase transition takes place\nbetween a free and a congested phase. The transition point was shown to\nexhibit optimal information transfer and wide fluctuations in time,\nwith scale-free properties. In this paper, we further extend our\nanalysis by considering a generalization of the previous model in which\nthe rate of packet emission is regulated by the local congestion\nperceived by each node. As a result of the feedback between traffic\ncongestion and packet release, the system is poised at criticality.\nMany well-known statistical features displayed by Internet traffic are\nrecovered from our model in a natural way\n', ['self-organized critical traffic', 'traffic flow dynamics', 'phase transition', 'free phase', 'congested phase', 'transition point', 'optimal information transfer', 'wide fluctuations', 'scale-free properties', 'generalization', 'packet emission', 'packet release', 'statistical features', 'Internet traffic', 'square lattice architecture', 'parallel computer networks', 'fluctuations', 'Internet', 'lattice theory', 'parallel processing', 'phase transformations', 'self-organised criticality']), ("At your service [agile businesses]\nSenior software executives from three of the world's leading software\ncompanies, and one smaller, entrepreneurial software developer, explain\nthe impact that web services, business process management and\nintegrated application architectures are having on their product\ndevelopment plans, and share their vision of the roles these products\nwill play in creating agile businesses\n", ['agile businesses', 'integrated application architectures', 'business process management', 'web services', 'software companies', 'application program interfaces', 'management information systems']), ("DAML+OIL: an ontology language for the Semantic Web\nBy all measures, the Web is enormous and growing at a staggering rate, which\nhas made it increasingly difficult-and important-for both people and\nprograms to have quick and accurate access to Web information and\nservices. The Semantic Web offers a solution, capturing and exploiting\nthe meaning of terms to transform the Web from a platform that focuses\non presenting information, to a platform that focuses on understanding\nand reasoning with information. To support Semantic Web development,\nthe US Defense Advanced Research Projects Agency launched the DARPA\nAgent Markup Language (DAML) initiative to fund research in languages,\ntools, infrastructure, and applications that make Web content more\naccessible and understandable. Although the US government funds DAML,\nseveral organizations-including US and European businesses and\nuniversities, and international consortia such as the World Wide Web\nConsortium-have contributed to work on issues related to DAML's\ndevelopment and deployment. We focus on DAML's current markup language,\nDAML+OIL, which is a proposed starting point for the W3C's Semantic Web\nActivity's Ontology Web Language (OWL). We introduce DAML+OIL syntax\nand usage through a set of examples, drawn from a wine knowledge base\nused to teach novices how to build ontologies\n", ['Semantic Web', 'DARPA Agent Markup Language', 'DAML+OIL', 'Ontology Web Language', 'syntax', 'wine knowledge base', 'computational linguistics', 'hypermedia markup languages', 'information resources', 'Internet', 'knowledge engineering', 'software agents']), ("Achieving competitive capabilities in e-services\nWhat implications does the Internet have for service operations strategy? How\ncan business performance of e-service companies be improved in today's\nknowledge-based economy? These research questions are the subject of\nthe paper. We propose a model that links the e-service company's\nknowledge-based competencies with their competitive capabilities.\nDrawing from the current literature, our analysis suggests that\nservices that strategically build a portfolio of knowledge-based\ncompetencies, namely human capital, structural capital, and absorptive\ncapacity have more operations-based options, than their counterparts\nwho are less apt to invest. We assume that the combinative capabilities\nof service quality, delivery, flexibility, and cost are determined by\nthe investment in intellectual capital. Arguably, with the advent of\nthe Internet, different operating models (e.g., bricks-and-mortar,\nclicks-and-mortar, or pure dot-com) have different strategic\nimperatives in terms of knowledge-based competencies. Thus, the new\ne-operations paradigm can be viewed as a configuration of\nknowledge-based competencies and capabilities\n", ['competitive capabilities', 'e-services', 'Internet', 'service operations strategy', 'business performance', 'knowledge-based economy', 'knowledge-based competencies', 'human capital', 'structural capital', 'absorptive capacity', 'operations-based options', 'combinative capabilities', 'service quality', 'delivery', 'flexibility', 'cost', 'investment', 'intellectual capital', 'bricks-and-mortar', 'clicks-and-mortar', 'dot-com', 'strategic imperatives', 'corporate modelling', 'electronic commerce', 'Internet', 'technological forecasting']), ('Schedulability analysis of real-time traffic in WorldFIP networks: an\nintegrated approach\nThe WorldFIP protocol is one of the profiles that constitute the European\nfieldbus standard EN-50170. It is particularly well suited to be used\nin distributed computer-controlled systems where a set of process\nvariables must be shared among network devices. To cope with the\nreal-time requirements of such systems, the protocol provides\ncommunication services based on the exchange of periodic and aperiodic\nidentified variables. The periodic exchanges have the highest priority\nand are executed at run time according to a cyclic schedule. Therefore,\nthe respective schedulability can be determined at pre-run-time when\nbuilding the schedule table. Concerning the aperiodic exchanges, the\nsituation is different since their priority is lower and they are\nbandied according to a first-come-first-served policy. In this paper, a\nresponse-time-based schedulability analysis for the real-time traffic\nis presented. Such analysis considers both types of traffic in an\nintegrated way, according to their priorities. Furthermore, a\nfixed-priorities-based policy is also used to schedule the periodic\ntraffic. The proposed analysis represents an improvement relative to\nprevious work and it can be evaluated online as part of a traffic\nonline admission control. This feature is of particular importance when\na planning scheduler is used, instead of the typical offline static\nscheduler, to allow online changes to the set of periodic process\nvariables\n', ['WorldFIP Networks', 'EN-50170 European fieldbus standard', 'real-time traffic schedulability analysis', 'distributed computer-controlled systems', 'communication services', 'aperiodic exchanges', 'traffic online admission control', 'first-come-first-served policy', 'periodic process variables', 'real-time communication', 'scheduling algorithms', 'response time', 'field buses', 'protocols', 'real-time systems', 'scheduling', 'telecommunication congestion control', 'telecommunication traffic']), ('Controlling in between the Lorenz and the Chen systems\nThis letter investigates a new chaotic system and its role as a joint function\nbetween two complex chaotic systems, the Lorenz and the Chen systems,\nusing a simple variable constant controller. With the gradual tuning of\nthe controller, the controlled system evolves from the canonical Lorenz\nattractor to the Chen attractor through the new transition chaotic\nattractor. This evolving procedure reveals the forming mechanisms of\nall similar and closely related chaotic systems, and demonstrates that\na simple control technique can be very useful in generating and\nanalyzing some complex chaotic dynamical phenomena\n', ['Chen attractors', 'Chen system', 'Lorenz system', 'tuning', 'Lorenz attractor', 'transition chaotic attractor', 'chaos', 'nonlinear control systems']), ('Time-resolved contrast-enhanced imaging with isotropic resolution and broad\ncoverage using an undersampled 3D projection trajectory\nTime-resolved contrast-enhanced 3D MR angiography (MRA) methods have gained in\npopularity but are still limited by the tradeoff between spatial and\ntemporal resolution. A method is presented that greatly reduces this\ntradeoff by employing undersampled 3D projection reconstruction\ntrajectories. The variable density k-space sampling intrinsic to this\nsequence is combined with temporal k-space interpolation to provide\ntime frames as short as 4 s. This time resolution reduces the need for\nexact contrast timing while also providing dynamic information. Spatial\nresolution is determined primarily by the projection readout resolution\nand is thus isotropic across the FOV, which is also isotropic. Although\nundersampling the outer regions of k-space introduces aliased energy\ninto the image, which may compromise resolution, this is not a limiting\nfactor in high-contrast applications such as MRA. Results from phantom\nand volunteer studies are presented demonstrating isotropic resolution,\nbroad coverage with an isotropic field of view (FOV), minimal\nprojection reconstruction artifacts, and temporal information. In one\napplication, a single breath-hold exam covering the entire pulmonary\nvasculature generates high-resolution, isotropic imaging volumes\ndepicting the bolus passage\n', ['time-resolved contrast-enhanced imaging', 'isotropic resolution', 'broad coverage', 'undersampled 3D projection trajectory', '3D MRI angiography', 'variable density k-space sampling', 'temporal k-space interpolation', 'isotropic field of view', 'pulmonary vasculature', 'bolus passage', 'abdomen', 'thorax', 'image artifacts', 'breath-hold imaging', 'biomedical MRI', 'blood vessels', 'image reconstruction', 'image sampling', 'interpolation', 'lung', 'medical image processing']), ('Closed-loop persistent identification of linear systems with unmodeled dynamics\nand stochastic disturbances\nThe essential issues of time complexity and probing signal selection are\nstudied for persistent identification of linear time-invariant systems\nin a closed-loop setting. By establishing both upper and lower bounds\non identification accuracy as functions of the length of observation,\nsize of unmodeled dynamics, and stochastic disturbances, we demonstrate\nthe inherent impact of unmodeled dynamics on identification accuracy,\nreduction of time complexity by stochastic averaging on disturbances,\nand probing capability of full rank periodic signals for closed-loop\npersistent identification. These findings indicate that the mixed\nformulation, in which deterministic uncertainty of system dynamics is\nblended with random disturbances, is beneficial to reduction of\nidentification complexity\n', ['closed-loop persistent identification', 'unmodeled dynamics', 'linear time-invariant systems', 'upper bounds', 'lower bounds', 'identification accuracy', 'full rank periodic signals', 'stochastic disturbances', 'time complexity', 'probing signal selection', 'adaptive control', 'closed loop systems', 'computational complexity', 'identification', 'linear systems', 'matrix algebra', 'robust control', 'stochastic processes']), ('Development through gaming\nMainstream observers commonly underestimate the role of fringe activities in\npropelling science and technology. Well-known examples are how wars\nhave fostered innovation in areas such as communications, cryptography,\nmedicine and aerospace; and how erotica has been a major factor in\npioneering visual media, from the first printed books to photography,\ncinematography, videotape, or the latest online video streaming. The\narticle aims to be a sampler of a less controversial, but still often\nunderrated, symbiosis between scientific computing and computing for\nleisure and entertainment\n', ['computer games', 'scientific computing', 'leisure', 'entertainment', 'graphics', 'computer games', 'computer graphics', 'entertainment', 'natural sciences computing']), ('High-density remote storage: the Ohio State University Libraries depository\nThe article describes a high-density off-site book storage facility operated by\nthe Ohio State University Libraries. Opened in 1995, it has the\ncapacity to house nearly 1.5 million items in only 9000 square feet by\nshelving books by size on 30-foot tall shelving. A sophisticated\nclimate control system extends the life of stored materials up to 12\ntimes. An online catalog record for each item informs patrons that the\nitem is located in a remote location. Regular courier deliveries from\nthe storage facility bring requested materials to patrons with minimal\ndelay\n', ['high-density remote storage', 'Ohio State University Libraries', 'high-density off-site book storage facility', 'shelving', 'climate control system', 'stored materials', 'patrons', 'circulation', 'online catalog record', 'remote location', 'courier deliveries', 'academic libraries', 'cataloguing', 'environmental engineering']), ('Subject access to government documents in an era of globalization: intellectual\nbundling of entities affected by the decisions of supranational\norganizations\nAs a result of the growing influence of supranational organizations, there is a\nneed for a new model for subject access to government information in\nacademic libraries. Rulings made by supranational bodies such as the\nWorld Trade Organization (WTO) and rulings determined under the\nauspices of transnational economic agreements such as the North\nAmerican Free Trade Agreement (NAFTA) often supersede existing law,\nresulting in obligatory changes to national, provincial, state, and\nmunicipal legislation. Just as important is the relationship among\nprivate sector companies, third party actors such as nongovernmental\norganizations (NGOs), and governments. The interaction among the\nvarious entities affected by supranational rulings could potentially\nform the basis of a new model for subject access to government\ninformation\n', ['government documents', 'globalization', 'intellectual bundling', 'supranational organizations', 'academic libraries', 'World Trade Organization', 'transnational economic agreements', 'North American Free Trade Agreement', 'municipal legislation', 'national legislation', 'provincial legislation', 'state legislation', 'government data processing', 'legislation']), ("Law librarians' survey: are academic law librarians in decline?\nThe author reports on the results of one extra element in the BIALL/SPTL\nsurvey, designed to acquire further information about academic law\nlibrarians. The survey has fulfilled the aim of providing a snapshot of\nthe academic law library profession and has examined the concerns that\nhave been raised. Perhaps most importantly, it has shown that more\nlong-term work needs to be done to monitor the situation effectively.\nWe hope that BIALL will take on this challenge and help to maintain the\nstatus of academic law librarians and aid them in their work\n", ['survey', 'BIALL/SPTL', 'academic law library', 'academic law librarians', 'academic libraries', 'law administration', 'professional aspects']), ('Structure of weakly invertible semi-input-memory finite automata with delay 1\nSemi-input-memory finite automata, a kind of finite automata introduced by the\nfirst author of this paper for studying error propagation, are a\ngeneralization of input memory finite automata by appending an\nautonomous finite automaton component. In this paper, we give a\ncharacterization of the structure of weakly invertible\nsemi-input-memory finite automata with delay 1, in which the state\ngraph of each autonomous finite automaton is a cycle. From a result on\nmutual invertibility of finite automata obtained by the authors\nrecently, it leads to a characterization of the structure of\nfeedforward inverse finite automata with delay 1\n', ['finite automata', 'semi-input-memory', 'invertibility', 'semi-input-memory finite automata', 'weakly invertible', 'delay 1', 'state graph', 'feedforward inverse finite automata', 'finite automata']), ("Transformation rules and strategies for functional-logic programs\nThis paper abstracts the contents of a PhD dissertation entitled\n'Transformation Rules and Strategies for Functional-Logic Programs'\nwhich has been recently defended. These techniques are based on\nfold/unfold transformations and they can be used to optimize integrated\n(functional-logic) programs for a wide class of applications.\nExperimental results show that typical examples in the field of\nartificial intelligence are successfully enhanced by our transformation\nsystem SYNTH. The thesis presents the first approach of these methods\nfor declarative languages that integrate the best features from\nfunctional and logic programming\n", ['program transformation rules', 'functional-logic programs', 'logic programming', 'functional programming', 'fold-unfold transformations', 'experimental results', 'artificial intelligence', 'SYNTH', 'declarative languages', 'artificial intelligence', 'functional programming', 'logic programming']), ('A novel preterm respiratory mechanics active simulator to test the performances\nof neonatal pulmonary ventilators\nA patient active simulator is proposed which is capable of reproducing values\nof the parameters of pulmonary mechanics of healthy newborns and\npreterm pathological infants. The implemented prototype is able to: (a)\nlet the operator choose the respiratory pattern, times of apnea,\nepisodes of cough, sobs, etc., (b) continuously regulate and control\nthe parameters characterizing the pulmonary system; and, finally, (c)\nreproduce the attempt of breathing of a preterm infant. Taking into\naccount both the limitation due to the chosen application field and the\npreliminary autocalibration phase automatically carried out by the\nproposed device, accuracy and reliability on the order of 1% is\nestimated. The previously indicated value has to be considered\nsatisfactory in light of the field of application and the small values\nof the simulated parameters. Finally, the achieved metrological\ncharacteristics allow the described neonatal simulator to be adopted as\na reference device to test performances of neonatal ventilators and,\nmore specifically, to measure the time elapsed between the occurrence\nof a potentially dangerous condition to the patient and the activation\nof the corresponding alarm of the tested ventilator\n', ['preterm respiratory mechanics active simulator', 'neonatal pulmonary ventilators', 'patient active simulator', 'healthy newborns', 'preterm pathological infants', 'apnea times', 'autocalibration phase', 'accuracy', 'reliability', 'respiratory diseases', 'ventilatory support', 'intensive care equipment', 'electronic unit', 'pneumatic/mechanical unit', 'software control', 'double compartment model', 'artificial trachea', 'pressure transducer', 'variable clamp resistance', 'upper airway resistance', 'compliance', 'biocontrol', 'biomedical transducers', 'calibration', 'lung', 'paediatrics', 'patient treatment', 'pneumatic control equipment', 'pneumodynamics', 'simulation']), ('A knowledge-based approach for business process reengineering, SHAMASH\nWe present an overview of SHAMASH, a process modelling tool for business\nprocess reengineering. The main features that differentiate it from\nmost current related tools are its ability to define and use\norganisation standards, functional structure, and develop automatic\nmodel simulation and optimisation. SHAMASH is a knowledge-based system,\nand we include a discussion on how knowledge acquisition takes place.\nFurthermore, we introduce a high level description of the architecture,\nthe conceptual model, and other important modules of the system\n', ['knowledge-based approach', 'business process reengineering', 'SHAMASH', 'process modelling tool', 'organisation standards', 'functional structure', 'automatic model simulation', 'optimisation', 'knowledge-based system', 'knowledge acquisition', 'conceptual model', 'business data processing', 'knowledge acquisition', 'knowledge based systems', 'optimisation', 'systems re-engineering']), ('On bandlimited scaling function\nThis paper discusses band-limited scaling function, especially the single\ninterval band case and three interval band cases. Their relationship to\noversampling property and weakly translation invariance are also\nstudied. At the end, we propose an open problem\n', ['bandlimited scaling function', 'interval band case', 'oversampling property', 'weakly translation invariance', 'invariance', 'wavelet transforms']), ('The social impact of Internet gambling\nTechnology has always played a role in the development of gambling practices\nand continues to provide new market opportunities. One of the fastest\ngrowing areas is that of Internet gambling. The effect of such\ntechnologies should not be accepted uncritically, particularly as there\nmay be areas of potential concern based on what is known about problem\ngambling offline. This article has three aims. First, it overviews some\nof the main social concerns about the rise of Internet gambling.\nSecond, it looks at the limited research that has been carried out in\nthis area. Third, it examines whether Internet gambling is doubly\naddictive, given research that suggests that the Internet can be\naddictive itself. It is concluded that technological developments in\nInternet gambling will increase the potential for problem gambling\nglobally, but that many of the ideas and speculations outlined in this\narticle need to be addressed further by large-scale empirical studies\n', ['social impact', 'Internet gambling', 'market opportunities', 'technological developments', 'addiction', 'electronic cash', 'psychology', 'electronic money', 'Internet', 'personal computing', 'psychology', 'social aspects of automation']), ("On the use of neural network ensembles in QSAR and QSPR\nDespite their growing popularity among neural network practitioners, ensemble\nmethods have not been widely adopted in structure-activity and\nstructure-property correlation. Neural networks are inherently\nunstable, in that small changes in the training set and/or training\nparameters can lead to large changes in their generalization\nperformance. Recent research has shown that by capitalizing on the\ndiversity of the individual models, ensemble techniques can minimize\nuncertainty and produce more stable and accurate predictors. In this\nwork, we present a critical assessment of the most common ensemble\ntechnique known as bootstrap aggregation, or bagging, as applied to\nQSAR and QSPR. Although aggregation does offer definitive advantages,\nwe demonstrate that bagging may not be the best possible choice and\nthat simpler techniques such as retraining with the full sample can\noften produce superior results. These findings are rationalized using\nKrogh and Vedelsby's (1995) decomposition of the generalization error\ninto a term that measures the average generalization performance of the\nindividual networks and a term that measures the diversity among them.\nFor networks that are designed to resist over-fitting, the benefits of\naggregation are clear but not overwhelming\n", ['neural network ensembles', 'QSAR', 'QSPR', 'training set', 'training parameters', 'generalization performance', 'uncertainty', 'bootstrap aggregation', 'bagging', 'retraining', 'generalization error decomposition', 'structure-activity correlation', 'structure-property correlation', 'chemical structure', 'chemistry computing', 'generalisation (artificial intelligence)', 'learning (artificial intelligence)', 'neural nets', 'uncertainty handling']), ("Agreeing with automated diagnostic aids: a study of users' concurrence\nstrategies\nAutomated diagnostic aids that are less than perfectly reliable often produce\nunwarranted levels of disuse by operators. In the present study, users'\ntendencies to either agree or disagree with automated diagnostic aids\nwere examined under conditions in which: (1) the aids were less than\nperfectly reliable but aided-diagnosis was still more accurate that\nunaided diagnosis; and (2) the system was completely opaque, affording\nusers no additional information upon which to base a diagnosis. The\nresults revealed that some users adopted a strategy of always agreeing\nwith the aids, thereby maximizing the number of correct diagnoses made\nover several trials. Other users, however, adopted a\nprobability-matching strategy in which agreement and disagreement rates\nmatched the rate of correct and incorrect diagnoses of the aids. The\nprobability-matching strategy, therefore, resulted in diagnostic\naccuracy scores that were lower than was maximally possible. Users who\nadopted the maximization strategy had higher self-ratings of\nproblem-solving and decision-making skills, were more accurate in\nestimating aid reliabilities, and were more confident in their\ndiagnosis on trials in which they agreed with the aids. The potential\napplications of these findings include the design of interface and\ntraining solutions that facilitate the adoption of the most effective\nconcurrence strategies by users of automated diagnostic aids\n", ['automated diagnostic aids', 'user concurrence strategy', 'probability-matching', 'disagreement rates', 'maximization', 'problem-solving', 'reliability', 'complex systems', 'fault diagnosis', 'automatic test equipment', 'human factors', 'large-scale systems', 'probability', 'reliability']), ('Adaptive scheduling of batch servers in flow shops\nBatch servicing is a common way of benefiting from economies of scale in\nmanufacturing operations. Good examples of production systems that\nallow for batch processing are ovens found in the aircraft industry and\nin semiconductor manufacturing. In this paper we study the issue of\ndynamic scheduling of such systems within the context of multi-stage\nflow shops. So far, a great deal of research has concentrated on the\ndevelopment of control strategies, which only address the batch stage.\nThis paper proposes an integral scheduling approach that also includes\nsucceeding stages. In this way, we aim for shop optimization, instead\nof optimizing performance for a single stage. Our so-called look-ahead\nstrategy adapts its scheduling decision to shop status, which includes\ninformation on a limited number of near-future arrivals. In particular,\nwe study a two-stage flow shop, in which the batch stage is succeeded\nby a serial stage. The serial stage may be realized by a single machine\nor by parallel machines. Through an extensive simulation study it is\ndemonstrated how shop performance can be improved by the proposed\nstrategy relative to existing strategies\n', ['adaptive scheduling', 'batch servers', 'flow shops', 'batch servicing', 'manufacturing operations', 'production systems', 'ovens', 'aircraft industry', 'semiconductor manufacturing', 'dynamic scheduling', 'multi-stage flow shops', 'control strategies', 'integral scheduling approach', 'shop optimization', 'look-ahead strategy', 'near-future arrivals', 'two-stage flow shop', 'single machine', 'parallel machines', 'simulation study', 'batch processing (industrial)', 'optimisation', 'process heating', 'production control']), ("Integration - no longer a barrier? [agile business]\nWeb services will be a critical technology for enabling the 'agile business'\n", ['agile business', 'Web services', 'integration middleware', 'Iona', 'AMR Research', 'application program interfaces', 'management information systems']), ("Knowledge acquisition for expert systems in accounting and financial problem\ndomains\nSince the mid-1980s, expert systems have been developed for a variety of\nproblems in accounting and finance. The most commonly cited problems in\ndeveloping these systems are the unavailability of the experts and\nknowledge engineers and difficulties with the rule extraction process.\nWithin the field of artificial intelligence, this has been called the\n'knowledge acquisition' (KA) problem and has been identified as a major\nbottleneck in the expert system development process. Recent empirical\nresearch reveals that certain KA techniques are significantly more\nefficient than others in helping to extract certain types of knowledge\nwithin specific problem domains. This paper presents a mapping between\nthese empirical studies and a generic taxonomy of expert system problem\ndomains. To accomplish this, we first examine the range of problem\ndomains and suggest a mapping of accounting and finance tasks to a\ngeneric problem domain taxonomy. We then identify and describe the most\nprominent KA techniques employed in developing expert systems in\naccounting and finance. After examining and summarizing the existing\nempirical KA work, we conclude by showing how the empirical KA research\nin the various problem domains can be used to provide guidance to\ndevelopers of expert systems in the fields of accounting and finance\n", ['knowledge acquisition', 'expert systems', 'accounting', 'finance', 'rule extraction process', 'artificial intelligence', 'problem domain taxonomy', 'expert systems', 'financial data processing', 'knowledge acquisition']), ('Modeling group foraging: individual suboptimality, interference, and a kind of\nmatching\nA series of agent-based models support the hypothesis that behaviors adapted to\na group situation may be suboptimal (or "irrational") when expressed by\nan isolated individual. These models focus on two areas of current\nconcern in behavioral ecology and experimental psychology: the\n"interference function" (which relates the intake rate of a focal\nforager to the density of conspecifics) and the "matching law" (which\nformalizes the observation that many animals match the frequency of\ntheir response to different stimuli in proportion to the reward\nobtained from each stimulus type). Each model employs genetic\nalgorithms to evolve foraging behaviors for multiple agents in\nspatially explicit environments, structured at the level of situated\nperception and action. A second concern of the article is to extend the\nunderstanding of both matching and interference per se by modeling at\nthis level\n', ['group foraging', 'individual suboptimality', 'agent-based models', 'group situation', 'suboptimal behavior', 'isolated individual', 'behavioral ecology', 'experimental psychology', 'interference function', 'focal forager', 'matching law', 'genetic algorithms', 'multiple agents', 'spatially explicit environments', 'situated perception', 'situated action', 'genetic algorithms', 'multi-agent systems']), ("Strobbe Graphics' next frontier: CTP for commercial printers\nStrobbe is one of the more successful makers of newspaper platesetters, which\nare sold by Agfa under the Polaris name. But the company also has a\ngrowing presence in commercial printing markets, where it sells under\nits own name\n", ['Strobbe Graphics', 'Punch International', 'Agfa', 'commercial printing', 'Polaris', 'platesetters', 'workflow', 'printing', 'publishing']), ("An experimental evaluation of comprehensibility aspects of knowledge structures\nderived through induction techniques: a case study of industrial fault\ndiagnosis\nMachine induction has been extensively used in order to develop knowledge bases\nfor decision support systems and predictive systems. The extent to\nwhich developers and domain experts can comprehend these knowledge\nstructures and gain useful insights into the basis of decision making\nhas become a challenging research issue. This article examines the\nknowledge structures generated by the C4.5 induction technique in a\nfault diagnostic task and proposes to use a model of human learning in\norder to guide the process of making comprehensive the results of\nmachine induction. The model of learning is used to generate\nhierarchical representations of diagnostic knowledge by adjusting the\nlevel of abstraction and varying the goal structures between 'shallow'\nand 'deep' ones. Comprehensibility is assessed in a global way in an\nexperimental comparison where subjects are required to acquire the\nknowledge structures and transfer to new tasks. This method of\naddressing the issue of comprehensibility appears promising especially\nfor machine induction techniques that are rather inflexible with regard\nto the number and sorts of interventions allowed to system developers\n", ['experimental evaluation', 'knowledge structure comprehensibility aspects', 'induction techniques', 'case study', 'industrial fault diagnosis', 'knowledge bases', 'decision support systems', 'predictive systems', 'C4.5 induction technique', 'industrial plants', 'human learning model', 'diagnostic knowledge representations', 'decision support systems', 'diagnostic expert systems', 'diagnostic reasoning', 'fault diagnosis', 'industrial plants', 'knowledge representation', 'learning by example']), ("Formal verification of human-automation interaction\nThis paper discusses a formal and rigorous approach to the analysis of operator\ninteraction with machines. It addresses the acute problem of detecting\ndesign errors in human-machine interaction and focuses on verifying the\ncorrectness of the interaction in complex and automated control\nsystems. The paper describes a systematic methodology for evaluating\nwhether the interface provides the necessary information about the\nmachine to enable the operator to perform a specified task successfully\nand unambiguously. It also addresses the adequacy of information\nprovided to the user via training materials (e.g., user manual) about\nthe machine's behavior. The essentials of the methodology, which can be\nautomated and applied to the verification of large systems, are\nillustrated by several examples and through a case study of pilot\ninteraction with an autopilot aboard a modern commercial aircraft. The\nexpected application of this methodology is an augmentation and\nenhancement, by formal verification, of human-automation interfaces\n", ['formal verification', 'human-automation interaction', 'man-machine interaction', 'automated control systems', 'user interface', 'autopilot', 'commercial aircraft', 'aircraft control', 'formal verification', 'human factors', 'man-machine systems', 'user interfaces']), ("A brief guide to competitive intelligence: how to gather and use information on\ncompetitors\nThe author outlines the processes involved in competitive intelligence, and\ndiscusses what it is, how to do it and gives examples of what happens\nwhen companies fail to monitor their competitive environment\neffectively. The author presents a case study, showing how the company\nthat produced the pre-cursor to the Barbie doll failed to look at their\nbusiness environment and how this led to the firm's failure. The author\ndiscusses what competitive intelligence is, and what it is not, and why\nit is important for businesses, and presents three models used to\ndescribe the competitive intelligence process, going through the\nvarious steps involved in defining intelligence requirements and\ncollecting, analyzing, communicating and utilizing competitive\nintelligence\n", ['competitive intelligence', 'competitor information', 'Barbie doll', 'business environment', 'intelligence collection', 'intelligence analysis', 'intelligence communication', 'intelligence utilization', 'business data processing', 'information analysis', 'information needs', 'information retrieval', 'information use']), ('Delayed-choice entanglement swapping with vacuum-one-photon quantum states\nWe report the experimental realization of a recently discovered\nquantum-information protocol by Peres implying an apparent nonlocal\nquantum mechanical retrodiction effect. The demonstration is carried\nout by a quantum optical method by which each singlet entangled state\nis physically implemented by a two-dimensional subspace of Fock states\nof a mode of the electromagnetic field, specifically the space spanned\nby the vacuum and the one-photon state, along lines suggested recently\nby E. Knill et al. [Nature (London) 409, 46 (2001)] and by M. Duan et\nal. [ibid. 414, 413 (2001)]\n', ['delayed-choice entanglement', 'quantum-information', 'nonlocal quantum mechanical retrodiction effect', 'quantum optical method', 'singlet entangled state', 'two-dimensional subspace', 'state entanglement', 'Fock states', 'electromagnetic field mode', 'vacuum-one-photon quantum states', 'one-photon state', 'vacuum state', 'information theory', 'quantum communication', 'quantum optics', 'quantum theory']), ('Knowledge organisation of product design blackboard systems via graph\ndecomposition\nKnowledge organisation plays an important role in building a knowledge-based\nproduct design blackboard system. Well-organised knowledge sources will\nfacilitate the effectiveness and efficiency of communication and data\nexchange in a blackboard system. In a previous investigation, an\napproach for constructing blackboard systems for product design using a\nnon-directed graph decomposition algorithm was proposed. In this paper,\nthe relationship between graph decomposition and the resultant\nblackboard system is further studied. A case study of a number of\nhypothetical blackboard systems that comprise different knowledge\norganisations is provided\n', ['knowledge organisation', 'product design blackboard systems', 'graph decomposition', 'knowledge-based product design', 'data exchange', 'case study', 'blackboard architecture', 'CAD/CAM', 'graph theory', 'intelligent design assistants']), ('Using extended logic programming for alarm-correlation in cellular phone\nnetworks\nAlarm correlation is a necessity in large mobile phone networks, where the\nalarm bursts resulting from severe failures would otherwise overload\nthe network operators. We describe how to realize alarm-correlation in\ncellular phone networks using extended logic programming. To this end,\nwe describe an algorithm and system solving the problem, a model of a\nmobile phone network application, and a detailed solution for a\nspecific scenario\n', ['extended logic programming', 'alarm-correlation', 'cellular phone networks', 'large mobile phone networks', 'network operators', 'fault diagnosis', 'alarm systems', 'cellular radio', 'fault diagnosis', 'logic programming', 'telecommunication computing', 'telecommunication network reliability']), ('Time-domain reconstruction for thermoacoustic tomography in a spherical\ngeometry\nReconstruction-based microwave-induced thermoacoustic tomography in a spherical\nconfiguration is presented. Thermoacoustic waves from biological tissue\nsamples excited by microwave pulses are measured by a wide-band\nunfocused ultrasonic transducer, which is set on a spherical surface\nenclosing the sample. Sufficient data are acquired from different\ndirections to reconstruct the microwave absorption distribution. An\nexact reconstruction solution is derived and approximated to a modified\nbackprojection algorithm. Experiments demonstrate that the\nreconstructed images agree well with the original samples. The spatial\nresolution of the system reaches 0.5 mm\n', ['medical diagnostic imaging', 'thermoacoustic tomography', 'time-domain reconstruction', 'modified backprojection algorithm', 'exact reconstruction solution', 'biological tissue samples', 'wide-band unfocused ultrasonic transducer', 'spherical surface enclosing sample', 'reconstructed images', 'system spatial resolution', 'spherical geometry', '0.5 mm', 'backpropagation', 'image reconstruction', 'medical image processing', 'microwave imaging', 'thermoacoustics']), ("Novel active noise-reducing headset using earshell vibration control\nActive noise-reducing (ANR) headsets are available commercially in applications\nvarying from aviation communication to consumer audio. Current ANR\nsystems use passive attenuation at high frequencies and\nloudspeaker-based active noise control at low frequencies to achieve\nbroadband noise reduction. This paper presents a novel ANR headset in\nwhich the external noise transmitted to the user's ear via earshell\nvibration is reduced by controlling the vibration of the earshell using\nforce actuators acting against an inertial mass or the earshell\nheadband. Model-based theoretical analysis using velocity feedback\ncontrol showed that current piezoelectric actuators provide sufficient\nforce but require lower stiffness for improved low-frequency\nperformance. Control simulations based on experimental data from a\nlaboratory headset showed that good performance can potentially be\nachieved in practice by a robust feedback controller, while a\nsingle-frequency real-time control experiment verified that noise\nreduction can be achieved using earshell vibration control\n", ['active noise-reducing headset', 'earshell vibration control', 'aviation communication', 'consumer audio', 'passive attenuation', 'broadband noise reduction', 'external noise transmission', 'force actuators', 'inertial mass', 'velocity feedback control', 'piezoelectric actuators', 'stiffness', 'robust feedback controller', 'single-frequency real-time control', 'acoustic field', 'acoustic transducers', 'acoustic wave absorption', 'acoustic wave transmission', 'active noise control', 'force feedback', 'piezoelectric transducers', 'robust control', 'vibration control']), ('Ultra-high speed positioning control of a gravure engraving unit using a\ndiscrete-time two-degree-of-freedom H/sub infinity / control\nThe piezoelectric actuator has high-speed response in comparison with the\nelectro-magnetic actuator. However, it is not easy to achieve both\nhigh-speed and high-precision response by feedforward control only\nbecause the piezoelectric element has nonlinear properties such as the\nhysteresis effect. Thus, feedback control is required to achieve good\nperformance. We develop a control design method to achieve both\nhigh-speed and high-precision response for piezoelectric actuators\nusing the discrete-time H/sub infinity / control method and the\ntwo-degree-of-freedom control scheme. The effectiveness of our proposed\nmethod has been shown by simulation and experimental results. The most\nimportant contribution of our study is that our method can be directly\napplied to commercial machines\n', ['ultra-high speed positioning control', 'gravure engraving unit', 'discrete-time two-degree-of-freedom H/sub infinity / control', 'piezoelectric actuator', 'control design method', 'high-precision response', 'feedforward', 'nonlinear properties', 'hysteresis', 'feedback control', 'digital control system', 'digital control', 'discrete time systems', 'H/sup infinity / control', 'manufacturing processes', 'piezoelectric actuators', 'position control']), ('Pattern recognition strategies for molecular surfaces. II. Surface\ncomplementarity\nFor pt.I see ibid., vol.23, p.1176-87 (2002). Fuzzy logic based algorithms for\nthe quantitative treatment of complementarity of molecular surfaces are\npresented. Therein, the overlapping surface patches defined in part I\nof this series are used. The identification of complementary surface\npatches can be considered as a first step for the solution of molecular\ndocking problems. Standard technologies can then be used for further\noptimization of the resulting complex structures. The algorithms are\napplied to 33 biomolecular complexes. After the optimization with a\ndownhill simplex method, for all these complexes one structure was\nfound, which is in very good agreement with the experimental results\n', ['pattern recognition strategies', 'surface complementarity', 'fuzzy logic based algorithms', 'quantitative treatment', 'molecular surfaces', 'overlapping surface', 'biomolecular complexes', 'optimization', 'downhill simplex method', 'chemistry computing', 'fuzzy logic', 'fuzzy set theory', 'molecular configurations', 'pattern recognition', 'physics computing']), ('A simple graphic approach for observer decomposition\nBased upon the proposition that the roles of inputs and outputs in a physical\nsystem and those in the corresponding output-injection observer do not\nreally have to be consistent, a systematic procedure is developed in\nthis work to properly divide a set of sparse system models and\nmeasurement models into a number of independent subsets with the help\nof a visual aid. Several smaller sub-observers can then be constructed\naccordingly to replace the original one. The size of each sub-observer\nmay be further reduced by strategically selecting one or more appended\nstates. These techniques are shown to be quite effective in relieving\non-line computation load of the output-injection observers and also in\nidentifying detectable sub-systems\n', ['graphic approach', 'observer decomposition', 'output-injection observer', 'sparse system models', 'measurement models', 'independent subsets', 'sub-observers', 'online computation load', 'detectable subsystems', 'computational complexity', 'observers']), ('Editorial system vendors focus on Adobe and the future\nLooking over the newspaper-system market, we note that the Mac is getting new\nrespect. Adobe InDesign has established itself as a solid alternative\nto Quark XPress for pagination. Positioning themselves for the long\nrun, developers are gradually shifting to new software architectures\n', ['newspaper-system market', 'Adobe InDesign', 'pagination', 'Macintosh', 'publishing', 'publishing', 'software packages']), ('Quadratic interpolation on spheres\nRiemannian quadratics are C/sup 1/ curves on Riemannian manifolds, obtained by\nperforming the quadratic recursive deCastlejeau algorithm in a\nRiemannian setting. They are of interest for interpolation problems in\nRiemannian manifolds, such as trajectory-planning for rigid body\nmotion. Some interpolation properties of Riemannian quadratics are\nanalysed when the ambient manifold is a sphere or projective space,\nwith the usual Riemannian metrics\n', ['quadratic interpolation', 'Riemannian manifolds', 'trajectory-planning', 'rigid body motion', 'ambient manifold', 'corner-cutting', 'parallel translation', 'approximation theory', 'approximation theory', 'curve fitting', 'interpolation']), ("Comparative statistical analysis of hole taper and circularity in laser\npercussion drilling\nInvestigates the relationships and parameter interactions between six\ncontrollable variables on the hole taper and circularity in laser\npercussion drilling. Experiments have been conducted on stainless steel\nworkpieces and a comparison was made between stainless steel and mild\nsteel. The central composite design was employed to plan the\nexperiments in order to achieve required information with reduced\nnumber of experiments. The process performance was evaluated. The ratio\nof minimum to maximum Feret's diameter was considered as circularity\ncharacteristic of the hole. The models of these three process\ncharacteristics were developed by linear multiple regression technique.\nThe significant coefficients were obtained by performing analysis of\nvariance (ANOVA) at 1, 5 and 7% levels of significance. The final\nmodels were checked by complete residual analysis and finally were\nexperimentally verified. It was found that the pulse frequency had a\nsignificant effect on the hole entrance diameter and hole circularity\nin drilling stainless steel unlike the drilling of mild steel where the\npulse frequency had no significant effect on the hole characteristics\n", ['comparative statistical analysis', 'hole taper', 'circularity', 'laser percussion drilling', 'stainless steel workpieces', 'mild steel', 'laser peak power', 'laser pulse width', 'pulse frequency', 'assist gas pressure', 'focal plane position', 'central composite design', 'process performance', 'equivalent entrance diameter', 'Ferets diameter', 'linear multiple regression technique', 'least squares procedure', 'stepwise regression method', 'analysis of variance', 'ANOVA', 'complete residual analysis', 'carbon steel', 'laser beam machining', 'stainless steel', 'statistical analysis']), ('IT challenge: cross selling [finance]\nLike most financial institutions, FleetBoston, Fidelity and Berkshire Group of\nCompanies are being charged with developing a strong technology\nplatform that will allow them to cross sell their products and\nservices. They discuss their solutions, advice and technology choices\n', ['cross selling', 'financial institutions', 'FleetBoston', 'Fidelity', 'Berkshire Group', 'investment']), ('Web talk is cheap\nWeb technology provides a wealth of opportunities for reaching potential\ncustomers. So how do you make it work for your business?\n', ['website', 'web chat', 'collaborative browsing', 'customer service representative', 'call centre', 'call centres', 'information resources']), ('Modular and visual specification of hybrid systems: an introduction to HyCharts\nVisual description techniques are particularly important for the design of\nhybrid systems, because specifications of such systems usually have to\nbe discussed between engineers from a number of different disciplines.\nModularity is vital for hybrid systems not only because it allows to\nhandle large systems, but also because it permits to think in terms of\ncomponents, which is familiar to engineers. Based on two different\ninterpretations for hierarchic graphs and on a clear hybrid computation\nmodel, we develop HyCharts. HyCharts consist of two modular visual\nformalisms, one for the specification of the architecture and one for\nthe specification of the behavior of hybrid systems. The operators on\nhierarchic graphs enable us to give a surprisingly simple denotational\nsemantics for many concepts known from statechart-like formalisms. Due\nto a very general composition operator, HyCharts can easily be composed\nwith description techniques from other engineering disciplines. Such\nheterogeneous system specifications seem to be particularly appropriate\nfor hybrid systems because of their interdisciplinary character\n', ['visual specification', 'modular specification', 'hybrid systems', 'HyCharts', 'visual description techniques', 'components', 'hierarchic graphs', 'hybrid computation model', 'denotational semantics', 'statechart', 'heterogeneous system specifications', 'formal specification', 'diagrams', 'formal specification', 'graph theory', 'programming language semantics', 'visual programming']), ('Pareto-optimal formulations for cost versus colorimetric accuracy trade-offs in\nprinter color management\nColor management for the printing of digital images is a challenging task, due\nprimarily to nonlinear ink-mixing behavior and the presence of\nredundant solutions for print devices with more than three inks.\nAlgorithms for the conversion of image data to printer-specific format\nare typically designed to achieve a single predetermined rendering\nintent, such as colorimetric accuracy. We present two CIELAB to CMYK\ncolor conversion schemes based on a general Pareto-optimal formulation\nfor printer color management. The schemes operate using a 149-color\ncharacterization data set selected to efficiently capture the entire\nCMYK gamut. The first scheme uses artificial neural networks as\ntransfer functions between the CIELAB and CMYK spaces. The second\nscheme is based on a reformulation of tetrahedral interpolation as an\noptimization problem. Characterization data are divided into tetrahedra\nfor the interpolation-based approach using the program Qhull, which\nremoves the common restriction that characterization data be well\norganized. Both schemes offer user control over trade-off problems such\nas cost versus reproduction accuracy, allowing for user-specified print\nobjectives and the use of constraints such as maximum allowable ink and\nmaximum allowable AE*/sub ab/. A formulation for minimization of ink is\nshown to be particularly favorable, integrating both a clipping and\ngamut compression features into a single methodology\n', ['printer color management', 'digital image printing', 'nonlinear ink-mixing behavior', 'redundant solutions', 'Pareto-optimal formulations', 'cost versus colorimetric accuracy trade-offs', 'image data conversion', 'CIELAB to CMYK color conversion schemes', 'color characterization data set', 'artificial neural networks', 'transfer functions', 'tetrahedral interpolation', 'optimization', 'tetrahedra', 'interpolation-based approach', 'Qhull program', 'user control', 'cost versus reproduction accuracy', 'user-specified print objectives', 'maximum allowable ink', 'constraints', 'gamut compression features', 'clipping', 'MacBeth ColorChecker chart', 'grey component replacement', 'rendering intent', 'colour graphics', 'image colour analysis', 'interpolation', 'neural nets', 'optimisation', 'printers', 'rendering (computer graphics)']), ('Impossible choice [web hosting service provider]\nSelecting a telecoms and web hosting service provider has become a high-stakes\ngame of chance\n', ['web hosting service provider', 'IT managers', 'selection', 'customer service', 'computer network management', 'Internet']), ("Hybrid broadcast for the video-on-demand service\nMulticast offers an efficient means of distributing video contents/programs to\nmultiple clients by batching their requests and then having them share\na server's video stream. Batching customers' requests is either\nclient-initiated or server-initiated. Most advanced client-initiated\nvideo multicasts are implemented by patching. Periodic broadcast, a\ntypical server-initiated approach, can be entirety-based or\nsegment-based. This paper focuses on the performance of the VoD service\nfor popular videos. First, we analyze the limitation of conventional\npatching when the customer request rate is high. Then, by combining the\nadvantages of each of the two broadcast schemes, we propose a hybrid\nbroadcast scheme for popular videos, which not only lowers the service\nlatency but also improves clients' interactivity by using an active\nbuffering technique. This is shown to be a good compromise for both\nlowering service latency and improving the VCR-like interactivity\n", ['quality-of-service', 'interactivity', 'scheduling', 'video-on-demand', 'multicast', 'conventional patching', 'customer request rate', 'hybrid broadcast scheme', 'multicast communication', 'quality of service', 'video on demand']), ("Modeling privacy control in context-aware systems\nSignificant complexity issues challenge designers of context-aware systems with\nprivacy control. Information spaces provide a way to organize\ninformation, resources, and services around important privacy-relevant\ncontextual factors. In this article, we describe a theoretical model\nfor privacy control in context-aware systems based on a core\nabstraction of information spaces. We have previously focused on\nderiving socially based privacy objectives in pervasive computing\nenvironments. Building on Ravi Sandhu's four-layer OM-AM (objectives,\nmodels, architectures, and mechanisms) idea, we aim to use information\nspaces to construct a model for privacy control that supports our\nsocially based privacy objectives. We also discuss how we can introduce\ndecentralization, a desirable property for many pervasive computing\nsystems, into our information space model, using unified privacy\ntagging\n", ['privacy', 'pervasive computing', 'context-aware systems', 'privacy control', 'smart office', 'business data processing', 'data privacy', 'mobile computing']), ('Making it to the major leagues: career movement between library and archival\nprofessions and from small college to large university libraries\nIssues of career movement and change are examined between library and archival\nfields and from small colleges to large universities. Issues examined\ninclude professional education and training, initial career-planning\nand placement, continuing education, scouting and mentoring, job market\nconditions, work experience and personal skills, professional\ninvolvement, and professional association self-interest. This\nexamination leads to five observations: 1. It is easier, in terms of\ncareer transitions, for a librarian to become an archivist than it is\nfor an archivist to become a librarian; 2. The progression from a small\ncollege venue to a large research university is very manageable with\nthe proper planning and experience; 3. At least three of the career\nelements-professional education, career-planning, and professional\nassociation self-interest-in their best moments provide a foundation\nthat enables a future consideration of change between institutional\ntypes and professional areas and in their worst moments conspire\nagainst the midcareer professional in terms of change; 4. The elements\nof scouting, continuing education, work experience, and professional\ninvolvement offer the greatest assistance in career transitions; 5. The\njob market is the wildcard that either stymies or stimulates\noccupational development\n', ['career movement', 'library profession', 'archival profession', 'small college library', 'large university libraries', 'professional education', 'training', 'continuing education', 'job market', 'work experience', 'personal skills', 'librarian', 'occupational development', 'midcareer', 'academic libraries', 'continuing education', 'employment', 'human resource management', 'information science', 'personnel', 'professional aspects']), ('The paradigm of viral communication\nThe IIW Institute of Information Management (www.IIW.de) is dealing with\ncommercial applications of digital technologies, such as the Internet,\ndigital printing, and many more. A study which has been carried out by\nthe institute, identifies viral messages as a new paradigm of\ncommunication, mostly found in the area of Direct Marketing, and - who\nwonders - mainly within the USA. Viral messages underlie certain\nprinciples: (1) prospects and customers of the idea are offered a\ntechnology platform providing a possibility to send a message to a\nmajority of persons; (2) there is an emotional or pecuniary incentive\nto participate. Ideally, niches of needs and market vacua are filled\nwith funny ideas; (3) also, the recipients are facing emotional or\npecuniary incentives to contact a majority of further recipients - this\ninduces a snowball effect and the message is spread virally; and (4)\nthe customer is activated as an "ambassador" of the piece of\ninformation, for instance promoting a product or a company. It is\nevident that there has been a long lasting history of what we call\n"word-of-mouth" ever since, however bundles of digital technologies\nempower the viral communication paradigm\n', ['viral communication paradigm', 'commercial applications', 'viral messages', 'e-mails', 'Internet', 'direct marketing', 'business', 'computer virus', 'business communication', 'business data processing', 'computer viruses', 'electronic mail', 'Internet', 'marketing data processing']), ('People who make a difference: mentors and role models\nThe literature of gender issues in computing steadfastly and uniformly has\nadvocated the use of mentors and role models (M&RM) for recruiting and\nretaining women in computer science. This paper, therefore, accepts the\nresults of research studies and avoids reiterating details of the\nprojects but offers instead a practical guide for using M&RM to recruit\nand retain women in computer science. The guide provides pragmatic\nadvice, describing several different facets of the M&RM concept\n', ['mentors', 'role models', 'gender issues', 'computing', 'women retention', 'women recruitment', 'computer science', 'computer science education', 'gender issues']), ('Exploratory study of the adoption of manufacturing technology innovations in\nthe USA and the UK\nManufacturing technologies, appropriately implemented, provide competitive\nadvantage to manufacturers. The use of manufacturing technologies\nacross countries is difficult to compare. One such comparison has been\nprovided in the literature with a study of US and Japanese practices in\nadvanced manufacturing technology use using a common questionnaire. The\npresent study compares the use of 17 different technologies in similar\nindustries in the USA (n=1025) and UK (n=166) using a common\nquestionnaire. Largely, there are remarkable similarities between the\ntwo countries. This may partly correlate with the heavy traffic in\nforeign direct investment between the two nations. Notable differences\nare (1) across-the-board, US manufacturers are ahead of the UK firms in\ncomputerized integration with units inside and outside manufacturing\norganizations; (2) US manufacturers show higher labour productivity,\nwhich is consistent with macro-economic data, and (3) more UK\nmanufacturers report the use of soft technologies such as just-in-time,\ntotal quality manufacturing and manufacturing cells. Hypotheses for\nfuture investigation are proposed\n', ['manufacturing technology innovations', 'USA', 'UK', 'competitive advantage', 'foreign direct investment', 'labour productivity', 'macro-economic data', 'soft technologies', 'just-in-time', 'total quality manufacturing', 'manufacturing cells', 'computer integrated manufacturing', 'economics', 'flexible manufacturing systems', 'investment']), ('Option pricing formulas based on a non-Gaussian stock price model\nOptions are financial instruments that depend on the underlying stock. We\nexplain their non-Gaussian fluctuations using the nonextensive\nthermodynamics parameter q. A generalized form of the Black-Scholes\n(BS) partial differential equation (1973) and some closed-form\nsolutions are obtained. The standard BS equation (q = 1) which is used\nby economists to calculate option prices requires multiple values of\nthe stock volatility (known as the volatility smile). Using q = 1.5\nwhich well models the empirical distribution of returns, we get a good\ndescription of option prices using a single volatility\n', ['nonGaussian stock price model', 'financial instruments', 'option pricing formulas', 'nonextensive thermodynamics parameter', 'Black-Scholes partial differential equation', 'closed-form solutions', 'stock volatility', 'volatility smile', 'empirical distribution', 'economic cybernetics', 'fluctuations', 'partial differential equations', 'stock markets']), ('Searching a scalable approach to cerebellar based control\nDecades of research into the structure and function of the cerebellum have led\nto a clear understanding of many of its cells, as well as how learning\nmight take place. Furthermore, there are many theories on what signals\nthe cerebellum operates on, and how it works in concert with other\nparts of the nervous system. Nevertheless, the application of\ncomputational cerebellar models to the control of robot dynamics\nremains in its infant state. To date, few applications have been\nrealized. The currently emerging family of light-weight robots poses a\nnew challenge to robot control: due to their complex dynamics\ntraditional methods, depending on a full analysis of the dynamics of\nthe system, are no longer applicable since the joints influence each\nother dynamics during movement. Can artificial cerebellar models\ncompete here?\n', ['scalable approach', 'cerebellar based control', 'nervous system', 'computational cerebellar models', 'light-weight robots', 'robot control', 'adaptive control', 'biocontrol', 'brain models', 'cerebellar model arithmetic computers', 'learning (artificial intelligence)', 'robot dynamics', 'robot kinematics']), ('Metaschemas for ER, ORM and UML data models: a comparison\nThis paper provides metaschemas for some of the main database modeling\nnotations used in industry. Two Entity Relationship (ER) notations\n(Information Engineering and Barker) are examined in detail, as well as\nObject Role Modeling (ORM) conceptual schema diagrams. The discussion\nof optionality, cardinality and multiplicity is widened to include\nUnified Modeling Language (UML) class diagrams. Issues addressed in the\nmetamodel analysis include the normalization impact of non-derived\nconstraints on derived associations, the influence of orthogonality on\nlanguage transparency, and trade-offs between simplicity and\nexpressibility. To facilitate comparison, the same modeling notation is\nused to display each metaschema. For this purpose, ORM is used because\nof its greater expressibility and clarity\n', ['metaschemas', 'data models', 'UML', 'database modeling notations', 'Entity Relationship modeling', 'Information Engineering', 'Barker notation', 'Object Role Modeling', 'conceptual schema diagrams', 'optionality', 'cardinality', 'multiplicity', 'Unified Modeling Language', 'class diagrams', 'normalization', 'orthogonality', 'language transparency', 'ORM', 'data models', 'diagrams', 'entity-relationship modelling', 'object-oriented databases', 'relational databases', 'specification languages']), ('VoIP: leveraging existing cable architecture\nAs operators prepare to enter the voice-over-IP fray, they are searching for\nways to leverage their existing two-way, interactive infrastructure.\nThere are several approaches for supporting VoIP on top of the core IP\ntransport network. The one garnering the most interest, especially in\nthe United States, is based on the PacketCable 1.x architecture. This\narticle discusses the PacketCable-based approach\n', ['cable architecture', 'VoIP', 'voice-over-IP', 'two-way interactive infrastructure', 'core IP transport network', 'United States', 'PacketCable 1.x architecture', 'PacketCable-based approach', 'Internet telephony', 'protocols']), ('Development of a real-time monitoring system\nThis paper describes a pattern recognition (PR) technique, which uses learning\nvector quantization (LVQ). This method is adapted for practical\napplication to solve problems in the area of condition monitoring and\nfault diagnosis where a number of fault signatures are involved. In\nthese situations, the aim is health monitoring, including\nidentification of deterioration of the healthy condition and\nidentification of causes of the failure in real-time. For this reason a\nfault database is developed which contains the collected information\nabout various states of operation of the system in the form of pattern\nvectors. The task of the real-time monitoring system is to correlate\npatterns of unknown faults with the known fault signatures in the fault\ndatabase. This will determine cause of failure and degree of\ndeterioration of the system under test. The problem of fault diagnosis\nmay involve a large number of patterns and large sampling time, which\naffects the learning stage of neural networks. The study here also aims\nto find a fast learning model of neural networks for instances when a\nhigh number of patterns and numerous processing elements are involved.\nIt begins searching for an appropriate solution. The study is extended\nto the enforcement learning models and considers LVQ as a network\nemerged from the competitive learning model through enforcement\ntraining. Finally, tests show an accuracy of 92.3 per cent in the fault\ndiagnostic capability of the technique\n', ['real-time monitoring system', 'LVQ', 'pattern recognition technique', 'learning vector quantization', 'condition monitoring', 'fault diagnosis', 'fault signatures', 'health monitoring', 'deterioration identification', 'real-time failure cause identification', 'fault database', 'pattern vectors', 'pattern correlation', 'large sampling time', 'fast learning model', 'neural networks', 'competitive learning model', 'enforcement training', 'fault diagnostic capability', 'CNC machine centre', 'coolant system', 'computerised monitoring', 'computerised numerical control', 'condition monitoring', 'fault diagnosis', 'machine tools', 'neural nets', 'pattern recognition', 'real-time systems', 'unsupervised learning', 'vector quantisation']), ('Efficient two-level image thresholding method based on Bayesian formulation and\nthe maximum entropy principle\nAn efficient method for two-level thresholding is proposed based on the Bayes\nformula and the maximum entropy principle, in which no assumptions of\nthe image histogram are made. An alternative criterion is derived based\non maximizing entropy and used for speeding up the searching algorithm.\nFive forms of conditional probability distributions-simple, linear,\nparabola concave, parabola convex, and S-function-are employed and\ncompared to each other for optimal threshold determination. The effect\nof precision on optimal threshold determination is discussed and a\ntrade-off precision epsilon =0.001 is selected experimentally. Our\nexperiments demonstrate that the proposed method achieves a significant\nimprovement in speed from 26 to 57 times faster than the exhaustive\nsearch method\n', ['two-level image thresholding method', 'Bayesian formulation', 'maximum entropy principle', 'image histogram', 'entropy', 'searching algorithm', 'conditional probability distributions', 'parabola concave', 'parabola convex', 'S-function', 'optimal threshold determination', 'trade-off precision', 'image segmentation', 'image thresholding', 'Bayes methods', 'image segmentation', 'maximum entropy methods', 'probability']), ('MTD-PLS: a PLS-based variant of the MTD method. II. Mapping ligand-receptor\ninteractions. Enzymatic acetic acid esters hydrolysis\nThe PLS variant of the MTD method (T.I. Oprea et al., SAR QSAR Environ. Res.\n2001, 12, 75-92) was applied to a series of 25 acetylcholinesterase\nhydrolysis substrates. Statistically significant MTD-PLS models (q/sup\n2/ between 0.7 and 0.8) are in agreement with previous MTD models, with\nthe advantage that local contributions are understood beyond the\noccupancy/nonoccupancy interpretation in MTD. A "chemically intuitive"\napproach further forces MTD-PLS coefficients to assume only negative\n(or zero) values for fragmental volume descriptors and positive (or\nzero) values for fragmental hydrophobicity descriptors. This further\nseparates the various kinds of local interactions at each vertex of the\nMTD hypermolecule, making this method suitable for medicinal chemistry\nsynthesis planning\n', ['minimum topological difference method', 'MTD-PLS models', 'PLS-based variant', 'ligand-receptor interactions mapping', 'enzymatic acetic acid esters hydrolysis', 'acetylcholinesterase hydrolysis substrates', 'chemically intuitive approach', 'fragmental volume descriptors', 'fragmental hydrophobicity descriptors', 'hypermolecule', 'medicinal chemistry synthesis planning', 'steric misfit', 'additive approach', 'intermolecular force categories', 'regression coefficients', 'ligand binding affinity', 'hydrogen bonding', 'polarizabilities', 'statistical model stability', 'binding energy', 'biochemistry', 'chemistry computing', 'combinatorial mathematics', 'hydrogen bonds', 'molecular biophysics', 'polarisability', 'statistical analysis', 'topology', 'van der Waals forces']), ('A strategy for a payoff-switching differential game based on fuzzy reasoning\nIn this paper, a new concept of a payoff-switching differential game is\nintroduced. In this new game, any one player at any time may have\nseveral choices of payoffs for the future. Moreover, the\npayoff-switching process, including the time of payoff switching and\nthe outcome payoff, of any one player is unknown to the other. Indeed,\nthe overall payoff, which is a sequence of several payoffs, is unknown\nuntil the game ends. An algorithm for determining a reasoning strategy\nbased on fuzzy reasoning is proposed. In this algorithm, the fuzzy\ntheory is used to estimate the behavior of one player during a past\ntime interval. By deriving two fuzzy matrices GSM, game similarity\nmatrix, and VGSM, variation of GSM, the behavior of the player can be\nquantified. Two weighting vectors are selected to weight the relative\nimportance of the player\'s behavior at each past time instant. Finally\na simple fuzzy inference rule is adopted to generate a linear reasoning\nstrategy. The advantage of this algorithm is that it provides a\nflexible way for differential game specialists to convert their\nknowledge into a "reasonable" strategy. A practical example of guarding\nthree territories is given to illustrate our main ideas\n', ['payoff-switching differential game', 'payoff switching', 'outcome payoff', 'reasoning strategy', 'fuzzy reasoning', 'fuzzy matrices', 'game similarity matrix', 'weighting vectors', 'fuzzy inference', 'differential game', 'differential games', 'fuzzy logic', 'inference mechanisms']), ('A 0.8-V 128-kb four-way set-associative two-level CMOS cache memory using\ntwo-stage wordline/bitline-oriented tag-compare (WLOTC/BLOTC) scheme\nThis paper reports a 0.8-V 128-kb four-way set-associative two-level CMOS cache\nmemory using a novel two-stage wordline/bitline-oriented tag-compare\n(WLOTC/BLOTC) and sense wordline/bitline (SWL/SBL) tag-sense amplifiers\nwith an eight-transistor (8-T) tag cell in Level 2 (L2) and a 10-T\nshrunk logic swing (SLS) memory cell. with the ground/floating (G/F)\ndata sense amplifier in Level 1 (L1) for high-speed operation for\nlow-voltage low-power VLSI system applications. Owing to the reduced\nloading at the SWL in the new 11-T tag cell using the WLOTC scheme, the\n10-T SLS memory cell with G/F sense amplifier in L1, and the split\ncomparison of the index signal in the 8-T tag cells with SWL/SBL tag\nsense amplifiers in L2, this 0.8-V cache memory implemented in a 1.8-V\n0.18- mu m CMOS technology has a measured L1/L2 hit time of 11.6/20.5\nns at the average dissipation of 0.77 mW at 50 MHz\n', ['four-way set-associative memory', 'two-level CMOS cache memory', 'cache memory architecture', 'wordline/bitline-oriented tag-compare', 'sense wordline/bitline amplifiers', 'tag-sense amplifiers', 'eight-transistor tag cell', 'ten-transistor memory cell', 'shrunk logic swing memory cell', 'ground/floating data sense amplifier', 'high-speed operation', 'low-voltage VLSI system applications', 'low-power VLSI system applications', '0.8 V', '128 kbit', '50 MHz', '0.77 mW', '1.8 V', '0.18 micron', '11.6 ns', '20.5 ns', 'cache storage', 'CMOS memory circuits', 'content-addressable storage', 'low-power electronics', 'memory architecture', 'VLSI']), ("Entrepreneurs in Action: a Web-case model\nMuch of the traditional schooling in America is built around systems of\ncompliance and control, characteristics which stifle the creative and\nentrepreneurial instincts of the children who are subjected to these\ntactics. The article explores a different approach to education, one\nthat involves capturing the interest of the student through the use of\nproblem and project-based instruction delivered via the Internet.\nCalled Entrepreneurs in Action, this program seeks to involve students\nin a problem at the outset and to promote the learning of traditional\nsubject areas as a process of the problem-solving activities that are\nundertaken. The program's details are explained, from elementary school\nthrough university level courses, and the authors outline their plans\nto test the efficacy of the program at each level\n", ['Entrepreneurs in Action', 'Web-case model', 'traditional schooling', 'America', 'entrepreneurial instincts', 'project-based instruction', 'Internet', 'traditional subject areas', 'problem-solving activities', 'elementary school', 'university level courses', 'educational computing', 'Internet', 'teaching']), ('Acquiring materials in the history of science, technology, and medicine\nThis article provides detailed advice on acquiring new, out-of-print, and rare\nmaterials in the history of science, technology, and medicine for the\nbeginner in these fields. The focus is on the policy formation, basic\nreference tools, and methods of collection development and acquisitions\nthat are the necessary basis for success in this endeavor\n', ['out-of-print books', 'special collections', 'library acquisitions', 'science', 'technology', 'medicine', 'rare materials', 'policy formation', 'basic reference tools', 'collection development', 'library automation', 'special libraries']), ('Evaluation of the usability of digital maintenance manuals developed without\neither user input or a task analysis\nThe primary objective was to investigate the value that can be added to a\nlow-cost digital maintenance manual by the addition of a navigational\naid. Two versions of a digital maintenance manual were developed, the\ndifference between them being the number of design heuristics observed\nwhen designing navigational aids. Neither version was based on an\nanalysis of the tasks carried out by users, nor were users involved in\nthe design process. Instead, the manuals were developed directly from\nthe digital information used to produce the existing paper manual.\nUsability trials were carried out to test both versions according to\nthe time taken and errors committed by users during typical information\nretrieval tasks. Users were questioned to determine their ease of use\n(EOU) perceptions for each manual. The main outcomes were that the\nnavigation aid used in the second version reduced the time taken to use\nthe manual but increased the number of errors made by users. The\nnavigational aid also seemed to reduce the perceived EOU compared with\nthe first version. In both cases, the perceived EOU was lower than for\na previous digital manual that had been developed using a task analysis\nand user input. The paper concludes by recommending the development of\na generic task model of user interaction with digital maintenance\nmanuals\n', ['digital maintenance manuals usability', 'navigational aid', 'usability trials', 'information retrieval', 'generic task model', 'user interaction', 'task analysis', 'information retrieval', 'task analysis', 'user manuals']), ('Quantum Zeno subspaces\nThe quantum Zeno effect is recast in terms of an adiabatic theorem when the\nmeasurement is described as the dynamical coupling to another quantum\nsystem that plays the role of apparatus. A few significant examples are\nproposed and their practical relevance discussed. We also focus on\ndecoherence-free subspaces\n', ['quantum Zeno subspaces', 'adiabatic theorem', 'dynamical coupling', 'measurement', 'decoherence-free subspaces', 'quantum computing', 'quantum theory']), ("Don't always believe what you Reed [optimisation techniques for Web sites and\ntrade mark infringement]\nOn 20 May 2002, Mr Justice Pumfrey gave judgment in the case of (1) Reed\nExecutive Plc (2) Reed Solutions Plc versus (1) Reed Business\nInformation Limited (2) Reed Elsevier (UK) Limited (3) totaljobs.com\nLimited. The case explored for the first time in any detail the extent\nto which the use of various optimisation techniques for Web sites could\ngive rise to new forms of trade mark infringement and passing off. The\nauthor reports on the case and offers his comments\n", ['Reed Executive Plc', 'Reed Solutions Plc', 'Reed Business Information Limited', 'Reed Elsevier (UK) Limited', 'totaljobs.com Limited', 'optimisation techniques', 'Web sites', 'trade mark infringement', 'passing off', 'copyright', 'information resources', 'legislation']), ('Verification of non-functional properties of a composable architecture with\nPetri nets\nIn this paper, we introduce our concept of composability and present the MSS\narchitecture as an example for a composable architecture. MSS claims to\nbe composable with respect to timing properties. We discuss, how to\nmodel and prove properties in such an architecture with time-extended\nPetrinets. As a result, the first step of a proof of composability is\npresented as well as a new kind of Petri net, which is more suitable\nfor modeling architectures like MSS\n', ['non-functional properties verification', 'MSS architecture', 'timing properties', 'composable architecture', 'Petri nets', 'proof of composability', 'formal specification', 'formal verification', 'Petri nets']), ('Raising the standard of management education for electronic commerce\nprofessionals\nThe teaching of electronic commerce in universities has become a growth\nindustry in itself. The rapid expansion of electronic commerce\nprogrammes raises the question of what actually is being taught. The\nassociation of electronic commerce as primarily a technical or\ninformation technology (IT) phenomenon has not been sufficient to\nconstrain it to IT and information systems departments. Business\nschools have been keen entrants into the electronic commerce coursework\nrace and they are developing electronic commerce programmes in an\nenvironment where there is no agreed definition of the term. This paper\ndraws on the work of Kenneth Boulding who argued that the dynamics of\nchange in society are largely a product of changing skills and the way\nthese skills are arranged into roles at the organizational level. It is\nargued that an overly technical interpretation of electronic commerce\nnarrows the skills being acquired as part of formal education.\nUniversities, under pressure from the market and technological change,\nare changing their roles resulting in a further narrowing of the\nbreadth of issues that is seen as legitimate to be included as\nelectronic commerce. The outcome is that aspiring electronic commerce\nprofessionals are not being exposed to a wide enough agenda of ideas\nand concepts that will assist them to make better business decisions\n', ['electronic commerce professionals', 'universities', 'information technology', 'IT', 'information systems', 'business schools', 'management education standards improvement', 'organizational level', 'formal education', 'Kenneth Boulding', 'accreditation', 'computer science education', 'electronic commerce', 'management education', 'professional aspects', 'standards', 'teaching']), ('Matched-filter template generation via spatial filtering: application to fetal\nbiomagnetic recordings\nWe have developed a two-step procedure for signal processing of fetal\nbiomagnetic recordings that removes cardiac interference and noise.\nFirst, a modified matched filter (MF) is applied to remove maternal\ncardiac interference; then, a simple signal space projection (SSP) is\napplied to remove noise. The key difference between our MF and a\nconventional one is that the interference template and the template\nscaling are derived from a signal that has been spatially filtered to\nisolate the interference, rather than from the raw signal. Unlike\nconventional MFs, ours is able to separate maternal and fetal cardiac\ncomplexes, even when they have similar morphology and overlap strongly.\nWhen followed by a SSP that preserves only the signal subspace, the\nnoise is reduced to a low level\n', ['maternal cardiac interference removal', 'simple signal space projection', 'noise removal', 'signal subspace preservation', 'fetal magnetocardiography', 'spatial filtering', 'interference template', 'raw signal', 'template scaling', 'modified matched filter', 'maternal cardiac interference', 'interference (signal)', 'magnetocardiography', 'matched filters', 'medical signal processing', 'obstetrics', 'spatial filters']), ('An adaptive time step procedure for a parabolic problem with blow-up\nIn this paper we introduce and analyze a fully discrete approximation for a\nparabolic problem with a nonlinear boundary condition which implies\nthat the solutions blow up in finite time. We use standard linear\nelements with mass lumping for the space variable. For the time\ndiscretization we write the problem in an equivalent form which is\nobtained by introducing an appropriate time re-scaling and then, we use\nexplicit Runge-Kutta methods for this equivalent problem. In order to\nmotivate our procedure we present it first in the case of a simple\nordinary differential equation and show how the blow up time is\napproximated in this case. We obtain necessary and sufficient\nconditions for the blowup of the numerical solution and prove that the\nnumerical blow-up time converges to the continuous one. We also study,\nfor the explicit Euler approximation, the localization of blow-up\npoints for the numerical scheme\n', ['adaptive time step procedure', 'parabolic problem', 'fully discrete approximation', 'nonlinear boundary condition', 'standard linear elements', 'Runge-Kutta methods', 'explicit Euler approximation', 'approximation theory', 'polynomials', 'Runge-Kutta methods', 'stability']), ("OMS battle heating up as Chicago Equity ousts LongView for Macgregor\nChicago Equity Partners LLC has gone into full production with Macgregor's\nFinancial Trading Platform. This marks a concentrated effort to achieve\nstraight-through processing\n", ['Chicago Equity Partners', 'Macgregor', 'Financial Trading Platform', 'straight-through processing', 'LongView', 'investment']), ('A model of periodic oscillation for genetic regulatory systems\nIn this paper, we focus on modeling and explaining periodic oscillations in\ngene-protein systems with a simple nonlinear model and on analyzing\neffects of time delay on the stability of oscillations. Our main model\nof genetic regulation comprises of a two-gene system with an\nautoregulatory feedback loop. We exploit multiple time scales and\nhysteretic properties of the model to construct periodic oscillations\nwith jumping dynamics and analyze the possible mechanism according to\nthe singular perturbation theory. As shown in this paper, periodic\noscillations are mainly generated by nonlinearly negative and positive\nfeedback loops in gene regulatory systems, whereas the jumping dynamics\nis generally caused by time scale differences among biochemical\nreactions. This simple model may actually act as a genetic oscillator\nor switch in gene-protein networks because the dynamics are robust for\nparameter perturbations or environment variations. We also explore\neffects of time delay on the stability of the dynamics, showing that\nthe time delay generally increases the stability region of the\noscillations, thereby making the oscillations robust to parameter\nchanges. Two examples are also provided to numerically demonstrate our\ntheoretical results\n', ['modeling', 'periodic oscillations', 'gene-protein systems', 'nonlinear model', 'time delay', 'oscillations stability', 'genetic regulation', 'two-gene system', 'autoregulatory feedback loop', 'hysteretic properties', 'jumping dynamics', 'singular perturbation theory', 'nonlinearly negative feedback loops', 'nonlinearly positive feedback loops', 'genetic regulatory system', 'biochemical reactions', 'stability region', 'bifurcation', 'circadian rhythm', 'relaxation oscillator', 'bifurcation', 'delays', 'feedback', 'genetics', 'hysteresis', 'oscillations', 'physiological models', 'proteins', 'relaxation oscillators', 'stability']), ('Linear, parameter-varying control and its application to a turbofan engine\nThis paper describes application of parameter-dependent control design methods\nto a turbofan engine. Parameter-dependent systems are linear systems,\nwhose state-space descriptions are known functions of time-varying\nparameters. The time variation of each of the parameters is not known\nin advance, but is assumed to be measurable in real-time. Three linear,\nparameter-varying (LPV) approaches to control design are discussed. The\nfirst method is based on linear fractional transformations which relies\non the small gain theorem for bounds on performance and robustness. The\nother methods make use of either a single (SQLF) or parameter-dependent\n(PDQLF) quadratic Lyapunov function to bound the achievable level of\nperformance. The latter two techniques are used to synthesize\ncontrollers for a high-performance turbofan engine. A LPV model of the\nturbofan engine is constructed from Jacobian linearizations at fixed\npower codes for control design. The control problem is formulated as a\nmodel matching problem in the H/sub infinity / and LPV framework. The\nobjective is decoupled command response of the closed-loop system to\npressure and rotor speed requests. The performance of linear, H/sub\ninfinity / point designs are compared with the SQLF and PDQLF\ncontrollers. Nonlinear simulations indicate that the controller\nsynthesized using the SQLF approach is slightly more conservative than\nthe PDQLF controller. Nonlinear simulations with the SQLF and PDQLF\ncontrollers show very robust designs that achieve all desired\nperformance objectives\n', ['turbofan engine', 'linear parameter-varying control', 'parameter-dependent control design methods', 'state-space descriptions', 'time-varying parameters', 'linear fractional transformations', 'small gain theorem', 'performance bounds', 'robustness bounds', 'single quadratic Lyapunov function', 'parameter-dependent quadratic Lyapunov function', 'Jacobian linearizations', 'decoupled command response', 'closed-loop system', 'model matching problem', 'H/sub infinity / framework', 'nonlinear simulations', 'very robust designs', 'aerospace control', 'aerospace engines', 'closed loop systems', 'control system synthesis', 'H/sup infinity / control', 'Lyapunov methods', 'nonlinear control systems', 'robust control', 'state-space methods', 'time-varying systems']), ('Sufficient conditions on nonemptiness and boundedness of the solution set of\nthe P/sub 0/ function nonlinear complementarity problem\nThe P/sub 0/ function nonlinear complementarity, problem (NCP) has attracted a\nlot of attention among researchers. Various assumed conditions, which\nensure that the NCP has a solution have been proposed. In this paper,\nby using the notion of an exceptional family of elements we develop a\nsufficient condition which ensures that the solution set of the P/sub\n0/ function NCP is nonempty and bounded. In particular, we prove that\nmany existing assumed conditions imply this sufficient condition. Thus,\nthese conditions imply that the solution set of the P/sub 0/ function\nNCP is nonempty and bounded. In addition, we also prove directly that a\nfew existence conditions imply that the solution set of the P/sub 0/\nfunction NCP is bounded\n', ['sufficient conditions', 'nonemptiness', 'boundedness', 'solution set', 'P/sub 0/ function nonlinear complementarity problem', 'functions', 'minimisation', 'set theory']), ("Multi-timescale Internet traffic engineering\nThe Internet is a collection of packet-based hop-by-hop routed networks.\nInternet traffic engineering is the process of allocating resources to\nmeet the performance requirements of users and operators for their\ntraffic. Current mechanisms for doing so, exemplified by TCP's\ncongestion control or the variety of packet marking disciplines,\nconcentrate on allocating resources on a per-packet basis or at data\ntimescales. This article motivates the need for traffic engineering in\nthe Internet at other timescales, namely control and management\ntimescales, and presents three mechanisms for this. It also presents a\nscenario to show how these mechanisms increase the flexibility of\noperators' service offerings and potentially also ease problems of\nInternet management\n", ['multi-timescale Internet traffic engineering', 'packet-based hop-by-hop routed networks', 'TCP congestion control', 'packet marking disciplines', 'resource allocation', 'control timescale', 'operator services', 'Internet management', 'admission control', 'ECN proxy', 'BGP routing protocol', 'computer network management', 'Internet', 'internetworking', 'packet switching', 'telecommunication congestion control', 'telecommunication network routing', 'telecommunication traffic', 'transport protocols']), ('Reply to "Comment on: Teleportation of an unknown state by W state" [Phys.\nLett. A 300 (2002) 324]\nIn our letter (see ibid., vol. 296, p. 161 (2002)), the main question we\nconsider is whether a general three-particle W state can be used to\nrealize the teleportation of an unknown qubit state. We give the\npositive answer to this question in our letter, and show that W state\ncan be used to realize to do that probabilistically. We also discuss\nhow to do it in detail in our letter. In the previous comment (see\nibid., vol. 300, p. 324 (2002)), authors check carefully the\nmathematics calculation of our letter, find and point out a simple\nmathematics error about normalization coefficient of Eq. (1). This\nmathematics error induces the incorrect probability calculation of Eq.\n(6), and also an incorrect claim in first part of our letter\n', ['teleportation', 'unknown state', 'three-particle W state', 'qubit state', 'normalization coefficient', 'probability calculation', 'probability', 'quantum communication', 'quantum theory']), ('Reconstruction of MR images from data acquired on an arbitrary k-space\ntrajectory using the same-image weight\nA sampling density compensation function denoted "same-image (SI) weight" is\nproposed to reconstruct MR images from the data acquired on an\narbitrary k-space trajectory. An equation for the SI weight is\nestablished on the SI criterion and an iterative scheme is developed to\nfind the weight. The SI weight is then used to reconstruct images from\nthe data calculated on a random trajectory in a numerical phantom case\nand from the data acquired on interleaved spirals in an in vivo\nexperiment, respectively. In addition, Pipe and Menon\'s weight (MRM\n1999;41:179-186) is also used in the reconstructions to make a\ncomparison. The images obtained with the SI weight were found to be\nslightly more accurate than those obtained with Pipe\'s weight\n', ['sampling density compensation', 'MRI image reconstruction', 'arbitrary k-space trajectory', 'spiral trajectory', 'same-image weight', 'random trajectory', 'convolution function', 'numerical phantom', 'Nyquist sampling conditions', 'iterative algorithm', 'weighting function', 'biomedical MRI', 'convolution', 'image reconstruction', 'image sampling', 'iterative methods', 'medical image processing']), ("Data quality - unlocking the ROI in CRM\nWhile many organisations realise their most valuable asset is their customers,\nmany more fail to realise the importance of auditing, maintaining and\nupdating the information contained in their customer databases. Today's\ngrowing awareness in the importance of data quality in relation to CRM\nand ROI will help change this attitude. In response, CRM vendors will\nfollow suit and begin to differentiate themselves by offering data\nquality as part of an enterprise-wide data management methodology\n", ['customer databases', 'data management', 'customer relationships', 'CRM', 'return on investment', 'database management systems', 'marketing']), ("Presenting-a better mousetrap [Leeza outboard video signal processor]\nScaling interlaced video to match high-resolution plasma, LCD, and DLP displays\nis a tough job, but Key Digital's Leeza is zip to the tack. And it's\ndigitally bilingual, too. There's no question that outboard video\nsignal processors like Leeza help overcome the inherent limitations of\nfixed-pixel displays. Being able to match a native display rate with\nheavily processed video makes the viewing experience much more\nenjoyable. But it seemed that 70% of the improvement in image quality\ncame from using a digital interface to the DVD player, as most noise\nand picture artifacts are introduced in the analog video encoding\nprocess\n", ['DLP displays', 'LCD displays', 'plasma displays', 'Leeza', 'outboard video signal processors', 'fixed-pixel displays', 'heavily processed video', 'liquid crystal displays', 'plasma displays', 'video signal processing']), ('On a general constitutive description for the inelastic and failure behavior of\nfibrous laminates. I. Lamina theory\nIt is well known that a structural design with isotropic materials can only be\naccomplished based on a stress failure criterion. This is, however,\ngenerally not true with laminated composites. Only when the laminate is\nsubjected to an in-plane load, can the ultimate failure of the laminate\ncorrespond to its last-ply failure, and hence a stress failure\ncriterion may be sufficient to detect the maximum load that can be\nsustained by the laminate. Even in such a case, the load shared by each\nlamina in the laminate cannot be correctly determined if the lamina\ninstantaneous stiffness matrix is inaccurately provided, since the\nlamina is always statically indeterminate in the laminate. If, however,\nthe laminate is subjected to a lateral load, its ultimate failure\noccurs before last-ply failure and use of the stress failure criterion\nis no longer sufficient; an additional critical deflection or curvature\ncondition must also be employed. This necessitates development of an\nefficient constitutive relationship for laminated composites in order\nthat the laminate strains/deflections up to ultimate failure can be\naccurately calculated. A general constitutive description for the\nthermomechanical response of a fibrous laminate up to ultimate failure\nwith applications to various fibrous laminates is presented in the two\npapers. The constitutive relationship is obtained by combining\nclassical lamination theory with a recently developed bridging\nmicromechanics model, through a layer-by-layer analysis. This paper\nfocuses on lamina analysis\n', ['general constitutive description', 'inelastic behavior', 'failure behavior', 'fibrous laminates', 'lamina theory', 'structural design', 'isotropic materials', 'stress failure criterion', 'in-plane load', 'instantaneous stiffness matrix', 'lateral load', 'last-ply failure', 'critical deflection condition', 'critical curvature condition', 'composites', 'laminate strains', 'laminate deflections', 'thermomechanical response', 'layer-by-layer analysis', 'micromechanics model', 'multidirectional tape laminae', 'woven fabric composites', 'braided fabric composites', 'knitted fabric reinforced composites', 'elastoplasticity', 'elastic-viscoplasticity', 'elastoplasticity', 'fibre reinforced composites', 'internal stresses', 'laminates', 'mechanical engineering computing', 'micromechanics', 'viscoplasticity']), ('Asymptotic normality for the K/sub phi /-divergence goodness-of-fit tests\nIn this paper for a wide class of goodness-of-fit statistics based K/sub phi\n/-divergences, the asymptotic normality is established under the\nassumption n/m/sub n/ to a in (0, infinity ), where n denotes sample\nsize and m/sub n/ the number of cells. This result is extended to\ncontiguous alternatives to study asymptotic efficiency\n', ['asymptotic normality', 'asymptotic efficiency', 'K/sub phi /-divergence goodness-of-fit tests', 'probability', 'statistical analysis']), ('Taking it to the max [ventilation systems]\nRaising the volumetric air supply rate is one way of increasing the cooling\ncapacity of displacement ventilation systems. David Butler and Michael\nSwainson explore how different types of diffusers can help make this\nwork\n', ['volumetric air supply rate', 'cooling capacity', 'displacement ventilation systems', 'diffusers', 'ventilation']), ('Separation and tracking of multiple broadband sources with one electromagnetic\nvector sensor\nA structure for adaptively separating, enhancing and tracking uncorrelated\nsources with an electromagnetic vector sensor (EMVS) is presented. The\nstructure consists of a set of parallel spatial processors, one for\neach individual source. Two stages of processing are involved in each\nspatial processor. The first preprocessing stage rejects all other\nsources except the one of interest, while the second stage is an\nadaptive one for maximizing the signal-to-noise ratio (SNR) and\ntracking the desired source. The preprocessings are designed using the\nlatest source parameter estimates obtained from the source trackers,\nand a redesign is activated periodically or whenever any source has\nbeen detected by the source trackers to have made significant movement.\nCompared with conventional adaptive beamforming, the algorithm has the\nadvantage that no a priori information on any desired signal location\nis needed, the sources are separated at maximum SNR, and their\nlocations are available. The structure is also well suited for parallel\nimplementation. Numerical examples are included to illustrate the\ncapability and performance of the algorithm\n', ['multiple broadband sources separation', 'multiple broadband sources tracking', 'uncorrelated sources', 'electromagnetic vector sensor', 'single EM vector sensor', 'parallel spatial processors', 'preprocessing stage', 'adaptive second stage', 'signal-to-noise ratio', 'SNR maximization', 'maximum SNR', 'signal source location', 'parallel implementation', 'adaptive source enhancement', 'adaptive signal processing', 'electromagnetic interference', 'matrix algebra', 'parallel algorithms', 'parameter estimation', 'tracking']), ('Community technology and democratic rationalization\nThe objective of the paper is to explore questions of human agency and\ndemocratic process in the technical sphere through the example of\n"virtual community." The formation of relatively stable long-term group\nassociations (community in the broad sense of the term), is the scene\non which a large share of human development occurs. As such it is a\nfundamental human value mobilizing diverse ideologies and\nsensitivities. The promise of realizing this value in a new domain\nnaturally stirs up much excitement among optimistic observers of the\nInternet. At the same time, the eagerness to place hopes for community\nin a technical system flies in the face of an influential intellectual\ntradition of technology criticism. This eagerness seems even more naive\nin the light of the recent commercialization of so much Internet\nactivity. Despite the widespread skepticism, we believe the growth of\nvirtual community is significant for an inquiry into the\ndemocratization of technology. We show that conflicting answers to the\ncentral question of the present theoretical debate - Is community\npossible on computer networks? epsilon neralize from particular\nfeatures of systems and software prevalent at different stages in the\ndevelopment of computer networking. We conclude that research should\nfocus instead on how to design computer networks to better support\ncommunity activities and values\n', ['community technology', 'democratic rationalization', 'human agency', 'democratic process', 'technical sphere', 'virtual community', 'stable long-term group associations', 'human development', 'human value', 'diverse ideologies', 'optimistic observers', 'technical system', 'intellectual tradition', 'technology criticism', 'Internet activity', 'conflicting answers', 'computer networks', 'computer networking', 'community activities', 'computer networks', 'groupware', 'human factors', 'information resources', 'Internet', 'public administration', 'social sciences computing']), ('Hit the road, Jack\nGoing freelance offers the potential of higher earnings, variety and\nindependence - but also removes the benefits of permanent employment\nand can mean long distance travel and periods out of work. The author\nlooks at the benefits and drawbacks - and how to get started as an IT\ncontractor\n', ['IT contractor', 'freelance working', 'DP industry', 'economics', 'employment']), ("Improvements and critique on Sugeno's and Yasukawa's qualitative modeling\nInvestigates Sugeno's and Yasukawa's (1993) qualitative fuzzy modeling\napproach. We propose some easily implementable solutions for the\nunclear details of the original paper, such as trapezoid approximation\nof membership functions, rule creation from sample data points, and\nselection of important variables. We further suggest an improved\nparameter identification algorithm to be applied instead of the\noriginal one. These details are crucial concerning the method's\nperformance as it is shown in a comparative analysis and helps to\nimprove the accuracy of the built-up model. Finally, we propose a\npossible further rule base reduction which can be applied successfully\nin certain cases. This improvement reduces the time requirement of the\nmethod by up to 16% in our experiments\n", ['qualitative modeling', 'fuzzy modeling', 'trapezoid approximation', 'membership functions', 'rule creation', 'parameter identification algorithm', 'rule base reduction', 'Sugeno-Yasukawa method', 'function approximation', 'fuzzy logic', 'fuzzy set theory', 'modelling', 'parameter estimation']), ('A note on vector cascade algorithm\nThe focus of this paper is on the relationship between accuracy of multivariate\nrefinable vector and vector cascade algorithm. We show that, if the\nvector cascade algorithm (1.5) with isotropic dilation converges to a\nvector-valued function with regularity, then the initial function must\nsatisfy the Strang-Fix conditions\n', ['vector cascade algorithm', 'multivariate refinable vector', 'matrix algebra', 'isotropic dilation', 'vector-valued function', 'Strang-fix conditions', 'matrix algebra', 'polynomials', 'wavelet transforms']), ('Neural networks in optimal filtration\nThe combined use and mutual influence of neural networks and optimal filtering\nis considered; the neural-network and filtering approaches are compared\nby solving two simple optimal-filtering problems: linear filtering and\nthe filtering of a binary telegraph signal corresponding to\nobservations in discrete white noise\n', ['optimal filtering', 'neural networks', 'linear filtering', 'binary telegraph signal', 'observations', 'discrete white noise', 'filtering theory', 'neural nets', 'optimisation', 'signal processing', 'telegraphy', 'white noise']), ("British Standard 7666 as a framework for geocoding land and property\ninformation the UK\nThe article examines the role of British Standard 7666 in the development of a\nnational framework for geocoding land and property information in the\nUnited Kingdom. The author assesses how local authorities, and other\nagencies concerned with property and address datasets, are coping with\nthe introduction of British Standard 7666, and examines the prospects\nand limitations of this development. British Standard 7666 has four\nparts, comprising specifications for street gazetteer; land and\nproperty gazetteer; addresses; and public rights of way. The\norganisation coordinating the introduction of British Standard 7666,\nImprovement and Development Agency (IDeA), is also overseeing the\ndevelopment and maintenance of a National Land and Property Gazetteer\n(NLPG) based on British Standard 7666. The introduction of the new\naddressing standard has mainly been prompted by Britain's effort to set\nup a national cadastral service to replace the obsolescent property\nregistration system currently in place\n", ['British Standard 7666', 'geocoding', 'property information', 'land information', 'UK', 'national framework', 'United Kingdom', 'local authorities', 'address datasets', 'property datasets', 'street gazetteer', 'property gazetteer', 'land gazetteer', 'addresses', 'public rights of way', 'Improvement and Development Agency', 'IDeA', 'National Land and Property Gazetteer', 'NLPG', 'addressing standard', 'national cadastral service', 'property registration system', 'land information systems', 'encoding', 'geographic information systems', 'government policies', 'standards', 'town and country planning']), ('A new method of regression on latent variables. Application to spectral data\nSeveral applications are based on the assessment of a linear model linking a\nset of variables Y to a set of predictors X. In the presence of strong\ncolinearity among predictors, as in the case with spectral data,\nseveral alternative procedures to ordinary least squares (OLS) are\nproposed, We discuss a new alternative approach which we refer to as\nregression models through constrained principal components analysis\n(RM-CPCA). This method basically shares certain common characteristics\nwith PLS regression as the dependent variables play a central role in\ndetermining the latent variables to be used as predictors. Unlike PLS,\nhowever, the approach discussed leads to straightforward models. This\nmethod also bears some similarity to latent root regression analysis\n(LRR) that was discussed by several authors. Moreover, a tuning\nparameter that ranges between 0 and 1 is introduced and the family of\nmodels thus formed includes several other methods as particular cases\n', ['latent variables', 'spectral data', 'linear model', 'near-IR spectroscopy', 'predictors', 'strong colinearity', 'regression models through constrained principal components analysis', 'dependent variables', 'latent root regression analysis', 'tuning parameter', 'infrared spectroscopy', 'principal component analysis', 'spectroscopy computing']), ('Optimal multi-degree reduction of Bezier curves with constraints of endpoints\ncontinuity\nGiven a Bezier curve of degree n, the problem of optimal multi-degree reduction\n(degree reduction of more than one degree) by a Bezier curve of degree\nm (m<n-1) with constraints of endpoint continuity is investigated.\nWith respect to L/sub 2/ norm, this paper presents an approximate\nmethod (MDR by L/sub 2/) that gives an explicit solution to deal with\nit. The method has good properties of endpoint interpolation:\ncontinuity of any r, s (r, s>or=0) orders can be preserved at two\nendpoints respectively. The method in the paper performs multi-degree\nreduction at one time and does not need stepwise computing. When\napplied to multi-degree reduction with endpoint continuity of any\norder, the MDR by L/sub 2/ obtains the best least squares\napproximation. Comparison with another method of multi-degree reduction\n(MDR by L/sub infinity /), which achieves the nearly best uniform\napproximation with respect to L/sub infinity / norm, is also given. The\napproximate effect of the MDR by L/sub 2/ is better than that of the\nMDR by L/sub infinity /. Explicit approximate error analysis of the\nmulti-degree reduction methods is presented\n', ['optimal multi-degree reduction', 'Bezier curves', 'endpoint continuity constraints', 'approximate method', 'explicit solution', 'endpoint interpolation', 'least squares approximation', 'uniform approximation', 'explicit approximate error analysis', 'CAD/CAM', 'computational geometry', 'engineering graphics', 'interpolation', 'least squares approximations']), ('The semantic Web: differentiating between taxonomies and ontologies\nThere is a new vision of the WWW - the semantic Web - that will dramatically\nimprove Web-based services and products. It creates a setting where\nsoftware agents perform everyday jobs for end-users. Deploying\nhierarchies, metadata, and structured vocabularies, the semantic Web\nexpands basic Internet functions\n', ['WWW', 'semantic Web', 'software agents', 'hierarchies', 'metadata', 'structured vocabularies', 'Internet', 'information resources', 'meta data', 'software agents', 'vocabulary']), ('The perils of privacy\nThe recent string of failures among dotcom companies has heightened fears of\nprivacy abuse. What should happen to the names and addresses on a\ncustomer list if these details were obtained under a privacy policy\nwhich specified no disclosure to any third party? Should the personal\ndata in the list be deemed to be an asset of a failing company which\ncan be transferred to any future (third party) purchaser for its\npurposes? Or should the privacy policy take precedence over the\ncommercial concerns of the purchaser?\n', ['privacy abuse', 'customer list', 'privacy policy', 'disclosure', 'data privacy', 'security of data', 'social aspects of automation']), ('Setup cost and lead time reductions on stochastic inventory models with a\nservice level constraint\nThe stochastic inventory models analyzed in this paper explore the problem of\nlead time associated with setup cost reductions for the continuous\nreview and periodic review inventory models. For these two models with\na mixture of backorders and lost sales, we respectively assume that\ntheir mean and variance of the lead time demand and protection interval\n(i.e., lead time plus review period) demand are known, but their\nprobability distributions are unknown. We develop a minimax\ndistribution free procedure to find the optimal solution-for each case\n', ['setup cost reductions', 'lead time reductions', 'stochastic inventory models', 'service level constraint', 'continuous review inventory models', 'periodic review inventory models', 'backorders', 'lost sales', 'lead time demand', 'protection interval', 'probability distributions', 'minimax distribution free procedure', 'minimax techniques', 'production control', 'statistical analysis', 'stochastic processes', 'stock control']), ('SPARC ignites scholarly publishing\nDuring the past several years, initiatives which bring together librarians,\nresearchers, university administrators and independent publishers have\nre-invigorated the scholarly publishing marketplace. These initiatives\ntake advantage of electronic technology and show great potential for\nrestoring science to scientists. The author outlines SPARC (the\nScholarly Publishing and Academic Resources Coalition), an initiative\nto make scientific journals more accessible\n', ['electronic publishing', 'initiative', 'scientific journal access', 'SPARC', 'Scholarly Publishing and Academic Resources Coalition', 'electronic publishing', 'research initiatives']), ('A scalable intelligent takeoff controller for a simulated running jointed leg\nRunning with jointed legs poses a difficult control problem in robotics. Neural\ncontrollers are attractive because they allow the robot to adapt to\nchanging environmental conditions. However, scalability is an issue\nwith many neural controllers. The paper describes the development of a\nscalable neurofuzzy controller for the takeoff phase of the running\nstride. Scalability is achieved by selecting a controller whose size\ndoes not grow with the dimensionality of the problem. Empirical results\nshow that with proper design the takeoff controller scales from a leg\nwith a single movable link to one with three movable links without a\ncorresponding growth in size and without a loss of accuracy\n', ['scalable intelligent takeoff controller', 'simulated running jointed leg', 'neural controllers', 'changing environmental conditions', 'scalability', 'scalable neurofuzzy controller', 'takeoff phase', 'running stride', 'intelligent robotic control', 'backpropagation', 'cerebellar model arithmetic computers', 'fuzzy control', 'intelligent control', 'learning (artificial intelligence)', 'legged locomotion', 'neurocontrollers']), ('Asymptotical stability in discrete-time neural networks\nIn this work, we present a proof of the existence of a fixed point and a\ngeneralized sufficient condition that guarantees the stability of it in\ndiscrete-time neural networks by using the Lyapunov function method. We\nalso show that for both symmetric and asymmetric connections, the\nunique attractor is a fixed point when several conditions are\nsatisfied. This is an extended result of Chen and Aihara (see Physica\nD, vol. 104, no. 3/4, p. 286-325, 1997). In particular, we further\nstudy the stability of equilibrium in discrete-time neural networks\nwith the connection weight matrix in form of an interval matrix.\nFinally, several examples are shown to illustrate and reinforce our\ntheory\n', ['asymptotical stability', 'fixed point', 'Lyapunov function method', 'symmetric connections', 'asymmetric connections', 'unique attractor', 'stability', 'equilibrium stability', 'connection weight matrix', 'interval matrix', 'generalized sufficient condition', 'discrete-time neural networks', 'asymptotic stability', 'discrete time systems', 'Lyapunov methods', 'matrix algebra', 'neural nets']), ('Breast MR imaging with high spectral and spatial resolutions: preliminary\nexperience\nThe authors evaluated magnetic resonance (MR) imaging with high spectral and\nspatial resolutions (HSSR) of water and fat in breasts of healthy\nvolunteers (n=6) and women with suspicious lesions (n=6). Fat\nsuppression, edge delineation, and image texture were improved on MR\nimages derived from HSSR data compared with those on conventional MR\nimages. HSSR MR imaging data acquired before and after contrast medium\ninjection showed spectrally inhomogeneous changes in the water\nresonances in small voxels that were not detectable with conventional\nMR imaging\n', ['breast magnetic resonance imaging', 'high spectral spatial resolutions', 'healthy volunteers', 'edge delineation', 'image texture', 'magnetic resonance images', 'magnetic resonance imaging data', 'contrast medium injection', 'water resonances', 'small voxels', 'women', 'suspicious lesions', 'fat suppression', 'biomedical MRI', 'image texture', 'medical image processing']), ('Calibrated initials for an EM applied to recursive models of categorical\nvariables\nThe estimates from an EM, when it is applied to a large causal model of 10 or\nmore categorical variables, are often subject to the initial values for\nthe estimates. This phenomenon becomes more serious as the model\nstructure becomes more complicated involving more variables. As a\nmeasure of compensation for this, it has been recommended in literature\nthat EMs are implemented several times with different sets of initial\nvalues to obtain more appropriate estimates. We propose an improved\napproach for initial values. The main idea is that we use initials that\nare calibrated to data. A simulation result strongly indicates that the\ncalibrated initials give rise to the estimates that are far closer to\nthe true values than the initials that are not calibrated\n', ['EM', 'recursive models', 'categorical variables', 'calibrated initials', 'large causal model', 'initial values', 'simulation', 'calibration', 'parameter estimation', 'probability', 'simulation']), ('No-go areas? [content management]\nAlex Fry looks at how content management systems can be used to ensure website\naccess for one important customer group, the disabled\n', ['disabled', 'content management systems', 'website access', 'handicapped aids', 'information resources']), ("New wrinkle on the Web? Hmm. [banking]\nThe financial sector produced its share of technology hype during the new\neconomy years. You. can't blame folks if the next next thing, a wave of\nInternet-related innovation called Web services, is being met with\nhealthy skepticism. Many gurus are placing their bets on Web services\nto drive the next chapter of finance technology, dramatically upgrading\ndisappointing automated customer management strategies by\nelectronically breaking down barriers between products, firms and\ncustomers, and perhaps creating a whole new line of business in the\nprocess. But it's not a magic wand. It doesn't change the need for a\nbank to reorganize and streamline its operations\n", ['Web services', 'bank', 'banking', 'information resources', 'intranets']), ("Organization design: The continuing influence of information technology\nDrawing from an information processing perspective, this paper examines how\ninformation technology (IT) has been a catalyst in the development of\nnew forms of organizational structures. The article draws a historical\nlinkage between the relative stability of an organization's task\nenvironment starting after the Second World War to the present\nenvironmental instability that now characterizes many industries.\nSpecifically, the authors suggest that advances in IT have enabled\nmanagers to adapt existing forms and create new models for\norganizational design that better fit requirements of an unstable\nenvironment. Time has seemingly borne out this hypothesis as the\nbureaucratic structure evolved to the matrix to the network and now to\nthe emerging shadow structure. IT has gone from a support mechanism to\na substitute for organizational structures in the form of the shadow\nstructure. The article suggests that the evolving and expanding role of\nIT will continue for organizations that face unstable environments\n", ['organization design', 'information processing perspective', 'organizational structures', 'organization task environment', 'environmental instability', 'information technology', 'DP management', 'information technology', 'social aspects of automation']), ('Content all clear [workflow & content management]\nGraeme Muir of SchlumbergerSema cuts through the confusion between content,\ndocument and records management\n', ['SchlumbergerSema', 'content management', 'document management', 'records management', 'document handling', 'information resources', 'records management']), ('Simulation study of the cardiovascular functional status in hypertensive\nsituation\nAn extended cardiovascular model was established based on our previous work to\nstudy the consequences of physiological or pathological changes to the\nhomeostatic functions of the cardiovascular system. To study\nhemodynamic changes in hypertensive situations, the impacts of\ncardiovascular parameter variations (peripheral vascular resistance,\narterial vessel wall stiffness and baroreflex gain) upon hemodynamics\nand the short-term regulation of the cardiovascular system were\ninvestigated. For the purpose of analyzing baroregulation function, the\nshort-term regulation of arterial pressure in response to moderate\ndynamic exercise for normotensive and hypertensive cases was studied\nthrough computer simulation and clinical experiments. The simulation\nresults agree well with clinical data. The results of this work suggest\nthat the model presented in this paper provides a useful tool to\ninvestigate the functional status of the cardiovascular system in\nnormal or pathological conditions\n', ['extended cardiovascular model', 'pathological changes', 'physiological changes', 'homeostatic functions', 'cardiovascular functional status', 'hypertensive situation', 'cardiovascular parameter variations', 'peripheral vascular resistance', 'arterial vessel wall stiffness', 'baroreflex gain', 'hemodynamics', 'short-term regulation', 'arterial pressure', 'moderate dynamic exercise', 'normotensive cases', 'computer simulation', 'clinical experiments', 'blood vessels', 'cardiovascular system', 'digital simulation', 'haemodynamics', 'medical computing']), ('Dependence graphs: dependence within and between groups\nThis paper applies the two-party dependence theory (Castelfranchi, Cesta and\nMiceli, 1992, in Y. Demazeau and E. Werner (Eds.) Decentralized AI-3,\nElsevier, North Holland) to modelling multiagent and group dependence.\nThese have theoretical potentialities for the study of emerging groups\nand collective structures, and more generally for understanding social\nand organisational complexity, and practical utility for both\nsocial-organisational and agent systems purposes. In the paper, the\ndependence theory is extended to describe multiagent links, with a\nspecial reference to group and collective phenomena, and is proposed as\na framework for the study of emerging social structures, such as groups\nand collectives. In order to do so, we propose to extend the notion of\ndependence networks (applied to a single agent) to dependence graphs\n(applied to an agency). In its present version, the dependence theory\nis argued to provide (a) a theoretical instrument for the study of\nsocial complexity, and (b) a computational system for managing the\nnegotiation process in competitive contexts and for monitoring\ncomplexity in organisational and other cooperative contexts\n', ['dependence graphs', 'group dependence', 'two-party dependence theory', 'multiagent dependence', 'emerging groups', 'collective structures', 'multiagent systems', 'organisational complexity', 'social complexity', 'agent systems', 'dependence networks', 'graph theory', 'multi-agent systems', 'social sciences']), ('Multichannel scaler for general statistical analysis of dynamic light\nscattering\nA four channel scaler for counting applications has been designed and built\nusing a standard high transfer rate parallel computer interface bus\nparallel data card. The counter section is based on standard complex\nprogrammable logic device integrated circuits. With a 200 MHz Pentium\nbased host PC a sustained counting and data transfer with channel\nwidths as short as 200 ns for a single channel is realized. The use of\nthe multichannel scaler is demonstrated in dynamic light scattering\nexperiments. The recorded traces are analyzed with wavelet and other\nstatistical techniques to obtain transient changes in the properties of\nthe scattered light\n', ['multichannel scaler', 'general statistical analysis', 'dynamic light scattering', 'correlation spectroscopy', 'optical spectroscopic techniques', 'photon signal statistical properties', 'four channel scaler', 'standard high transfer rate parallel computer interface', 'interface bus parallel data card', 'complex programmable logic device', 'standard CPLD ICs', 'Pentium based host PC', 'windowed Fourier transform', '200 MHz', '200 ns', 'computerised instrumentation', 'light scattering', 'peripheral interfaces', 'photon correlation spectroscopy', 'programmable logic devices', 'scaling circuits', 'signal processing equipment', 'statistical analysis', 'timing']), ("Evaluating alternative manufacturing control strategies using a benchmark\nsystem\nThis paper describes an investigation of the effects of dynamic job routing and\njob sequencing decisions on the performance of a distributed control\nsystem and its adaptability against disturbances. This experimental\nwork was carried out to compare the performance of alternative control\nstrategies in various manufacturing environments and to investigate the\nrelationship between the 'control' and 'controlled' systems. The\nexperimental test-bed presented in this paper consists of an\nagent-based control system (implemented in C++) and a discrete-event\nsimulation model. Using this test-bed, various control strategies were\ntested on a benchmark manufacturing system by varying production\nvolumes (to model the production system with looser/tighter schedules)\nand disturbance frequencies. It was found that hybrid strategies that\ncombine reactive agent mechanisms (and allocation strategies such as\nthe contract net) with appropriate job sequencing heuristics provide\nthe best performance, particularly when job congestion increases on a\nshop-floor\n", ['alternative manufacturing control strategies', 'benchmark system', 'dynamic job routing', 'job sequencing decisions', 'distributed control system', 'disturbance adaptability', 'agent-based control system', 'discrete-event simulation model', 'experimental test-bed', 'benchmark manufacturing system', 'production volumes', 'disturbance frequencies', 'hybrid strategies', 'reactive agent mechanisms', 'allocation strategies', 'contract net', 'job congestion', 'control system analysis computing', 'discrete event simulation', 'distributed control', 'multi-agent systems', 'production control']), ('A formal framework for viewpoint consistency\nMultiple viewpoint models of system development are becoming increasingly\nimportant. Each viewpoint offers a different perspective on the target\nsystem and system development involves parallel refinement of the\nmultiple views. Viewpoint related approaches have been considered in a\nnumber of different guises by a spectrum of researchers. Our work\nparticularly focuses on the use of viewpoints in open distributed\nprocessing (ODP) which is an ISO/ITU standardisation framework. The\nrequirements of viewpoint modelling in ODP are very broad and, hence,\ndemanding. Multiple viewpoints, though, prompt the issue of consistency\nbetween viewpoints. This paper describes a very general interpretation\nof consistency which we argue is broad enough to meet the requirements\nof consistency in ODP. We present a formal framework for this general\ninterpretation; highlight basic properties of the interpretation and\nlocate restricted classes of consistency. Strategies for checking\nconsistency are also investigated. Throughout we illustrate our theory\nusing the formal description technique LOTOS. Thus, the paper also\ncharacterises the nature of and options for consistency checking in\nLOTOS\n', ['multiple viewpoint models', 'formal framework', 'viewpoint consistency', 'system development', 'open distributed processing', 'ODP', 'ISO/ITU standardisation framework', 'consistency checking', 'formal description technique', 'LOTOS', 'development models', 'process algebra', 'distributed processing', 'formal specification', 'formal verification', 'ISO standards', 'process algebra', 'specification languages']), ('Nonlinear extrapolation algorithm for realization of a scalar random process\nA method of construction of a nonlinear extrapolation algorithm is proposed.\nThis method makes it possible to take into account any nonlinear random\ndependences that exist in an investigated process and are described by\nmixed central moment functions. The method is based on the V. S.\nPugachev canonical decomposition apparatus. As an example, the problem\nof nonlinear extrapolation is solved for a moment function of third\norder\n', ['nonlinear extrapolation algorithm', 'scalar random process', 'nonlinear random dependences', 'mixed central moment functions', 'canonical decomposition apparatus', 'moment function', 'extrapolation', 'mean square error methods']), ('Estimating long-range dependence: finite sample properties and confidence\nintervals\nA major issue in financial economics is the behavior of asset returns over long\nhorizons. Various estimators of long-range dependence have been\nproposed. Even though some have known asymptotic properties, it is\nimportant to test their accuracy by using simulated series of different\nlengths. We test R/S analysis, detrended fluctuation analysis and\nperiodogram regression methods on samples drawn from Gaussian white\nnoise. The DFA statistics turns out to be the unanimous winner.\nUnfortunately, no asymptotic distribution theory has been derived for\nthis statistics so far. We were able, however, to construct empirical\n(i.e. approximate) confidence intervals for all three methods. The\nobtained values differ largely from heuristic values proposed by some\nauthors for the R/S statistics and are very close to asymptotic values\nfor the periodogram regression method\n', ['long-range dependence', 'finite sample properties', 'confidence intervals', 'financial economics', 'asset returns', 'long horizons', 'asymptotic properties', 'detrended fluctuation analysis', 'periodogram regression methods', 'Gaussian white noise', 'heuristic values', 'economics', 'finance', 'fluctuations', 'Gaussian noise', 'investment', 'white noise']), ('Matching PET and CT scans of the head and neck area: Development of method and\nvalidation\nPositron emission tomography (PET) provides important information on tumor\nbiology, but lacks detailed anatomical information. Our aim in the\npresent study was to develop and validate an automatic registration\nmethod for matching PET and CT scans of the head and neck. Three\ndifficulties in achieving this goal are (1) nonrigid motions of the\nneck can hamper the use of automatic ridged body transformations; (2)\nemission scans contain too little anatomical information to apply\nstandard image fusion methods; and (3) no objective way exists to\nquantify the quality of the match results. These problems are solved as\nfollows: accurate and reproducible positioning of the patient was\nachieved by using a radiotherapy treatment mask. The proposed method\nmakes use of the transmission rather than the emission scan. To obtain\nsufficient (anatomical) information for matching, two bed positions for\nthe transmission scan were included in the protocol. A mutual\ninformation-based algorithm was used as a registration technique. PET\nand CT data were obtained in seven patients. Each patient had two CT\nscans and one PET scan. The datasets were used to estimate the\nconsistency by matching PET to CT/sub 1/, CT/sub 1/ to CT/sub 2/, and\nCT/sub 2/ to PET using the full circle consistency test. It was found\nthat using our method, consistency could be obtained of 4 mm and 1.3\ndegrees on average. The PET voxels used for registration were 5.15 mm,\nso the errors compared quite favorably with the voxel size. Cropping\nthe images (removing the scanner bed from images) did not improve the\nconsistency of the algorithm. The transmission scan, however, could\npotentially be reduced to a single position using this approach. In\nconclusion, the represented algorithm and validation technique has\nseveral features that are attractive from both theoretical and\npractical point of view, it is a user-independent, automatic validation\ntechnique for matching CT and PET scans of the head and neck, which\ngives the opportunity to compare different image enhancements\n', ['positron emission tomography scans', 'tumor biology', 'anatomical information', 'automatic registration method', 'computerised tomography scans', 'head', 'neck', 'nonrigid motions', 'automatic ridged body transformations', 'standard image fusion methods', 'radiotherapy treatment mask', 'bed positions', 'transmission scan', 'mutual information-based algorithm', 'registration technique', 'patients', 'full circle consistency test', 'errors', 'scanner bed', 'user-independent automatic validation technique', 'image enhancements', 'computerised tomography', 'diagnostic radiography', 'dosimetry', 'image registration', 'medical image processing', 'positron emission tomography', 'radiation therapy']), ('E-government\nThe author provides an introduction to the main issues surrounding E-government\nmodernisation and electronic delivery of all public services by 2005.\nThe author makes it clear that E-government is about transformation,\nnot computers and hints at the special legal issues which may arise\n', ['E-government', 'modernisation', 'electronic delivery', 'public services', 'legal issues', 'government data processing', 'legislation']), ('Scale-invariant segmentation of dynamic contrast-enhanced perfusion MR images\nwith inherent scale selection\nSelection of the best set of scales is problematic when developing\nsignal-driven approaches for pixel-based image segmentation. Often,\ndifferent possibly conflicting criteria need to be fulfilled in order\nto obtain the best trade-off between uncertainty (variance) and\nlocation accuracy. The optimal set of scales depends on several\nfactors: the noise level present in the image material, the prior\ndistribution of the different types of segments, the class-conditional\ndistributions associated with each type of segment as well as the\nactual size of the (connected) segments. We analyse, theoretically and\nthrough experiments, the possibility of using the overall and\nclass-conditional error rates as criteria for selecting the optimal\nsampling of the linear and morphological scale spaces. It is shown that\nthe overall error rate is optimized by taking the prior class\ndistribution in the image material into account. However, a uniform\n(ignorant) prior distribution ensures constant class-conditional error\nrates. Consequently, we advocate for a uniform prior class distribution\nwhen an uncommitted, scale-invariant segmentation approach is desired.\nExperiments with a neural net classifier developed for segmentation of\ndynamic magnetic resonance (MR) images, acquired with a paramagnetic\ntracer, support the theoretical results. Furthermore, the experiments\nshow that the addition of spatial features to the classifier, extracted\nfrom the linear or morphological scale spaces, improves the\nsegmentation result compared to a signal-driven approach based solely\non the dynamic MR signal. The segmentation results obtained from the\ntwo types of features are compared using two novel quality measures\nthat characterize spatial properties of labelled images\n', ['scale-invariant segmentation', 'dynamic contrast-enhanced perfusion MR images', 'inherent scale selection', 'pixel-based image segmentation', 'noise level', 'class-conditional error rates', 'optimal sampling', 'experiments', 'neural net classifier', 'dynamic magnetic resonance images', 'paramagnetic tracer', 'quality measures', 'labelled images', 'class-conditional distributions', 'biomedical MRI', 'image classification', 'image sampling', 'image segmentation', 'medical image processing', 'neural nets']), ("Information-processing and computing systems at thermal power stations in China\nThe development and commissioning of information-processing and computing\nsystems (IPCSs) at four power units, each of 500 MW capacity at the\nthermal power stations Tszisyan' and Imin' in China, are considered.\nThe functional structure and the characteristics of the functions of\nthe IPCSs are presented as is information on the technology of\ndevelopment and experience in adjustments. Ways of using the experience\ngained in creating a comprehensive functional firmware system are shown\n", ['China', 'thermal power stations', 'information-processing systems', 'computing systems', 'commissioning', 'development', 'functional structure', 'functions characteristics', 'firmware system', '500 MW', 'commissioning', 'firmware', 'power station control', 'thermal power stations']), ('Effective moving cast shadow detection for monocular color traffic image\nsequences\nFor an accurate scene analysis using monocular color traffic image sequences, a\nrobust segmentation of moving vehicles from the stationary background\nis generally required. However, the presence of moving cast shadow may\nlead to an inaccurate vehicle segmentation, and as a result, may lead\nto further erroneous scene analysis. We propose an effective method for\nthe detection of moving cast shadow. By observing the characteristics\nof cast shadow in the luminance, chrominance, gradient density, and\ngeometry domains, a combined probability map, called a shadow\nconfidence score (SCS), is obtained. From the edge map of the input\nimage, each edge pixel is examined to determine whether it belongs to\nthe vehicle region based on its neighboring SCSs. The cast shadow is\nidentified as those regions with high SCSs, which are outside the\nconvex hull of the selected vehicle edge pixels. The proposed method is\ntested on 100 vehicle images taken under different lighting conditions\n(sunny and cloudy), viewing angles (roadside and overhead), vehicle\nsizes (small, medium, and large), and colors (similar to the road and\nnot). The results indicate that an average error rate of around 14% is\nobtained while the lowest error rate is around 3% for large vehicles\n', ['effective moving cast shadow detection', 'monocular color traffic image sequences', 'accurate scene analysis', 'robust segmentation', 'moving vehicles', 'stationary background', 'moving cast shadow', 'inaccurate vehicle segmentation', 'erroneous scene analysis', 'luminance', 'chrominance', 'gradient density', 'geometry domains', 'combined probability map', 'shadow confidence score', 'input image', 'cast shadow', 'convex hull', 'selected vehicle edge pixels', 'lighting conditions', 'vehicle images', 'sunny', 'cloudy', 'viewing angles', 'vehicle sizes', 'average error rate', 'image segmentation', 'brightness', 'image colour analysis', 'image recognition', 'image segmentation', 'image sensors', 'image sequences', 'road vehicles', 'transportation']), ("Cyberobscenity and the ambit of English criminal law\nThe author looks at a recent case and questions the Court of Appeal's approach.\nIn the author's submission, the Court of Appeal's decision in Perrin\nwas wrong. P published no material in England and Wales, and should not\nhave been convicted of any offence under English law, even if it were\nproved that he sought to attract English subscribers to his site. That\nmay be an unpalatable conclusion but, if the content of foreign-hosted\nInternet sites is to be controlled, the only sensible way forward is\nthrough international agreement and cooperation. The Council of\nEurope's Cybercrime Convention provides some indication of the limited\nareas over which widespread international agreement might be achieved\n", ['cyberobscenity', 'criminal law', 'Court of Appeal', 'Internet sites', 'Cybercrime Convention', 'Council of Europe', 'international agreement', 'England', 'computer crime', 'government policies', 'information resources', 'legislation']), ('Supply chain optimisation in the paper industry\nWe describe the formulation and development of a supply-chain optimisation\nmodel for Fletcher Challenge Paper Australasia (FCPA). This model,\nknown as the paper industry value optimisation tool (PIVOT), is a large\nmixed integer program that finds an optimal allocation of supplier to\nmill, product to paper machine, and paper machine to customer, while at\nthe same time modelling many of the supply chain details and nuances\nwhich are peculiar to FCPA. PIVOT has assisted FCPA in solving a number\nof strategic and tactical decision problems, and provided significant\neconomic benefits for the company\n', ['supply chain optimisation', 'Fletcher Challenge Paper Australasia', 'paper industry value optimisation tool', 'PIVOT', 'large mixed integer program', 'optimal allocation', 'strategic decision problems', 'tactical decision problems', 'economic benefits', 'goods distribution', 'integer programming', 'paper industry', 'production control']), ("The eyes have it [hotel security]\nCCTV systems can help lodging establishments accomplish a range of objectives,\nfrom deterring criminals to observing staff interactions with\nclientele. But pitfalls can arise if the CCTV system has not been\nproperly integrated into the overall hotel security plan. CCTV system\ndesigns at new hotel properties are often too sophisticated, too\ncomplicated, and too costly, and do not take into consideration the\nsecurity realities of site management. These problems arise when the\nprofessionals designing or installing the system, including architects,\nconstruction engineers, integrators, and consultants, are not familiar\nwith a hotel's operating strategies or security standards\n", ['hotel security', 'CCTV system', 'site management', 'operating strategies', 'closed circuit television', 'hotel industry', 'security', 'surveillance']), ('Nonlinear modeling and adaptive fuzzy control of MCFC stack\nTo improve availability and performance of fuel cells, the operating\ntemperature of the molten carbonate fuel cells (MCFC) stack should be\ncontrolled within a specified range. However, most existing models of\nMCFC are not ready to be applied in synthesis. In the paper, a radial\nbasis function neural networks identification model of a MCFC stack is\ndeveloped based on the input-output sampled data. An adaptive fuzzy\ncontrol procedure for the temperature of the MCFC stack is also\ndeveloped. The parameters of the fuzzy control system are regulated by\nback-propagation algorithm, and the rule database of the fuzzy system\nis also adaptively adjusted by the nearest-neighbor-clustering\nalgorithm. Finally using the neural networks model of MCFC stack, the\nsimulation results of the control algorithm are presented. The results\nshow the effectiveness of the proposed modeling and design procedures\nfor the MCFC stack based on neural networks identification and the\nnovel adaptive fuzzy control\n', ['nonlinear modeling', 'adaptive fuzzy control', 'MCFC stack', 'fuel cells', 'molten carbonate fuel cells stack', 'radial basis function neural networks identification model', 'input-output sampled data', 'backpropagation algorithm', 'rule database', 'nearest-neighbor-clustering algorithm', 'adaptive control', 'backpropagation', 'fuzzy control', 'identification', 'molten carbonate fuel cells', 'radial basis function networks', 'temperature control']), ('A knowledge intensive multi-agent framework for cooperative/collaborative\ndesign modeling and decision support of assemblies\nMulti-agent modeling has emerged as a promising discipline for dealing with the\ndecision making process in distributed information system applications.\nOne of such applications is the modeling of distributed design or\nmanufacturing processes which can link up various designs or\nmanufacturing processes to form a virtual consortium on a global basis.\nThis paper proposes a novel knowledge intensive multi-agent\ncooperative/collaborative framework for concurrent intelligent design\nand assembly planning, which integrates product design, design for\nassembly, assembly planning, assembly system design, and assembly\nsimulation subjected to econo-technical evaluations. An AI protocol\nbased method is proposed to facilitate the integration of intelligent\nagents for assembly design, planning, evaluation and simulation\nprocesses. A unified class of knowledge intensive Petri nets is defined\nusing the OO knowledge-based Petri net approach and used as an AI\nprotocol for handling both the integration and the negotiation problems\namong multi-agents. The detailed cooperative/collaborative mechanism\nand algorithms are given based on the knowledge object cooperation\nformalisms. As such, the assembly-oriented design system can easily be\nimplemented under the multi-agent-based knowledge-intensive Petri net\nframework with concurrent integration of multiple cooperative knowledge\nsources and software. Thus, product design and assembly planning can be\ncarried out simultaneously and intelligently in an entirely\ncomputer-aided concurrent design and assembly planning system\n', ['knowledge intensive multi-agent framework', 'collaborative design modeling', 'decision support', 'distributed information system applications', 'distributed design', 'virtual consortium', 'cooperative framework', 'concurrent intelligent design', 'assembly planning', 'product design', 'design for assembly', 'assembly simulation', 'AI protocol', 'knowledge intensive Petri nets', 'agent negotiation', 'knowledge object cooperation', 'assembly planning', 'CAD/CAM', 'concurrent engineering', 'decision support systems', 'intelligent design assistants', 'multi-agent systems', 'Petri nets']), ('The web services agenda\nEven the most battle-scarred of CIOs have become excited at the prospect of\nwhat web services can do for their businesses. But there are still some\nshortcomings to be addressed\n', ['web services', 'transaction support', 'security', 'DP management', 'intranets', 'management information systems']), ("Integrating virtual and physical context to support knowledge workers\nThe Kimura system augments and integrates independent tools into a pervasive\ncomputing system that monitors a user's interactions with the computer,\nan electronic whiteboard, and a variety of networked peripheral devices\nand data sources\n", ['pervasive computing', 'knowledge workers', 'networked peripheral devices', 'electronic whiteboard', 'Kimura system', 'data sources', 'distributed processing', 'groupware', 'management information systems', 'user interfaces']), ('View from the top [workflow & content management]\nInternational law firm Linklaters has installed a global document and content\nmanagement system that is accessible to clients and which has helped it\nmove online\n', ['international law firm', 'Linklaters', 'document management', 'content management', 'online', 'document handling', 'information resources', 'law administration']), ('Robotically enhanced placement of left ventricular epicardial electrodes during\nimplantation of a biventricular implantable cardioverter defibrillator\nsystem\nBiventricular pacing has gained increasing acceptance in advanced heart failure\npatients. One major limitation of this therapy is positioning the left\nventricular stimulation lead via the coronary sinus. This report\ndemonstrates the feasibility of totally endoscopic direct placement of\nan epicardial stimulation lead on the left ventricle using the daVinci\nsurgical system\n', ['epicardial leads', 'left ventricular pacing', 'left ventricular epicardial electrodes', 'biventricular implantable cardioverter defibrillator system implantation', 'coronary sinus', 'daVinci surgical system', 'totally endoscopic direct placement', 'advanced heart failure patients', 'left ventricular stimulation lead positioning', 'biomedical electrodes', 'defibrillators', 'medical robotics']), ('Strategic implementation of IT/IS projects in construction: a case study\nThe need for improved implementation of Information Technology (IT) and\nInformation Systems (IS) has been emphasised in both empirical and\nprescriptive research studies. This problem is magnified in the\nconstruction industry, which has been slow to embrace and utilise new\ntechnologies with negative consequences on productivity and innovation.\nThis paper presents a strategic implementation framework for IT/IS\nprojects in construction. The framework builds upon recent published\nworks and encompasses well-documented predictors for effective IT/IS\nimplementation. A case study with a large multi-national construction\norganisation is used to demonstrate the strategic implementation of a\nProject Management Information System (PMIS) used for the construction\nof a mobile phone telecommunications network in the South East of\nQueensland, Australia\n', ['strategic implementation', 'construction industry', 'strategic implementation framework', 'predictors', 'large multi-national construction organisation', 'SWOT analysis', 'information technology', 'information systems', 'Project Management Information System', 'mobile phone telecommunications network', 'analytical hierarchy process', 'civil engineering computing', 'management information systems', 'project management', 'strategic planning']), ('Module placement with boundary constraints using B*-trees\nThe module placement problem is to determine the co-ordinates of logic modules\nin a chip such that no two modules overlap and some cost (e.g. silicon\narea, interconnection length, etc.) is optimised. To shorten\nconnections between inputs and outputs and/or make related modules\nadjacent, it is desired to place some modules along the specific\nboundaries of a chip. To deal with such boundary constraints, we\nexplore the feasibility conditions of a B*-tree with boundary\nconstraints and develop a simulated annealing-based algorithm using\nB*-trees. Unlike most previous work, the proposed algorithm guarantees\na feasible B*-tree with boundary constraints for each perturbation.\nExperimental results show that the algorithm can obtain a smaller\nsilicon area than the most recent work based on sequence pairs\n', ['logic module placement', 'boundary constraints', 'B*-tree', 'simulated annealing algorithm', 'silicon area', 'interconnection length', 'logic CAD', 'modules', 'simulated annealing', 'trees (mathematics)']), ('Temelin casts its shadow [nuclear power plant]\nReservations about Temelin nuclear plant in the Czech Republic are political\nrather than technical. This paper discusses the problems of\nturbogenerator vibrations and how they were diagnosed. The paper also\ndiscusses some of the other problems of commissioning the power plant.\nThe simulator used for training new staff is also mentioned\n', ['Temelin nuclear plant', 'Czech Republic', 'turbogenerator vibrations', 'power plant commissioning', 'training simulator', 'computer based training', 'digital simulation', 'machine testing', 'nuclear engineering computing', 'nuclear power stations', 'turbogenerators', 'vibration measurement']), ('Why information departments are becoming academic\nThis article outlines the increasing convergence between academia and business\nover the last decade or so, and the mutual benefits that this closer\nassociation has brought. It also looks at the growing importance of the\ninformation profession, suggesting that this is leading to a greater\nneed for specialist skills, as reflected by the rise in academic\ncourses in this area. However, it argues that increasing specialization\nmust not lead to insularity; if information professionals are truly\nconcerned with gaining a competitive advantage, they must not close\ntheir minds to the potential benefits of working with external, non\nspecialist, partners. The benefits that business has reaped from\nacademia, it is contended, suggest that this may also be a fruitful\navenue for information departments to explore\n', ['information science', 'universities', 'academia', 'business', 'information profession', 'specialist skills', 'academic courses', 'information departments', 'commerce', 'educational administrative data processing', 'educational courses', 'information science', 'professional aspects']), ('Vehicle travel time models for AGV systems under various dispatching rules\nThe design and evaluation of AGV-based material handling systems are highly\ncomplex because of the randomness and the large number of variables\ninvolved. Vehicle travel time is a fundamental parameter for solving\nvarious flexible manufacturing system (FMS) design problems. This\narticle presents stochastic vehicle travel time models for AGV-based\nmaterial handling systems with emphasis on the empty travel times of\nvehicles. Various vehicle dispatching rules examined here include the\nnearest vehicle selection rule and longest idle vehicle selection rule.\nA simulation experiment is used to evaluate and demonstrate the\npresented models\n', ['automatic guided vehicle', 'AGV', 'material handling systems', 'vehicle travel time', 'flexible manufacturing system', 'FMS', 'vehicle dispatching rules', 'nearest vehicle selection rule', 'longest idle vehicle selection rule', 'automatic guided vehicles', 'flexible manufacturing systems', 'materials handling', 'optimisation', 'scheduling', 'transportation']), ("A look at MonacoProfiler 4\nThe newest profiling program from Monaco Software adds some valuable features:\nsupport for up to 8-color printing, profiling for digital cameras,\nfine-tuning of black generation and tweaking of profile transforms. We\ntested its ease of use and a few of the advanced functions. In all,\nit's pretty good\n", ['MonacoProfiler 4', 'color-correction', 'Pantone Hexachrome', 'commercial printers', 'colour graphics', 'desktop publishing', 'software reviews']), ("Quadratic Newton iteration for systems with multiplicity\nNewton's iterator is one of the most popular components of polynomial equation\nsystem solvers, either from the numeric or symbolic point of view. This\niterator usually handles smooth situations only (when the Jacobian\nmatrix associated to the system is invertible). This is often a\nrestrictive factor. Generalizing Newton's iterator is still an open\nproblem: How to design an efficient iterator with a quadratic\nconvergence even in degenerate cases? We propose an answer for an\nm-adic topology when the ideal m can be chosen generic enough: compared\nto a smooth case we prove quadratic convergence with a small overhead\nthat grows with the square of the multiplicity of the root\n", ['quadratic Newton iteration', 'systems with multiplicity', "Newton's iterator", 'polynomial equation system solvers', 'Jacobian matrix', 'quadratic convergence', 'm-adic topology', 'computational complexity', 'Jacobian matrices', 'polynomials', 'process algebra', 'series (mathematics)']), ('Benchmarking of the Dose Planning Method (DPM) Monte Carlo code using electron\nbeams from a racetrack microtron\nA comprehensive set of measurements and calculations has been conducted to\ninvestigate the accuracy of the Dose Planning Method (DPM) Monte Carlo\ncode for dose calculations from 10 and 50 MeV scanned electron beams\nproduced from a racetrack microtron. Central axis depth dose\nmeasurements and a series of profile scans at various depths were\nacquired in a water phantom using a Scanditronix type RK ion chamber.\nSource spatial distributions for the Monte Carlo calculations were\nreconstructed from in-air ion chamber measurements carried out across\nthe two-dimensional beam profile at 100 cm downstream from the source.\nThe in-air spatial distributions were found to have full width at half\nmaximum of 4.7 and 1.3 cm, at 100 cm from the source, for the 10 and 50\nMeV beams, respectively. Energy spectra for the 10 and 50 MeV beams\nwere determined by simulating the components of the microtron treatment\nhead using the code MCNP4B. DPM calculations are on average within\n+or-2% agreement with measurement for all depth dose and profile\ncomparisons conducted in this study. The accuracy of the DPM code\nillustrated in this work suggests that DPM may be used as a valuable\ntool for electron beam dose calculations\n', ['dose planning method Monte Carlo code', 'scanned electron beams', 'racetrack microtron', 'benchmarking', 'central axis depth dose measurements', 'profile scans', 'water phantom', 'ion chamber', 'source spatial distributions', 'two-dimensional beam profile', 'in-air spatial distributions', 'MCNP4B', 'electron beam dose calculations', 'radiotherapy treatment planning', 'electron transport', 'scoring parameters', '10 MeV', '50 MeV', 'dosimetry', 'electron beam applications', 'electron transport theory', 'medical computing', 'Monte Carlo methods', 'radiation therapy']), ('Access privilege management in protection systems\nWe consider the problem of managing access privileges on protected objects. We\nassociate one or more locks with each object, one lock for each access\nright defined by the object type. Possession of an access right on a\ngiven object is certified by possession of a key for this object, if\nthis key matches one of the object locks. We introduce a number of\nvariants to this basic key-lock technique. Polymorphic access rights\nmake it possible to decrease the number of keys required to certify\npossession of complex access privileges that are defined in terms of\nseveral access rights. Multiple locks on the same access right allow us\nto exercise forms of selective revocation of access privileges. A lock\nconversion function can be used to reduce the number of locks\nassociated with any given object to a single lock. The extent of the\nresults obtained is evaluated in relation to alternative methodologies\nfor access privilege management\n', ['access privilege management', 'protection systems', 'protected objects', 'locks', 'key-lock technique', 'polymorphic access rights', 'complex access privilege possession certification', 'selective revocation', 'lock conversion function', 'authorisation']), ('One-step digit-set-restricted modified signed-digit adder using an incoherent\ncorrelator based on a shared content-addressable memory\nAn efficient one-step digit-set-restricted modified signed-digit (MSD) adder\nbased on symbolic substitution is presented. In this technique, carry\npropagation is avoided by introducing reference digits to restrict the\nintermediate carry and sum digits to {1,0} and {0,1}, respectively. The\nproposed technique requires significantly fewer minterms and simplifies\nsystem complexity compared to the reported one-step MSD addition\ntechniques. An incoherent correlator based on an optoelectronic shared\ncontent-addressable memory processor is suggested to perform the\naddition operation. In this technique, only one set of minterms needs\nto be stored, independent of the operand length\n', ['one-step digit-set-restricted modified signed-digit adder', 'incoherent correlator', 'shared content-addressable memory', 'symbolic substitution', 'reference digits', 'intermediate carry', 'sum digits', 'minterms', 'system complexity', 'optoelectronic shared content-addressable memory processor', 'addition operation', 'operand length', 'adders', 'content-addressable storage', 'light coherence', 'optical storage', 'symbolic substitution']), ("Modelling user acceptance of building management systems\nThis study examines user acceptance of building management systems (BMS) using\na questionnaire survey. These systems are crucial for optimising\nbuilding performance and yet it has been widely reported that users are\nnot making full use of their systems' facilities. Established models of\ntechnology acceptance have been employed in this research, and the\npositive influence of user perceptions of ease of use and compatibility\nhas been demonstrated. Previous research has indicated differing levels\nof importance of perceived ease of use relative to other factors. Here,\nperceived ease of use is shown generally to be more important, though\nthe balance between this and compatibility is moderated by the user\nperceptions of voluntariness\n", ['user acceptance modelling', 'building management systems', 'technology acceptance model', 'innovation characteristics', 'information systems', 'questionnaire survey', 'user perceptions', 'ease of use', 'compatibility', 'voluntariness', 'building management systems', 'information systems', 'user modelling']), ('Design and implementation of a brain-computer interface with high transfer\nrates\nThis paper presents a brain-computer interface (BCI) that can help users to\ninput phone numbers. The system is based on the steady-state visual\nevoked potential (SSVEP). Twelve buttons illuminated at different rates\nwere displayed on a computer monitor. The buttons constituted a virtual\ntelephone keypad, representing the ten digits 0-9, BACKSPACE, and\nENTER. Users could input phone number by gazing at these buttons. The\nfrequency-coded SSVEP was used to judge which button the user desired.\nEight of the thirteen subjects succeeded in ringing the mobile phone\nusing the system. The average transfer rate over all subjects was 27.15\nbits/min. The attractive features of the system are noninvasive signal\nrecording, little training required for use, and high information\ntransfer rate. Approaches to improve the performance of the system are\ndiscussed\n', ['brain-computer interface with high transfer rates', 'phone numbers input', 'steady-state visual evoked potential', 'illuminated buttons', 'system performance improvement', 'virtual telephone keypad', 'frequency-coded SSVEP', 'mobile phone ringing', 'computer monitor', 'electroencephalography', 'handicapped aids', 'medical computing', 'mobile radio', 'visual evoked potentials']), ("Internet-based psychological experimenting: five dos and five don'ts\nInternet-based psychological experimenting is presented as a method that needs\ncareful consideration of a number of issues-from potential data\ncorruption to revealing confidential information about participants.\nTen issues are grouped into five areas of actions to be taken when\ndeveloping an Internet experiment (dos) and five errors to be avoided\n(don'ts). Dos include: (a) utilizing dropout as a dependent variable,\n(b) the use of dropout to detect motivational confounding, (c)\nplacement of questions for personal information, (d) using a collection\nof techniques, and (e) using Internet-based tools. Don'ts are about:\n(a) unprotected directories, (b) public access to confidential data,\n(c) revealing the experiment's structure, (d) ignoring the Internet's\ntechnical variance, and (e) improper use of form elements\n", ['Internet-based psychological experimenting', 'data corruption', 'data confidentiality', 'dropout', 'motivational confounding', 'personal information', 'unprotected directories', 'Web experiment', 'online research techniques', 'psychology', 'data privacy', 'information resources', 'Internet', 'psychology', 'security of data']), ("A study of hospitality and tourism information technology education and\nindustrial applications\nThe purpose of this study was to examine the subject relevance of information\ntechnology (IT) in hospitality and tourism management programs with\nskills deployed in the workplace. This study aimed at investigating\ngraduates' transition from education to employment, and to determine\nhow well they appear to be equipped to meet the needs of the\nhospitality and tourism industry. One hundred and seventeen graduates\nresponded to a mail survey. These graduates rated the importance of IT\nskills in the workplace, the level of IT teaching in hotel and tourism\nmanagement programs, and the self-competence level in IT. This study\nconcluded that a gap exists between the IT skills required at work and\nthose acquired at university\n", ['hospitality and tourism management programs', 'education', 'employment', 'hospitality industry', 'tourism industry', 'mail survey', 'graduates', 'IT skills', 'university', 'IT teaching', 'computer literacy', 'employment', 'hotel industry', 'management education', 'travel industry']), ("The numerical solution of an evolution problem of second order in time on a\nclosed smooth boundary\nWe consider an initial value problem for the second-order differential equation\nwith a Dirichlet-to-Neumann operator coefficient. For the numerical\nsolution we carry out semi-discretization by the Laguerre\ntransformation with respect to the time variable. Then an infinite\nsystem of the stationary operator equations is obtained. By potential\ntheory, the operator equations are reduced to boundary integral\nequations of the second kind with logarithmic or hypersingular kernels.\nThe full discretization is realized by Nystrom's method which is based\non the trigonometric quadrature rules. Numerical tests confirm the\nability of the method to solve these types of nonstationary problems\n", ['initial value problem', 'second-order differential equation', 'evolution problem', 'closed smooth boundary', 'Laguerre transformation', 'hypersingular kernels', 'boundary integral equations', 'stationary operator equations', 'boundary integral equations', 'differential equations', 'initial value problems']), ('Nonlinear control of a shape memory alloy actuated manipulator\nThis paper presents a nonlinear, robust control algorithm for accurate\npositioning of a single degree of freedom rotary manipulator actuated\nby Shape Memory Alloy (SMA). A model for an SMA actuated manipulator is\npresented. The model includes nonlinear dynamics of the manipulator, a\nconstitutive model of Shape Memory Alloy, and electrical and heat\ntransfer behavior of SMA wire. This model is used for open and closed\nloop motion simulations of the manipulator. Experiments are presented\nthat show results similar to both closed and open loop simulation\nresults. Due to modeling uncertainty and nonlinear behavior of the\nsystem, classic control methods such as\nProportional-Integral-Derivative control are not able to present fast\nand accurate performance. Hence a nonlinear, robust control algorithm\nis presented based on Variable Structure Control. This algorithm is a\ncontrol gain switching technique based on the weighted average of\nposition and velocity feedbacks. This method has been designed through\nsimulation and tested experimentally. Results show fast, accurate, and\nrobust performance of the control system. Computer simulation and\nexperimental results for different stabilization and tracking\nsituations are also presented\n', ['shape memory alloy', 'manipulator', 'open loop', 'closed loop', 'variable structure control', 'control gain switching', 'nonlinear control', 'feedback', 'stabilization', 'tracking', 'positioning', 'nonlinear dynamics', 'feedback', 'manipulators', 'nonlinear control systems', 'nonlinear dynamical systems', 'shape memory effects']), ('Online masquerade: whose e-mail is it?\nE-mails carrying viruses like the recent Klez worm use deceptively simple\ntechniques and known vulnerabilities to spread from one computer to\nanother with ease\n', ['e-mail', 'Klez worm', 'viruses', 'vulnerabilities', 'computer viruses', 'electronic mail', 'security of data']), ('Using technology to facilitate the design and delivery of warnings\nThis paper describes several ways in which new technologies can assist in the\ndesign and delivery of warnings. There are four discussion points: (1)\ncurrent product information can be delivered via the Internet; (2)\ncomputer software and hardware are available to assist in the design,\nconstruction, and production of visual and auditory warnings; (3)\nvarious detection devices can be used to recognize instances in which\nwarnings might be delivered; and (4) a warning presentation can be\nmodified to fit conditions and persons. Implications, example\napplications and future prospects of these points are described\n', ['product information', 'Internet', 'computer software', 'computer hardware', 'auditory warnings', 'warning presentation', 'graphical user interfaces', 'system documentation', 'user manuals']), ('Limits for computational electromagnetics codes imposed by computer\narchitecture\nThe algorithmic complexity of the innermost loops that determine the complexity\nof algorithms in computational electromagnetics (CEM) codes are\nanalyzed according to their operation count and the impact of the\nunderlying computer hardware. As memory chips are much slower than\narithmetic processors, codes that involve a high data movement compared\nto the number of arithmetic operations are executed comparatively\nslower. Hence, matrix-matrix multiplications are much faster than\nmatrix-vector multiplications. It is seen that it is not sufficient to\ncompare only the complexity, but also the actual performance of\nalgorithms to judge on faster execution. Implications involve FDTD\nloops, LU factorizations, and iterative solvers for dense matrices. Run\ntimes on two reference platforms, namely an Athlon 900 MHz and an HP PA\n8600 processor, verify the findings\n', ['computational electromagnetics codes', 'computer architecture', 'algorithmic complexity', 'innermost loops', 'CEM codes', 'operation count', 'computer hardware', 'memory chips', 'data movement', 'matrix-matrix multiplications', 'matrix-vector multiplications', 'FDTD loops', 'LU factorizations', 'iterative solvers', 'dense matrices', 'computational complexity', 'computer architecture', 'electrical engineering computing', 'electromagnetism', 'finite difference time-domain analysis', 'iterative methods', 'matrix decomposition', 'matrix multiplication']), ('Information architecture for bilingual Web sites\nCreating an information architecture for a bilingual Web site presents\nparticular challenges beyond those that exist for single and\nmultilanguage sites. This article reports work in progress on the\ndevelopment of a content-based bilingual Web site to facilitate the\nsharing of resources and information between Speech and Language\nTherapists. The development of the information architecture is based on\na combination of two aspects: an abstract structural analysis of\nexisting bilingual Web designs focusing on the presentation of\nbilingual material, and a bilingual card-sorting activity conducted\nwith potential users. Issues for bilingual developments are discussed,\nand some observations are made regarding the use of card-sorting\nactivities\n', ['information architecture', 'content-based bilingual Web site', 'speech therapists', 'language therapists', 'bilingual card-sorting activity', 'bilingual developments', 'World Wide Web', 'electronic publishing', 'information resources', 'natural languages']), ('Analogue realizations of fractional-order controllers\nAn approach to the design of analogue circuits, implementing fractional-order\ncontrollers, is presented. The suggested approach is based on the use\nof continued fraction expansions; in the case of negative coefficients\nin a continued fraction expansion, the use of negative impedance\nconverters is proposed. Several possible methods for obtaining suitable\nrational approximations and continued fraction expansions are\ndiscussed. An example of realization of a fractional-order I/sup lambda\n/ controller is presented and illustrated by obtained measurements. The\nsuggested approach can be used for the control of very fast processes,\nwhere the use of digital controllers is difficult or impossible\n', ['analogue realizations', 'fractional-order controllers', 'continued fraction expansions', 'negative coefficients', 'fraction expansion', 'negative impedance converters', 'rational approximations', 'fast processes', 'digital controllers', 'fractional differentiation', 'fractional integration', 'differentiation', 'digital control', 'integration', 'nonlinear control systems', 'nonlinear dynamical systems', 'realisation theory']), ('Scalable secure group communication over IP multicast\nWe introduce and analyze a scalable rekeying scheme for implementing secure\ngroup communications Internet protocol multicast. We show that our\nscheme incurs constant processing, message, and storage overhead for a\nrekey operation when a single member joins or leaves the group, and\nlogarithmic overhead for bulk simultaneous changes to the group\nmembership. These bounds hold even when group dynamics are not known a\npriori. Our rekeying algorithm requires a particular clustering of the\nmembers of the secure multicast group. We describe a protocol to\nachieve such clustering and show that it is feasible to efficiently\ncluster members over realistic Internet-like topologies. We evaluate\nthe overhead of our own rekeying scheme and also of previously\npublished schemes via simulation over an Internet topology map\ncontaining over 280 000 routers. Through analysis and detailed\nsimulations, we show that this rekeying scheme performs better than\nprevious schemes for a single change to group membership. Further, for\nbulk group changes, our algorithm outperforms all previously known\nschemes by several orders of magnitude in terms of actual bandwidth\nusage, processing costs, and storage requirements\n', ['scalable secure group communication', 'IP multicast', 'Internet protocol multicast', 'storage overhead', 'logarithmic overhead', 'cryptography', 'access control server', 'authentication', 'group membership', 'group dynamics', 'rekeying algorithm', 'secure multicast group', 'Internet-like topologies', 'overhead', 'simulation', 'Internet topology map', 'network routers', 'bandwidth usage', 'processing costs', 'storage requirements', 'cryptography', 'Internet', 'message authentication', 'multicast communication', 'network topology', 'telecommunication network routing', 'telecommunication security', 'transport protocols']), ('A framework for image deblurring using wavelet packet bases\nWe show that the average over translations of an operator diagonal in a wavelet\npacket basis is a convolution. We also show that an operator diagonal\nin a wavelet packet basis can be decomposed into several operators of\nthe same kind, each of them being better conditioned. We investigate\nthe possibility of using such a convolution to approximate a given\nconvolution (in practice an image blur). Then we use these\napproximations to deblur images. First, we show that this framework\npermits us to redefine existing deblurring methods. Then, we show that\nit permits us to define a new variational method which combines the\nwavelet packet and the total variation approaches. We argue and show by\nexperiments that this permits us to avoid the drawbacks of both\napproaches which are, respectively, ringing and staircasing\n', ['image deblurring', 'wavelet packet bases', 'convolution', 'operator diagonal', 'total variation approach', 'ringing', 'staircasing', 'deconvolution', 'convolution', 'deconvolution', 'image restoration', 'wavelet transforms']), ('An automated parallel image registration technique based on the correlation of\nwavelet features\nWith the increasing importance of multiple multiplatform remote sensing\nmissions, fast and automatic integration of digital data from disparate\nsources has become critical to the success of these endeavors. Our work\nutilizes maxima of wavelet coefficients to form the basic features of a\ncorrelation-based automatic registration algorithm. Our wavelet-based\nregistration algorithm is tested successfully with data from the\nNational Oceanic and Atmospheric Administration (NOAA) Advanced Very\nHigh Resolution Radiometer (AVHRR) and the Landsat Thematic Mapper\n(TM), which differ by translation and/or rotation. By the choice of\nhigh-frequency wavelet features, this method is similar to an\nedge-based correlation method, but by exploiting the multiresolution\nnature of a wavelet decomposition, our method achieves higher\ncomputational speeds for comparable accuracies. This algorithm has been\nimplemented on a single-instruction multiple-data (SIMD) massively\nparallel computer, the MasPar MP-2, as well as on the CrayT3D, the Cray\nT3E, and a Beowulf cluster of Pentium workstations\n', ['geophysical measurement technique', 'land surface', 'terrain mapping', 'optical imaging', 'microwave radiometry', 'image processing', 'automated parallel image registration', 'correlation', 'wavelet feature', 'remote sensing', 'automatic registration algorithm', 'AVHRR', 'Landsat Thematic Mapper', 'wavelet decomposition', 'SIMD massively parallel computing', 'geophysical signal processing', 'geophysical techniques', 'geophysics computing', 'image registration', 'remote sensing', 'terrain mapping', 'wavelet transforms']), ("The two populations' cellular automata model with predation based on the Penna\nmodel\nIn Penna's (1995) single-species asexual bit-string model of biological ageing,\nthe Verhulst factor has too strong a restraining effect on the\ndevelopment of the population. Danuta Makowiec gave an improved model\nbased on the lattice, where the restraining factor of the four\nneighbours take the place of the Verhulst factor. Here, we discuss the\ntwo populations' Penna model with predation on the planar lattice of\ntwo dimensions. A cellular automata model containing movable wolves and\nsheep has been built. The results show that both the quantity of the\nwolves and the sheep fluctuate in accordance with the law that one\nquantity increases while the other one decreases\n", ['cellular automata model', 'population', 'Penna model', 'single-species asexual bit-string model', 'biological ageing', 'Verhulst factor', 'restraining effect', 'lattice', 'wolves', 'sheep', 'fluctuation', 'Lotka-Volterra model', 'predation', 'cellular automata', 'fluctuations', 'lattice theory', 'probability', 'zoology']), ('Design patterns for high availability\nIt is possible to achieve five-nines reliability with everyday\ncommercial-quality hardware and software. The key is the way in which\nthese components are combined. The design of high availability systems\nis based on a combination of redundant hardware components and software\nto manage fault detection and correction without human intervention.\nThe author quickly reviews some definitions tied to high availability\nand fault management, and then goes on to discuss some hardware and\nsoftware design patterns for fault tolerant systems\n', ['hardware reliability', 'software reliability', 'high availability systems', 'redundant hardware components', 'fault detection', 'fault correction', 'software design patterns', 'checkpointing', 'software redundancy', 'fault tolerant systems', 'fault diagnosis', 'fault tolerant computing', 'redundancy', 'software reliability', 'system recovery']), ('Social percolation and the influence of mass media\nIn the marketing model of Solomon and Weisbuch, people buy a product only if\ntheir neighbours tell them of its quality, and if this quality is\nhigher than their own quality expectations. Now we introduce additional\ninformation from the mass media, which is analogous to the ghost field\nin percolation theory. The mass media shift the percolative phase\ntransition observed in the model, and decrease the time after which the\nstationary state is reached\n', ['social percolation', 'mass media influence', 'Solomon-Weisbuch marketing model', 'quality expectations', 'ghost field', 'percolative phase transition', 'stationary state', 'customers', 'cinema', 'external field', 'cinematography', 'marketing', 'percolation', 'phase transformations', 'socio-economic effects']), ('The diameter of a long-range percolation graph\nWe consider the following long-range percolation model: an undirected graph\nwith the node set {0, 1, . . . , N}/sup d/, has edges (x, y) selected\nwith probability approximately= beta /||x - y||/sup s/ if ||x - y||\n> 1, and with probability 1 if ||x - y|| = 1, for some parameters\nbeta , s > 0. This model was introduced by who obtained bounds on\nthe diameter of this graph for the one-dimensional case d = 1 and for\nvarious values of s, but left cases s = 1, 2 open. We show that, with\nhigh probability, the diameter of this graph is Theta (log N/log log N)\nwhen s = d, and, for some constants 0 < eta /sub 1/ < eta /sub 2/\n< 1, it is at most N/sup eta 2/ when s = 2d, and is at least N/sup\neta 1/ when d = 1, s = 2, beta < 1 or when s > 2d. We also\nprovide a simple proof that the diameter is at most log/sup O(1)/ N\nwith high probability, when d < s < 2d, established previously in\nBenjamini and Berger (2001)\n', ['long-range percolation model', 'undirected graph', 'probability', 'percolation', 'positive probability', 'networks', 'random graph', 'graph theory', 'percolation']), ('Generalized mosaicing: wide field of view multispectral imaging\nWe present an approach to significantly enhance the spectral resolution of\nimaging systems by generalizing image mosaicing. A filter transmitting\nspatially varying spectral bands is rigidly attached to a camera. As\nthe system moves, it senses each scene point multiple times, each time\nin a different spectral band. This is an additional dimension of the\ngeneralized mosaic paradigm, which has demonstrated yielding high\nradiometric dynamic range images in a wide field of view, using a\nspatially varying density filter. The resulting mosaic represents the\nspectrum at each scene point. The image acquisition is as easy as in\ntraditional image mosaics. We derive an efficient scene sampling rate,\nand use a registration method that accommodates the spatially varying\nproperties of the filter. Using the data acquired by this method, we\ndemonstrate scene rendering under different simulated illumination\nspectra. We are also able to infer information about the scene\nillumination. The approach was tested using a standard 8-bit\nblack/white video camera and a fixed spatially varying spectral\n(interference) filter\n', ['generalized mosaicing', 'wide field of view multispectral imaging', 'spatially varying spectral bands', 'spatially varying density filter', 'image acquisition', 'scene sampling rate', 'registration method', 'scene rendering', 'simulated illumination spectra', 'scene illumination', 'hyperspectral imaging', 'color balance', 'image fusion', 'physics-based vision', 'image-based rendering', 'computer vision', 'image enhancement', 'image registration', 'image sampling', 'image segmentation', 'lighting', 'rendering (computer graphics)']), ('Naomi Campbell: drugs, distress and the Data Protection Act\nIn the first case of its kind, Naomi Campbell successfully sued Mirror Group\nNewspapers for damage and distress caused by breach of the Data\nProtection Act 1998. Partner N. Wildish and assistant M. Turle of City\nlaw firm Field Fisher Waterhouse discuss the case and the legal\nimplications of which online publishers should be aware\n', ['drugs', 'distress', 'Data Protection Act', 'Naomi Campbell', 'online publishers', 'legislation']), ('Control of transient thermal response during sequential open-die forging: a\ntrajectory optimization approach\nA trajectory optimization approach is applied to the design of a sequence of\nopen-die forging operations in order to control the transient thermal\nresponse of a large titanium alloy billet. The amount of time the\nbillet is soaked in furnace prior to each successive forging operation\nis optimized to minimize the total process time while simultaneously\nsatisfying constraints on the maximum and minimum values of the billet\ntemperature distribution to avoid microstructural defects during\nforging. The results indicate that a "differential" heating profile is\nthe most effective at meeting these design goals\n', ['transient thermal response control', 'trajectory optimization', 'open-die forging', 'titanium alloy billet', 'temperature distribution', 'microstructural defects', 'heating profile', 'forging', 'metallurgical industries', 'optimisation', 'process control', 'temperature distribution']), ('Development of a 3.5 inch magneto-optical disk with a capacity of 2.3 GB\nThe recording capacity of GIGAMO media was enlarged from 1.3 GB to 2.3 GB for\n3.5 inch magneto-optical (MO) disks while maintaining downward\ncompatibility. For the new GIGAMO technology, a land and groove\nrecording method was applied in addition to magnetically induced super\nresolution (MSR) media. Furthermore, a novel address format suitable\nfor the land and groove recording method was adopted. The\nspecifications of the new GIGAMO media were examined to satisfy\nrequirements for practical use with respect to margins. Durability of\nmore than 10/sup 6/ rewritings and an enough lifetime were confirmed\n', ['magneto-optical disk', 'recording capacity', 'GIGAMO media', 'MO disks', 'land-groove recording method', 'magnetically induced super resolution', 'MSR', 'address format', 'rewriting durability', 'lifetime', 'crosstalk', '3.5 inch', '2.3 GB', 'SiN-GdFeCo-GdFe-TbFeCo-SiN-Al', 'magneto-optical recording', 'optical crosstalk', 'optical disc storage']), ('Prospecting virtual collections\nVirtual collections are a distinct sub-species of digital collections and\ndigital archives. Archivists and curators as archivists and curators do\nnot construct virtual collections; rather they enable virtual\ncollections through the application of descriptive and other standards.\nVirtual collections are constructed by end users\n', ['virtual collections', 'digital collections', 'digital archives', 'archivists', 'curators', 'descriptive standards', 'end users', 'digitization', 'digital libraries', 'information retrieval systems', 'records management', 'standards']), ('A study on an automatic seam tracking system by using an electromagnetic sensor\nfor sheet metal arc welding of butt joints\nMany sensors, such as the vision sensor and the laser displacement sensor, have\nbeen developed to automate the arc welding process. However, these\nsensors have some problems due to the effects of arc light, fumes and\nspatter. An electromagnetic sensor, which utilizes the generation of an\neddy current, was developed for detecting the weld line of a butt joint\nin which the root gap size was zero. An automatic seam tracking system\ndesigned for sheet metal arc welding was constructed with a sensor.\nThrough experiments, it was revealed that the system had an excellent\nseam tracking accuracy of the order of +or-0.2 mm\n', ['sheet metal arc welding', 'butt joints', 'automatic seam tracking system', 'electromagnetic sensor', 'eddy current generation', 'weld line detection', 'root gap size', 'seam tracking accuracy', 'arc welding', 'eddy currents', 'electromagnetic devices', 'process control', 'sensors', 'signal processing', 'tracking']), ("Estimation of the gradient of the solution of an adjoint diffusion equation by\nthe Monte Carlo method\nFor the case of isotropic diffusion we consider the representation of the\nweighted concentration of trajectories and its space derivatives in the\nform of integrals (with some weights) of the solution to the\ncorresponding boundary value problem and its directional derivative of\na convective velocity. If the convective velocity at the domain\nboundary is degenerate and some other additional conditions are imposed\nthis representation allows us to construct an efficient 'random walk by\nspheres and balls' algorithm. When these conditions are violated,\ntransition to modelling the diffusion trajectories by the Euler scheme\nis realized, and the directional derivative of velocity is estimated by\nthe dependent testing method, using the parallel modelling of two\nclosely-spaced diffusion trajectories. We succeeded in justifying this\nmethod by statistically equivalent transition to modelling a single\ntrajectory after the first step in the Euler scheme, using a suitable\nweight. This weight also admits direct differentiation with respect to\nthe initial coordinate along a given direction. The resulting weight\nalgorithm for calculating concentration derivatives is especially\nefficient if the initial point is in the subdomain in which the\ncoefficients of the diffusion equation are constant\n", ['isotropic diffusion', 'weighted trajectory concentration', 'space derivatives', 'integrals', 'boundary value problem', 'directional derivative', 'convective velocity', 'domain boundary', 'gradient estimation', 'adjoint diffusion equation', 'Monte Carlo method', 'random walk by spheres and balls algorithm', 'diffusion trajectories', 'Euler scheme', 'dependent testing method', 'parallel modelling', 'closely-spaced diffusion trajectories', 'statistically equivalent transition', 'weight', 'direct differentiation', 'initial coordinate', 'concentration derivatives', 'boundary-value problems', 'differentiation', 'diffusion', 'integral equations', 'Monte Carlo methods', 'physics computing', 'random processes']), ('The effects of asynchronous computer-mediated group interaction on group\nprocesses\nThis article reports a study undertaken to investigate some of the social\npsychological processes underlying computer-supported group discussion\nin natural computer-mediated contexts. Based on the concept of\ndeindividuation, it was hypothesized that personal identifiability and\ngroup identity would be important factors that affect the perceptions\nand behavior of members of computer-mediated groups. The degree of\npersonal identifiability and the strength of group identity were\nmanipulated across groups of geographically dispersed computer users\nwho took part in e-mail discussions during a 2-week period. The results\ndo not support the association between deindividuation and uninhibited\nbehavior cited in much previous research. Instead, the data provide\nsome support for a social identity perspective of computer-mediated\ncommunication, which explains the higher levels uninhibited in\nidentifiable computer-mediated groups. However, predictions based on\nsocial identity theory regarding group polarization and group cohesion\nwere not supported. Possible explanations for this are discussed and\nfurther research is suggested to resolve these discrepancies\n', ['asynchronous computer-mediated group interaction', 'group processes', 'social issues', 'psychology', 'deindividuation', 'Internet', 'personal identifiability', 'group identity', 'geographically dispersed computer users', 'e-mail discussions', 'social identity theory', 'group polarization', 'group cohesion', 'electronic mail', 'groupware', 'human factors', 'Internet', 'psychology', 'social aspects of automation', 'user interfaces']), ('Indexing-neglected and poorly understood\nThe growth of the Internet has highlighted the use of machine indexing. The\ndifficulties in using the Internet as a searching device can be\nfrustrating. The use of the term "python" is given as an example.\nMachine indexing is noted as "rotten" and human indexing as\n"capricious." The problem seems to be a lack of a theoretical\nfoundation for the art of indexing. What librarians have learned over\nthe last hundred years has yet to yield a consistent approach to what\nreally works best in preparing index terms and in the ability of our\ncustomers to search the various indexes. An attempt is made to consider\nthe elements of indexing, their pros and cons. The argument is made\nthat machine indexing is far too prolific in its production of index\nterms. Neither librarians nor computer programmers have made much\nprogress to improve Internet indexing. Human indexing has had the same\nproblems for over fifty years\n', ['Internet', 'machine indexing', 'searching', 'index terms', 'human indexing', 'indexing', 'information resources', 'information retrieval', 'Internet', 'vocabulary']), ('SPTL/BIALL academic law library survey 2000/2001\nThe paper outlines the activities and funding of academic law libraries in the\nUK and Ireland in the academic year 2000/2001. The figures have been\ntaken from the results of a postal questionnaire undertaken by\ninformation services staff at Cardiff University on behalf of BIALL\n', ['SPTL/BIALL', 'academic law libraries', 'funding', 'UK', 'Ireland', 'postal questionnaire', 'information services', 'Cardiff University', 'survey', 'academic libraries', 'law administration']), ('Optimal allocation of runs in a simulation metamodel with several independent\nvariables\nCheng and Kleijnen (1999) propose a very general regression metamodel for\nmodelling the output of a queuing system. Its main limitations are that\nthe regression function is based on a polynomial and that it can use\nonly one independent variable. These limitations are removed here. We\nderive an explicit formula for the optimal way of assigning simulation\nruns to the different design points\n', ['optimal runs allocation', 'simulation metamodel', 'independent variables', 'general regression metamodel', 'queuing system', 'regression function', 'design of experiments', 'minimisation', 'simulation']), ('Robust stability analysis for current-programmed regulators\nUncertainty models for the three basic switch-mode converters: buck, boost, and\nbuck-boost are given in this paper. The resulting models are\nrepresented by linear fractional transformations with structured\ndynamic uncertainties. Uncertainties are assumed for the load\nresistance R=R/sub O/(1+ delta /sub R/), inductance L=L/sub O/(1+ delta\n/sub L/), and capacitance C=C/sub O/(1+ delta /sub C/). The interest in\nthese models is clearly motivated by the need to have models for\nswitch-mode DC-DC converters that are compatible with robust control\nanalysis, which require a model structure consisting of a nominal model\nand a norm-bounded modeling uncertainty. Therefore, robust stability\nanalysis can be realized using standard mu -tools. At the end of the\npaper, an illustrative example is given which shows the simplicity of\nthe procedure\n', ['current-programmed regulators', 'robust stability analysis', 'uncertainty models', 'buck converters', 'boost converters', 'buck-boost converters', 'linear fractional transformations', 'structured dynamic uncertainties', 'load resistance', 'capacitance', 'inductance', 'switch-mode DC-DC converters', 'control analysis', 'nominal model', 'norm-bounded modeling uncertainty', 'control system analysis', 'DC-DC power convertors', 'electric current control', 'PWM power convertors', 'robust control']), ('The service side of systems librarianship\nDescribes the role of a systems librarian at a small academic library. Although\nonline catalogs and the Internet are making library accessibility more\nconvenient, the need for library buildings and professionals has not\ndiminished. Typical duties of a systems librarian and the effects of\nnew technology on librarianship are discussed. Services provided to\nother constituencies on campus and the blurring relationship between\nthe library and computer services are also presented\n', ['systems librarianship', 'service side', 'small academic library', 'online catalogs', 'Internet', 'academic libraries', 'Internet', 'library automation']), ('New approach to standing phase angle reduction for power system restoration\nDuring power system restoration, it is necessary to check the phase angle\nbetween two buses before closing circuit breakers to connect a line\nbetween them. These angles may occur across a tie line between two\nsystems or between two connected subsystems within a system. In case of\nlarge standing phase angle (SPA) difference the synchro-check relay\ndoes not allow closing of the breaker for this line. Therefore, this\nexcessive SPA has to be reduced before attempting to connect the line.\nIn this paper, a new and fast method for reducing SPA is presented. For\nthis purpose, the standing phase angle difference between two specific\nbuses is represented in terms of sensitivity factors associated with\nthe change in active power generations and consumption at the buses.\nThen, the proposed method reschedule generation of selected units or\nshed load of selected buses to reduce excessive SPA difference between\ntwo buses based on sensitivity factors\n', ['power system restoration', 'standing phase angle reduction approach', 'circuit breaker closing', 'synchrocheck relay', 'power line connection', 'sensitivity factors', 'circuit breakers', 'control system analysis computing', 'control system synthesis', 'load shedding', 'power system analysis computing', 'power system control', 'power system restoration']), ('Universal dynamic synchronous self-stabilization\nWe prove the existence of a "universal" synchronous self-stabilizing protocol,\nthat is, a protocol that allows a distributed system to stabilize to a\ndesired nonreactive behaviour (as long as a protocol stabilizing to\nthat behaviour exists). Previous proposals required drastic increases\nin asymmetry and knowledge to work, whereas our protocol does not use\nany additional knowledge, and does not require more symmetry-breaking\nconditions than available; thus, it is also stabilizing with respect to\ndynamic changes in the topology. We prove an optimal quiescence time n\n+ D for a synchronous network of n processors and diameter D; the\nprotocol can be made finite state with a negligible loss in quiescence\ntime. Moreover, an optimal D + 1 protocol is given for the case of\nunique identifiers. As a consequence, we provide an effective proof\ntechnique that allows one to show whether self-stabilization to a\ncertain behaviour is possible under a wide range of models\n', ['universal dynamic synchronous self-stabilization', 'synchronous self-stabilizing protocol', 'distributed system', 'nonreactive behaviour', 'topology', 'dynamic changes', 'optimal quiescence time', 'synchronous network', 'finite state', 'quiescence time', 'optimal protocol', 'unique identifiers', 'proof technique', 'self-stabilization', 'anonymous networks', 'graph fibrations', 'distributed processing', 'protocols', 'self-adjusting systems', 'theorem proving']), ('WEXTOR: a Web-based tool for generating and visualizing experimental designs\nand procedures\nWEXTOR is a Javascript-based experiment generator and teaching tool on the\nWorld Wide Web that can be used to design laboratory and Web\nexperiments in a guided step-by-step process. It dynamically creates\nthe customized Web pages and Javascripts needed for the experimental\nprocedure and provides experimenters with a print-ready visual display\nof their experimental design. WEXTOR flexibly supports complete and\nincomplete factorial designs with between-subjects, within-subjects,\nand quasi-experimental factors, as well as mixed designs. The software\nimplements client-side response time measurement and contains a content\nwizard for creating interactive materials, as well as dependent\nmeasures (graphical scales, multiple-choice items, etc.), on the\nexperiment pages. However, it does not aim to replace a full-fledged\nHTML editor. Several methodological features specifically needed in Web\nexperimental design have been implemented in the Web-based tool and are\ndescribed in this paper. WEXTOR is platform independent. The created\nWeb pages can be uploaded to any type of Web server in which data may\nbe recorded in logfiles or via a database. The current version of\nWEXTOR is freely available for educational and noncommercial purposes.\nIts Web address is http://www.genpsylab.unizh.ch/wextor/index.html\n', ['WEXTOR', 'Web-based tool', 'experimental design visualization', 'Javascript-based experiment generator', 'teaching tool', 'World Wide Web', 'customized Web pages', 'print-ready visual display', 'factorial designs', 'client-side response time measurement', 'content wizard', 'HTML', 'Web server', 'logfiles', 'database', 'free software', 'authoring languages', 'educational computing', 'information resources', 'Internet', 'Java', 'public domain software', 'student experiments']), ('Laguerre approximation of fractional systems\nSystems characterised by fractional power poles can be called fractional\nsystems. Here, Laguerre orthogonal polynomials are employed to\napproximate fractional systems by minimum phase, reduced order,\nrational transfer functions. Both the time and the frequency-domain\nanalysis exhibit the accuracy of the approximation\n', ['Laguerre approximation', 'fractional systems', 'fractional power poles', 'orthogonal polynomials', 'minimum phase', 'reduced order', 'robust controllers', 'closed-loop system', 'rational transfer functions', 'frequency-domain analysis', 'time-domain analysis', 'closed loop systems', 'frequency-domain analysis', 'poles and zeros', 'polynomials', 'reduced order systems', 'robust control', 'stochastic systems', 'time-domain analysis', 'transfer functions']), ('Re-examining the machining frictional boundary conditions using fractals\nPresents experimental evidence for the existence of non-Euclidean contact\ngeometry at the tool-chip interface in the machining of aluminium\nalloy, which challenges conventional assumptions. The geometry of\ncontact at the tool rake face is modelled using fractals and a\ndimension is computed for its description. The variation in the fractal\ndimension with the cutting speed is explored\n', ['machining frictional boundary conditions', 'fractals', 'nonEuclidean contact geometry', 'tool-chip interface', 'aluminium alloy', 'contact geometry', 'tool rake face', 'cutting speed', 'Al', 'aluminium alloys', 'cutting', 'fractals', 'friction', 'geometry', 'machine tools', 'machining']), ('Tactical airborne reconnaissance goes dual-band and beyond\nMultispectral imaging technologies are satisfying the need for a "persistent"\nlook at the battlefield. We highlight the need to persistently monitor\na battlefield to determine exactly who and what is there. For example,\ninfrared imaging can be used to expose the fuel status of an aircraft\non the runway. A daytime, visible-spectrum image of the same aircraft\nwould offer information about external details, such as the plane\'s\nmarkings and paint scheme. A dual-band camera enables precision image\nregistration by fusion and frequently yields more information than is\npossible by evaluating the images separately\n', ['tactical airborne reconnaissance', 'multispectral imaging technologies', 'battlefield', 'infrared imaging', 'fuel status', 'aircraft', 'daytime visible-spectrum image', 'dual-band camera', 'precision image registration', 'sensor fusion', 'image registration', 'image sensors', 'infrared imaging', 'military equipment', 'remote sensing', 'sensor fusion']), ('Plug-ins for critical media literacy: a collaborative program\nInformation literacy is important in academic and other libraries. The paper\nlooks at whether it would be more useful to librarians and to\ninstructors, as well as the students, to deal with information-literacy\nskill levels of students beginning their academic careers, rather than\nchecking them at the end. Approaching the situation with an eye toward\nthe broader scope of critical media literacy opens the discussion\nbeyond a skills inventory to the broader range of intellectual activity\n', ['information literacy', 'critical media literacy', 'collaborative program', 'academic libraries', 'instructors', 'academic libraries', 'information science', 'teaching']), ('Standard protocol for exchange of health-checkup data based on SGML: the\nHealth-checkup Data Markup Language (HDML)\nThe objectives are to develop a health/medical data interchange model for\nefficient electronic exchange of data among health-checkup facilities.\nA Health-checkup Data Markup Language (HDML) was developed on the basis\nof the Standard Generalized Markup Language (SGML), and a feasibility\nstudy carried out, involving data exchange between two health checkup\nfacilities. The structure of HDML is described. The transfer of\nnumerical lab data, summary findings and health status assessment was\nsuccessful. HDML is an improvement to laboratory data exchange. Further\nwork has to address the exchange of qualitative and textual data\n', ['health checkup data exchange', 'SGML', 'Health-checkup Data Markup Language', 'data interchange model', 'numerical lab data', 'summary findings', 'health status assessment', 'electronic data interchange', 'health care', 'medical administrative data processing', 'medical computing', 'page description languages']), ('Gossip is synteny: Incomplete gossip and the syntenic distance between genomes\nThe syntenic distance between two genomes is given by the minimum number of\nfusions, fissions, and translocations required to transform one into\nthe other, ignoring the order of genes within chromosomes. Computing\nthis distance is NP-hard. In the present work, we give a tight\nconnection between syntenic distance and the incomplete gossip problem,\na novel generalization of the classical gossip problem. In this\nproblem, there are n gossipers, each with a unique piece of initial\ninformation; they communicate by phone calls in which the two\nparticipants exchange all their information. The goal is to minimize\nthe total number of phone calls necessary to inform each gossiper of\nhis set of relevant gossip which he desires to learn. As an application\nof the connection between syntenic distance and incomplete gossip, we\nderive an O(2/sup O(n log n)/) algorithm to exactly compute the\nsyntenic distance between two genomes with at most n chromosomes each.\nOur algorithm requires O(n/sup 2/+2/sup O(d log d)/) time when this\ndistance is d, improving the O(n/sup 2/+2(O(d//sup 2/))) running time\nof the best previous exact algorithm\n', ['syntenic distance', 'genomes', 'NP-hard', 'incomplete gossip problem', 'comparative genomics', 'running time', 'chromosomes', 'algorithm theory', 'biology computing', 'cellular biophysics', 'computational complexity']), ('Iterative regularized least-mean mixed-norm image restoration\nWe develop a regularized mixed-norm image restoration algorithm to deal with\nvarious types of noise. A mixed-norm functional is introduced, which\ncombines the least mean square (LMS) and the least mean fourth (LMF)\nfunctionals, as well as a smoothing functional. Two regularization\nparameters are introduced: one to determine the relative importance of\nthe LMS and LMF functionals, which is a function of the kurtosis, and\nanother to determine the relative importance of the smoothing\nfunctional. The two parameters are chosen in such a way that the\nproposed functional is convex, so that a unique minimizer exists. An\niterative algorithm is utilized for obtaining the solution, and its\nconvergence is analyzed. The novelty of the proposed algorithm is that\nno knowledge of the noise distribution is required, and the relative\ncontributions of the LMS, the LMF, and the smoothing functionals are\nadjusted based on the partially restored image. Experimental results\ndemonstrate the effectiveness of the proposed algorithm\n', ['iterative regularized least-mean mixed-norm image restoration', 'noise', 'mixed-norm functional', 'least mean square functionals', 'mean fourth functionals', 'smoothing functional', 'regularization parameters', 'kurtosis', 'convex functional', 'unique minimizer', 'iterative algorithm', 'convergence', 'noise distribution', 'partially restored image', 'Gaussian noise', 'image restoration', 'iterative methods', 'least mean squares methods', 'smoothing methods']), ('Absorption of long waves by nonresonant parametric microstructures\nUsing simple acoustical and mechanical models, we consider the conceptual\npossibility of designing an active absorbing (nonreflecting) coating in\nthe form of a thin layer with small-scale stratification and fast time\nmodulation of parameters. Algorithms for space-time modulation of the\ncontrolled-layer structure are studied in detail for a one-dimensional\nboundary-value problem. These algorithms do not require wave-field\nmeasurements, which eliminates the self-excitation problem that is\ncharacteristic of active systems. The majority of the considered\nalgorithms of parametric control transform the low-frequency incident\nwave to high-frequency waves of the technological band for which the\nwaveguiding medium inside the layer is assumed to be opaque\n(absorbing). The efficient use conditions are found for all the\nalgorithms. It is shown that the absorbing layer can be as thin as\ndesired with respect to the minimum spatial scale of the incident wave\nand ensures efficient absorption in a wide frequency interval (starting\nfrom zero frequency) that is bounded from above only by a finite\nspace-time resolution of the parameter-control operations. The\nstructure of a three-dimensional parametric "\'black" coating whose\nefficiency is independent of the angle of incidence of an incoming wave\nis developed on the basis of the studied one-dimensional problems. The\ngeneral solution of the problem of diffraction of incident waves from\nsuch a coating is obtained. This solution is analyzed in detail for the\ncase of a disk-shaped element\n', ['acoustical models', 'mechanical models', 'active absorbing coating', 'nonreflecting coating', 'thin layer', 'small-scale stratification', 'fast time modulation', 'space-time modulation', 'controlled-layer structure', 'one-dimensional boundary-value problem', 'parametric control', 'low-frequency incident wave', 'high-frequency waves', 'waveguiding medium', 'absorbing layer', 'angle of incidence', 'one-dimensional problems', 'diffraction', 'disk-shaped element', 'acoustic parametric devices', 'acoustic wave absorption', 'active noise control', 'antireflection coatings', 'boundary-value problems']), ('Estimation of the vanishing point for automatic driving system using a cross\nratio\nThis paper proposes a new method to estimate the vanishing point used as the\nvehicle heading, which is essential in automatic driving systems. The\nproposed method uses a cross ratio comprised of a ratio of lengths from\nfour collinear points for extracting the edges that shape the vanishing\npoint. Then, lines that intersect at one point are fitted to the edges\nin a Hough space. Consequently, the vanishing point is estimated\nrobustly even when the lane markings are occluded by other vehicles. In\nthe presence of lane markings, the road boundaries are also estimated\nat the same time. Experimental results from images of a real road scene\nshow the effectiveness of the proposed method\n', ['vanishing point estimation', 'automatic driving system', 'cross ratio', 'automatic driving systems', 'collinear points', 'Hough space', 'lane markings', 'real road scene', 'driver information systems', 'edge detection', 'feature extraction']), ("Advanced optimization strategies in the Rice dHPF compiler\nHigh-Performance Fortran (HPF) was envisioned as a vehicle for modernizing\nlegacy Fortran codes to achieve scalable parallel performance. To a\nlarge extent, today's commercially available HPF compilers have failed\nto deliver scalable parallel performance for a broad spectrum of\napplications because of insufficiently powerful compiler analysis and\noptimization. Substantial restructuring and hand-optimization can be\nrequired to achieve acceptable performance with an HPF port of an\nexisting Fortran application, even for regular data-parallel\napplications. A key goal of the Rice dHPF compiler project has been to\ndevelop optimization techniques that enable a wide range of existing\nscientific applications to be ported easily to efficient HPF with\nminimal restructuring. This paper describes the challenges to effective\nparallelization presented by complex (but regular) data-parallel\napplications, and then describes how the novel analysis and\noptimization technologies in the dHPF compiler address these challenges\neffectively, without major rewriting of the applications. We illustrate\nthe techniques by describing their use for parallelizing the NAS SP and\nBT benchmarks. The dHPF compiler generates multipartitioned\nparallelizations of these codes that are approaching the scalability\nand efficiency of sophisticated hand-coded parallelizations\n", ['Mgh-Performance Fortran', 'legacy Fortran codes', 'parallel performance', 'HPF compilers', 'compiler analysis', 'compiler optimization', 'Rice dHPF compiler', 'multipartitioning', 'automatic parallelization', 'FORTRAN', 'optimising compilers', 'parallelising compilers']), ('Web ad explosion\nFinanced by advertising dollars from big names, online marketers are embracing\nmore aggressive tactics\n', ['Web advertising', 'online marketers', 'advertising', 'information resources', 'Internet', 'marketing']), ('Analysis of nonlinear time-delay systems using modules over non-commutative\nrings\nThe theory of non-commutative rings is introduced to provide a basis for the\nstudy of nonlinear control systems with time delays. The left Ore ring\nof non-commutative polynomials defined over the field of a meromorphic\nfunction is suggested as the framework for such a study. This approach\nis then generalized to a broader class of nonlinear systems with delays\nthat are called generalized Roesser systems. Finally, the theory is\napplied to analyze nonlinear time-delay systems. A weak observability\nis defined and characterized, generalizing the well-known linear\nresult. Properties of closed submodules are then developed to obtain a\nresult on the accessibility of such systems\n', ['nonlinear time-delay systems', 'modules', 'noncommutative rings', 'nonlinear control systems', 'left Ore ring', 'noncommutative polynomials', 'meromorphic function', 'generalized Roesser systems', 'weak observability', 'control system analysis', 'delay systems', 'nonlinear control systems', 'observability', 'polynomials']), ('ePsych: interactive demonstrations and experiments in psychology\nePsych (http://epsych.msstate.edu), a new Web site currently under active\ndevelopment, is intended to teach students about the discipline of\npsychology. The site presumes little prior knowledge about the field\nand so may be used in introductory classes, but it incorporates\nsufficient depth of coverage to be useful in more advanced classes as\nwell. Numerous interactive and dynamic elements are incorporated into\nvarious modules, orientations, and guidebooks. These elements include\nJava-based experiments and demonstrations, video clips, and animated\ndiagrams. Rapid access to all material is provided through a\nlayer-based navigation system that allows users to visit various\n"Worlds of the Mind." Active learning is encouraged, by challenging\nstudents with puzzles and problems and by providing the opportunity to\n"dig deeper" to learn more about the phenomena at hand\n', ['ePsych', 'interactive demonstrations', 'psychology experiments', 'Web site', 'teaching', 'Java-based experiments', 'video clips', 'animated diagrams', 'layer-based navigation system', 'Worlds of the Mind', 'active learning', 'educational computing', 'information resources', 'interactive systems', 'Internet', 'Java', 'psychology', 'student experiments']), ("Women in computing history\nExciting inventions, innovative technology, human interaction, and intriguing\npolitics fill computing history. However, the recorded history is\nmainly composed of male achievements and involvements, even though\nwomen have played substantial roles. This situation is not unusual.\nMost science fields are notorious for excluding, undervaluing, or\noverlooking the accomplishments of their female scientists. As Lee\npoints out, it is up to the historians and others to remedy this\nimbalance. Steps have been taken towards this goal through publishing\nbiographies on women in technology, and through honoring the pioneers\nwith various awards such as the GHC'97 Pioneering Awards, the WITI Hall\nof Fame, and the AWC Lovelace Award. A few online sites contain\nbiographies of women in technology. However, even with these resources,\nmany women who have contributed significantly to computer science are\nstill to be discovered\n", ['women', 'computing history', 'computer software', 'gender issues', 'history']), ('Outsourced backup saves time\nTo increase the efficiency of its data backup and to free staff to concentrate\non core business, The Gadget Shop is relying on a secure, automated\nsystem hosted by a third party\n', ['data backup', 'The Gadget Shop', 'e-business', 'outsourced', 'back-up procedures', 'electronic commerce', 'outsourcing', 'retailing']), ('Three-dimensional periodic Voronoi grain models and micromechanical\nFE-simulations of a two-phase steel\nA three-dimensional model is proposed for modeling of microstructures. The\nmodel is based on the finite element method with periodic boundary\nconditions. The Voronoi algorithm is used to generate the geometrical\nmodel, which has a periodic grain structure that follows the original\nboundaries of the Voronoi cells. As an application, the model is used\nto model a two-phase ferrite/pearlite steel. It is shown that periodic\ncells with only five grains generate representative stress-strain\ncurves\n', ['two-phase steel', 'three-dimensional model', 'periodic Voronoi grain models', 'micromechanical FEM simulations', 'microstructures modeling', 'periodic boundary conditions', 'Voronoi algorithm', 'geometrical model', 'ferrite-pearlite steel', 'stress-strain curves', 'Voronoi tessellation', 'adaptive mesh generator', 'quadtree/octree-based algorithm', 'kinematic constraints', 'computational time', 'computational geometry', 'crystal microstructure', 'internal stresses', 'mesh generation', 'micromechanics', 'physics computing', 'steel', 'stress-strain relations']), ('Design of an adaptive vibration absorber to reduce electrical transformer\nstructural vibration\nThis paper considers the design of a vibration absorber to reduce structural\nvibration at multiple frequencies, with an enlarged bandwidth control\nat these target frequencies. While the basic absorber is a passive\ndevice a control system has been added to facilitate tuning,\neffectively giving the combination of a passive and active device,\nwhich leads to far greater stability and robustness. Experimental\nresults demonstrating the effectiveness of the absorber are also\ndescribed\n', ['adaptive vibration absorber', 'electrical transformer', 'structural vibration', 'bandwidth control', 'damping', 'transformers', 'vibration control']), ('Surface textures improve the robustness of stereoscopic depth cues\nThis research develops design recommendations for surface textures (patterns of\ncolor on object surfaces) rendered with stereoscopic displays. In 3\nmethod-of-adjustment procedure experiments, 8 participants matched the\ndisparity of a circular probe and a planar stimulus rendered using a\nsingle visible edge. The experiments varied stimulus orientation and\nsurface texture. Participants more accurately matched the depth of\nvertical stimuli than that of horizontal stimuli, consistent with\nprevious studies and existing theory. Participants matched the depth of\nsurfaces with large pixel-to-pixel luminance variations more accurately\nthan they did surfaces with a small pixel-to-pixel luminance variation.\nFinally, they matched the depth of surfaces with vertical line patterns\nmore accurately than they did surfaces with horizontal-striped texture\npatterns. These results suggest that designers can enhance depth\nperception in stereoscopic displays, and also reduce undesirable\nsensitivity to orientation, by rendering objects with surface textures\nusing large pixel-to-pixel luminance variations\n', ['surface textures', 'robustness', 'stereoscopic depth cues', 'circular probe disparity', 'planar stimulus disparity', 'single visible edge', 'stimulus orientation', 'surface texture', 'pixel-to-pixel luminance variations', 'vertical line patterns', 'horizontal-striped texture patterns', 'depth perception', 'stereoscopic displays', 'orientation sensitivity reduction', 'object rendering', 'human factors', 'rendering (computer graphics)', 'three-dimensional displays']), ('Universal parametrization in constructing smoothly-connected B-spline surfaces\nIn this paper, we explore the feasibility of universal parametrization in\ngenerating B-spline surfaces, which was proposed recently in the\nliterature (Lim, 1999). We present an interesting property of the new\nparametrization that it guarantees Go continuity on B-spline surfaces\nwhen several independently constructed patches are put together without\nimposing any constraints. Also, a simple blending method of patchwork\nis proposed to construct C/sup n-1/ surfaces, where overlapping control\nnets are utilized. It takes into account the semi-localness property of\nuniversal parametrization. It effectively helps us construct very\nnatural looking B-spline surfaces while keeping the deviation from\ngiven data points very low. Experimental results are shown with several\nsets of surface data points\n', ['universal parametrization', 'smoothly-connected B-spline surface generation', 'G/sup 0/ continuity', 'patches', 'patchwork blending method', 'C/sup n-1/ surfaces', 'overlapping control nets', 'semi-localness property', 'surface data points', 'computational geometry', 'interpolation', 'splines (mathematics)']), ("Control in active systems based on criteria and motivation\nFor active systems where the principal varies the agents' goal functions by\nadding to them appropriately weighted goal functions of other agents or\na balanced system of inter-agent transfers, the paper formulated and\nsolved the problems of control based on criteria and motivation. Linear\nactive systems were considered by way of example\n", ['goal functions', 'inter-agent transfers', 'linear active systems', 'criteria-based control', 'motivation-based control', 'decision theory', 'game theory', 'linear systems']), ('High-level language support for user-defined reductions\nThe optimized handling of reductions on parallel supercomputers or clusters of\nworkstations is critical to high performance because reductions are\ncommon in scientific codes and a potential source of bottlenecks. Yet\nin many high-level languages, a mechanism for writing efficient\nreductions remains surprisingly absent. Further, when such mechanisms\ndo exist, they often do not provide the flexibility a programmer needs\nto achieve a desirable level of performance. In this paper, we present\na new language construct for arbitrary reductions that lets a\nprogrammer achieve a level of performance equal to that achievable with\nthe highly flexible, but low-level combination of Fortran and MPI. We\nhave implemented this construct in the ZPL language and evaluate it in\nthe context of the initialization of the NAS MG benchmark. We show a 45\ntimes speedup over the same code written in ZPL without this construct.\nIn addition, performance on a large number of processors surpasses that\nachieved in the NAS implementation showing that our mechanism provides\nprogrammers with the needed flexibility\n', ['parallel supercomputers', 'clusters of workstations', 'reductions', 'parallel programming', 'scientific computing', 'language construct', 'parallel languages', 'parallel programming', 'program compilers']), ("VONNA(HBP): a multimedia learning package on hotel budget planning\nIn this paper, a new learning package, VONNA(HBP), which provides an\ninteractive and online environment for novices to study and practice\nhotel budget planning, is introduced. Its design philosophy will be\ndiscussed thoughtfully with special focus on how to make use of the\nmultimedia and Internet. According to literatures, learning packages\nare faced to be more effective in delivering teaching material.\nResearchers indicate that students using a self-paced learning package\nscore higher than in a traditional classroom setting. Moreover, the\nlearning package provides different scenarios for students to explore\nthemselves in a practical environment and is more cost effective and\nsystematic than lectures. Currently, most learning packages in hotel\neducation are not implemented using multimedia with Internet access.\nOur paper describes a new learning package that fills the gaps.\nVONNA(HBP) requires participants to investigate operational budgets on\nvarious areas such as sales levels, payroll, inventory level, promotion\nstrategies, and facilities planning, etc. Eventually, the\nstudents/novices are required to practice their skills in a\ncomprehensive case about a hypothetical hotel. They need to solve\nmanagerial problems by a combination of budgetary planning on human\nresources, staff training programmes, facilities' maintenance and\nreplacement, or promotion schemes. Analytical tools are available for\nstudents/novices to judge an appropriate decision in handling\nconstrained resources\n", ['multimedia learning package', 'VONNA(HBP)', 'interactive online environment', 'hotel budget planning', 'Internet', 'teaching material delivery', 'self-paced learning package', 'hotel education', 'sales', 'payroll', 'inventory', 'promotion strategies', 'facilities planning', 'managerial problems', 'human resources', 'staff training programmes', 'facility maintenance', 'facility replacement', 'constrained resource handling', 'budgeting data processing', 'courseware', 'hotel industry', 'Internet', 'management education', 'multimedia computing', 'planning']), ('Prediction of ultraviolet spectral absorbance using quantitative\nstructure-property relationships\nHigh performance liquid chromatography (HPLC) with ultraviolet (UV)\nspectrophotometric detection is a common method for analyzing reaction\nproducts in organic chemistry. This procedure would benefit from a\ncomputational model for predicting the relative response of organic\nmolecules. Models are now reported for the prediction of the integrated\nUV absorbance for a diverse set of organic compounds using a\nquantitative structure-property relationship (QSPR) approach. A\nseven-descriptor linear correlation with a squared correlation\ncoefficient (R/sup 2/) of 0.815 is reported for a data set of\n521.compounds. Using the sum of ZINDO oscillator strengths in the\nintegration range as an additional descriptor allowed reduction in the\nnumber of descriptors producing a robust model for 460 compounds with\nfive descriptors and a squared correlation coefficient 0.857. The\ndescriptors used in the models are discussed with respect to the\nphysical nature of the UV absorption process\n', ['ultraviolet spectral absorbance prediction', 'quantitative structure-property relationship', 'high performance liquid chromatography', 'ultraviolet spectrophotometric detection', 'reaction products', 'organic chemistry', 'computational model', 'relative response', 'seven-descriptor linear correlation', 'squared correlation coefficient', 'ZINDO oscillator strengths', 'combinatorial chemistry', 'generic quantitation', 'configuration interaction calculation', 'CODESSA program', 'MOS-F package', 'chromatography', 'combinatorial mathematics', 'correlation methods', 'INDO calculations', 'iterative methods', 'organic compounds', 'oscillator strengths', 'solvent effects', 'spectrochemical analysis', 'spectrophotometry', 'spectroscopy computing', 'ultraviolet spectroscopy']), ('A framework of electronic tendering for government procurement: a lesson\nlearned in Taiwan\nTo render government procurement efficient, transparent, nondiscriminating, and\naccountable, an electronic government procurement system is required.\nAccordingly, Taiwan government procurement law (TGPL) states that\nsuppliers may employ electronic devices to forward a tender. This\ninvestigation demonstrates how the electronic government procurement\nsystem functions and reengineers internal procurement processes, which\nin turn benefits both government bodies and vendors. The system\nfeatures explored herein include posting/receiving bids via the\nInternet, vendor registration, certificate authorization, contract\ndevelopment tools, bid/request for proposal (RFP) development, online\nbidding, and online payment, all of which can be integrated easily\nwithin most existing information infrastructures\n', ['electronic tendering', 'electronic government procurement system', 'Taiwan government procurement law', 'reengineering', 'internal procurement processes', 'Internet bids', 'vendor registration', 'certificate authorization', 'contract development tools', 'request for proposal development', 'RFP development', 'online bidding', 'online payment', 'certification authority', 'payment gateway', 'public key infrastructure', 'certification', 'contracts', 'government data processing', 'Internet', 'public key cryptography', 'purchasing', 'systems re-engineering']), ('Gender benders [women in computing profession]\nAs a minority in the upper levels of the computing profession, women are\nsometimes mistreated through ignorance or malice. Some women have\nlearned to respond with wit and panache\n', ['computing profession', 'women', 'computer science education', 'gender issues', 'professional aspects']), ('Numerical solution of forward and backward problem for 2-D heat conduction\nequation\nFor a two-dimensional heat conduction problem, we consider its initial boundary\nvalue problem and the related inverse problem of determining the\ninitial temperature distribution from transient temperature\nmeasurements. The conditional stability for this inverse problem and\nthe error analysis for the Tikhonov regularization are presented. An\nimplicit inversion method, which is based on the regularization\ntechnique and the successive over-relaxation (SOR) iteration process,\nis established. Due to the explicit difference scheme for a direct heat\nproblem developed in this paper, the inversion process is very\nefficient, while the application of SOR technique makes our inversion\nconvergent rapidly. Numerical results illustrating our method are also\ngiven\n', ['heat conduction', 'initial boundary value problem', 'inverse problem', 'initial temperature distribution', 'transient temperature measurements', 'conditional stability', 'Tikhonov regularization', 'successive over-relaxation iteration process', 'error analysis', 'error analysis', 'heat conduction', 'inverse problems', 'iterative methods', 'parabolic equations', 'relaxation theory']), ('Polarization of the RF field in a human head at high field: a study with a\nquadrature surface coil at 7.0 T\nThe RF field intensity distribution in the human brain becomes inhomogeneous\ndue to wave behavior at high field. This is further complicated by the\nspatial distribution of RF field polarization that must be considered\nto predict image intensity distribution. An additional layer of\ncomplexity is involved when a quadrature coil is used for transmission\nand reception. To study such complicated RF field behavior, a computer\nmodeling method was employed to investigate the RF field of a\nquadrature surface coil at 300 MHz. Theoretical and experimental\nresults for a phantom and the human head at 7.0 T are presented. The\nresults are theoretically important and practically useful for\nhigh-field quadrature coil design and application\n', ['RF field intensity distribution', 'human brain', 'high field MRI', 'RF field polarization', 'quadrature surface coil', 'spatial distribution', 'image intensity distribution', 'computer modeling', 'high-field coil design', 'whole-body MRI', 'phantom samples', 'segmented images', '3D multitissue head model', 'gradient echo images', 'finite difference time domain method', 'Maxwell wave equations', 'reception fields', 'transmission fields', '7.0 T', '300 MHz', 'bioelectric phenomena', 'biomedical equipment', 'biomedical MRI', 'brain', 'coils', 'electromagnetic wave polarisation', 'finite difference time-domain analysis', 'image segmentation', 'Maxwell equations', 'medical image processing', 'physiological models', 'solid modelling']), ('Does social capital determine innovation? To what extent?\nThis paper deals with two questions: Does social capital determine innovation\nin manufacturing firms? If it is the case, to what extent? To deal with\nthese questions, we review the literature on innovation in order to see\nhow social capital came to be added to the other forms of capital as an\nexplanatory variable of innovation. In doing so, we have been led to\nfollow the dominating view of the literature on social capital and\ninnovation which claims that social capital cannot be captured through\na single indicator, but that it actually takes many different forms\nthat must be accounted for. Therefore, to the traditional explanatory\nvariables of innovation, we have added five forms of structural social\ncapital (business network assets, information network assets, research\nnetwork assets, participation assets, and relational assets) and one\nform of cognitive social capital (reciprocal trust). In a context where\nempirical investigations regarding the relations between social capital\nand innovation are still scanty, this paper makes contributions to the\nadvancement of knowledge in providing new evidence regarding the impact\nand the extent of social capital on innovation at the two\ndecisionmaking stages considered in this study\n', ['innovation', 'manufacturing firms', 'business network assets', 'information network assets', 'research network assets', 'participation assets', 'relational assets', 'cognitive social capital', 'structural social capital', 'reciprocal trust', 'two-stage decision-making process', 'degree of radicalness', 'corporate modelling', 'decision theory', 'manufacturing industries', 'technological forecasting']), ('An on-line distributed intelligent fault section estimation system for\nlarge-scale power networks\nIn this paper, a novel distributed intelligent system is suggested for on-line\nfault section estimation (FSE) of large-scale power networks. As the\nfirst step, a multi-way graph partitioning method based on weighted\nminimum degree reordering is proposed for effectively partitioning the\noriginal large-scale power network into the desired number of connected\nsub-networks with quasi-balanced FSE burdens and minimum frontier\nelements. After partitioning, a distributed intelligent system based on\nradial basis function neural network (RBF NN) and companion fuzzy\nsystem is suggested for FSE. The relevant theoretical analysis and\nprocedure are presented in the paper. The proposed distributed\nintelligent FSE method has been implemented with sparse storage\ntechnique and tested on the IEEE 14, 30 and 118-bus systems,\nrespectively. Computer simulation results show that the proposed FSE\nmethod works successfully for large-scale power networks\n', ['on-line distributed intelligent fault section estimation system', 'large-scale power networks', 'on-line fault section estimation', 'multi-way graph partitioning method based', 'weighted minimum degree reordering', 'connected sub-networks', 'quasi-balanced FSE burdens', 'minimum frontier elements', 'distributed intelligent system', 'radial basis function neural network', 'fuzzy system', 'sparse storage technique', 'IEEE 14-bus systems', 'IEEE 30-bus systems', 'IEEE 118-bus systems', 'computer simulation', 'fault simulation', 'fuzzy systems', 'graph theory', 'parameter estimation', 'power system faults', 'power system simulation', 'radial basis function networks']), ('Robust Kalman filter design for discrete time-delay systems\nThe problem of finite- and infinite-horizon robust Kalman filtering for\nuncertain discrete-time systems with state delay is addressed. The\nsystem under consideration is subject to time-varying norm-bounded\nparameter uncertainty in both the state and output matrices. We develop\na new methodology for designing a linear filter such that the error\nvariance of the filter is guaranteed to be within a certain upper bound\nfor any allowed uncertainty and time delay. The solution is given in\nterms of two Riccati equations. Multiple time-delay systems are also\ninvestigated\n', ['robust Kalman filter', 'discrete time-delay systems', 'uncertain systems', 'state delay', 'time-varying parameter uncertainty', 'norm-bounded parameter uncertainty', 'state matrices', 'output matrices', 'linear filter', 'Riccati equations', 'robust state estimation', 'delays', 'discrete time systems', 'estimation theory', 'filtering theory', 'Kalman filters', 'matrix algebra', 'Riccati equations', 'state estimation', 'time-varying systems', 'uncertain systems']), ('Smart collision information processing sensors for fast moving objects\nIn this technical note we survey the area of smart collision information\nprocessing sensors. We review the existing technologies to detect\ncollision or overlap between fast moving physical objects or objects in\nvirtual environments, physical environments or a combination of\nphysical and virtual objects. We report developments in the collision\ndetection of fast moving objects at discrete time steps such as two\nconsecutive time frames, as well as continuous time intervals such as\nin an interframe collision detection system. Our discussion of\ncomputational techniques in this paper is limited to convex objects.\nTechniques exist however to efficiently decompose non-convex objects\ninto convex objects. We also discuss the tracking technologies for\nobjects from the standpoint of collision detection or avoidance\n', ['virtual environments', 'physical environments', 'collision detection', 'discrete time steps', 'consecutive time frames', 'continuous time intervals', 'interframe collision detection', 'nonconvex objects', 'collision information processing', 'fast moving objects', 'air traffic control', 'smart sensors', 'military training', 'high speed machining', 'convex objects', 'tracking', 'air traffic control', 'automated highways', 'collision avoidance', 'digital simulation', 'industrial control', 'intelligent control', 'intelligent sensors', 'military computing', 'virtual reality']), ('Does classicism explain universality? Arguments against a pure classical\ncomponent of mind\nOne of the hallmarks of human cognition is the capacity to generalize over\narbitrary constituents. Marcus (Cognition 66, p.153; Cognitive\nPsychology 37, p. 243, 1998) argued that this capacity, called\n"universal generalization" (universality), is not supported by\nconnectionist models. Instead, universality is best explained by\nclassical symbol systems, with connectionism as its implementation.\nHere it is argued that universality is also a problem for classicism in\nthat the syntax-sensitive rules that are supposed to provide causal\nexplanations of mental processes are either too strict, precluding\npossible generalizations; or too lax, providing no information as to\nthe appropriate alternative. Consequently, universality is not\nexplained by a classical theory\n', ['classicism', 'universality', 'classical component of mind', 'human cognition', 'universal generalization', 'connectionist models', 'classical symbol systems', 'syntax-sensitive rules', 'causal explanations', 'mental processes', 'cognitive systems', 'neural nets', 'philosophical aspects']), ('Acceptance of a price discount: the role of the semantic relatedness between\npurchases and the comparative price format\nTwo studies are reported where people are asked to accept or not a price\nreduction on a target product. In the high (low) relative saving\nversion, the regular price of the target product is low (high). In both\nversions, the absolute value of the price reduction is the same as well\nas the total of regular prices of planned purchases. As first reported\nby Tversky and Kahneman (1981), findings show that the majority of\npeople accept the price discount in the high-relative saving version\nwhereas the minority do it in the low one. In Study 1, findings show\nthat the previous preference reversal disappears when planned purchases\nare strongly related. Also, a previously unreported preference reversal\nis found. The majority of people accept the price discount when the\nproducts are weakly related whereas the minority accept when the\nproducts are strongly related. In Study 2, findings show that the\nclassic preference reversal disappears as a function of the comparative\nprice format. Also, another previously unreported preference reversal\nis found. When the offered price reduction relates to a low-priced\nproduct, people are more inclined to accept it with a control than a\nminimal comparative price format. Findings reported in Studies 1 and 2\nare interpreted in terms of mental accounting shifts\n', ['price discount acceptance', 'preference reversal', 'semantic relatedness hypothesis', 'mental accounting shifts', 'high relative saving version', 'low relative saving version', 'planned purchases', 'comparative price format', 'low-priced product', 'behavioural sciences', 'decision theory', 'marketing']), ('Vector algebra proofs for geometry theorems\nVector mathematics can generate simple and powerful proofs of theorems in plane\ngeometry. These proofs can also be used to generalize plane geometry\ntheorems to higher dimensions. We present three vector proofs that show\nthe power of this technique. 1. For any quadrilateral, the sum of the\nsquares of the diagonals is less than or equal to the sum of the\nsquares of the sides. 2. The area of a quadrilateral is half the\nproduct of the diagonals multiplied by the sine of an included angle.\n3. One quarter of all triangles are acute (Based upon the options\ndetailed below, with respect to the relative lengths of the sides).\nThis paper presents a set of examples of vector mathematics applied to\ngeometry problems. Some of the most beautiful and sophisticated proofs\nin mathematics involve using multiple representations of the same data.\nBy leveraging the advantages of each representation one finds new and\nuseful mathematical facts\n', ['vector mathematics', 'plane geometry', 'multiple representations', 'quadrilateral', 'proofs', 'vector algebra proofs', 'algebra', 'geometry', 'vectors']), ('48 Gbit/s InP DHBT MS-DFF with very low time jitter\nA master-slave D-type flip-flop (MS DFF) fabricated in a self-aligned InP DHBT\ntechnology is presented. The packaged circuit shows full-rate clock\noperation at 48 Gbit/s. Very low time jitter and good retiming\ncapabilities are observed. Layout aspects, packaging and measurement\nissues are discussed in particular\n', ['DHBT MS-DFF', 'InP DHBT technology', 'low time jitter', 'master-slave D-type flip-flop', 'self-aligned DHBT technology', 'packaged circuit', 'retiming capabilities', 'layout aspects', '48 Gbit/s', 'InP', 'bipolar logic circuits', 'flip-flops', 'heterojunction bipolar transistors', 'high-speed integrated circuits', 'III-V semiconductors', 'indium compounds', 'integrated circuit layout', 'integrated circuit packaging', 'timing jitter']), ('Design and implementation of a reusable and extensible HL7 encoding/decoding\nframework\nThe Health Level Seven (HL7), an international standard for electronic data\nexchange in all health care environments, enables disparate computer\napplications to exchange key sets of clinical and administrative\ninformation. Above all, it defines the standard HL7 message formats\nprescribed by the standard encoding rules. In this paper, we propose a\nflexible, reusable, and extensible HL7 encoding and decoding framework\nusing a message object model (MOM) and message definition repository\n(MDR). The MOM provides an abstract HL7 message form represented by a\ngroup of objects and their relationships. It reflects logical\nrelationships among the standard HL7 message elements such as segments,\nfields, and components, while enforcing the key structural constraints\nimposed by the standard. Since the MOM completely eliminates the\ndependency of the HL7 encoder and decoder on platform-specific data\nformats, it makes it possible to build the encoder and decoder as\nreusable standalone software components, enabling the interconnection\nof arbitrary heterogeneous hospital information systems (HIS) with\nlittle effort. Moreover, the MDR, an external database of key\ndefinitions for HL7 messages, helps make the encoder and decoder as\nresilient as possible to future modifications of the standard HL7\nmessage formats. It is also used by the encoder and decoder to perform\na well-formedness check for their respective inputs (i.e., HL7 message\nobjects expressed in the MOM and encoded HL7 message strings). Although\nwe implemented a prototype version of the encoder and decoder using\nJAVA, they can be easily packaged and delivered as standalone\ncomponents using the standard component frameworks\n', ['reusable framework', 'extensible encoding/decoding framework', 'Health Level Seven', 'message object model', 'message definition repository', 'MDR', 'MOM', 'abstract message form', 'logical relationships', 'structural constraints', 'international standard', 'standalone software components', 'heterogeneous hospital information systems', 'HIS', 'external database', 'key definitions', 'JAVA', 'ActiveX', 'JAVABEAN', 'CORBA', 'electronic data exchange', 'health care environments', 'clinical information', 'administrative information', 'HL7 message formats', 'decoding', 'distributed object management', 'electronic data interchange', 'encoding', 'file organisation', 'health care', 'Java', 'medical information systems', 'message passing', 'software reusability', 'software standards']), ('The set of stable polynomials of linear discrete systems: its geometry\nThe multidimensional stability domain of linear discrete systems is studied.\nIts configuration is determined from the parameters of its intersection\nwith coordinate axes, coordinate planes, and certain auxiliary planes.\nCounterexamples for the discrete variant of the Kharitonov theorem are\ngiven\n', ['stable polynomials', 'Kharitonov theorem', 'characteristic polynomial', 'linear discrete systems', 'geometry', 'multidimensional stability domain', 'discrete systems', 'geometry', 'linear systems', 'polynomials', 'stability']), ('Knowledge-based structures and organisational commitment\nOrganisational commitment, the emotional attachment of an employee to the\nemploying organisation, has attracted a substantial body of literature,\nrelating the concept to various antecedents, including organisational\nstructure, and to a range of consequences, including financially\nimportant performance factors such as productivity and staff turnover.\nThe new areas of knowledge management and learning organisations offer\nsubstantial promise as imperatives for the organisation of business\nenterprises. As organisations in the contemporary environment adopt\nknowledge-based structures to improve their competitive position, there\nis value in examining these structures against other performance\nrelated factors. Theoretical knowledge-based structures put forward by\nR. Miles et al. (1997) and J. Quinn et al. (1996) and an existing\nimplementation are examined to determine common features inherent in\nthese approaches. These features are posited as a typical form and\ntheir impact on organisational commitment and hence on individual and\norganisational performance is examined\n', ['knowledge-based structures', 'emotional attachment', 'performance factors', 'productivity', 'staff turnover', 'earning organisations', 'organisational commitment', 'DP management', 'knowledge based systems', 'personnel', 'social aspects of automation']), ("Synthetic simultaneity - natural and artificial\nIn control loops, each element introduces time delays. If those time delays are\nlarger than the critical times for control of the system, a problem\nexists. I show a simple approach to mitigating this problem by basing\nthe controller's decisions not on the observations themselves but on\nour projections as to what the observations will be at the time our\ncontrols reach the controlled system. Finally, I argue that synthetic\nsimultaneity explains Libet's (1993) results better than Libet's\nexplanation\n", ['control loops', 'time delays', 'critical times', 'controller decisions', 'observations', 'synthetic simultaneity', 'control systems', 'control theory', 'delays', 'planning (artificial intelligence)', 'software agents']), ('Theoretical and experimental investigations on coherence of traffic noise\ntransmission through an open window into a rectangular room in\nhigh-rise buildings\nA method for theoretically calculating the coherence between sound pressure\ninside a rectangular room in a high-rise building and that outside the\nopen window of the room is proposed. The traffic noise transmitted into\na room is generally dominated by low-frequency components, to which\nactive noise control (ANC) technology may find an application. However,\ngood coherence between reference and error signals is essential for an\neffective noise reduction and should be checked first. Based on traffic\nnoise prediction methods, wave theory, and mode coupling theory, the\nresults of this paper enabled one to determine the potentials and\nlimitations of ANC used to reduce such a transmission. Experimental\ncoherence results are shown for two similar, empty rectangular rooms\nlocated on the 17th and 30th floors of a 34 floor high-rise building.\nThe calculated results with the proposed method are generally in good\nagreement with the experimental results and demonstrate the usefulness\nof the method for predicting the coherence\n', ['traffic noise transmission', 'open window', 'rectangular room', 'high-rise buildings', 'sound pressure', 'low-frequency components', 'active noise control technology', 'traffic noise prediction methods', 'mode coupling theory', 'wave theory', 'active noise control', 'architectural acoustics', 'noise pollution']), ("Security crisis management - the basics\nOf the more pervasive problems in any kind of security event is how the\nsecurity event is managed from the inception to the end. There's a lot\nwritten about how to manage a specific incident or how to deal with a\npoint problem such as a firewall log, but little tends to be written\nabout how to deal with the management of a security event as part of\ncorporate crisis management. This article discusses the basics of\nsecurity crisis management and of the logical steps required to ensure\nthat a security crisis does not get out of hand\n", ['security crisis management', 'security event', 'firewall log', 'corporate crisis management', 'DP management', 'security of data']), ('A component-based software configuration management model and its supporting\nsystem\nSoftware configuration management (SCM) is an important key technology in\nsoftware development. Component-based software development (CBSD) is an\nemerging paradigm in software development. However, to apply CBSD\neffectively in real world practice, supporting SCM in CBSD needs to be\nfurther investigated. In this paper, the objects that need to be\nmanaged in CBSD is analyzed and a component-based SCM model is\npresented. In this model, components, as the integral logical\nconstituents in a system, are managed as the basic configuration items\nin SCM, and the relationships between/among components are defined and\nmaintained. Based on this model, a configuration management system is\nimplemented\n', ['component-based software configuration management model', 'software development', 'integral logical constituents', 'software reuse', 'version control', 'concurrency control', 'configuration management', 'software reusability']), ('Intelligent control of life support for space missions\nFuture manned space operations will include a greater use of automation than we\ncurrently see. For example, semiautonomous robots and software agents\nwill perform difficult tasks while operating unattended most of the\ntime. As these automated agents become more prevalent, human contact\nwith them will occur more often and become more routine, so designing\nthese automated agents according to the principles of human-centered\ncomputing is important. We describe two cases of semiautonomous control\nsoftware developed and fielded in test environments at the NASA Johnson\nSpace Center. This software operated continuously at the JSC and\ninteracted closely with humans for months at a time\n', ['life support', 'software agents', 'semiautonomous robots', 'space missions', 'intelligent control', 'manned space operations', 'automation', 'automated agents', 'semiautonomous control software', 'NASA Johnson Space Center', 'crew air regeneration', 'crew water recovery', 'human intervention', 'aerospace computing', 'aerospace control', 'aerospace robotics', 'intelligent control', 'software agents']), ('The agile revolution [business agility]\nThere is a new business revolution in the air. The theory is there, the\ntechnology is evolving, fast. It is all about agility\n', ['business agility', 'software design', 'software deployment', 'organisational structures', 'supply chains', 'management information systems']), ('A static semantics for Haskell\nThis paper gives a static semantics for Haskell 98, a non-strict purely\nfunctional programming language. The semantics formally specifies\nnearly all the details of the Haskell 98 type system, including the\nresolution of overloading, kind inference (including defaulting) and\npolymorphic recursion, the only major omission being a proper treatment\nof ambiguous overloading and its resolution. Overloading is translated\ninto explicit dictionary passing, as in all current implementations of\nHaskell. The target language of this translation is a variant of the\nGirard-Reynolds polymorphic lambda calculus featuring higher order\npolymorphism. and explicit type abstraction and application in the term\nlanguage. Translated programs can thus still be type checked, although\nthe implicit version of this system is impredicative. A surprising\nresult of this formalization effort is that the monomorphism\nrestriction, when rendered in a system of inference rules, compromises\nthe principal type property\n', ['static semantics', 'Haskell 98', 'nonstrict purely functional programming language', 'formal specification', 'type system', 'overloading', 'kind inference', 'polymorphic recursion', 'explicit dictionary passing', 'polymorphic lambda calculus', 'higher order polymorphism', 'explicit type abstraction', 'term language', 'type checking', 'monomorphism restriction', 'inference rules', 'functional languages', 'lambda calculus', 'programming language semantics']), ('Craigslist: virtual community maintains human touch\nIf it works why change it? This might have been the thought on the minds of dot\ncom executives back when Internet businesses were booming, and most of\nthe Web content was free. Web sites were overflowing with\nadvertisements of every kind and size. Now that dot com principals know\nbetter, Web ads are no longer the only path to revenue generation.\nCommunity portals, however, never seemed to have many ads to begin\nwith, and their content stayed truer to who they served. Many of them\nstarted off as simple places for users to list announcements, local\nevents, want ads, real estate, and mingle with other local users. The\nauthor saw the need for San Franciscans to have a place to do all of\nthat for free, without any annoying advertising, and ended up offering\nmuch more to his community with the creation of craigslist. "[Polling\nusers] was a good way for us to connect with our members, this is the\nway to operate successfully in situations like these - your members\ncome first."\n', ['virtual community', 'craigslist', 'Internet businesses', 'Web content', 'revenue generation', 'community portals', 'announcements', 'local events', 'want ads', 'real estate', 'San Francisco Bay community', 'advertising data processing', 'information resources']), ('Lossy to lossless object-based coding of 3-D MRI data\nWe propose a fully three-dimensional (3-D) object-based coding system\nexploiting the diagnostic relevance of the different regions of the\nvolumetric data for rate allocation. The data are first decorrelated\nvia a 3-D discrete wavelet transform. The implementation via the\nlifting steps scheme allows to map integer-to-integer values, enabling\nlossless coding, and facilitates the definition of the object-based\ninverse transform. The coding process assigns disjoint segments of the\nbitstream to the different objects, which can be independently accessed\nand reconstructed at any up-to-lossless quality. Two fully 3-D coding\nstrategies are considered: embedded zerotree coding (EZW-3D) and\nmultidimensional layered zero coding (MLZC), both generalized for\nregion of interest (ROI)-based processing. In order to avoid artifacts\nalong region boundaries, some extra coefficients must be encoded for\neach object. This gives rise to an overheading of the bitstream with\nrespect to the case where the volume is encoded as a whole. The amount\nof such extra information depends on both the filter length and the\ndecomposition depth. The system is characterized on a set of head\nmagnetic resonance images. Results show that MLZC and EZW-3D have\ncompetitive performances. In particular, the best MLZC mode outperforms\nthe others state-of-the-art techniques on one of the datasets for which\nresults are available in the literature\n', ['lossy object-based coding', 'lossless object-based coding', '3-D MRI data', 'diagnostic relevance', 'volumetric data', 'rate allocation', '3-D discrete wavelet transform', 'lifting steps scheme', 'integer-to-integer values', 'object-based inverse transform', 'disjoint segments', 'bitstream', 'embedded zerotree coding', 'EZW-3D', 'multidimensional layered zero coding', 'NMZQ', 'region of interest-based processing', 'ROI-based processing', 'region boundaries', 'filter length', 'decomposition depth', 'head magnetic resonance images', 'biomedical MRI', 'decorrelation', 'discrete wavelet transforms', 'image coding', 'medical image processing']), ("RISCy business. Part 1: RISC projects by Cornell students\nThe author looks at several projects that Cornell University students entered\nin the Atmel Design 2001 contest. Those covered include a vertical\nplotter; BiLines, an electronic game; a wireless Internet pager;\nCooking Coach; Barbie's zip drive; and a model train controller\n", ["Atmel's Design Logic 2001 contest", 'RISC projects', 'Cornell students', 'vertical plotter', 'BiLines', 'electronic game', 'wireless Internet pager', 'Cooking Coach', "Barbie's zip drive", 'model train controller', 'logic design', 'reduced instruction set computing']), ('Image fusion between /sup 18/FDG-PET and MRI/CT for radiotherapy planning of\noropharyngeal and nasopharyngeal carcinomas\nAccurate diagnosis of tumor extent is important in three-dimensional conformal\nradiotherapy. This study reports the use of image fusion between\n(18)F-fluoro-2-deoxy-D-glucose positron emission tomography (/sup\n18/FDG-PET) and magnetic resonance imaging/computed tomography (MRI/CT)\nfor better targets delineation in radiotherapy planning of\nhead-and-neck cancers. The subjects consisted of 12 patients with\noropharyngeal carcinoma and 9 patients with nasopharyngeal carcinoma\n(NPC) who were treated with radical radiotherapy between July 1999 and\nFebruary 2001. Image fusion between /sup 18/FDG-PET and MRI/CT was\nperformed using an automatic multimodality image registration\nalgorithm, which used the brain as an internal reference for\nregistration. Gross tumor volume (GTV) was determined based on clinical\nexamination and /sup 18/FDG uptake on the fusion images. Clinical\ntarget volume (CTV) was determined following the usual pattern of lymph\nnode spread for each disease entity along with the clinical\npresentation of each patient. Except for 3 cases with superficial\ntumors, all the other primary tumors were detected by /sup 18/FDG-PET.\nThe GTV volumes for primary tumors were not changed by image fusion in\n19 cases (89%), increased by 49% in one NPC, and decreased by 45% in\nanother NPC. Normal tissue sparing was more easily performed based on\nclearer GTV and CTV determination on the fusion images. In particular,\nparotid sparing became possible in 15 patients (71%) whose upper neck\nareas near the parotid glands were tumor-free by /sup 18/FDG-PET.\nWithin a mean follow-up period of 18 months, no recurrence occurred in\nthe areas defined as CTV, which was treated prophylactically, except\nfor 1 patient who experienced nodal recurrence in the CTV and\nsimultaneous primary site recurrence. In conclusion, this preliminary\nstudy showed that image fusion between /sup 18/FDG-PET and MRI/CT was\nuseful in GTV and CTV determination in conformal RT, thus sparing\nnormal tissues\n', ['image fusion', '/sup 18/FDG-PET', 'MRI/CT', 'radiotherapy planning', 'nasopharyngeal carcinomas', 'oropharyngeal carcinomas', 'parotid glands', 'simultaneous primary site recurrence', 'normal tissues sparing', 'superficial tumors', 'primary tumors', 'F', 'biomedical MRI', 'cancer', 'computerised tomography', 'image registration', 'medical image processing', 'positron emission tomography', 'radiation therapy', 'tumours']), ('Optical actuation of a bistable MEMS\nThis paper presents an optical actuation scheme for MEMS devices based on the\nwell-established fact that light possesses momentum, and hence, imparts\na force equal to 2 W/c when reflected by a surface. Here, W is the\ntotal power of the reflected light, and c is the speed of light.\nRadiation pressure, as it is known, is nearly insignificant for most\nmacroscale applications, but it can be quite significant for MEMS\ndevices. In addition, light actuation offers a new paradigm. First,\nintersecting light beams do not interfere, in contrast to electrical\nconductors, which short when they come into contact. Second, light can\noperate in high temperature and high radiation environments far outside\nthe capability of solid state electronic components. This actuation\nmethod is demonstrated, both in air and in vacuum, by switching the\nstate of a bistable MEMS device. The associated heat transfer model is\nalso presented\n', ['bistable MEMS', 'optical actuation scheme', 'radiation pressure', 'MEMS devices', 'intersecting light beams', 'high temperature environments', 'high radiation environments', 'heat transfer model', 'electromagnetic actuators', 'microactuators', 'radiation pressure', 'semiconductor device models']), ('A fast implementation of correlation of long data sequences for coherent\nreceivers\nCoherent reception depends upon matching of phase between the transmitted and\nreceived signal. Fast convolution techniques based on fast Fourier\ntransform (FFT) are widely used for extracting time delay information\nfrom such matching. The latency in processing a large data window of\nthe received signal is a serious overhead for mission critical real\ntime applications. The implementation of a parallel algorithm for\ncorrelation of long data sequences in multiprocessor environment is\ndemonstrated here. The algorithm does processing while acquiring the\nreceived signal and reduces the computation overhead considerably\nbecause of inherent parallelism\n', ['correlation', 'long data sequences', 'coherent receivers', 'transmitted signal', 'received signal', 'fast Fourier transform', 'time delay information', 'latency', 'mission critical real time applications', 'parallel algorithm', 'multiprocessor environment', 'computation', 'convolution', 'correlation methods', 'fast Fourier transforms', 'parallel algorithms', 'real-time systems', 'signal detection']), ("ICANN and Internet governance: leveraging technical coordination to realize\nglobal public policy\nThe Internet Corporation for Assigned Names and Numbers (ICANN) was created in\n1998 to perform technical coordination of the Internet. ICANN also lays\nthe foundations for governance, creating capabilities for promulgating\nand enforcing global regulations on Internet use. ICANN leverages the\ncapabilities in the Internet domain name system (DNS) to implement four\nmechanisms of governance: authority, law, sanctions, and jurisdictions.\nThese governance-related features are embodied in seemingly technical\nfeatures of ICANN's institutional design. Recognition of ICANN's\ngovernance mechanisms allows us to better understand the Internet's\nemerging regulatory regime\n", ['ICANN', 'Internet governance', 'technical coordination', 'global public policy', 'Internet Corporation for Assigned Names and Numbers', 'global regulations', 'Internet use', 'Internet domain name system', 'Internet DNS', 'governance-related features', 'institutional design', 'regulatory regime', 'bibliographies', 'government policies', 'Internet', 'management', 'naming services']), ('Estimating the intrinsic dimension of data with a fractal-based method\nIn this paper, the problem of estimating the intrinsic dimension of a data set\nis investigated. A fractal-based approach using the\nGrassberger-Procaccia algorithm is proposed. Since the\nGrassberger-Procaccia algorithm (1983) performs badly on sets of high\ndimensionality, an empirical procedure that improves the original\nalgorithm has been developed. The procedure has been tested on data\nsets of known dimensionality and on time series of Santa Fe competition\n', ['data intrinsic dimension estimation', 'fractal-based method', 'time series', 'Santa Fe competition', 'pattern recognition', 'fractals', 'pattern recognition', 'time series']), ('Basin configuration of a six-dimensional model of an electric power system\nAs part of an ongoing project on the stability of massively complex electrical\npower systems, we discuss the global geometric structure of contacts\namong the basins of attraction of a six-dimensional dynamical system.\nThis system represents a simple model of an electrical power system\ninvolving three machines and an infinite bus. Apart from the possible\noccurrence of attractors representing pathological states, the contacts\nbetween the basins have a practical importance, from the point of view\nof the operation of a real electrical power system. With the aid of a\nglobal map of basins, one could hope to design an intervention strategy\nto boot the power system back into its normal state. Our method\ninvolves taking two-dimensional sections of the six-dimensional state\nspace, and then determining the basins directly by numerical simulation\nfrom a dense grid of initial conditions. The relations among all the\nbasins are given for a specific numerical example, that is, choosing\nparticular values for the parameters in our model\n', ['basin configuration', 'six-dimensional model', 'electric power system', 'massively complex electrical power systems', 'global geometric structure', 'infinite bus', 'attractors', 'pathological states', 'global map', 'state space', 'power system stability', 'bifurcation', 'chaos', 'geometry', 'limit cycles', 'nonlinear dynamical systems', 'power system dynamic stability', 'transforms']), ("Regularity of some 'incomplete' Pal-type interpolation problems\nIn this paper the regularity of nine Pal-type interpolation problems is proved.\nIn the literature interpolation on the zeros of the pair W/sub n//sup (\nalpha )/(z) = (z + alpha )/sup n/ + (1 + alpha z)/sup n/, v/sub n//sup\n( alpha )/(z) = (z + alpha )/sup n/ - (1 + alpha z)/sup n/ with 0 <\nalpha < 1 has been studied. Here the nodes form a subset of these\nsets of zeros\n", ['Pal-type interpolation problems', 'zeros', 'interpolation', 'poles and zeros']), ('Post-projected Runge-Kutta methods for index-2 differential-algebraic equations\nA new projection technique for Runge-Kutta methods applied to index-2\ndifferential-algebraic equations is presented in which the numerical\napproximation is projected only as part of the output process. It is\nshown that for methods that are strictly stable at infinity, the order\nof convergence is unaffected compared to standard projected methods.\nGauss methods, for which this technique is of special interest when\nsome symmetry is to be preserved, are studied in more detail\n', ['post-projected Runge-Kutta methods', 'numerical approximation', 'order of convergence', 'projected methods', 'index-2 differential-algebraic equations', 'approximation theory', 'differential equations', 'Runge-Kutta methods']), ("Customer in-reach and library strategic systems: the case of ILLiad\nLibraries have walls. Recognizing this fact, the Interlibrary Loan Department\nat Virginia Tech is creating systems and services that enable our\ncustomers to reach past our walls at anytime from anywhere. Customer\nin-reach enables Virginia Tech faculty, students, and staff anywhere in\nthe world to obtain information and services heretofore available only\nto our on-campus customers. ILLiad, Virginia Tech's interlibrary\nborrowing system, is the library strategic system that attains this\ngoal. The principles that guided development of ILLiad are widely\napplicable\n", ['library strategic systems', 'Interlibrary Loan Department', 'Virginia Tech', 'customer in-reach', 'ILLiad', 'interlibrary borrowing system', 'academic libraries', 'interlibrary loan']), ('Firewall card shields data\nThe SlotShield 3000 firewall on a PCI card saves power and space, but might not\noffer enough security for large networks\n', ['SlotShield 3000 firewall', 'PCI card', 'security', 'large networks', 'authorisation', 'computer network management', 'security of data']), ("User-appropriate tyre-modelling for vehicle dynamics in standard and limit\nsituations\nWhen modelling vehicles for the vehicle dynamic simulation, special attention\nmust be paid to the modelling of tyre forces and -torques, according to\ntheir dominant influence on the results. This task is not only about\nsufficiently exact representation of the effective forces but also\nabout user-friendly and practical relevant applicability, especially\nwhen the experimental tyre-input-data is incomplete or missing. This\ntext firstly describes the basics of the vehicle dynamic tyre model,\nconceived to be a physically based, semi-empirical model for\napplication in connection with multi-body-systems (MBS). On the basis\nof tyres for a passenger car and a heavy truck the simulated steady\nstate tyre characteristics are shown together and compared with the\nunderlying experimental values. The possibility to link the tyre model\nTMeasy to any MBS-program is described, as far as it supports the\n'Standard Tyre Interface'. As an example, the simulated and\nexperimental data of a heavy truck doing a standardized driving\nmanoeuvre are compared\n", ['tyre modelling', 'vehicle dynamics', 'standard situations', 'limit situations', 'tyre torques', 'semi-empirical model', 'multi-body-systems', 'passenger car', 'heavy truck', 'simulated steady state tyre characteristics', 'TMeasy', 'Standard Tyre Interface', 'standardized driving manoeuvre', 'digital simulation', 'dynamics', 'force', 'road vehicles', 'torque']), ('Code generator for the HPF Library and Fortran 95 transformational functions\nOne of the language features of the core language of HPF 2.0 (High Performance\nFortran) is the HPF Library. The HPF Library consists of 55 generic\nfunctions. The implementation of this library presents the challenge\nthat all data types, data kinds, array ranks and input distributions\nneed to be supported. For instance, more than 2 billion separate\nfunctions are required to support COPY-SCATTER fully. The efficient\nsupport of these billions of specific functions is one of the\noutstanding problems of HPF. We have solved this problem by developing\na library generator which utilizes the mechanism of parameterized\ntemplates. This mechanism allows the procedures to be instantiated at\ncompile time for arguments with a specific type, kind, rank and\ndistribution over a specific processor array. We describe the\nalgorithms used in the different library functions. The implementation\ngives the ease of generating a large number of library routines from a\nsingle template. The templates can be extended with special code for\nspecific combinations of the input arguments. We describe in detail the\nimplementation and performance of the matrix multiplication template\nfor the Fujitsu VPP5000 platform\n', ['HPF', 'High Performance Fortran', 'HPF Library', 'generic functions', 'data types', 'library generator', 'library functions', 'parallel computing', 'parallel languages', 'code generation', 'parameterized templates', 'matrix multiplication', 'FORTRAN', 'matrix multiplication', 'parallel languages', 'parallel programming', 'software libraries']), ('A systematic review of the efficacy of telemedicine for making diagnostic and\nmanagement decisions\nWe conducted a systematic review of the literature to evaluate the efficacy of\ntelemedicine for making diagnostic and management decisions in three\nclasses of application: office/hospital-based, store-and-forward, and\nhome-based telemedicine. We searched the MEDLINE, EMBASE, CINAHL and\nHealthSTAR databases and printed resources, and interviewed\ninvestigators in the field. We excluded studies where the service did\nnot historically require face-to-face encounters (e.g. radiology or\npathology diagnosis). A total of 58 articles met the inclusion\ncriteria. The articles were summarized and graded for the quality and\ndirection of the evidence. There were very few high-quality studies.\nThe strongest evidence for the efficacy of telemedicine for diagnostic\nand management decisions came from the specialties of psychiatry and\ndermatology. There was also reasonable evidence that general medical\nhistory and physical examinations performed via telemedicine had\nrelatively good sensitivity and specificity. Other specialties in which\nsome evidence for efficacy existed were cardiology and certain areas of\nophthalmology. Despite the widespread use of telemedicine in most major\nmedical specialties, there is strong evidence in only a few of them\nthat the diagnostic and management decisions provided by telemedicine\nare comparable to face-to-face care\n', ['telemedicine', 'medical diagnosis', 'management decision making', 'literature review', 'MEDLINE', 'EMBASE', 'CINAHL', 'HealthSTAR', 'psychiatry', 'dermatology', 'cardiology', 'ophthalmology', 'information analysis', 'information services', 'medical computing', 'reviews', 'telemedicine']), ("A study of computer attitudes of non-computing students of technical colleges\nin Brunei Darussalam\nThe study surveyed 268 non-computing students among three technical colleges in\nBrunei Darussalam. The study validated an existing instrument to\nmeasure computer attitudes of non-computing students, and identified\nfactors that contributed to the formation of their attitudes. The\nfindings show that computer experience and educational qualification\nare associated with students' computer attitudes. In contrast,\nvariables such as gender, age, ownership of a personal computer (PC),\ngeographical location of institution, and prior computer training\nappeared to have no impact on computer attitudes\n", ['computer attitudes', 'noncomputing students', 'survey', 'computer experience', 'educational qualification', 'gender', 'age', 'personal computer ownership', 'computer training', 'educational computing', 'end user computing', 'technical colleges', 'computer literacy', 'educational computing', 'gender issues', 'human factors', 'personal computing', 'training']), ('The use of the SPSA method in ECG analysis\nThe classification, monitoring, and compression of electrocardiogram (ECG)\nsignals recorded of a single patient over a relatively long period of\ntime is considered. The particular application we have in mind is\nhigh-resolution ECG analysis, such as late potential analysis,\nmorphology changes in QRS during arrythmias, T-wave alternants, or the\nstudy of drug effects on ventricular activation. We propose to apply a\nmodification of a classical method of cluster analysis or vector\nquantization. The novelty of our approach is that we use a new\ndistortion measure to quantify the distance of two ECG cycles, and the\nclass-distortion measure is defined using a min-max criterion. The new\nclass-distortion-measure is much more sensitive to outliers than the\nusual distortion measures using average-distance. The price of this\npractical advantage is that computational complexity is significantly\nincreased. The resulting nonsmooth optimization problem is solved by an\nadapted version of the simultaneous perturbation stochastic\napproximation (SPSA) method of J. Spall (IEEE Trans. Automat. Contr.,\nvol. 37, p. 332-41, Mar. 1992). The main idea is to generate a smooth\napproximation by a randomization procedure. The viability of the method\nis demonstrated on both simulated and real data. An experimental\ncomparison with the widely used correlation method is given on real\ndata\n', ['class-distortion-measure', 'nonsmooth optimization problem', 'simultaneous perturbation stochastic approximation method', 'cluster analysis', 'randomization procedure', 'correlation method', 'electrodiagnostics', 'ECG signals compression', 'distortion measure', 'ECG cycles', 'computational complexity', 'data compression', 'electrocardiography', 'medical signal processing', 'minimax techniques', 'vector quantisation']), ('The rise and fall and rise again of customer care\nTaking care of customers has never gone out of style, but as the recession\nfades, interest is picking up in a significant retooling of the CRM\nsolutions banks have been using. The goal: usable knowledge to help\nimprove service\n', ['usable knowledge', 'customer relationship management', 'banks', 'banking', 'marketing']), ('The archival imagination of David Bearman, revisited\nMany archivists regard the archival imagination evidenced in the writings of\nDavid Bearman as avant-garde. Archivist L. Henry (1998) has sharply\ncriticized Bearman for being irreverent toward the archival theory and\npractice outlined by classical American archivist T. R. Schellenberg.\nAlthough Bearman is sometimes credited (and sometimes berated) for\nestablishing "a new paradigm" centered on the archival management of\nelectronic records, his methods and strategies are intended to\nencompass all forms of record keeping. The article provides general\nobservations on Bearman\'s archival imagination, lists some of its\ncomponents, and addresses elements of Henry\'s critique. Although the\nlong lasting impact of Bearman\'s imagination upon the archival\nprofession might be questioned, it nonetheless deserves continued\nconsideration by archivists and inclusion as a component of graduate\narchival education\n', ['archival imagination', 'David Bearman', 'archival theory', 'classical American archivist', 'Schellenberg', 'archival management', 'electronic records', 'record keeping', 'archival profession', 'graduate archival education', 'information retrieval systems', 'professional aspects', 'records management']), ('Predictive control of a high temperature-short time pasteurisation process\nModifications on the dynamic matrix control (DMC) algorithm are presented to\ndeal with transfer functions with varying parameters in order to\ncontrol a high temperature-short time pasteurisation process. To\ncontrol processes with first order with pure time delay models whose\nparameters present an exogenous variable dependence, a new method of\nfree response calculation, using multiple model information, is\ndeveloped. Two methods, to cope with those nonlinear models that allow\na generalised Hammerstein model description, are proposed. The proposed\nmethods have been tested, both in simulation and in real cases, in\ncomparison with PID and DMC classic controllers, showing important\nimprovements on reference tracking and disturbance rejection\n', ['high temperature-short time pasteurisation process', 'predictive control', 'dynamic matrix control algorithm', 'transfer functions', 'first order processes', 'time delay models', 'exogenous variable dependence', 'free response calculation', 'multiple model information', 'nonlinear models', 'generalised Hammerstein model description', 'reference tracking', 'disturbance rejection', 'delays', 'food processing industry', 'nonlinear control systems', 'predictive control', 'step response', 'transfer function matrices']), ('Adaptive multiresolution approach for solution of hyperbolic PDEs\nThis paper establishes an innovative and efficient multiresolution adaptive\napproach combined with high-resolution methods, for the numerical\nsolution of a single or a system of partial differential equations. The\nproposed methodology is unconditionally bounded (even for hyperbolic\nequations) and dynamically adapts the grid so that higher spatial\nresolution is automatically allocated to domain regions where strong\ngradients are observed, thus possessing the two desired properties of a\nnumerical approach: stability and accuracy. Numerical results for five\ntest problems are presented which clearly show the robustness and cost\neffectiveness of the proposed method\n', ['multiresolution adaptive approach', 'high-resolution methods', 'numerical solution', 'hyperbolic partial differential equations', 'dynamic grid adaptation', 'unconditionally bounded methodology', 'spatial resolution', 'strong gradients', 'stability', 'accuracy', 'robustness', 'cost effectiveness', 'hyperbolic equations', 'numerical stability', 'partial differential equations']), ('The acquisition of out-of-print music\nNon-specialist librarians are alerted to factors important in the successful\nacquisition of out-of-print music, both scholarly editions and\nperformance editions. The appropriate technical music vocabulary, the\nmusic publishing industry, specialized publishers and vendors, and\nmethods of acquisition of out-of-print printed music are introduced,\nand the need for familiarity with them is emphasized\n', ['out-of-print music', 'scholarly editions', 'performance editions', 'technical music vocabulary', 'music publishing industry', 'specialized publishers', 'specialized vendors', 'out-of-print printed music', 'music', 'special libraries', 'vocabulary']), ('Solving the multiple competitive facilities location problem\nIn this paper we propose five heuristic procedures for the solution of the\nmultiple competitive facilities location problem. A franchise of\nseveral facilities is to be located in a trade area where competing\nfacilities already exist. The objective is to maximize the market share\ncaptured by the franchise as a whole. We perform extensive\ncomputational tests and conclude that a two-step heuristic procedure\ncombining simulated annealing and an ascent algorithm provides the best\nsolutions\n', ['multiple competitive facilities location problem', 'heuristic procedures', 'facilities franchise', 'market share maximization', 'computational tests', 'two-step heuristic procedure', 'simulated annealing', 'ascent algorithm', 'facility location', 'heuristic programming', 'marketing', 'retailing', 'simulated annealing']), ('FLID-DL: congestion control for layered multicast\nWe describe fair layered increase/decrease with dynamic layering (FLID-DL): a\nnew multirate congestion control algorithm for layered multicast\nsessions. FLID-DL generalizes the receiver-driven layered congestion\ncontrol protocol (RLC) introduced by Vicisano et al. (Proc. IEEE\nINFOCOM, San Francisco, CA, , p.996-1003, Mar. 1998)ameliorating the\nproblems associated with large Internet group management protocol\n(IGMP) leave latencies and abrupt rate increases. Like RLC, FLID-DL, is\na scalable, receiver-driven congestion control mechanism in which\nreceivers add layers at sender-initiated synchronization points and\nleave layers when they experience congestion. FLID-DL congestion\ncontrol coexists with transmission control protocol (TCP) flows as well\nas other FLID-DL sessions and supports general rates on the different\nmulticast layers. We demonstrate via simulations that our congestion\ncontrol scheme exhibits better fairness properties and provides better\nthroughput than previous methods. A key contribution that enables\nFLID-DL and may be useful elsewhere is dynamic layering (DL), which\nmitigates the negative impact of long IGMP leave latencies and\neliminates the need for probe intervals present in RLC. We use DL to\nrespond to congestion much faster than IGMP leave operations, which\nhave proven to be a bottleneck in practice for prior work\n', ['FLID-DL', 'congestion control', 'fair layered increase/decrease with dynamic layering', 'multirate congestion control algorithm', 'layered multicast sessions', 'receiver-driven layered congestion control protocol', 'Internet group management protocol', 'scalable congestion control', 'IGMP', 'transmission control protocol', 'sender-initiated synchronization', 'multicast layers', 'simulations', 'throughput', 'dynamic layering', 'Internet protocol multicast', 'TCP fairness', 'digital simulation', 'multicast communication', 'telecommunication congestion control', 'transport protocols']), ('Local search with constraint propagation and conflict-based heuristics\nSearch algorithms for solving CSP (Constraint Satisfaction Problems) usually\nfall into one of two main families: local search algorithms and\nsystematic algorithms. Both families have their advantages. Designing\nhybrid approaches seems promising since those advantages may be\ncombined into a single approach. In this paper, we present a new hybrid\ntechnique. It performs a local search over partial assignments instead\nof complete assignments, and uses filtering techniques and\nconflict-based techniques to efficiently guide the search. This new\ntechnique benefits from both classical approaches: a priori pruning of\nthe search space from filtering-based search and possible repair of\nearly mistakes from local search. We focus on a specific version of\nthis technique: tabu decision-repair. Experiments done on open-shop\nscheduling problems show that our approach competes well with the best\nhighly specialized algorithms\n', ['search algorithms', 'CSP', 'Constraint Satisfaction Problems', 'local search algorithms', 'systematic algorithms', 'partial assignments', 'filtering techniques', 'tabu decision-repair', 'constraint handling', 'search problems']), ('A genetic approach to the optimization of automatic generation control\nparameters for power systems\nThis paper presents a method based on genetic algorithm for the automatic\ngeneration control of power systems. The technique is applied to\ncontrol a system, which includes two areas tied together through a\npower line. As a consequence of continuous load variation, the\nfrequency of the power system changes with time. In conventional\nstudies, frequency transients are minimized by using integral\ncontrollers and thus zero steady-state error is obtained. In this\npaper, integral controller gains and frequency bias factors are\ndetermined by using the genetic algorithm. The results of simulation\nreveal the application of the genetic algorithm having easy\nimplementation to find the global optimum values of the control\nparameters\n', ['power systems automatic generation control parameters optimization', 'genetic algorithm', 'power line', 'control design', 'interconnected power networks', 'continuous load variation', 'frequency transients', 'integral controller gains', 'frequency bias factors', 'control simulation', 'control system analysis', 'control system synthesis', 'frequency control', 'genetic algorithms', 'optimal control', 'power generation control', 'power system interconnection', 'power system transients']), ('Fast broadcasting and gossiping in radio networks\nWe establish an O(n log/sup 2/ n) upper bound on the time for deterministic\ndistributed broadcasting in multi-hop radio networks with unknown\ntopology. This nearly matches the known lower bound of Omega (n log n).\nThe fastest previously known algorithm for this problem works in time\nO(n/sup 3/2/). Using our broadcasting algorithm, we develop an O(n/sup\n3/2/ log/sup 2/ n) algorithm for gossiping in the same network model\n', ['fast broadcasting', 'upper bound', 'deterministic distributed broadcasting', 'gossiping', 'radio networks', 'computational complexity', 'deterministic algorithms', 'radio networks']), ('Healthy, wealthy and wise? [health sector document management]\nNHS spending will rise from Pounds 65.4bn in 2002 to Pounds 87.2bn in 2006, and\nby 2008, spending will total Pounds 105.6bn. David Tyler looks at how\nthe health sector is already beginning to exploit IT, and particularly\ndocument management, to improve service and cut costs\n', ['NHS spending', 'document management', 'IT', 'ScanSoft PaperPort', 'document image processing', 'health care']), ('Some recent advances in validated methods for IVPs for ODEs\nCompared to standard numerical methods for initial value problems (IVPs) for\nordinary differential equations (ODEs), validated methods (often called\ninterval methods) for IVPs for ODEs have two important advantages: if\nthey return a solution to a problem, then (1) the problem is guaranteed\nto have a unique solution, and (2) an enclosure of the true solution is\nproduced. We present a brief overview of interval Taylor series (ITS)\nmethods for IVPs for ODEs and discuss some recent advances in the\ntheory of validated methods for IVPs for ODEs. In particular, we\ndiscuss an interval Hermite-Obreschkoff (IHO) scheme for computing\nrigorous bounds on the solution of an IVP for an ODE, the stability of\nITS and IHO methods, and a new perspective on the wrapping effect,\nwhere we interpret the problem of reducing the wrapping effect as one\nof finding a more stable scheme for advancing the solution\n', ['validated methods', 'initial value problems', 'ordinary differential equations', 'interval methods', 'interval Taylor series', 'interval Hermite-Obreschkoff scheme', 'wrapping effect', 'QR algorithm', 'differential equations', 'Hermitian matrices', 'initial value problems', 'series (mathematics)']), ('Linguistic knowledge and new technologies\nModern language studies are characterized by a variety of forms, ways, and\nmethods of their development. In this connection, it is necessary to\nspecify the problem of the development of their internal\ndifferentiation and classification, which lead to the formation of\nspecific areas knowledge. An example of such an area is speechology-a\nfield of science belonging to fundamental, theoretical, and applied\nlinguistics\n', ['modern language studies', 'internal differentiation', 'internal classification', 'speechology', 'applied linguistics', 'theoretical linguistics', 'fundamental linguistics', 'linguistic knowledge', 'linguistics', 'speech recognition']), ('A mechanism for inferring approximate solutions under incomplete knowledge\nbased on rule similarity\nThis paper proposes an inference method which can obtain an approximate\nsolution even if the knowledge stored in the problem-solving system is\nincomplete. When a rule needed for solving the problem does not exist,\nthe problem can be solved by using rules similar to the existing rules.\nIn an implementation using the SLD procedure, a resolution is executed\nbetween a subgoal and a rule if an atom of the subgoal is similar to\nthe consequence atom of the rule. Similarities between atoms are\ncalculated using a knowledge base of words with account of the\nreasoning situation, and the reliability of the derived solution is\ncalculated based on these similarities. If many solutions are obtained,\nthey are grouped into classes of similar solutions and a representative\nsolution is then selected for each class. The proposed method was\nverified experimentally by solving simple problems\n', ['inference method', 'approximate solution', 'incomplete knowledge', 'rule similarity', 'problem solving', 'SLD procedure', 'subgoal atom', 'consequence atom', 'word knowledge base', 'reasoning', 'reliability', 'representative solution', 'common sense knowledge', 'common-sense reasoning', 'knowledge based systems', 'knowledge representation', 'problem solving', 'uncertainty handling']), ('Groove Networks. Matching technology with human needs\nIf what has been occurring in information technology during the past decade or\nso can be classified as the "Information Age," then going forward, I\nbelieve it\'s going to be viewed more as the "connection age"," says Ray\nOzzie, CEO and chairman of Groove Networks, the Beverly, Massachusetts\ncompany that produces collaboration software. "We\'re all going to be\nthinking more about the connections between people and the connections\nbetween companies," Ozzie says. "Our mission has two parts: to help\nbusinesses achieve a greater "return on connection" from their\nrelationships with customers, vendors, and partners; and to help\nindividuals strengthen online connections with the people with whom\nthey interact."\n', ['businesses', 'online connections', 'server products', 'Groove Networks', 'organizational perspective', 'personal perspective', 'online collaboration', 'knowledge work', 'collaborative technologies', 'inking technology', 'groupware', 'human factors', 'user interfaces']), ("L/sub p/ stability and linearization\nA theorem by Hadamard gives a two-part condition under which a map from one\nBanach space to another is a homeomorphism. The theorem, while often\nvery useful, is incomplete in the sense that it does not explicitly\nspecify the family of maps for which the condition is met. Recently,\nunder a typically weak additional assumption on the map, it was shown\nthat Hadamard's condition is met if and only if the map is a\nhomeomorphism with a Lipschitz continuous inverse. Here, an application\nis given concerning the relation between the L/sub p/ stability (with 1\n<or= p < infinity ) of a nonlinear system and the stability of\nrelated linear systems. We also give a result that directs attention to\na fundamental limitation concerning what can be proved about\nlinearization and stability for a related familiar family of feedback\nsystems\n", ['Banach space', "Hadamard's condition", 'Lipschitz continuous inverse', 'L/sub p/ stability', 'nonlinear system', 'linear systems', 'feedback systems', 'feedback', 'input-output stability', 'linearisation techniques', 'nonlinear systems']), ('Electronic reserves at University College London: understanding the needs of\nacademic departments\nThis article describes a recent project at University College London to explore\nthe feasibility of providing a service to improve access to electronic\ncourse materials. Funded by the Higher Education Funding Council for\nEngland (HEFCE), the project was not simply to set up an electronic\nreserve. By undertaking a needs analysis of academic departments, the\nproject was able to tailor the design of the new service appropriately.\nWhile new initiatives in libraries are often established using project\nfunding, this work was unique in being research-led. It also involved\ncollaboration between library and computing staff and learning\ntechnologists\n', ['University College London', 'electronic course materials', 'electronic reserves', 'academic department needs', 'Higher Education Funding Council for England', 'computing staff', 'learning technologists', 'academic libraries', 'information needs', 'library automation']), ('Risk theory with a nonlinear dividend barrier\nIn the framework of classical risk theory we investigate a surplus process in\nthe presence of a nonlinear dividend barrier and derive equations for\ntwo characteristics of such a process, the probability of survival and\nthe expected sum of discounted dividend payments. Number-theoretic\nsolution techniques are developed for approximating these quantities\nand numerical illustrations are given for exponential claim sizes and a\nparabolic dividend barrier\n', ['risk theory', 'nonlinear dividend barrier', 'surplus process', 'probability of survival', 'discounted dividend payments', 'number-theoretic solution', 'numerical illustrations', 'exponential claim sizes', 'parabolic dividend barrier', 'mean square error methods', 'Monte Carlo methods', 'probability', 'risk management', 'simulation', 'stochastic processes']), ('Sensitivity calibration of ultrasonic detectors based using ADD diagrams\nThe paper considers basic problems related to utilization of ADD diagrams in\ncalibrating sensitivity of ultrasonic detectors. We suggest that a\nconvenient tool for solving such problems can be the software package\nADD Universal. Version 2.1 designed for plotting individual ADD\ndiagrams for normal and slanted transducers. The software is compatible\nwith the contemporary operational system Windows-95(98). Reference\nsignals for calibration are generated in a sample with cylindrical\nholes\n', ['ADD diagrams', 'ultrasonic detectors', 'software package', 'slanted transducers', 'normal transducers', 'contemporary operational system Windows-95(98', 'calibration', 'cylindrical holes', 'reference signals', 'sensitivity calibration', 'ultrasonic testing', 'calibration', 'physics computing', 'sensitivity', 'software packages', 'ultrasonic materials testing', 'ultrasonic transducers']), ('Control of integral processes with dead-time. 1. Disturbance observer-based 2\nDOF control scheme\nA disturbance observer-based control scheme (a version of 2 DOF internal model\ncontrol) which is very effective in controlling integral processes with\ndead time is presented. The controller can be designed to reject ramp\ndisturbances as well as step disturbances and even arbitrary\ndisturbances. When the plant model is available only two parameters are\nleft to tune. One is the time constant of the set-point response and\nthe other is the time constant of the disturbance response. The latter\nis tuned according to the compromise between disturbance response and\nrobustness. This control scheme has a simple, clear, easy-to-design,\neasy-to-implement structure and good performance. It is compared to the\nbest results (so far) using some simulation examples\n', ['integral processes', 'dead-time', 'disturbance observer-based 2 DOF control scheme', '2 DOF internal model control', 'ramp disturbances rejection', 'set-point response', 'time constant', 'disturbance response', 'robustness', 'control system synthesis', 'observers', 'robust control', 'transfer functions', 'tuning']), ('A collocation formulation of multistep methods for variable step-size\nextensions\nMultistep methods are classically constructed by specially designed difference\noperators on an equidistant time grid. To make them practically useful,\nthey have to be implemented by varying the step-size according to some\nerror-control algorithm. It is well known how to extend Adams and BDF\nformulas to a variable step-size formulation. In this paper we present\na collocation approach to construct variable step-size formulas. We\nmake use of piecewise polynomials to show that every k-step method of\norder k+1 has a variable step-size polynomial collocation formulation\n', ['multistep methods', 'variable step-size extensions', 'collocation formulation', 'difference operators', 'equidistant time grid', 'error-control algorithm', 'piecewise polynomials', 'k-step method', 'variable step-size polynomial collocation formulation', 'difference equations', 'iterative methods', 'mathematical operators', 'piecewise polynomial techniques']), ('Document-based workflow modeling: a case-based reasoning approach\nA workflow model is useful for business process analysis. A well-built workflow\ncan help a company streamline its internal processes by reducing\noverhead. The results of workflow modeling need to be managed as\ninformation assets in a systematic fashion. Reusing these results is\nlikely to enhance the quality of the modeling. Therefore, this paper\nproposes a document-based workflow modeling mechanism, which employs a\ncase-based reasoning (CBR) technique for the effective reuse of design\noutputs. A repository is proposed to support this CBR process. A\nreal-life case is illustrated to demonstrate the usefulness of our\napproach\n', ['document-based workflow modeling', 'case-based reasoning', 'business process analysis', 'company', 'information assets', 'design output reuse', 'business data processing', 'case-based reasoning', 'document handling', 'workflow management software']), ('Using constructed types in C++ unions\nThe C++ Standard states that a union type cannot have a member with a\nnontrivial constructor or destructor. While at first this seems\nunreasonable, further thought makes it clear why this is the case: The\ncrux of the problem is that unions don\'t have built-in semantics for\ndenoting when a member is the "current" member of the union. Therefore,\nthe compiler can\'t know when it\'s appropriate to call constructors or\ndestructors on the union members. Still, there are good reasons for\nwanting to use constructed object types in a union. For example, you\nmight want to implement a scripting language with a single variable\ntype that can either be an integer, a string, or a list. A union is the\nperfect candidate for implementing such a composite type, but the\nrestriction on constructed union members may prevent you from using an\nexisting string or list class (for example, from the STL) to provide\nthe underlying functionality. Luckily, a feature of C++ called\nplacement new can provide a workaround\n', ['C++ Standard', 'union type', 'constructors', 'destructors', 'union members', 'scripting language', 'placement new', 'C++ language', 'type theory']), ('HPCVIEW: a tool for top-down analysis of node performance\nIt is increasingly difficult for complex scientific programs to attain a\nsignificant fraction of peak performance on systems that are based on\nmicroprocessors with substantial instruction-level parallelism and deep\nmemory hierarchies. Despite this trend, performance analysis and tuning\ntools are still not used regularly by algorithm and application\ndesigners. To a large extent, existing performance tools fail to meet\nmany user needs and are cumbersome to use. To address these issues, we\ndeveloped HPCVIEW - a toolkit for combining multiple sets of program\nprofile data, correlating the data with source code, and generating a\ndatabase that can be analyzed anywhere with a commodity Web browser. We\nargue that HPCVIEW addresses many of the issues that have limited the\nusability and the utility of most existing tools. We originally built\nHPCVIEW to facilitate our own work on data layout and optimizing\ncompilers. Now, in addition to daily use within our group, HPCVIEW is\nbeing used by several code development teams in DoD and DoE\nlaboratories as well as at NCSA\n', ['HPCView', 'top-down analysis', 'node performance', 'complex scientific programs', 'peak performance', 'instruction-level parallelism', 'deep memory hierarchies', 'performance analysis', 'source code', 'commodity Web browser', 'data layout', 'optimizing compilers', 'software tools', 'binary analysis', 'optimising compilers', 'software performance evaluation', 'software tools']), ('Loudspeaker voice-coil inductance losses: circuit models, parameter estimation,\nand effect on frequency response\nWhen the series resistance is separated and treated as a separate element, it\nis shown that losses in an inductor require the ratio of the flux to\nMMF in the core to be frequency dependent. For small-signal operation,\nthis dependence leads to a circuit model composed of a lossless\ninductor and a resistor in parallel, both of which are frequency\ndependent. Mathematical expressions for these elements are derived\nunder the assumption that the ratio of core flux to MMF varies as omega\n/sup n-1/, where n is a constant. A linear regression technique is\ndescribed for extracting the model parameters from measured data.\nExperimental data are presented to justify the model for the lossy\ninductance of a loudspeaker voice-coil. A SPICE example is presented to\nillustrate the effects of voice-coil inductor losses on the frequency\nresponse of a typical driver\n', ['loudspeaker voice-coil inductance losses', 'circuit models', 'parameter estimation', 'frequency response', 'series resistance', 'small-signal operation', 'linear regression', 'lossy inductance', 'SPICE', 'loudspeaker driver', 'lossless inductor', 'core flux to MMF ratio', 'frequency response', 'inductors', 'losses', 'loudspeakers', 'network parameters', 'parameter estimation', 'SPICE']), ('A formal model of correctness in a cadastre\nA key issue for cadastral systems is the maintenance of their correctness.\nCorrectness is defined to be the proper correspondence between the\nvalid legal situation and the content of the cadastre. This\ncorrespondence is generally difficult to achieve, since the cadastre is\nnot a complete representation of all aspects influencing the legal\nsituation in reality. The goal of the paper is to develop a formal\nmodel comprising representations of the cadastre and of reality that\nallows the simulation and investigation of cases where this\ncorrespondence is potentially violated. For this purpose the model\nconsists of two parts, the first part represents the valid legal\nsituation and the second part represents the cadastre. This makes it\nfeasible to mark the differences between reality and the cadastre. The\nmarking together with the two parts of the model facilitate the\ndiscussion of issues in "real-world" cadastral systems where\nincorrectness occurs. In order to develop a formal model, the paper\nuses the transfer of ownership of a parcel between two persons as\nminimal case study. The foundation for the formalization is a modern\nversion of the situation calculus. The focus moves from the analysis of\nthe cadastre to the preparation of a conceptual and a formalized model\nand the implementation of a prototype\n', ['formal correctness model', 'cadastre', 'cadastral systems', 'correctness maintenance', 'legal situation', 'formal model', 'transfer of ownership', 'minimal case study', 'situation calculus', 'formalized model', 'cartography', 'formal languages', 'formal verification', 'geography', 'law administration', 'logic programming', 'town and country planning']), ('Recovering lost efficiency of exponentiation algorithms on smart cards\nAt the RSA cryptosystem implementation stage, a major security concern is\nresistance against so-called side-channel attacks. Solutions are known\nbut they increase the overall complexity by a non-negligible factor\n(typically, a protected RSA exponentiation is 133% slower). For the\nfirst time, protected solutions are proposed that do not penalise the\nrunning time of an exponentiation\n', ['smart cards', 'exponentiation algorithms', 'RSA cryptosystem implementation stage', 'security', 'side-channel attack resistance', 'public-key encryption', 'computational complexity', 'public key cryptography', 'smart cards']), ('Word spotting based on a posterior measure of keyword confidence\nIn this paper, an approach of keyword confidence estimation is developed that\nwell combines acoustic layer scores and syllable-based statistical\nlanguage model (LM) scores. An a posteriori (AP) confidence measure and\nits forward-backward calculating algorithm are deduced. A zero false\nalarm (ZFA) assumption is proposed for evaluating relative confidence\nmeasures by word spotting task. In a word spotting experiment with a\nvocabulary of 240 keywords, the keyword accuracy under the AP measure\nis above 94%, which well approaches its theoretical upper limit. In\naddition, a syllable lattice Hidden Markov Model (SLHMM) is formulated\nand a unified view of confidence estimation, word spotting, optimal\npath search, and N-best syllable re-scoring is presented. The proposed\nAP measure can be easily applied to various speech recognition systems\nas well\n', ['word spotting', 'a posterior measure', 'keyword confidence', 'acoustic layer scores', 'syllable-based statistical language model scores', 'a posteriori confidence measure', 'forward-backward calculating algorithm', 'zero false alarm assumption', 'relative confidence measures', 'word spotting task', 'syllable lattice hidden Markov model', 'confidence estimation', 'optimal path search', 'N-best syllable re-scoring', 'speech recognition systems', 'hidden Markov models', 'speech recognition']), ('Application of Sugeno fuzzy-logic controller to the stator field-oriented\ndoubly-fed asynchronous motor drive\nThis study deals with the application of the fuzzy-control theory to\nwound-rotor asynchronous motor with both its stator and rotor fed by\ntwo PWM voltage-source inverters, in which the system operates in\nstator field-oriented control. Thus, after determining the model of the\nmachine, we present two types of fuzzy controller: Mamdani and Sugeno\ncontrollers. The training of the last one is carried out starting from\nthe first. Simulation study is conducted to show the effectiveness of\nthe proposed method\n', ['machine modelling', 'Sugeno fuzzy-logic controller', 'stator field-oriented doubly-fed asynchronous motor drive', 'fuzzy-control', 'wound-rotor asynchronous motor', 'PWM voltage-source inverters', 'stator field-oriented control', 'Mamdani controller', 'training', 'speed regulation', 'angular velocity control', 'fuzzy control', 'induction motor drives', 'machine theory', 'machine vector control', 'PWM invertors', 'rotors', 'stators']), ('Implications of document-level literacy skills for Web site design\nThe proliferation of World Wide Web (Web) sites and the low cost of publishing\ninformation on the Web have placed a tremendous amount of information\nat the fingertips of millions of people. Although most of this\ninformation is at least intended to be accurate, there is much that is\nrumor, innuendo, urban legend, and outright falsehood. This raises\nproblems especially for students (of all ages) trying to do research or\nlearn about some topic. Finding accurate, credible information requires\ndocument level literacy skills, such as integration, sourcing,\ncorroboration, and search. This paper discusses these skills and offers\na list of simple ways that designers of educational Web sites can help\ntheir visitors utilize these skills\n', ['document-level literacy skills', 'rumor', 'innuendo', 'urban legend', 'falsehood', 'students', 'accurate credible information', 'integration', 'sourcing', 'corroboration', 'search', 'educational Web site design', 'computer aided instruction', 'data integrity', 'information resources', 'information retrieval', 'user interfaces']), ('Regression testing of database applications\nDatabase applications features such as Structured Query Language or SQL,\nexception programming, integrity constraints, and table triggers pose\ndifficulties for maintenance activities; especially for regression\ntesting that follows modifications to database applications. In this\nwork, we address these difficulties and propose a two phase regression\ntesting methodology. In phase 1, we explore control flow and data flow\nanalysis issues of database applications. Then, we propose an impact\nanalysis technique that is based on dependencies that exist among the\ncomponents of database applications. This analysis leads to selecting\ntest cases from the initial test suite for regression testing the\nmodified application. In phase 2, further reduction in the regression\ntest cases is performed by using reduction algorithms. We present two\nsuch algorithms. The Graph Walk algorithm walks through the control\nflow graph of database modules and selects a safe set of test cases to\nretest. The Call Graph Firewall algorithm uses a firewall for the inter\nprocedural level. Finally, a maintenance environment for database\napplications is described. Our experience with this regression testing\nmethodology shows that the impact analysis technique is adequate for\nselecting regression tests and that phase 2 techniques can be used for\nfurther reduction in the number of theses tests\n', ['database applications', 'control flow analysis', 'data flow analysis', 'reduction algorithms', 'Graph Walk algorithm', 'control flow graph', 'Call Graph Firewall algorithm', 'impact analysis', 'Structured Query Language', 'SQL', 'exception programming', 'integrity constraints', 'table triggers', 'two phase regression testing methodology', 'data flow analysis', 'data flow graphs', 'data integrity', 'database management systems', 'program testing', 'software maintenance', 'SQL']), ('The chromatic spectrum of mixed hypergraphs\nA mixed hypergraph is a triple H = (X, C, D), where X is the vertex set, and\neach of C, D is a list of subsets of X. A strict k-coloring of H is a\nsurjection c : X {1,..., k} such that each member of le has two\nvertices assigned a common value and each member of D has two vertices\nassigned distinct values. The feasible set of H is {k: H has a strict\nk-coloring}. Among other results, we prove that a finite set of\npositive integers is the feasible set of some mixed hypergraph if and\nonly if it omits the number I or is an interval starting with 1. For\nthe set {s, t} with 2 <or= s <or= t - 2, the smallest realization\nhas 2t - s vertices. When every member of C union D is a single\ninterval in an underlying linear order on the vertices, the feasible\nset is also a single interval of integers\n', ['chromatic spectrum', 'mixed hypergraphs', 'vertex set', 'strict k-coloring', 'positive integers', 'mixed hypergraph', 'graph colouring']), ('Individual decision making using fuzzy set theory\nThe paper shows the importance of decision making by an individual and\nhighlights the prime domain of decision making where fuzzy set theory\ncan be used as a tool. Fuzzy set theory has been used on rational model\nof decision making to arrive at the desired conclusion\n', ['individual decision making', 'fuzzy set theory', 'rational decision making model', 'decision theory', 'fuzzy set theory']), ('Dynamic testing of inflatable structures using smart materials\nIn this paper we present experimental investigations of the vibration testing\nof an inflated, thin-film torus using smart materials. Lightweight,\ninflatable structures are very attractive in satellite applications.\nHowever, the lightweight, flexible and highly damped nature of inflated\nstructures poses difficulties in ground vibration testing. In this\nstudy, we show that polyvinylidene fluoride (PVDF) patches and recently\ndeveloped macro-fiber composite actuators may be used as sensors and\nactuators in identifying modal parameters. Both smart materials can be\nintegrated unobtrusively into the skin of a torus or space device\nforming an attractive testing arrangement. The addition of actuators\nand PVDF sensors to the torus does not significantly interfere with the\nsuspension modes of a free-free boundary condition, and can be\nconsidered an integral part of the inflated structure. The results\nindicate the potential of using smart materials to measure and control\nthe dynamic response of inflated structures\n', ['thin-film torus', 'smart materials', 'satellite applications', 'inflated structures', 'ground vibration testing', 'polyvinylidene fluoride patches', 'PVDF sensors', 'macro-fiber composite actuators', 'modal parameters', 'space device', 'boundary condition', 'dynamic response', 'Kapton torus', 'aerospace instrumentation', 'aerospace testing', 'dynamic testing', 'intelligent actuators', 'intelligent materials', 'intelligent sensors', 'intelligent structures', 'polymer films']), ("MEMS applications in computer disk drive dual-stage servo systems\nWe present a decoupled discrete time pole placement design method, which can be\ncombined with a self-tuning scheme to compensate variations in the\nmicroactuator's (MA's) resonance mode. Section I of the paper describes\nthe design and fabrication of a prototype microactuator with an\nintegrated gimbal structure. Section II presents a decoupled\ntrack-following controller design and a self-tuning control scheme to\ncompensate for the MA's resonance mode variations\n", ['computer disk drive dual-stage servo systems', 'MEMS', 'microactuator', 'servo control', 'hard disk drives', 'decoupled discrete time pole placement design method', 'self-tuning scheme', 'electrostatic design', 'fabrication process', 'track-following controller design', 'compensation', 'control system synthesis', 'disc drives', 'discrete time systems', 'hard discs', 'microactuators', 'pole assignment', 'position control', 'self-adjusting systems']), ('High-speed CMOS circuits with parallel dynamic logic and speed-enhanced skewed\nstatic logic\nIn this paper, we describe parallel dynamic logic (PDL) which exhibits high\nspeed without charge sharing problem. PDL uses only parallel-connected\ntransistors for fast logic evaluation and is a good candidate for\nhigh-speed low-voltage operation. It has less back-bias effect compared\nto other logic styles, which use stacked transistors. Furthermore, PDL\nneeds no signal ordering or tapering. PDL with speed-enhanced skewed\nstatic logic renders straightforward logic synthesis without the usual\narea penalty due to logic duplication. Our experimental results on two\n32-bit carry lookahead adders using 0.25- mu m CMOS technology show\nthat PDL with speed-enhanced skewed static (SSS) look reduces the delay\nover clock-delayed(CD)-domino by 15%-27% and the power-delay product by\n20%-37%\n', ['high-speed CMOS circuits', 'parallel dynamic logic', 'speed-enhanced skewed static logic', 'parallel-connected transistors', 'low-voltage operation', 'logic synthesis', 'carry lookahead adders', 'delay', 'power-delay product', 'back-bias effect', 'stacked transistors', '32 bit', '0.25 micron', 'adders', 'carry logic', 'CMOS logic circuits', 'delays', 'high-speed integrated circuits', 'logic CAD', 'low-power electronics']), ('Vibration control of structure by using tuned mass damper (development of\nsystem which suppress displacement of auxiliary mass)\nIn vibration control of a structure by using an active tuned mass damper\n(ATMD), stroke of the auxiliary mass is so limited that it is difficult\nto control the vibration in the case of large disturbance input. In\nthis paper, two methods are proposed for the problem. One of the\nmethods is a switching control system by two types of controllers. One\nof the controllers is a normal controller under small relative\ndisplacement of the auxiliary mass, and the other is not effective only\nfor first mode of vibration under large relative displacement of the\nauxiliary mass. New variable gain control system is constructed by\nswitching these two controllers. The other method is the brake system.\nIn active vibration control, it is necessary to use actuator for active\ncontrol. By using the actuator, the proposed system puts on the brake\nto suppress displacement increase of the auxiliary mass under large\ndisturbance input. Finally, the systems are designed and the\neffectiveness of the systems is confirmed by the simulation\n', ['tuned mass damper', 'vibration control', 'variable gain control system', 'brake system', 'actuator', 'active control', 'auxiliary mass displacement suppression', 'controllers', 'active noise control', 'actuators', 'brakes', 'controllers', 'damping', 'gain control', 'vibration control']), ('New methods for oscillatory problems based on classical codes\nThe numerical integration of differential equations with oscillatory solutions\nis a very common problem in many fields of the applied sciences. Some\nmethods have been specially devised for this kind of problem. In most\nof them, the calculation of the coefficients needs more computational\neffort than the classical codes because such coefficients depend on the\nstep-size in a not simple manner. On the contrary, in this work we\npresent new algorithms specially designed for perturbed oscillators\nwhose coefficients have a simple dependence on the step-size. The\nmethods obtained are competitive when comparing with classical and\nspecial codes\n', ['oscillatory problems', 'classical codes', 'numerical integration', 'differential equations', 'oscillatory solutions', 'perturbed oscillators', 'differential equations', 'initial value problems']), ("An accurate COG defuzzifier design using Lamarckian co-adaptation of learning\nand evolution\nThis paper proposes a design technique of optimal center of gravity (COG)\ndefuzzifier using the Lamarckian co-adaptation of learning and\nevolution. The proposed COG defuzzifier is specified by various design\nparameters such as the centers, widths, and modifiers of MFs. The\ndesign parameters are adjusted with the Lamarckian co-adaptation of\nlearning and evolution, where the learning performs a local search of\ndesign parameters in an individual COG defuzzifier, but the evolution\nperforms a global search of design parameters among a population of\nvarious COG defuzzifiers. This co-adaptation scheme allows to evolve\nmuch faster than the non-learning case and gives a higher possibility\nof finding an optimal solution due to its wider searching capability.\nAn application to the truck backer-upper control problem of the\nproposed co-adaptive design method of COG defuzzifier is presented. The\napproximation ability and control performance are compared with those\nof the conventionally simplified COG defuzzifier in terms of the fuzzy\nlogic controller's approximation error and the average tracing\ndistance, respectively\n", ['optimal center of gravity defuzzifier', 'learning', 'evolution', 'fuzzy logic controller', 'local search', 'control system synthesis', 'fuzzy control', 'learning (artificial intelligence)', 'parameter estimation']), ("Fault diagnosis and fault tolerant control of linear stochastic systems with\nunknown inputs\nThis paper presents an integrated robust fault detection and isolation (FDI)\nand fault tolerant control (FTC) scheme for a fault in actuators or\nsensors of linear stochastic systems subjected to unknown inputs\n(disturbances). As usual in this kind of works, it is assumed that\nsingle fault occurs at a time and the fault treated is of random bias\ntype. The FDI module is constructed using banks of robust two-stage\nKalman filters, which simultaneously estimate the state and the fault\nbias, and generate residual sets decoupled from unknown disturbances.\nAll elements of residual sets are evaluated by using a hypothesis\nstatistical test, and the fault is declared according to the prepared\ndecision logic. The FTC module is activated based on the fault\nindicator, and additive compensation signal is computed using the fault\nbias estimate and combined to the nominal control law for compensating\nthe fault's effect on the system. Simulation results for the simplified\nlongitudinal flight control system with parameter variations, process\nand measurement noises demonstrate the effectiveness of the approach\nproposed\n", ['fault detection', 'fault isolation', 'fault tolerant control', 'linear systems', 'stochastic systems', 'two-stage Kalman filters', 'state estimation', 'longitudinal flight control system', 'robust control', 'discrete-time system', 'discrete time systems', 'fault diagnosis', 'fault tolerance', 'Kalman filters', 'linear systems', 'robust control', 'state estimation', 'stochastic systems']), ('Development of a computer-aided manufacturing system for profiled edge\nlamination tooling\nProfiled edge lamination (PEL) tooling is a promising rapid tooling (RT) method\ninvolving the assembly of an array of laminations whose top edges are\nsimultaneously profiled and beveled based on a CAD model of the\nintended tool surface. To facilitate adoption of this RT method by\nindustry, a comprehensive PEL tooling development system is proposed.\nThe two main parts of this system are: (1) iterative tool design based\non thermal and structural models; and (2) fabrication of the tool using\na computer-aided manufacturing (CAM) software and abrasive water jet\ncutting. CAM software has been developed to take lamination slice data\n(profiles) from any proprietary RP software in the form of polylines\nand create smooth, kinematically desirable cutting trajectories for\neach tool lamination. Two cutting trajectory algorithms, called\nidentical equidistant profile segmentation and adaptively vector\nprofiles projection (AVPP), were created for this purpose. By comparing\nthe performance of both algorithms with a benchmark part shape, the\nAVPP algorithm provided better cutting trajectories for complicated\ntool geometries. A 15-layer aluminum PEL tool was successfully\nfabricated using a 5-axis CNC AWJ cutter and NC code generated by the\nCAM software\n', ['computer aided manufacturing', 'profiled edge lamination tooling', 'rapid tooling', 'abrasive water jet cutting', 'CAM software', 'cutting trajectory algorithms', 'identical equidistant profile segmentation', 'adaptively vector profiles projection', 'CAD/CAM', 'computational geometry', 'computerised numerical control', 'cutting', 'rapid prototyping (industrial)']), ('Average optimization of the approximate solution of operator equations and its\napplication\nIn this paper, a definition of the optimization of operator equations in the\naverage case setting is given. And the general result about the\nrelevant optimization problem is obtained. This result is applied to\nthe optimization of approximate solution of some classes of integral\nequations\n', ['operator equations', 'optimization', 'average case setting', 'integral equations', 'Gaussian measure', 'integral n-width', 'Fredholm integral equations', 'mathematical operators', 'optimisation']), ('High-performance servo systems based on multirate sampling control\nIn this paper, novel multirate two-degree-of-freedom controllers are proposed\nfor digital control systems, in which the sampling period of plant\noutput is restricted to be relatively longer than the control period of\nplant input. The proposed feedforward controller assures perfect\ntracking at M inter-sampling points. On the other hand, the proposed\nfeedback controller assures perfect disturbance rejection at M\ninter-sample points in the steady state. Illustrative examples of\nposition control for hard disk drive are presented, and advantages of\nthese approaches are demonstrated\n', ['multirate sampling control', 'servo system', 'digital control systems', 'tracking', 'feedback', 'feedforward', 'position control', 'hard disk drive', 'disturbance rejection', 'digital control', 'disc drives', 'feedback', 'feedforward', 'position control', 'sampled data systems', 'servomechanisms', 'tracking']), ('Application of an internally consistent material model to determine the effect\nof tool edge geometry in orthogonal machining\nIt is well known that the edge geometry of a cutting tool affects the forces\nmeasured in metal cutting. Two experimental methods have been suggested\nin the past to extract the ploughing (non-cutting) component from the\ntotal measured force: (1) the extrapolation approach, and (2) the dwell\nforce technique. This study reports the behavior of zinc during\northogonal machining using tools of controlled edge radius.\nApplications of both the extrapolation and dwell approaches show that\nneither produces an analysis that yields a material response consistent\nwith the known behavior of zinc. Further analysis shows that the edge\ngeometry modifies the shear zone of the material and thereby modifies\nthe forces. When analyzed this way, the measured force data yield the\nexpected material response without requiring recourse to an additional\nploughing component\n', ['tool edge geometry', 'cutting tool', 'metal cutting', 'ploughing component', 'extrapolation', 'dwell force', 'zinc', 'edge geometry', 'orthogonal machining', 'cutting', 'extrapolation', 'machine tools', 'machining']), ('Improving computer security for authentication of users: influence of proactive\npassword restrictions\nEntering a user name-password combination is a widely used procedure for\nidentification and authentication in computer systems. However, it is a\nnotoriously weak method, in that the passwords adopted by many users\nare easy to crack. In an attempt to, improve security, proactive\npassword checking may be used, in which passwords must meet several\ncriteria to be more resistant to cracking. In two experiments, we\nexamined the influence of proactive password restrictions on the time\nthat it took to generate an acceptable password and to use it\nsubsequently to log in. The required length was a minimum of five\ncharacters in experiment I and eight characters in experiment 2. In\nboth experiments, one condition had only the length restriction, and\nthe other had additional restrictions. The additional restrictions\ngreatly increased the time it took to generate the password but had\nonly a small effect on the time it took to use it subsequently to log\nin. For the five-character passwords, 75% were cracked when no other\nrestrictions were imposed, and this was reduced to 33% with the\nadditional restrictions. For the eight-character passwords, 17% were\ncracked with no other restrictions, and 12.5% with restrictions. The\nresults indicate that increasing the minimum character length reduces\ncrackability and increases security, regardless of whether additional\nrestrictions are imposed\n', ['computer security', 'user authentication', 'proactive password checking', 'proactive password restrictions', 'length restriction', 'five-character passwords', 'eight-character passwords', 'authorisation', 'message authentication']), ('Virtual projects at Halden [Reactor Project]\nThe Halden man-machine systems (MMS) programme for 2002 is intended to address\nissues related to human factors, control room design, computer-based\nsupport system areas and system safety and reliability. The Halden MMS\nprogramme is intended to address extensive experimental work in the\nhuman factors, control room design and computer-based support system\nareas. The work is based on experiments and demonstrations carried out\nin the experimental facility HAMMLAB. Pilot-versions of several\noperator aids are adopted and integrated to the HAMMLAB simulators and\ndemonstrated in a full dynamic setting. The Halden virtual reality\nlaboratory has recently become an integral and important part of the\nprogramme\n', ['Halden Reactor Project', 'man-machine systems programme', 'human factors', 'computer-based support system', 'safety', 'reliability', 'virtual reality', 'control room design', 'human factors', 'man-machine systems', 'nuclear engineering computing', 'virtual reality']), ('Vacuum-compatible vibration isolation stack for an interferometric\ngravitational wave detector TAMA300\nInterferometric gravitational wave detectors require a large degree of\nvibration isolation. For this purpose, a multilayer stack constructed\nof rubber and metal blocks is suitable, because it provides isolation\nin all degrees of freedom at once. In TAMA300, a 300 m interferometer\nin Japan, long-term dimensional stability and compatibility with an\nultrahigh vacuum environment of about 10/sup -6/ Pa are also required.\nTo keep the interferometer at its operating point despite ground strain\nand thermal drift of the isolation system, a thermal actuator was\nintroduced. To prevent the high outgassing rate of the rubber from\nspoiling the vacuum, the rubber blocks were enclosed by gas-tight\nbellows. Using these techniques, we have successfully developed a\nthree-layer stack which has a vibration isolation ratio of more than\n10/sup 3/ at 300 Hz with control of drift and enough vacuum\ncompatibility\n', ['vibration isolation stack', 'TAMA300 interferometer', 'interferometric gravitational wave detectors', 'rubber blocks', 'multilayer stack', 'metal blocks', 'long-term dimensional stability', 'ultrahigh vacuum environment', 'operating point', 'ground strain', 'thermal drift', 'thermal actuator', 'gas-tight bellows', 'rubber outgassing', 'vacuum compatibility', '300 m', '10/sup -6/ Pa', '300 Hz', 'actuators', 'gravitational wave detectors', 'light interferometers', 'mechanical stability', 'rubber', 'thermal stability', 'vibration isolation']), ('Centroid detection based on optical correlation\nWe propose three correlation-based methods to simultaneously detect the\ncentroids of multiple objects in an input scene. The first method is\nbased on the modulus of the moment function, the second method is based\non squaring the moment function, and the third method works with a\nsingle intensity filter. These methods are invariant to changes in the\nposition, orientation, and scale of the object and result in good\nnoise-smoothing performance. We use spatial light modulators (SLMs) to\ndirectly implement the input of the image and filter information for\nthe purpose of these approaches. We present results showing simulations\nfrom different approaches and provide comparisons between\noptical-correlation- and digital-moment-based methods. Experimental\nresults corresponding to an optical correlator using SLMs for the\ncentroid detection are also presented\n', ['optical correlation', 'centroid detection', 'correlation-based methods', 'centroids', 'multiple objects', 'input scene', 'moment function modulus', 'moment function squaring', 'single intensity filter', 'position', 'orientation', 'scale', 'noise-smoothing performance', 'spatial light modulators', 'digital-moment-based methods', 'optical correlator', 'image recognition', 'image registration', 'optical correlation', 'smoothing methods', 'spatial filters', 'spatial light modulators']), ('The set of just-in-time management strategies: an assessment of their impact on\nplant-level productivity and input-factor substitutability using\nvariable cost function estimates\nMany manufacturers in the automobile industry around the world have adopted the\njust-in-time (JIT) set of management strategies in an effort to improve\nproductivity, efficiency and product quality. The paper provides\nempirical evidence that supports the idea that JIT manufacturing\nenvironments are, in fact, more productive than their non-JIT\ncounterparts. Plant-level cross-sectional data from auto-parts\nmanufacturing firms are used to estimate variable cost functions for a\nJIT group as well as for a non-JIT group of plants. Differences in cost\nfunction characteristics between the two groups are examined and\ndiscussed\n', ['just-in-time management strategies', 'plant-level productivity', 'input-factor substitutability', 'variable cost function estimates', 'automobile industry', 'JIT', 'efficiency', 'product quality', 'auto-parts manufacturing firms', 'automobile industry', 'economics', 'production control', 'stock control']), ('Recommendations for implementing Internet inquiry projects\nThe purpose of the study presented was to provide recommendations to teachers\nwho are interested in implementing Internet inquiry projects. Four\nclasses of ninth- and tenth-grade honors students (N = 100)\nparticipated in an Internet inquiry project in which they were\npresented with an ecology question that required them to make a\ndecision based on information that they gathered, analyzed, and\nsynthesized from the Internet and their textbook. Students then\ncomposed papers with a rationale for their decision. Students in one\ngroup had access to pre-selected relevant Web sites, access to the\nentire Internet, and were provided with less online support. Students\nin the other group had access to only pre-selected relevant Web sites,\nbut were provided with more online support. Two of the most important\nrecommendations were: 1) to provide students with more online support;\nand 2) to provide students with pre-selected relevant Web sites and\nallow them to search the Internet for information\n', ['Internet inquiry projects', 'teachers', 'honors students', 'ecology question', 'pre-selected relevant Web sites', 'online support', 'biology computing', 'ecology', 'educational computing', 'information retrieval', 'Internet', 'teaching']), ('A regularized conjugate gradient method for symmetric positive definite system\nof linear equations\nA class of regularized conjugate gradient methods is presented for solving the\nlarge sparse system of linear equations of which the coefficient matrix\nis an ill-conditioned symmetric positive definite matrix. The\nconvergence properties of these methods are discussed in depth, and the\nbest possible choices of the parameters involved in the new methods are\ninvestigated in detail. Numerical computations show that the new\nmethods are more efficient and robust than both classical relaxation\nmethods and classical conjugate direction methods\n', ['regularized conjugate gradient method', 'symmetric positive definite system', 'linear equations', 'large sparse system', 'coefficient matrix', 'convergence properties', 'classical relaxation methods', 'classical conjugate direction methods', 'ill-conditioned linear system', 'conjugate gradient methods', 'sparse matrices']), ('Portfolio optimization and the random magnet problem\nDiversification of an investment into independently fluctuating assets reduces\nits risk. In reality, movements of assets are mutually correlated and\ntherefore knowledge of cross-correlations among asset price movements\nare of great importance. Our results support the possibility that the\nproblem of finding an investment in stocks which exposes invested funds\nto a minimum level of risk is analogous to the problem of finding the\nmagnetization of a random magnet. The interactions for this "random\nmagnet problem" are given by the cross-correlation matrix C of stock\nreturns. We find that random matrix theory allows us to make an\nestimate for C which outperforms the standard estimate in terms of\nconstructing an investment which carries a minimum level of risk\n', ['portfolio optimization', 'fluctuating assets', 'cross-correlations', 'price movements', 'investment', 'stocks', 'invested funds', 'magnetization', 'cross-correlation matrix', 'minimum risk level', 'spin glasses', 'random magnet problem', 'costing', 'fluctuations', 'investment', 'magnetisation', 'matrix algebra', 'optimisation', 'random processes', 'risk management', 'spin glasses', 'stock markets']), ('Real-time estimations of multi-modal frequencies for smart structures\nIn this paper, various methods for the real-time estimation of multi-modal\nfrequencies are realized in real time and compared through numerical\nand experimental tests. These parameter-based frequency estimation\nmethods can be applied to various engineering fields such as\ncommunications, radar and adaptive vibration and noise control.\nWell-known frequency estimation methods are introduced and explained.\nThe Bairstow method is introduced to find the roots of a characteristic\nequation for estimations of multi-modal frequencies, and the\ncomputational efficiency of the Bairstow method is shown\nquantitatively. For a simple numerical test, we consider two sinusoids\nof the same amplitudes mixed with various amounts of white noise. The\ntest results show that the auto regressive (AR) and auto regressive and\nmoving average (ARMA) methods are unsuitable in noisy environments. The\nother methods apart from the AR method have fast tracking capability.\nFrom the point of view of computational efficiency, the results reveal\nthat the ARMA method is inefficient, while the cascade notch filter\nmethod is very effective. The linearized adaptive notch filter and\nrecursive maximum likelihood methods have average performances.\nExperimental tests are devised to confirm the feasibility of real-time\ncomputations and to impose the severe conditions of drastically\ndifferent amplitudes and of considerable changes of natural\nfrequencies. We have performed experiments to extract the natural\nfrequencies from the vibration signal of wing-like composite plates in\nreal time. The natural frequencies of the specimen are changed by added\nmasses. Especially, the AR method exhibits a remarkable performance in\nspite of the severe conditions. This study will be helpful to anyone\nwho needs a frequency estimation algorithm for real-time applications\n', ['multi-modal frequencies', 'smart structures', 'real-time estimation', 'frequency estimation', 'adaptive vibration control', 'noise control', 'Bairstow method', 'characteristic equation', 'auto regressive and moving average methods', 'ARMA', 'cascade notch filter', 'linearized adaptive notch filter', 'recursive maximum likelihood methods', 'real-time computations', 'vibration signal', 'wing-like composite plates', 'frequency estimation algorithm', 'real-time applications', 'adaptive signal processing', 'autoregressive moving average processes', 'control system CAD', 'frequency estimation', 'intelligent structures']), ("Fusion of qualitative bond graph and genetic algorithms: A fault diagnosis\napplication\nIn this paper, the problem of fault diagnosis via integration of genetic\nalgorithms (GA's) and qualitative bond graphs (QBG's) is addressed. We\nsuggest that GA's can be used to search for possible fault components\namong a system of qualitative equations. The QBG is adopted as the\nmodeling scheme to generate a set of qualitative equations. The\nqualitative bond graph provides a unified approach for modeling\nengineering systems, in particular, mechatronic systems. In order to\ndemonstrate the performance of the proposed algorithm, we have tested\nthe proposed algorithm on an in-house designed and built floating disc\nexperimental setup. Results from fault diagnosis in the floating disc\nsystem are presented and discussed. Additional measurements will be\nrequired to localize the fault when more than one fault candidate is\ninferred. Fault diagnosis is activated by a fault detection mechanism\nwhen a discrepancy between measured abnormal behavior and predicted\nsystem behavior is observed. The fault detection mechanism is not\npresented here\n", ['qualitative bond graph', 'genetic algorithms', 'fault diagnosis', 'fault components', 'qualitative equations', 'engineering systems', 'mechatronic systems', 'floating disc', 'measured abnormal behavior', 'predicted system behavior', 'bond graphs', 'fault diagnosis', 'genetic algorithms', 'search problems']), ('Optimal linear control in stabilizer design\nThe most common method of improving stability of the power system is the\nsynthesis of the turbine and generator control systems, because of the\nhigh effectiveness and relatively low cost of these elements. The\nsynthesis and construction of the effective synchronous generator and\nturbine controller is a very difficult task. This paper describes the\nseven step mu -synthesis approach to PSS design enabling the\nsynchronous generator to remain stable over a wide range of system\noperating conditions\n', ['mu -synthesis approach', 'PSS design', 'optimal linear control', 'synchronous generator control system synthesis', 'turbine control system synthesis', 'control system synthesis', 'machine control', 'optimal control', 'power system control', 'power system stability', 'synchronous generators', 'turbines']), ('The effects of emotions on bounded rationality: a comment on Kaufman\nBruce Kaufman\'s article (1999), "Emotional arousal as a source of bounded\nrationality", objective is to present an additional source of bounded\nrationality, one that is not due to cognitive constraints, but to high\nemotional arousal. In doing so, Kaufman is following a long tradition\nof thinkers who have contrasted emotion with reason, claiming, for the\nmost part, that emotions are a violent force hindering rational\nthinking. This paper aims to challenge Kaufman\'s unidimensional idea\nregarding the connection between high emotional arousal and decision\nmaking\n', ['bounded rationality', 'decision making', 'rational thinking', 'psychology', 'emotion', 'Yerkes-Dodson law', 'psychology']), ('Stochastic optimization of acoustic response - a numerical and experimental\ncomparison\nThe objective of the work presented is to compare results from numerical\noptimization with experimental data and to highlight and discuss the\ndifferences between two fundamentally different optimization methods.\nThe problem domain is minimization of acoustic emission and the\nstructure used in the work is a closed cylinder with forced vibration\nof one end. The optimization method used in this paper is simulated\nannealing (SA), a stochastic method. The results are compared with\nthose from a gradient-based method used on the same structure in an\nearlier paper (Tinnsten, 2000)\n', ['numerical optimization', 'acoustic emission minimization', 'structure', 'closed cylinder', 'acoustic response', 'forced vibration', 'simulated annealing', 'stochastic optimization', 'gradient-based method', 'boundary-elements methods', 'finite element analysis', 'simulated annealing', 'stochastic processes', 'structural acoustics', 'structural engineering computing', 'vibrations']), ('An unconditionally stable extended (USE) finite-element time-domain solution of\nactive nonlinear microwave circuits using perfectly matched layers\nThis paper proposes an extension of the unconditionally stable finite-element\ntime-domain (FETD) method for the global electromagnetic analysis of\nactive microwave circuits. This formulation has two advantages. First,\nthe time-step size is no longer governed by the spatial discretization\nof the mesh, but rather by the Nyquist sampling criterion. Second, the\nimplementation of the truncation by the perfectly matched layers (PML)\nis straightforward. An anisotropic PML absorbing material is presented\nfor the truncation of FETD lattices. Reflection less than -50 dB is\nobtained numerically over the entire propagation bandwidth in waveguide\nand microstrip line. A benchmark test on a microwave amplifier\nindicates that this extended FETD algorithm is not only superior to\nfinite-difference time-domain-based algorithm in mesh flexibility and\nsimulation accuracy, but also reduces computation time dramatically\n', ['unconditionally stable FETD method', 'finite-element time-domain method', 'global electromagnetic analysis', 'global EM analysis', 'active nonlinear microwave circuits', 'Nyquist sampling criterion', 'time-step size', 'PML truncation', 'perfectly matched layers', 'anisotropic PML absorbing material', 'FETD lattices truncation', 'waveguide', 'microstrip line', 'microwave amplifier', 'mesh flexibility', 'simulation accuracy', 'computation time reduction', 'active networks', 'circuit simulation', 'equivalent circuits', 'finite element analysis', 'microwave amplifiers', 'microwave circuits', 'nonlinear network analysis', 'time-domain analysis']), ("Achieving performance under OpenMP on ccNUMA and software distributed shared\nmemory systems\nOpenMP is emerging as a viable high-level programming model for shared memory\nparallel systems. It was conceived to enable easy, portable application\ndevelopment on this range of systems, and it has also been implemented\non cache-coherent Non-Uniform Memory Access (ccNUMA) architectures.\nUnfortunately, it is hard to obtain high performance on the latter\narchitecture, particularly when large numbers of threads are involved.\nIn this paper, we discuss the difficulties faced when writing OpenMP\nprograms for ccNUMA systems, and explain how the vendors have attempted\nto overcome them. We focus on one such system, the SGI Origin 2000, and\nperform a variety of experiments designed to illustrate the impact of\nthe vendor's efforts. We compare codes written in a standard,\nloop-level parallel style under OpenMP with alternative versions\nwritten in a Single Program Multiple Data (SPMD) fashion, also realized\nvia OpenMP, and show that the latter consistently provides superior\nperformance. A carefully chosen set of language extensions can help us\ntranslate programs from the former style to the latter (or to compile\ndirectly, but in a similar manner). Syntax for these extensions can be\nborrowed from HPF, and some aspects of HPF compiler technology can help\nthe translation process. It is our expectation that an extended\nlanguage, if well compiled, would improve the attractiveness of OpenMP\nas a language for high-performance computation on an important class of\nmodern architectures\n", ['cache-coherent Non-Uniform Memory Access', 'OpenMP', 'programming model', 'shared memory parallel systems', 'HPF', 'Single Program Multiple Data', 'parallel programming', 'distributed shared memory systems', 'parallel programming', 'software performance evaluation']), ("Modification for synchronization of Rossler and Chen chaotic systems\nActive control is an effective method for making two identical Rossler and Chen\nsystems be synchronized. However, this method works only for a certain\nclass of chaotic systems with known parameters both in drive systems\nand response systems. Modification based on Lyapunov stability theory\nis proposed in order to overcome this limitation. An adaptive\nsynchronization controller, which can make the states of two identical\nRossler and Chen systems globally asymptotically synchronized in the\npresence of system's unknown constant parameters, is derived.\nEspecially, when some unknown parameters are positive, we can make the\ncontroller more simple, besides, the controller is independent of those\npositive uncertain parameters. At last, when the condition that\narbitrary unknown parameters in two systems are identical constants is\ncancelled, we demonstrate that it is possible to synchronize two\nchaotic systems. All results are proved using a well-known Lyapunov\nstability theorem. Numerical simulations are given to validate the\nproposed synchronization approach\n", ['synchronization', 'Rossler chaotic systems', 'Chen chaotic systems', 'active control', 'response systems', 'Lyapunov stability theory', 'adaptive synchronization controller', 'global asymptotic synchronization', 'asymptotic stability', 'chaos', 'Lyapunov matrix equations', 'nonlinear control systems', 'nonlinear dynamical systems', 'synchronisation']), ('Application of artificial intelligence to search ground-state geometry of\nclusters\nWe introduce a global optimization procedure, the neural-assisted genetic\nalgorithm (NAGA). It combines the power of an artificial neural network\n(ANN) with the versatility of the genetic algorithm. This method is\nsuitable to solve optimization problems that depend on some kind of\nheuristics to limit the search space. If a reasonable amount of data is\navailable, the ANN can "understand" the problem and provide the genetic\nalgorithm with a selected population of elements that will speed up the\nsearch for the optimum solution. We tested the method in a search for\nthe ground-state geometry of silicon clusters. We trained the ANN with\ninformation about the geometry and energetics of small silicon\nclusters. Next, the ANN learned how to restrict the configurational\nspace for larger silicon clusters. For Si/sub 10/ and Si/sub 20/, we\nnoticed that the NAGA is at least three times faster than the "pure"\ngenetic algorithm. As the size of the cluster increases, it is expected\nthat the gain in terms of time will increase as well\n', ['artificial intelligence', 'ground-state geometry', 'atomic clusters', 'global optimization procedure', 'neural-assisted genetic algorithm', 'artificial neural network', 'population', 'optimum solution', 'silicon clusters', 'Si/sub 10/', 'Si/sub 20/', 'cluster size', 'artificial intelligence', 'atomic clusters', 'genetic algorithms', 'ground states', 'molecular configurations', 'neural nets', 'optimisation', 'physics computing']), ('Theta functions with harmonic coefficients over number fields\nWe investigate theta functions attached to quadratic forms over a number field\nK. We establish a functional equation by regarding the theta functions\nas specializations of symplectic theta functions. By applying a\ndifferential operator to the functional equation, we show how theta\nfunctions with harmonic coefficients over K behave under modular\ntransformations\n', ['harmonic coefficients', 'differential operator', 'modular transformations', 'number fields', 'quadratic forms', 'functional equation', 'symplectic theta functions', 'differentiation', 'matrix algebra', 'number theory']), ("Getting the most out of intrusion detection systems\nIntrusion detection systems (IDS) can play a very valuable role in the defence\nof a network. However, it is important to understand not just what it\nwill do (and how it does it) - but what it won't do (and why). This\narticle does not go into the technical working of IDS in too much\ndetail, rather it limits itself to a discussion of some of the\ncapabilities and failings of the technology\n", ['intrusion detection systems', 'computer network security', 'network attacks', 'firewall', 'authorisation', 'computer networks', 'telecommunication security']), ('Rate allocation for video transmission over lossy correlated networks\nA novel rate allocation algorithm for video transmission over lossy networks\nsubject to bursty packet losses is presented. A Gilbert-Elliot model is\nused at the encoder to drive the selection of coding parameters.\nExperimental results using the H.26L test model show a significant\nperformance improvement with respect to the assumption of independent\npacket losses\n', ['rate allocation algorithm', 'video transmission', 'lossy correlated networks', 'bursty packet losses', 'Gilbert-Elliot model', 'coding parameters', 'H.26L test model', 'video coding', 'video coding', 'visual communication']), ('Virtual borders, real laws [Internet activity and treaties]\nNational governments are working to tame activity on the Internet. They have\nworked steadily to extend control over online activities that they\nbelieve affect their interests, even when the activities occur outside\ntheir borders. These usually involve what governments regard as their\ndomain: protecting public order, enforcing commercial laws, and,\noccasionally, protecting consumer interests. Methods have included\nassertions or legal jurisdiction based on where material is accessible\ninstead of where it originates, and the blocking of sites, service\nproviders, or entire high level domains from access by citizens. Such\ninstances are mentioned in this article. Whilst larger companies are\nable to defend themselves against overseas lawsuits, individuals and\nsmaller organizations lack the resources to defend what are often\nnormal business activities at home, but could violate the laws of local\njurisdictions in countries around the world. The problems of libel are\ndiscussed as are the blocking of certain sites by certain countries.\nEfforts to draw up Internet treaties are also mentioned\n', ['national governments', 'Internet activity', 'online activities', 'public order protection', 'commercial laws enforcement', 'legal jurisdiction', 'consumer interests protection', 'Internet sites blocking', 'lawsuits', 'Internet treaties', 'Internet', 'legislation']), ("An ACM-W literature review on women in computing\nThe pipeline shrinkage problem for women in computer science is a well-known\nand documented phenomenon where the ratio of women to men involved in\ncomputing shrinks dramatically from early student years to working\nyears. During the last decade, considerable research ensued to\nunderstand the reasons behind the existence of the shrinking pipeline\nand in some cases to take action to increase the numbers of women in\ncomputing. Through the work of a National Science Foundation funded\nproject, ACM's Committee on Women in Computing (ACM-W) has taken a\nfirst step towards pulling this research together. A large number of\narticles was gathered and processed on the topic of women in computing\nand the shrinking pipeline. The committee created a publicly available\nonline database to organize the references of this body of work by\ntopic, author, and reference information. The database, constantly\nbeing updated, is accessible through ACM-W's website\n<http://www.acm.org/women>. A final report is also available via\nthe ACM-W Web site which covers current statistics on women in\ncomputing, summaries of the literature in the database, and a set of\nrecommendations. The article is a brief synopsis of a subset of the\nliterature review as of August 2001\n", ['ACM-W literature review', 'ACM Committee on Women in Computing', 'pipeline shrinkage problem', 'bibliographies', 'computer science', 'computer science education', 'gender issues']), ('Formula-dependent equivalence for compositional CTL model checking\nWe present a polytime computable state equivalence that is defined with respect\nto a given CTL formula. Since it does not attempt to preserve all CTL\nformulas, like bisimulation does, we can expect to compute coarser\nequivalences. This equivalence can be used to reduce the complexity of\nmodel checking a system of interacting FSM. Additionally, we show that\nin some cases our techniques can detect if a formula passes or fails,\nwithout forming the entire product machine. The method is exact and\nfully automatic, and handles full CTL\n', ['formula-dependent equivalence', 'compositional minimization', 'CTL model checking', 'polytime computable state equivalence', 'CTL formula', 'coarse equivalence', 'complexity reduction', 'interacting FSM', 'automatic method', 'formal design verification', 'computation tree logic', 'bisimulation equivalence', 'finite state machines', 'formal verification', 'minimisation']), ('PGE helps customers reduce energy costs\nA new service from Portland General Electric (PGE, Portland, Oregon, US) is\nsaving customers tens of thousands of dollars in energy costs. PGE\ncreated E-Manager to allow facility managers to analyze their energy\nconsumption online at 15-minute intervals. Customers can go to the Web\nfor complete data, powerful analysis tools and charts, helping them\ndetect abnormal energy use and focus on costly problem areas\n', ['energy costs reduction', 'Portland General Electric', 'Oregon', 'E-Manager', 'online energy consumption analysis', 'abnormal energy use detection', 'power consumption', 'power system analysis computing', 'power system economics']), ('Building digital collections at the OAC: current strategies with a view to\nfuture uses\nProviding a context for the exploration of user defined virtual collections,\nthe article describes the history and recent development of the Online\nArchive of California (OAC). Stating that usability and user needs are\nprimary factors in digital resource development, issues explored\ninclude collaborations to build digital collections, reliance upon\nprofessional standards for description and encoding, system\narchitecture, interface design, the need for user tools, and the role\nof archivists as interpreters in the digital environment\n', ['digital collections', 'OAC', 'future uses', 'user defined virtual collections', 'history', 'Online Archive of California', 'user needs', 'digital resource', 'professional standards', 'system architecture', 'interface design', 'user tools', 'digital environment', 'Encoded Archival Description', 'archival descriptive standards', 'metadata standards', 'best practices', 'user studies', 'digital libraries', 'information retrieval systems', 'meta data', 'records management', 'standards']), ('Evaluation of combined dispatching and routeing strategies for a flexible\nmanufacturing system\nThis paper deals with the evaluation of combined dispatching and routeing\nstrategies on the performance of a flexible manufacturing system. Three\nrouteing policies - no alternative routings, alternative routeing\ndynamics and alternative routeing plans - are considered with four\ndispatching rules with finite buffer capacity. In addition, the effect\nof changing part mix ratios is also discussed. The performance measures\nconsidered are makespan, average machine utilization, average flow time\nand average delay at local input buffers. Simulation results indicate\nthat the alternative routings dynamic policy gives the best results in\nthree performance measures except for average delay at local input\nbuffers. Further, the effect of changing part mix ratios is not\nsignificant\n', ['alternative routings', 'flexible manufacturing system', 'FMS', 'dispatching rules', 'finite buffer capacity', 'part mix ratios', 'average flow time', 'computer aided production planning', 'flexible manufacturing systems', 'production control']), ('Toward an Experimental Timing Standards Lab: benchmarking precision in the real\nworld\nMuch discussion has taken place over the relative merits of various platforms\nand operating systems for real-time data collection. Most would agree\nthat, provided great care is taken, many are capable of millisecond\ntiming precision. However, to date, much of this work has focused on\nthe theoretical aspects of raw performance. It is our belief that\nresearchers would be better informed if they could place confidence\nlimits on their own specific paradigms in situ and without\nmodification. To this end, we have developed a millisecond precision\ntest rig that can control and time experiments on a second presentation\nmachine. We report on the specialist hardware and software used. We\nelucidate the importance of the approach in relation to real-world\nexperimentation\n', ['benchmarking precision', 'Experimental Timing Standards Lab', 'performance evaluation', 'operating systems', 'Event Generation software', 'real-time data collection', 'millisecond timing precision', 'computer testing', 'data acquisition', 'operating systems (computers)', 'performance evaluation', 'program testing', 'real-time systems']), ('The UPS as network management tool\nUninterrupted power supplies (UPS), or battery backup systems, once provided a\nrelatively limited, although important, function-continual battery\nsupport to connected equipment in the event of a power failure.\nHowever, yesterday\'s "battery in a box" has evolved into a\nsophisticated network power management tool that can monitor and\nactively correct many of the problems that might plague a healthy\nnetwork. This new breed of UPS system provides such features as\nautomatic voltage regulation, generous runtimes and unattended system\nshutdown, and now also monitors and automatically restarts critical\nservices and operating systems if they lock up or otherwise fail\n', ['uninterrupted power supplies', 'network power management', 'unattended system shutdown', 'automatic voltage regulation', 'computer network management', 'uninterruptible power supplies']), ('Fast and efficient algorithm for the multiplierless realisation of linear DSP\ntransforms\nA fast algorithm having a pseudopolynomial run-time and memory requirement in\nthe worst case is developed to generate multiplierless architectures at\nall wordlengths for constant multiplications in linear DSP transforms.\nIt is also re-emphasised that indefinitely reducing operators for\nmultiplierless architectures is not sufficient to reduce the final chip\narea. For a major reduction, techniques like resource folding must be\nused. Simple techniques for improving the results are also presented\n', ['multiplierless realisation', 'linear DSP transforms', 'pseudopolynomial run-time', 'memory requirement', 'wordlengths', 'constant multiplications', 'final chip area', 'resource folding', 'digital signal processing chips', 'iterative methods', 'processor scheduling']), ('Application of XML for neural network exchange\nThis article introduces a framework for the interchange of trained neural\nnetwork models. An XML-based language (Neural Network Markup Language)\nfor the neural network model description is offered. It allows to write\ndown all the components of neural network model which are necessary for\nits reproduction. We propose to use XML notation for the full\ndescription of neural models, including data dictionary, properties of\ntraining sample, preprocessing methods, details of network structure\nand parameters and methods for network output interpretation\n', ['XML', 'neural network exchange', 'neural network markup language', 'data dictionary', 'preprocessing methods', 'network structure', 'network output interpretation', 'hypermedia markup languages', 'neural nets', 'standards']), ('Cross-entropy and rare events for maximal cut and partition problems\nWe show how to solve the maximal cut and partition problems using a randomized\nalgorithm based on the cross-entropy method. For the maximal cut\nproblem, the proposed algorithm employs an auxiliary Bernoulli\ndistribution, which transforms the original deterministic network into\nan associated stochastic one, called the associated stochastic network\n(ASN). Each iteration of the randomized algorithm for the ASN involves\nthe following two phases: (1) generation of random cuts using a\nmultidimensional Ber(p) distribution and calculation of the associated\ncut lengths (objective functions) and some related quantities, such as\nrare-event probabilities; (2) updating the parameter vector p on the\nbasis of the data collected in the first phase. We show that the Ber(p)\ndistribution converges in distribution to a degenerated one, Ber(p/sub\nd/*), p/sub d/* = (p/sub d/,/sub 1/, p/sub d,n/) in the sense that some\nelements of p/sub d/*, will be unities and the rest zeros. The unity\nelements of p/sub d/* uniquely define a cut which will be taken as the\nestimate of the maximal cut. A similar approach is used for the\npartition problem. Supporting numerical results are given as well. Our\nnumerical studies suggest that for the maximal cut and partition\nproblems the proposed algorithm typically has polynomial complexity in\nthe size of the network\n', ['cross entropy method', 'Bernoulli distribution', 'deterministic network', 'rare event simulation', 'associated stochastic network', 'random cuts', 'probability', 'numerical results', 'polynomial complexity', 'combinatorial optimization', 'importance sampling', 'maximal cut problems', 'partition problems', 'randomized algorithm', 'computational complexity', 'discrete event simulation', 'graph theory', 'importance sampling', 'optimisation', 'probability', 'randomised algorithms']), ('Cataloguing to help law library users\nThe author takes a broader view of the catalogue than is usual; we can include\nwithin it items that have locations other than the office/library\nitself. This may well start with Internet resources, but can perfectly\nappropriately continue with standard works not held in the immediate\ncollection but available in some other accessible collection, such as\nthe local reference library. The essential feature is to include\nentries for the kind of material sought by users, with the addition of\na location mark indicating where they can find it\n', ['law library users', 'cataloguing', 'Internet resources', 'reference library', 'location mark', 'cataloguing', 'Internet', 'law administration', 'library automation']), ('Self-testing chips take a load off ATE\nLooks at how chipmakers get more life out of automatic test equipment by\nembedding innovative circuits in silicon\n', ['self-testing chips', 'ATE', 'automatic test equipment', 'innovative circuits', 'design-for-test techniques', 'embedded deterministic testing technique', 'automatic test equipment', 'design for testability', 'integrated circuit testing', 'logic testing']), ('Operator splitting and approximate factorization for taxis-diffusion-reaction\nmodels\nIn this paper we consider the numerical solution of 2D systems of certain types\nof taxis-diffusion-reaction equations from mathematical biology. By\nspatial discretization these PDE systems are approximated by systems of\npositive, nonlinear ODEs (Method of Lines). The aim of this paper is to\nexamine the numerical integration of these ODE systems for low to\nmoderate accuracy by means of splitting techniques. An important\nconsideration is maintenance of positivity. We apply operator splitting\nand approximate matrix factorization using low order explicit\nRunge-Kutta methods and linearly implicit Runge-Kutta-Rosenbrock\nmethods. As a reference method the general purpose solver VODPK is\napplied\n', ['operator splitting', 'numerical solution', 'mathematical biology', 'spatial discretization', 'PDE systems', 'nonlinear ODEs', 'numerical integration', 'approximate matrix factorization', 'Runge-Kutta methods', 'linearly implicit Runge-Kutta-Rosenbrock methods', 'approximate factorization', 'taxis-diffusion-reaction models', 'matrix decomposition', 'partial differential equations', 'Runge-Kutta methods', 'stability']), ('Networking without wires\nSeveral types of devices use radio transmitters to send data over thin air. Are\nWLANs, wireless local area networks, the end to all cables? Will\nDalrymple weighs up the costs and benefits\n', ['wireless local area networks', 'costs', 'benefits', 'wireless LAN']), ("A comparison of computational color constancy Algorithms. II. Experiments with\nimage data\nFor pt.I see ibid., vol. 11, no.9, p.972-84 (2002). We test a number of the\nleading computational color constancy algorithms using a comprehensive\nset of images. These were of 33 different scenes under 11 different\nsources representative of common illumination conditions. The\nalgorithms studied include two gray world methods, a version of the\nRetinex method, several variants of Forsyth's (1990) gamut-mapping\nmethod, Cardei et al.'s (2000) neural net method, and Finlayson et\nal.'s color by correlation method (Finlayson et al. 1997, 2001; Hubel\nand Finlayson 2000). We discuss a number of issues in applying color\nconstancy ideas to image data, and study in depth the effect of\ndifferent preprocessing strategies. We compare the performance of the\nalgorithms on image data with their performance on synthesized data.\nAll data used for this study are available online at\nhttp://www.cs.sfu.ca/~color/data, and implementations for most of the\nalgorithms are also available (http://www.cs.sfu.ca/~color/code).\nExperiments with synthesized data (part one of this paper) suggested\nthat the methods which emphasize the use of the input data statistics,\nspecifically color by correlation and the neural net algorithm, are\npotentially the most effective at estimating the chromaticity of the\nscene illuminant. Unfortunately, we were unable to realize comparable\nperformance on real images. Here exploiting pixel intensity proved to\nbe more beneficial than exploiting the details of image chromaticity\nstatistics, and the three-dimensional (3-D) gamut-mapping algorithms\ngave the best performance\n", ['computational color constancy algorithms', 'images', 'image data', 'illumination conditions', 'gray world methods', 'Retinex method', 'gamut-mapping method', 'neural net method', 'color by correlation method', 'preprocessing strategies', 'synthesized data', 'input data statistics', 'chromaticity', 'scene illuminant', 'pixel intensity', 'correlation methods', 'image colour analysis', 'neural nets']), ('Reachability sets of a class of multistep control processes: their design\nAn upper estimate and an iterative "restriction" algorithm for the reachability\nset for determining the optimal control for a class of multistep\ncontrol processes are designed\n', ['reachability sets', 'multistep control processes', 'discrete systems', 'upper estimate', 'iterative restriction algorithm', 'optimal control', 'discrete systems', 'optimal control', 'reachability analysis', 'set theory']), ('E-commerce-resources for doing business on the Internet\nThere are many different types of e-commerce depending upon who or what is\nselling and who or what is buying. In addition, e-commerce is more than\nan exchange of funds and goods or services, it encompasses an entire\ninfrastructure of services, computer hardware and software products,\ntechnologies, and communications formats. The paper discusses\ne-commerce terminology, types and information resources, including\nbooks and Web sites\n', ['business', 'Internet', 'e-commerce', 'terminology', 'information resources', 'books', 'Web sites', 'electronic commerce', 'information resources', 'Internet']), ('Citizen centric identity management: chip tricks?\nAccelerating and harmonizing the diffusion and acceptance of electronic\nservices in Europe in a secure and practical way has become a priority\nof several initiatives in the past few years and a critical factor for\ncitizen and business information society services. As identification\nand authentication is a critical element in accessing public services\nthe combination of public key infrastructure (PKI) and smart cards\nemerges as the solution of choice for eGovernment in Europe. National\ngovernments and private initiatives alike vouch their support for this\npowerful combination to deliver an essential layer of reliable\nelectronic services and address identity requirements in a broad range\nof application areas. A recent study suggests that several eGovernment\nimplementations point to the direction of electronic citizen identity\nmanagement as an up and coming challenge. The paper discusses the\neGovernment needs for user identification applicability and the need\nfor standardization\n', ['citizen centric identity management', 'electronic services', 'business information services', 'user identification', 'authentication', 'public key infrastructure', 'smart cards', 'legal framework', 'government', 'standardization', 'public information services', 'authorisation', 'government data processing', 'legislation', 'public key cryptography', 'smart cards', 'standardisation']), ('Multiple shooting using a dichotomically stable integrator for solving\ndifferential-algebraic equations\nIn previous work by the first author, it has been established that a\ndichotomically stable discretization is needed when solving a stiff\nboundary-value problem in ordinary differential equations (ODEs), when\nsharp boundary layers may occur at each end of the interval. A\ndichotomically stable implicit Runge-Kutta method, using the 3-stage,\nfourth-order, Lobatto IIIA formulae, has been implemented in a variable\nstep-size initial-value integrator, which could be used in a\nmultiple-shooting approach. In the case of index-one\ndifferential-algebraic equations (DAEs) the use of the Lobatto IIIA\nformulae has an advantage, over a comparable Gaussian method, that the\norder is the same for both differential and algebraic variables, and\nthere is no need to treat them separately. The ODE integrator has been\nadapted for the solution of index-one DAEs, and the resulting\nintegrator (SYMDAE) has been inserted into the multiple-shooting code\n(MSHDAE) previously developed by R. Lamour for differential-algebraic\nboundary-value problems. The standard version of MSHDAE uses a BDF\nintegrator, which is not dichotomically stable, and for some stiff test\nproblems this fails to integrate across the interval of interest, while\nthe dichotomically stable integrator SYMDAE encounters no difficulty.\nIndeed, for such problems, the modified version of MSHDAE produces an\naccurate solution, and within limits imposed by computer word length,\nthe efficiency of the solution process improves with increasing\nstiffness. For some nonstiff problems, the solution is also entirely\nsatisfactory\n', ['multiple shooting', 'stiff boundary-value problem', 'ordinary differential equations', 'implicit Runge-Kutta method', 'Lobatto IIIA formulae', 'initial-value integrator', 'dichotomically stable integrator', 'differential-algebraic equations', 'boundary-value problems', 'differential equations', 'Runge-Kutta methods', 'stability']), ('Phase transition for parking blocks, Brownian excursion and coalescence\nIn this paper, we consider hashing with linear probing for a hashing table with\nm places, n items (n<m), and l=m-n empty places. For a noncomputer\nscience-minded reader, we shall use the metaphore of n cars parking on\nm places: each car c/sub i/ chooses a place p/sub i/ at random, and if\np/sub i/ is occupied, c/sub i/ tries successively p/sub i/+1, p/sub\ni/+2, until it finds an empty place. Pittel [42] proves that when l/m\ngoes to some positive limit beta <1, the size B/sub 1//sup m,l/ of\nthe largest block of consecutive cars satisfies 2( beta -1-log beta )\nB/sub 1//sup m,l/=2 log m-3 log log m+ Xi /sub m/, where Xi /sub m/\nconverges weakly to an extreme-value distribution. In this paper we\nexamine at which level for n a phase transition occurs between B/sub\n1//sup m,l/=o(m) and m-B/sub 1//sup m,l/=o(m). The intermediate case\nreveals an interesting behavior of sizes of blocks, related to the\nstandard additive coalescent in the same way as the sizes of connected\ncomponents of the random graph are related to the multiplicative\ncoalescent\n', ['hashing', 'linear probing', 'Brownian excursion', 'coalescence', 'parking blocks', 'empirical processes', 'algorithm theory', 'file organisation']), ('Application of nonlinear time series analysis techniques to high-frequency\ncurrency exchange data\nIn this work we have applied nonlinear time series analysis to high-frequency\ncurrency exchange data. The time series studied are the exchange rates\nbetween the US Dollar and 18 other foreign currencies from within and\nwithout the Euro zone. Our goal was to determine if their dynamical\nbehaviours were in some way correlated. The nonexistence of\nstationarity called for the application of recurrence quantification\nanalysis as a tool for this analysis, and is based on the definition of\nseveral parameters that allow for the quantification of recurrence\nplots. The method was checked using the European Monetary System\ncurrency exchanges. The results show, as expected, the high correlation\nbetween the currencies that are part of the Euro, but also a strong\ncorrelation between the Japanese Yen, the Canadian Dollar and the\nBritish Pound. Singularities of the series are also demonstrated taking\ninto account historical events, in 1996, in the Euro zone\n', ['nonlinear time series', 'high-frequency currency exchange data', 'exchange rates', 'US Dollar', 'foreign currencies', 'Euro zone', 'stationarity', 'recurrence quantification analysis', 'recurrence plots', 'European Monetary System', 'Japanese Yen', 'Canadian Dollar', 'British Pound', 'historical events', 'econophysics', 'nonlinear dynamics', 'economics', 'fluctuations', 'foreign exchange trading', 'nonlinear dynamical systems', 'statistical mechanics', 'time series']), ('Waltzing through Port 80 [Web security]\nWeb services follow the trusting model of the Internet, but allow ever more\npowerful payloads to travel between businesses and consumers. Before\nyou leap online, the author advises to scan the security concerns and\nthe available fixes. He looks at how we define and store Web services\nand incorporate them into business processes\n', ['Web services', 'Internet', 'trust', 'data security', 'business processes', 'business data processing', 'information resources', 'Internet', 'security of data']), ('Detection of flaws in composites from scattered elastic-wave field using an\nimproved mu GA and a local optimizer\nAn effective technique for flaw detection of composites is proposed. In this\ntechnique, the detection problem is formulated as an optimization\nproblem minimizing the difference between the measured and calculated\nsurface displacement response derived from scattered elastic-wave\nfields. A combined optimization technique using an improved mu GA and a\nlocal optimizer is developed to solve the optimization problem so as to\nobtain the flaw parameters defining flaw configurations. Guidelines for\nimplementing the detection technique, including formulation of the\nobjective function of the optimization problem using different error\nnorms, improvement of mu GA convergence performance, switching from mu\nGA to local optimizer in the optimization process, and suppression of\nthe effect of noise on detection results, are addressed in detail.\nNumerical examples are presented to demonstrate the effectiveness and\nefficiency of the proposed detection technique\n', ['flaw detection', 'composites', 'scattered elastic-wave field', 'improved mu GA', 'local optimizer', 'optimization problem', 'surface displacement response', 'flaw configurations', 'objective function', 'error norms', 'convergence', 'noise effect suppression', 'composite materials', 'elastic waves', 'flaw detection', 'genetic algorithms', 'mechanical engineering computing']), ('The impact of the product mix on the value of flexibility\nProduct-mix flexibility is one of the major types of manufacturing flexibility,\nreferring to the ability to produce a broad range of products or\nvariants with presumed low changeover costs. The value of such a\ncapability is important to establish for an industrial firm in order to\nensure that the flexibility provided will be at the right level and\nused profitably rather than in excess of market requirements and\nconsequently costly. We use option-pricing theory to analyse the impact\nof various product-mix issues on the value of flexibility. The real\noptions model we use incorporates multiple products, capacity\nconstraints as well as set-up costs. The issues treated here include\nthe number of products, demand variability, correlation between\nproducts, and the relative demand distribution within the product mix.\nThus, we are interested in the nature of the input data to analyse its\neffect on the value of flexibility. We also check the impact at\ndifferent capacity levels. The results suggest that the value of\nflexibility (i) increases with an increasing number of products, (ii)\ndecreases with increasing volatility of product demand, (iii) decreases\nthe more positively correlated the demand is, and (iv) reduces for\nmarginal capacity with increasing levels of capacity. Of these, the\nimpact of positively correlated demand seems to be a major issue.\nHowever, the joint impact of the number of products and demand\ncorrelation showed some non-intuitive results\n', ['product-mix flexibility', 'flexible manufacturing', 'manufacturing flexibility', 'low changeover costs', 'industrial firm', 'option-pricing theory', 'real options model', 'multiple products', 'capacity constraints', 'set-up costs', 'demand variability', 'product correlation', 'relative demand distribution', 'product demand volatility', 'marginal capacity', 'positively correlated demand', 'demand correlation', 'capital budgeting', 'budgeting', 'flexible manufacturing systems', 'management science']), ('The development of a mobile manipulator imaging system for bridge crack\ninspection\nA mobile manipulator imaging system is developed for the automation of bridge\ncrack inspection. During bridge safety inspections, an eyesight\ninspection is made for preliminary evaluation and screening before a\nmore precise inspection. The inspection for cracks is an important part\nof the preliminary evaluation. Currently, the inspectors must stand on\nthe platform of a bridge inspection vehicle or a temporarily erected\nscaffolding to examine the underside of a bridge. However, such a\nprocedure is risky. To help automate the bridge crack inspection\nprocess, we installed two CCD cameras and a four-axis manipulator\nsystem on a mobile vehicle. The parallel cameras are used to detect\ncracks. The manipulator system is equipped with binocular charge\ncoupled devices (CCD) for examining structures that may not be\naccessible to the eye. The system also reduces the danger of accidents\nto the human inspectors. The manipulator system consists of four arms.\nBalance weights are placed at the ends of arms 2 and 4, respectively,\nto maintain the center of gravity during operation. Mechanically, arms\n2 and 4 can revolve smoothly. Experiments indicated that the system\ncould be useful for bridge crack inspections\n', ['mobile manipulator', 'imaging system', 'bridge crack inspection', 'automation', 'eyesight inspection', 'CCD cameras', 'four-axis manipulator', 'parallel cameras', 'binocular CCD', 'charge coupled devices', 'automatic optical inspection', 'cameras', 'CCD image sensors', 'crack detection', 'manipulators', 'mobile robots', 'structural engineering computing']), ('Sliding mode dynamics in continuous feedback control for distributed\ndiscrete-event scheduling\nA continuous feedback control approach for real-time scheduling of discrete\nevents is presented motivated by the need for control theoretic\ntechniques to analyze and design such systems in distributed\nmanufacturing applications. These continuous feedback control systems\nexhibit highly nonlinear and discontinuous dynamics. Specifically, when\nthe production demand in the manufacturing system exceeds the available\nresource capacity then the control system "chatters" and exhibits\nsliding modes. This sliding mode behavior is advantageously used in the\nscheduling application by allowing the system to visit different\nschedules within an infinitesimal region near the sliding surface. In\nthe paper, an analytical model is developed to characterize the sliding\nmode dynamics. This model is then used to design controllers in the\nsliding mode domain to improve the effectiveness of the control system\nto "search" for schedules with good performance. Computational results\nindicate that the continuous feedback control approach can provide\nnear-optimal schedules and that it is computationally efficient\ncompared to existing scheduling techniques\n', ['sliding mode dynamics', 'continuous feedback control', 'distributed discrete-event scheduling', 'real-time scheduling', 'control theoretic techniques', 'distributed manufacturing applications', 'highly nonlinear discontinuous dynamics', 'production demand', 'resource capacity', 'continuous time systems', 'discrete event systems', 'dynamics', 'feedback', 'production control', 'variable structure systems']), ('Control of a coupled map lattice model for vortex shedding in the wake of a\ncylinder\nThe flow behind a vibrating flexible cable at low Reynolds numbers can exhibit\ncomplex wake structures such as lace-like patterns, vortex dislocations\nand frequency cells. These structures have been observed in experiments\nand numerical simulations, and are predicted by a previously developed\nlow-order coupled map lattice (CML). The discrete (in time and space)\nCML models consist of a series of diffusively coupled circle map\noscillators along the cable span. Motivated by a desire to modify the\ncomplex wake patterns behind flexible vibrating cables, we have studied\nthe addition of control terms into the highly efficient CML models and\nexplored the resulting dynamics. Proportional, adaptive proportional\nand discontinuous non-linear (DNL) control methods were used to derive\nthe control laws. The first method employed occasional proportional\nfeedback. The adaptive method used spatio-temporal feedback control.\nThe DNL method used a discontinuous feedback linearization procedure,\nand the controller was designed for the resulting linearized system\nusing eigenvalue assignment. These techniques were applied to a modeled\nvortex dislocation structure in the wake of a vibrating cable in\nuniform freestream flow. Parallel shedding patterns were achieved for a\nrange of forcing frequency-forcing amplitude combinations studied to\nvalidate the control theory. The adaptive proportional and DNL methods\nwere found to be more effective than the proportional control method\ndue to the incorporation of a spatially varying feedback gain across\nthe cylinder span. The DNL method was found to be the most efficient\ncontroller of the low-order CML model. The required control level\nacross the cable span was correlated to the 1/1 lock-on behavior of the\ntemporal circle map\n', ['coupled map lattice', 'vortex shedding', 'wake', 'proportional feedback', 'coupled circle map oscillators', 'spatio-temporal feedback control', 'temporal circle map', 'vortex dislocation', '1/1 lock-on', 'discontinuous nonlinear control', 'cylinder', 'low Reynolds numbers', 'vortex dislocations', 'vibrating flexible cable', 'feedback', 'lattice theory', 'nonlinear control systems', 'proportional control', 'vortices', 'wakes']), ('Development of visual design steering as an aid in large-scale\nmultidisciplinary design optimization. II. Method validation\nFor pt. I see ibid., pp. 412-24. Graph morphing, the first concept developed\nunder the newly proposed paradigm of visual design steering (VDS), is\napplied to optimal design problems. Graph morphing, described in Part I\nof this paper, can be used to provide insights to a designer to improve\nefficiency, reliability, and accuracy of an optimal design in less\ncycle time. It is demonstrated in this part of the paper that graph\nmorphing can be used to provide insights into design variable impact,\nconstraint redundancy, reasonable values for constraint allowable\nlimits, and function smoothness, that otherwise might not be attainable\n', ['visual design steering', 'large-scale multidisciplinary design optimization', 'method validation', 'graph morphing', 'optimal design problems', 'reliability', 'accuracy', 'cycle time', 'design variable impact', 'constraint redundancy', 'constraint allowable limits', 'function smoothness', 'CAD', 'engineering graphics', 'graph theory', 'image morphing', 'optimisation']), ("Acts to facts catalogue\nThe paper shows a way to satisfy users' changing and specific information needs\nby providing the modified\nformat-author-collaborators-title-series-subject (FACTS). catalogue\ninstead of the traditional author-collaborator-title-series-subjects\n(ACTS) catalogue\n", ['information needs', 'format-author-collaborators-title-series-subject catalogue', 'author-collaborator-title-series-subjects catalogue', 'information needs', 'information retrieval', 'library automation']), ('Abundance of mosaic patterns for CNN with spatially variant templates\nThis work investigates the complexity of one-dimensional cellular neural\nnetwork mosaic patterns with spatially variant templates on finite and\ninfinite lattices. Various boundary conditions are considered for\nfinite lattices and the exact number of mosaic patterns is computed\nprecisely. The entropy of mosaic patterns with periodic templates can\nalso be calculated for infinite lattices. Furthermore, we show the\nabundance of mosaic patterns with respect to template periods and,\nwhich differ greatly from cases with spatially invariant templates\n', ['mosaic patterns', 'CNN', 'spatially variant templates', 'one-dimensional cellular neural network', 'infinite lattices', 'finite lattices', 'boundary conditions', 'spatial entropy', 'transition matrix', 'cellular neural nets', 'entropy', 'lattice theory', 'matrix algebra', 'piecewise linear techniques']), ("Extending Kamp's theorem to model time granularity\nIn this paper, a generalization of Kamp's theorem relative to the functional\ncompleteness of the until operator is proved. Such a generalization\nconsists in showing the functional completeness of more expressive\ntemporal operators with respect to the extension of the first-order\ntheory of linear orders MFO[<] with an extra binary relational\nsymbol. The result is motivated by the search of a modal language\ncapable of expressing properties and operators suitable to model time\ngranularity in omega -layered temporal structures\n", ["Kamp's theorem", 'functional completeness', 'until operator', 'temporal operators', 'first-order theory', 'linear orders', 'binary relational symbol', 'omega -layered temporal structures', 'model time granularity', 'decidability', 'temporal logic']), ('Design and implementation of a 3-D mapping system for highly irregular shaped\nobjects with application to semiconductor manufacturing\nThe basic technology for a robotic system is developed to automate the packing\nof polycrystalline silicon nuggets into fragile fused silica crucible\nin Czochralski (melt pulling) semiconductor wafer production. The\nhighly irregular shapes of the nuggets and the packing constraints make\nthis a difficult and challenging task. It requires the delicate\nmanipulation and packing of highly irregular polycrystalline silicon\nnuggets into a fragile fused silica crucible. For this application, a\ndual optical 3-D surface mapping system that uses active laser\ntriangulation has been developed and successfully tested. One part of\nthe system measures the geometry profile of a nugget being packed and\nthe other the profile of the nuggets already in the crucible. A\nresolution of 1 mm with 15-KHz sampling frequency is achieved. Data\nfrom the system are used by the packing algorithm, which determines\noptimal nugget placement. The key contribution is to describe the\ndesign and implementation of an efficient and robust 3-D imaging system\nto map highly irregular shaped objects using conventional components in\ncontext of real commercial manufacturing processes\n', ['3D mapping system', 'highly irregular shaped objects', 'semiconductor manufacturing', 'robotic system', 'polycrystalline silicon nuggets', 'fragile fused silica crucible', 'sampling frequency', 'packing algorithm', 'optical nugget placement', 'robust 3-D imaging system', 'irregular shaped objects', 'commercial manufacturing processes', 'Czochralski semiconductor wafer production', 'dual optical 3D surface mapping system', 'highly irregular polycrystalline silicon nuggets', 'active laser triangulation', 'calibration', 'crystal growth from melt', 'geometry', 'robot vision', 'semiconductor growth']), ('Finding performance bugs with the TNO HPF benchmark suite\nHigh-Performance Fortran (HPF) has been designed to provide portable\nperformance on distributed memory machines. An important aspect of\nportable performance is the behavior of the available HPF compilers.\nIdeally, a programmer may expect comparable performance between\ndifferent HPF compilers, given the same program and the same machine.\nTo test the performance portability between compilers, we have designed\na special benchmark suite, called the TNO HPF benchmark suite. It\nconsists of a set of HPF programs that test various aspects of\nefficient parallel code generation. The benchmark suite consists of a\nnumber of template programs that are used to generate test programs\nwith different array sizes, alignments, distributions, and iteration\nspaces. It ranges from very simple assignments to more complex\nassignments such as triangular iteration spaces, convex iteration\nspaces, coupled subscripts, and indirection arrays. We have run the TNO\nHPF benchmark suite on three compilers: the PREPARE prototype compiler,\nthe PGI-HPF compiler, and the GMD Adaptor HPF compiler. Results show\nperformance differences that can be quite large (up to two orders of\nmagnitude for the same test program). Closer inspection reveals that\nthe origin of most of the differences in performance is due to\ndifferences in local enumeration and storage of distributed array\nelements\n', ['High-Performance Fortran', 'portable performance', 'distributed memory machines', 'HPF compilers', 'performance portability', 'benchmark suite', 'parallel compilers', 'compiler optimizations', 'FORTRAN', 'parallel programming', 'program compilers', 'software performance evaluation', 'software portability']), ("Development and validation of user-adaptive navigation and information\nretrieval tools for an intranet portal organizational memory\ninformation system\nBased on previous research and properties of organizational memory, a\nconceptual model for navigation and retrieval functions in an intranet\nportal organizational memory information system was proposed, and two\nhuman-centred features (memory structure map and history-based tool)\nwere developed to support user's navigation and retrieval in a\nwell-known organizational memory. To test two hypotheses concerning the\nvalidity of the conceptual model and two human-centred features, an\nexperiment was conducted with 30 subjects. Testing of the two\nhypotheses indicated the following: (1) the memory structure map's\nusers showed 29% better performance in navigation, and (2) the\nhistory-based tool's users outperformed by 34% in identifying\ninformation. The results of the study suggest that a conceptual model\nand two human-centred features could be used in a user-adaptive\ninterface design to improve user's performance in an intranet portal\norganizational memory information system\n", ['user-adaptive navigation', 'information retrieval tools', 'intranet portal', 'organizational memory information system', 'conceptual model', 'human factors', 'memory structure map', 'history-based tool', 'experiment', 'user-adaptive interface design', 'user performance', 'business data processing', 'human factors', 'information retrieval', 'information systems', 'intranets', 'online front-ends', 'user interfaces']), ('Implementation and evaluation of HPF/SX V2\nWe are developing HPF/SX V2, a High Performance Fortran (HPF) compiler for\nvector parallel machines. It provides some unique extensions as well as\nthe features of HPF 2.0 and HPF/JA. In particular, this paper describes\nfour of them: (1) the ON directive of HPF 2.0; (2) the REFLECT and\nLOCAL directives of HPF/JA; (3) vectorization directives; and (4)\nautomatic parallelization. We evaluate these features through some\nbenchmark programs on NEC SX-5. The results show that each of them\nachieved a 5-8 times speedup in 8-CPU parallel execution and the four\nfeatures are useful for vector parallel execution. We also evaluate the\noverall performance of HPF/SX V2 by using over 30 well-known benchmark\nprograms from HPFBench, APR Benchmarks, GENESIS Benchmarks, and NAS\nParallel Benchmarks. About half of the programs showed good\nperformance, while the other half suggest weakness of the compiler,\nespecially on its runtimes. It is necessary to improve them to put the\ncompiler to practical use\n', ['HPF/SX V2', 'High Performance Fortran compiler', 'vector parallel machines', 'benchmark', 'parallelization', 'compiler', 'FORTRAN', 'parallel languages', 'parallelising compilers']), ("Unsafe at any speed?\nWhile Sun prides itself on Java's secure sandbox programming model, Microsoft\ntakes a looser approach. Its C# language incorporates C-like concepts,\nincluding pointers and memory management. But is unsafe code really a\nboon to programmers, or is it a step backward?\n", ['Microsoft C# language', 'C-like concepts', 'pointers', 'memory management', 'Sun Java secure sandbox programming model', 'C language', 'object-oriented programming', 'security']), ('Twenty years of the literature on acquiring out-of-print materials\nThis article reviews the last two-and-a-half decades of literature on acquiring\nout-of-print materials to assess recurring issues and identify changing\npractices. The out-of-print literature is uniform in its assertion that\nlibraries need to acquire o.p. materials to replace worn or damaged\ncopies, to replace missing copies, to duplicate copies of heavily used\nmaterials, to fill gaps in collections, to strengthen weak collections,\nto continue to develop strong collections, and to provide materials for\nnew courses, new programs, and even entire new libraries\n', ['out-of-print materials', 'recurring issues', 'changing practices', 'out-of-print books', 'library materials', 'acquisition', 'bibliographies', 'library automation']), ("C and C++: a case for compatibility\nModern C and C++ are sibling languages descended from Classic C. In many\npeople's minds, they are (wrongly, but understandably) fused into the\nmythical C/C++ programming language. There is no C/C++ language, but\nthere is a C/C++ community. Previously the author described some of the\nincompatibilities that complicate the work of developers within that\nC/C++ community. In this article, he discusses some of the underlying\nmyths that help perpetuate these incompatibilities. He also shows why\nmore compatibility (ideally, full compatibility) is in the best\ninterest of the C/C++ community. In the next paper, he presents some\nexamples of how the incompatibilities in C and C++ might be resolved\n", ['C++ language', 'C language', 'incompatibilities', 'object-oriented programming', 'class hierarchies', 'low-level programming', 'C++ libraries', 'C language', 'C++ language']), ('Computing transient gating charge movement of voltage-dependent ion channels\nThe opening of voltage-gated sodium, potassium, and calcium ion channels has a\nsteep relationship with voltage. In response to changes in the\ntransmembrane voltage, structural movements of an ion channel that\nprecede channel opening generate a capacitative gating current. The net\ngating charge displacement due to membrane depolarization is an index\nof the voltage sensitivity of the ion channel activation process.\nUnderstanding the molecular basis of voltage-dependent gating of ion\nchannels requires the measurement and computation of the gating charge,\nQ. We derive a simple and accurate semianalytic approach to computing\nthe voltage dependence of transient gating charge movement (Q-V\nrelationship) of discrete Markov state models of ion channels using\nmatrix methods. This approach allows rapid computation of Q-V curves\nfor finite and infinite length step depolarizations and is consistent\nwith experimentally measured transient gating charge. This\ncomputational approach was applied to Shaker potassium channel gating,\nincluding the impact of inactivating particles on potassium channel\ngating currents\n', ['transient gating charge movement', 'transmembrane voltage', 'gating current', 'action potentials', 'ion channels', 'charge movement', 'inactivation', 'immobilization', 'Markov state model', 'bioelectric potentials', 'Markov processes', 'neural nets']), ('K-12 instruction and digital access to archival materials\nProviding K-12 schools with digital access to archival materials can strengthen\nboth student learning and archival practice, although it cannot replace\ndirect physical access to records. The article compares a variety of\nelectronic and nonelectronic projects to promote teaching with primary\nsource materials. The article also examines some of the different\nhistoriographical and pedagogical approaches used in archival Web sites\ngeared for K-12 instruction, focusing on differences between the\neducational sites sponsored by the Library of Congress and the National\nArchives and Records Administration\n', ['K-12 instruction', 'digital access', 'archival materials', 'student learning', 'archival practice', 'electronic projects', 'nonelectronic projects', 'primary source materials', 'direct physical access', 'historiographical approaches', 'pedagogical approaches', 'archival Web', 'educational sites', 'Library of Congress', 'National Archives and Records Administration', 'education', 'information resources', 'information retrieval systems', 'records management', 'teaching']), ("Measuring keyboard response delays by comparing keyboard and joystick inputs\nThe response characteristics of PC keyboards have to be identified when they\nare used as response devices in psychological experiments. In the past,\nthe proposed method has been to check the characteristics independently\nby means of external measurement equipment. However, with the\navailability of different PC models and the rapid pace of model change,\nthere is an urgent need for the development of convenient and accurate\nmethods of checking. The method proposed consists of raising the\nprecision of the PC's clock to the microsecond level and using a\njoystick connected to the MIDI terminal of a sound board to give the PC\nan independent timing function. Statistical processing of the data\nprovided by this method makes it possible to estimate accurately the\nkeyboard scanning interval time and the average keyboard delay time.\nThe results showed that measured keyboard delay times varied from 11 to\n73 msec, depending on the keyboard model, with most values being less\nthan 30 msec\n", ['keyboard response delay measurement', 'joystick inputs', 'keyboard inputs', 'PC keyboards', 'psychological experiments', 'model change', 'checking', 'PC clock precision', 'MIDI terminal', 'sound board', 'independent timing function', 'statistical data processing', 'keyboard scanning interval time', 'average keyboard delay time', 'interactive devices', 'keyboards', 'microcomputer applications', 'psychology']), ('Faking it: simulating dependent types in Haskell\nDependent types reflect the fact that validity of data is often a relative\nnotion by allowing prior data to affect the types of subsequent data.\nNot only does this make for a precise type system, but also a highly\ngeneric one: both the type and the program for each instance of a\nfamily of operations can be computed from the data which codes for that\ninstance. Recent experimental extensions to the Haskell type class\nmechanism give us strong tools to relativize types to other types. We\nmay simulate some aspects of dependent typing by making counterfeit\ntype-level copies of data, with type constructors simulating data\nconstructors and type classes simulating datatypes. This paper gives\nexamples of the technique and discusses its potential\n', ['dependent types', 'Haskell', 'data validity', 'precise type system', 'type class mechanism', 'dependent typing', 'counterfeit type-level copies', 'type constructors', 'data constructors', 'datatypes', 'functional programming', 'data structures', 'functional languages', 'functional programming', 'type theory']), ('Building 3D anatomical scenes on the Web\nWe propose a new service for building user-defined 3D anatomical structures on\nthe Web. The Web server is connected to a database storing more than\n1000 3D anatomical models reconstructed from the Visible Human. Users\nmay combine existing models as well as planar oblique slices in order\nto create their own structured anatomical scenes. Furthermore, they may\nrecord sequences of scene construction and visualization actions. These\nactions enable the server to construct high-quality video animations,\ndownloadable by the user. Professionals and students in anatomy,\nmedicine and related disciplines are invited to use the server and\ncreate their own anatomical scenes\n', ['3D anatomical scenes', 'World Wide Web', 'user-defined 3D anatomical structures', 'Web server', 'database', '3D anatomical models', 'Visible Human', 'planar oblique slices', 'structured anatomical scenes', 'volume visualization', 'surface reconstruction', 'applet-based rendering engine', 'Java', 'visualization', 'scene construction', 'high-quality video animation', 'computer animation', 'data visualisation', 'information resources', 'Internet', 'medical computing', 'rendering (computer graphics)', 'solid modelling', 'visual databases']), ('High-speed consistency checking for hypothetical reasoning systems using\ninference path network\nHypothetical reasoning is popular in fault diagnostics and design systems, but\nslow reasoning speed is its drawback. The goal of the current study is\ndeveloping hypothetical reasoning based on an inference path network,\nwhich would overcome this drawback. In hypothetical reasoning systems\nbased on an inference path network, there is much room for improvement\nregarding the computing costs of connotation processing and consistency\nchecking. The authors of this study demonstrate improvement ideas\nregarding one of these problems, namely, consistency checking. First,\nthe authors obtained necessary and sufficient conditions under which\ninconsistencies occur during hypothesis composition. Based on the\nobtained results, the authors proposed an algorithm for speeding up the\nprocess of consistency checking. Processing with this algorithm in its\ncore consists of transforming the inference path network in such a way\nthat inconsistencies do not occur during the hypothesis composition,\nunder the condition of unchanged solution hypotheses. The efficiency of\nthis algorithm was confirmed by tests\n', ['hypothetical reasoning', 'fault diagnostics', 'high-speed consistency checking', 'inference path network', 'reasoning speed', 'inconsistencies', 'hypothesis composition', 'speed up', 'diagnostic reasoning', 'heuristic programming', 'uncertainty handling']), ("Oracle's Suite grows up\nOnce a low-cost Web offering, Oracle's Small Business Suite now carries a price\ntag to justify VAR interest\n", ['Oracle Small Business Suite', 'NetLedger', 'accounting', 'resellers', 'accounting', 'software packages']), ("Streaming, disruptive interference and power-law behavior in the exit dynamics\nof confined pedestrians\nWe analyze the exit dynamics of pedestrians who are initially confined in a\nroom. Pedestrians are modeled as cellular automata and compete to\nescape via a known exit at the soonest possible time. A pedestrian\ncould move forward, backward, left or right within each iteration time\ndepending on adjacent cell vacancy and in accordance with simple rules\nthat determine the compulsion to move and physical capability relative\nto his neighbors. The arching signatures of jamming were observed and\nthe pedestrians exited in bursts of various sizes. Power-law behavior\nis found in the burst-size frequency distribution for exit widths w\ngreater than one cell dimension (w > 1). The slope of the power-law\ncurve varies with w from -1.3092 (w = 2) to -1.0720 (w = 20). Streaming\nwhich is a diffusive behavior, arises in large burst sizes and is more\nlikely in a single-exit room with w = 1 and leads to a counterintuitive\nresult wherein an average exit throughput Q is obtained that is higher\nthan with w = 2, 3, or 4. For a two-exit room (w = 1), Q is not greater\nthan twice the yield of a single-exit room. If the doors are not\nseparated far enough (< 4w), Q becomes even significantly less due\nto a collective slow-down that emerges among pedestrians crossing in\neach other's path (disruptive interference effect). For the same w and\ndoor number, Q is also higher with relaxed pedestrians than with\nanxious ones\n", ['streaming', 'cellular automata', 'iteration time', 'adjacent cell vacancy', 'arching signatures', 'jamming', 'burst-size frequency distribution', 'collective slow-down', 'self-organised criticality', 'disruptive interference', 'power-law behavior', 'exit dynamics', 'confined pedestrians', 'cellular automata', 'iterative methods', 'nonlinear dynamical systems', 'self-organised criticality', 'transport processes', 'transportation']), ('Network intrusion and fault detection: a statistical anomaly approach\nWith the advent and explosive growth of the global Internet and electronic\ncommerce environments, adaptive/automatic network/service intrusion and\nanomaly detection in wide area data networks and e-commerce\ninfrastructures is fast gaining critical research and practical\nimportance. We present and demonstrate the use of a general-purpose\nhierarchical multitier multiwindow statistical anomaly detection\ntechnology and system that operates automatically, adaptively, and\nproactively, and can be applied to various networking technologies,\nincluding both wired and wireless ad hoc networks. Our method uses\nstatistical models and multivariate classifiers to detect anomalous\nnetwork conditions. Some numerical results are also presented that\ndemonstrate that our proposed methodology can reliably detect attacks\nwith traffic anomaly intensity as low as 3-5 percent of the typical\nbackground traffic intensity, thus promising to generate an effective\nearly warning\n', ['network intrusion', 'fault detection', 'Internet', 'computer network attacks', 'denial of service', 'early warning systems', 'neural network classification', 'ad hoc wireless experiments', 'backpropagation', 'perceptron-back propagation hybrid', 'electronic commerce environment', 'adaptive/automatic network/service intrusion', 'wide area data networks', 'e-commerce infrastructure', 'hierarchical multitier statistical anomaly detection', 'multiwindow anomaly detection', 'wired ad hoc networks', 'wireless ad hoc networks', 'statistical models', 'multivariate classifiers', 'traffic anomaly intensity', 'background traffic intensity', 'backpropagation', 'data communication', 'electronic commerce', 'neural nets', 'security of data', 'signal classification', 'statistical analysis', 'telecommunication security', 'telecommunication traffic', 'wide area networks']), ('LMI approach to digital redesign of linear time-invariant systems\nA simple design methodology for the digital redesign of static state feedback\ncontrollers by using linear matrix inequalities is presented. The\nproposed method provides close matching of the states between the\noriginal continuous-time system and those of the digitally redesigned\nsystem with a guaranteed stability. Specifically, the digital redesign\nproblem is reformulated as linear matrix inequalities (LMIs) and solved\nby a numerical optimisation technique. The main feature of the proposed\nmethod is that the closed-loop stability of the digitally redesigned\nsystem is explicitly guaranteed within the design procedure using the\nLMI-based approach. A numerical example of the position control of a\nsimple crane system is presented\n', ['LMI approach', 'digital redesign', 'linear time-invariant systems', 'design methodology', 'linear matrix inequalities', 'continuous-time system', 'guaranteed stability', 'numerical optimisation technique', 'closed-loop stability', 'position control', 'crane system', 'closed loop systems', 'continuous time systems', 'control system synthesis', 'cranes', 'digital control', 'discrete time systems', 'linear systems', 'matrix algebra', 'position control', 'state feedback']), ('Construction of information retrieval thesaurus for family planning terms using\nCDS/ISIS\nThe thesaurus as a tool for information retrieval and as an alternative to the\nexisting scheme of classifications in information retrieval is\ndiscussed. The paper considers the emergence of the information\nretrieval thesaurus and its definition. Family planning is a\nmultidisciplinary subject covering socio economic, cultural,\npsychological and medical fields. This necessitated the construction of\na thesaurus for the Family Planning discipline. The construction is\nbased on UNISIST, ISO 2788 and BS 5723 guidelines by using CDS/ISIS\nsoftware\n', ['information retrieval', 'thesaurus', 'family planning terms', 'CDS/ISIS software', 'bibliographic databases', 'classification', 'socio economic field', 'culture', 'psychology', 'medicine', 'Family Planning', 'ISO 2788', 'UNISIST', 'BS 5723', 'bibliographic systems', 'classification', 'indexing', 'information retrieval', 'ISO standards', 'medical information systems', 'thesauri']), ('Fuzzy business [Halden Reactor Project]\nThe Halden Reactor Project has developed two systems to investigate how signal\nvalidation and thermal performance monitoring techniques can be\nimproved. PEANO is an online calibration monitoring system that makes\nuse of artificial intelligence techniques. The system has been tested\nin cooperation with EPRI and Edan Engineering, using real data from a\nUS PWR plant. These tests showed that PEANO could reliably assess the\nperformance of the process instrumentation at different plant\nconditions. Real cases of zero and span drifts were successfully\ndetected by the system. TEMPO is a system for thermal performance\nmonitoring and optimisation, which relies on plant-wide first principle\nmodels. The system has been installed on a Swedish BWR plant. Results\nobtained show an overall rms deviation from measured values of a few\ntenths of a percent, and giving goodness-of-fits in the order of 95%.\nThe high accuracy demonstrated is a good basis for detecting possible\nfaults and efficiency losses in steam turbine cycles\n', ['Halden Reactor Project', 'PEANO', 'calibration', 'artificial intelligence', 'fuzzy logic', 'steam generators', 'feedwater flow', 'PWR', 'TEMPO', 'thermal performance monitoring', 'BWR', 'steam turbine cycles', 'calibration', 'fission reactor monitoring', 'fuzzy systems', 'nuclear engineering computing']), ('Adaptive wavelet methods. II. Beyond the elliptic case\nThis paper is concerned with the design and analysis of adaptive wavelet\nmethods for systems of operator equations. Its main accomplishment is\nto extend the range of applicability of the adaptive wavelet-based\nmethod developed previously for symmetric positive definite problems to\nindefinite or unsymmetric systems of operator equations. This is\naccomplished by first introducing techniques (such as the least squares\nformulation developed previously) that transform the original\n(continuous) problem into an equivalent infinite system of equations\nwhich is now well-posed in the Euclidean metric. It is then shown how\nto utilize adaptive techniques to solve the resulting infinite system\nof equations. It is shown that for a wide range of problems, this new\nadaptive method performs with asymptotically optimal complexity, i.e.,\nit recovers an approximate solution with desired accuracy at a\ncomputational expense that stays proportional to the number of terms in\na corresponding wavelet-best N-term approximation. An important\nadvantage of this adaptive approach is that it automatically stabilizes\nthe numerical procedure so that, for instance, compatibility\nconstraints on the choice of trial spaces, like the LBB condition, no\nlonger arise\n', ['adaptive wavelet methods', 'elliptic case', 'operator equations', 'least squares formulation', 'Euclidean metric', 'asymptotically optimal complexity', 'N-term approximation', 'computational complexity', 'wavelet transforms']), ('Analyzing the benefits of 300 mm conveyor-based AMHS\nWhile the need for automation in 300 mm fabs is not debated, the form and\nperformance of such automation is still in question. Software\nsimulation that compares conveyor-based continuous flow transport\ntechnology to conventional car-based wafer-lot delivery has detailed\ndelivery time and throughput advantages to the former\n', ['software simulation', 'car-based wafer-lot delivery', 'conveyor-based continuous flow transport technology', 'automated material handling system', 'semiconductor fab', 'throughput', 'wafer processing', 'delivery time', '300 mm', 'conveyors', 'semiconductor device manufacture']), ('Connection management for QoS service on the Web\nThe current Web service model treats all requests equivalently, both while\nbeing processed by servers and while being transmitted over the\nnetwork. For some uses, such as multiple priority schemes, different\nlevels of service are desirable. We propose application-level TCP\nconnection management mechanisms for Web servers to provide two\ndifferent levels of Web service, high and low service, by setting\ndifferent time-outs for inactive TCP connections. We evaluated the\nperformance of the mechanism under heavy and light loading conditions\non the Web server. Our experiments show that, though heavy traffic\nsaturates the network, high level class performance is improved by as\nmuch as 25-28%. Therefore, this mechanism can effectively provide QoS\nguaranteed services even in the absence of operating system and network\nsupports\n', ['connection management', 'Web service model', 'Internet', 'TCP connections', 'time-outs', 'quality of service', 'telecommunication traffic', 'client server system', 'Web transaction', 'client-server systems', 'Internet', 'quality of service', 'telecommunication network management', 'telecommunication traffic', 'transport protocols']), ('Dynamic neighborhood structures in parallel evolution strategies\nParallelizing is a straightforward approach to reduce the total computation\ntime of evolutionary algorithms. Finding an appropriate communication\nnetwork within spatially structured populations for improving\nconvergence speed and convergence probability is a difficult task. A\nnew method that uses a dynamic communication scheme in an evolution\nstrategy will be compared with conventional static and dynamic\napproaches. The communication structure is based on a so-called\ndiffusion model approach. The links between adjacent individuals are\ndynamically chosen according to deterministic or probabilistic rules.\nDue to self-organization effects, efficient and stable communication\nstructures are established that perform robustly and quickly on a\nmultimodal test function\n', ['evolutionary algorithms', 'multimodal test function', 'convergence probability', 'convergence speed', 'parallel evolutionary algorithms', 'parallelizing', 'evolutionary computation', 'genetic algorithms', 'parallel programming']), ('A discontinuous Galerkin method for transient analysis of wave propagation in\nunbounded domains\nA technique based on the discontinuous Galerkin finite element method is\ndeveloped and applied to the derivation of an absorbing boundary\ncondition for the analysis of transient wave propagation. The condition\nis exact in that only discretization error is involved. Furthermore,\nthe computational cost associated with use of the condition is an order\nof magnitude lower than for conditions based on Green functions. The\ntime-stepping scheme resulting from an implicit method in conjunction\nwith this boundary condition appears to be unconditionally stable\n', ['discontinuous Galerkin finite element method', 'transient analysis', 'transient wave propagation', 'absorbing boundary condition', 'unbounded domains', 'discretization error', 'computational cost', 'time-stepping scheme', 'implicit method', 'unconditional stability', 'civil engineering computing', 'finite element analysis', 'Galerkin method', 'numerical stability', 'wave propagation']), ('Approximation theory of fuzzy systems based upon genuine many-valued\nimplications - SISO cases\nIt is proved that the single input and single output (SISO) fuzzy systems based\nupon genuine many-valued implications are universal approximators. It\nis shown theoretically that fuzzy control systems based upon genuine\nmany-valued implications are equivalent to those based upon t-norm\nimplications, the general approach to construct fuzzy systems is given.\nIt is also shown that defuzzifier based upon center of areas is not\nappropriate to the fuzzy systems based upon genuine many-valued\nimplications\n', ['single input and single output fuzzy systems', 'SISO', 'many-valued implications', 'fuzzy systems', 'Boolean implication', 'universal approximator', 'approximation theory', 'fuzzy control', 'fuzzy systems', 'inference mechanisms']), ('A question of perspective: assigning Library of Congress subject headings to\nclassical literature and ancient history\nThis article explains the concept of world view and shows how the world view of\ncataloguers influences the development and assignment of subject\nheadings to works about other cultures and civilizations, using works\nfrom classical literature and ancient history as examples. Cataloguers\nare encouraged to evaluate the headings they assign to works in\nclassical literature and ancient history in terms of the world views of\nAncient Greece and Rome so that headings reflect the contents of the\nworks they describe and give fuller expression to the diversity of\nthoughts and themes that characterize these ancient civilizations\n', ['Library of Congress subject heading assignment', 'world view', 'cultures', 'civilizations', 'classical literature', 'ancient history', 'Ancient Greece', 'Ancient Rome', 'cataloguing', 'history', 'literature', 'vocabulary']), ('Alien Rescue: a problem-based hypermedia learning environment for middle school\nscience\nThe article describes an innovative hypermedia product for sixth graders in\nspace science: Alien Rescue. Using a problem-based learning approach\nthat is highly interactive, Alien Rescue engages students in scientific\ninvestigations aimed at finding solutions to complex and meaningful\nproblems. Problem-based learning (PBL) is an instructional strategy\nproven to be effective in medical and business fields, and it is\nincreasingly popular in education. However, using PBL in K-12\nclassrooms is challenging and requires access to rich knowledge bases\nand cognitive tools. Alien Rescue is designed to provide such cognitive\nsupport for successful use of PBL in sixth-grade classrooms. The design\nand development of Alien Rescue is guided by current educational\nresearch. Research is an integral part of this project. Results of\nformative evaluation and research studies are being integrated into the\ndevelopment and improvement of the program. Alien Rescue is designed in\naccordance with the National Science Standards and the Texas Essential\nKnowledge and Skills (TEKS) for science. So far Alien Rescue has been\nfield-tested by approximately 1400 sixth graders. More use in middle\nschools is in progress and more research on its use is planned\n', ['Alien Rescue', 'problem-based hypermedia learning environment', 'middle school science', 'space science', 'sixth graders', 'scientific investigations', 'PBL', 'instructional strategy', 'medical fields', 'business fields', 'K-12 classrooms', 'rich knowledge bases', 'cognitive tools', 'cognitive support', 'educational research', 'formative evaluation', 'middle schools', 'courseware', 'hypermedia', 'interactive systems', 'natural sciences computing', 'teaching']), ("Process pioneers [agile business]\nBy managing IT infrastructures along so-called 'top down' lines, organisations\ncan streamline their business processes, eliminate redundant tasks and\nincrease automation\n", ['agile business', 'managing IT infrastructures', 'business processes', 'increase automation', 'DP management', 'management information systems']), ('High dynamic control of a three-level voltage-source-converter drive for a main\nstrip mill\nA high dynamic control system for the Alspa VDM 7000 medium-voltage drive was\nimplemented, which provides fast torque response times of a few\nmilliseconds despite the typically low switching frequency of\ngate-turn-off thyristors which is necessary to achieve high efficiency.\nThe drive system consists of a three-level voltage-source converter\nwith active front end and a synchronous motor. The drive has most\nrecently been applied to a main strip mill. It provides a maximum of\n8.3-MW mechanical power with a rated motor voltage of 3 kV. Besides\nmotor torque as the main control objective, the control system has to\ncomply with a number of additional objectives and constraints like\nDC-link voltage regulation and balancing, current and torque harmonics,\nmotor flux, and excitation\n', ['medium-voltage drive', 'high dynamic control system', 'gate-turn-off thyristors', 'switching frequency', 'efficiency', 'three-level voltage-source converter', 'synchronous motor', 'strip mill', 'mechanical power', 'motor voltage', 'control objective', 'DC-link voltage regulation', 'DC-link voltage balancing', 'current harmonics', 'torque harmonics', 'motor flux', 'excitation', '8.3 MW', '3 kV', 'DC-AC power convertors', 'invertors', 'machine vector control', 'rolling mills', 'steel industry', 'switching circuits', 'synchronous motor drives', 'thyristor convertors', 'torque control']), ('Modelling dependencies in paired comparison data a log-linear approach\nIn many Bradley-Terry models a more or less explicit assumption is that all\ndecisions of the judges are independent. An assumption which might be\nquestionable at least for the decisions of a given judge. In paired\ncomparison studies, a judge chooses among objects several times, and in\nsuch cases, judgements made by the same judge are likely to be\ndependent. A log-linear representation for the Bradley-Terry model is\ndeveloped, which takes into account dependencies between judgements.\nThe modelling of the dependencies is embedded in the analysis of\nmultiple binomial responses, which has the advantage of\ninterpretability in terms of conditional odds ratios. Furthermore, the\nmodelling is done in the framework of generalized linear models, thus\nparameter estimation and the assessment of goodness of fit can be\nobtained in the standard way by using e.g. GLIM or another standard\nsoftware\n', ['paired comparison data dependency modelling', 'log-linear approach', 'Bradley-Terry model', 'judge decisions', 'multiple binomial responses', 'conditional odds ratios', 'generalized linear models', 'parameter estimation', 'goodness of fit', 'GLIM', 'data analysis', 'decision theory', 'parameter estimation', 'probability', 'statistical analysis']), ("Image reconstruction from fan-beam projections on less than a short scan\nThis work is concerned with 2D image reconstruction from fan-beam projections.\nIt is shown that exact and stable reconstruction of a given\nregion-of-interest in the object does not require all lines passing\nthrough the object to be measured. Complete (non-truncated) fan-beam\nprojections provide sufficient information for reconstruction when\n'every line passing through the region-of-interest intersects the\nvertex path in a non-tangential way'. The practical implications of\nthis condition are discussed and a new filtered-backprojection\nalgorithm is derived for reconstruction. Experiments with\ncomputer-simulated data are performed to support the mathematical\nresults\n", ['fan-beam projections', '2D image reconstruction', 'exact stable reconstruction', 'region-of-interest', 'vertex path', 'filtered-backprojection algorithm', 'X-ray computed tomography', 'short-scan condition', 'Hilbert transform', 'Radon transform', 'rebinning formula', 'convolution', 'linear interpolation', '3D head phantom', 'computerised tomography', 'convolution', 'diagnostic radiography', 'Hilbert transforms', 'image reconstruction', 'interpolation', 'medical image processing', 'Radon transforms']), ('Training for trouble\nIn a security context, one example of digitized video\'s integration into a\nnetworked knowledge base is found in the Accident Response Group (ARG)\nat Sandia National Labs. A "national security laboratory" headquartered\nin Albuquerque, New Mexico, Sandia is operated by Lockheed Martin and\nprimarily funded by the U.S. Department of Energy. The organization\nhandles research, design and development of all non-nuclear components\nused in U.S. nuclear weapons programs, and is involved as well in\nprograms related to energy, critical infrastructure, non-proliferation,\nmaterials control, and emerging threats. ARG\'s searchable video\ndatabase has been implemented using the Screening Room package of\napplications from Convera in Vienna, Virginia. Formed in December 2000\nfrom Excalibur Technologies and Intel\'s Interactive Media Services\nDivision, Convera targets corporate and institutional markets with\nproducts for securely accessing, indexing, and searching rich media\ncontent-text, images, audio, and video-across interconnected computer\nnetworks. Among its public-sector clients are the FBI, NASA, the\nNuclear Regulatory Commission, U.S. military services, the Departments\nof Justice and State, and various domestic and foreign intelligence\nagencies\n', ['digitized video integration', 'networked knowledge base', 'Accident Response Group', 'Sandia National Labs', 'national security laboratory', 'U.S. nuclear weapons programs', 'searchable video database', 'Screening Room package', 'rich media content', 'public-sector clients', 'computer based training', 'emergency services', 'security of data', 'video databases']), ('Approximate relaxed descent method for optimal control problems\nWe consider an optimal control problem for systems governed by ordinary\ndifferential equations with control constraints. Since no convexity\nassumptions are made on the data, the problem is reformulated in\nrelaxed form. The relaxed state equation is discretized by the implicit\ntrapezoidal scheme and the relaxed controls are approximated by\npiecewise constant relaxed controls. We then propose a combined descent\nand discretization method that generates sequences of discrete relaxed\ncontrols and progressively refines the discretization. Since here the\nadjoint of the discrete state equation is not defined, we use, at each\niteration, an approximate derivative of the cost functional defined by\ndiscretizing the continuous adjoint equation and the integral involved\nby appropriate trapezoidal schemes. It is proved that accumulation\npoints of sequences constructed by this method satisfy the strong\nrelaxed necessary conditions for optimality for the continuous problem.\nFinally, the computed relaxed controls can be easily approximated by\npiecewise constant classical controls\n', ['approximate relaxed descent method', 'optimal control problems', 'ordinary differential equations', 'relaxed state equation discretization', 'implicit trapezoidal scheme', 'piecewise constant relaxed controls', 'relaxed control approximation', 'discrete relaxed control sequences', 'discretization refinement', 'discrete state equation', 'cost functional approximate derivative', 'trapezoidal schemes', 'differential equations', 'gradient index optics', 'optimal control', 'piecewise constant techniques']), ('Mixture of experts classification using a hierarchical mixture model\nA three-level hierarchical mixture model for classification is presented that\nmodels the following data generation process: (1) the data are\ngenerated by a finite number of sources (clusters), and (2) the\ngeneration mechanism of each source assumes the existence of individual\ninternal class-labeled sources (subclusters of the external cluster).\nThe model estimates the posterior probability of class membership\nsimilar to a mixture of experts classifier. In order to learn the\nparameters of the model, we have developed a general training approach\nbased on maximum likelihood that results in two efficient training\nalgorithms. Compared to other classification mixture models, the\nproposed hierarchical model exhibits several advantages and provides\nimproved classification performance as indicated by the experimental\nresults\n', ['hierarchical mixture model', 'classification', 'data generation process', 'Bayes classifier', 'experts classifier', 'posterior probability of class membership', 'Bayes methods', 'pattern classification']), ('Algorithms for improving the quality of R-trees\nA novel approach to operation with a structure for spatial indexing of extended\nobjects shaped as R-trees is considered. It consists of the initial\nglobal construction of an efficient R-tree structure and the subsequent\noperation with it using conventional dynamic algorithms. A global\nstrategy for constructing an R-tree reduced to a problem of dividing a\nset of rectangular objects into K parts with minimum mutual overlay is\nsuggested. Base, box, and "Divide and Conquer" algorithms are\nsuggested. The results of experimental modeling of the execution of\nvarious algorithms are discussed\n', ['R-trees', 'spatial indexing', 'extended objects', 'dynamic algorithms', 'rectangular objects', 'minimum mutual overlay', 'graphical search', 'computational geometry', 'algorithm theory', 'computational geometry', 'minimisation', 'trees (mathematics)']), ('Hybrid decision tree\nIn this paper, a hybrid learning approach named hybrid decision tree (HDT) is\nproposed. HDT simulates human reasoning by using symbolic learning to\ndo qualitative analysis and using neural learning to do subsequent\nquantitative analysis. It generates the trunk of a binary HDT according\nto the binary information gain ratio criterion in an instance space\ndefined by only original unordered attributes. If unordered attributes\ncannot further distinguish training examples falling into a leaf node\nwhose diversity is beyond the diversity-threshold, then the node is\nmarked as a dummy node. After all those dummy nodes are marked, a\nspecific feedforward neural network named FANNC that is trained in an\ninstance space defined by only original ordered attributes is exploited\nto accomplish the learning task. Moreover, this paper distinguishes\nthree kinds of incremental learning tasks. Two incremental learning\nprocedures designed for example-incremental learning with different\nstorage requirements are provided, which enables HDT to deal gracefully\nwith data sets where new data are frequently appended. Also a\nhypothesis-driven constructive induction mechanism is provided, which\nenables HDT to generate compact concept descriptions\n', ['hybrid decision tree', 'hybrid learning approach', 'reasoning', 'symbolic learning', 'qualitative analysis', 'neural learning', 'quantitative analysis', 'binary information gain ratio criterion', 'feedforward neural network', 'FANNC', 'incremental learning', 'storage requirements', 'data sets', 'hypothesis-driven constructive induction', 'decision trees', 'feedforward neural nets', 'inference mechanisms', 'knowledge acquisition', 'learning (artificial intelligence)']), ('Real-time tissue characterization on the basis of in vivo Raman spectra\nThe application of in vivo Raman spectroscopy for clinical diagnosis demands\ndedicated software that can perform the necessary signal processing and\nsubsequent (multivariate) data analysis, enabling clinically relevant\nparameters to be extracted and made available in real time. Here we\ndescribe the design and implementation of a software package that\nallows for real-time signal processing and data analysis of Raman\nspectra. The design is based on automatic data exchange between Grams,\na spectroscopic data acquisition and analysis program, and Matlab, a\nprogram designed for array-based calculations. The data analysis\nsoftware has a modular design providing great flexibility in developing\ncustom data analysis routines for different applications. The\nimplementation is illustrated by a computationally demanding\napplication for the classification of skin spectra using principal\ncomponent analysis and linear discriminant analysis\n', ['real-time tissue characterization', 'clinically relevant parameters extraction', 'array-based calculations', 'computationally demanding application', 'modular design', 'data analysis software', 'clinical diagnosis', 'dedicated software', 'multivariate data analysis', 'automatic data exchange', 'Grams', 'Matlab', 'linear discriminant analysis', 'skin spectra classification', 'medical signal processing', 'parameter estimation', 'patient diagnosis', 'principal component analysis', 'Raman spectroscopy', 'skin', 'software packages']), ("Cleared for take-off [Hummingbird Enterprise]\nA recent Gartner report identifies Hummingbird in the first wave of vendors as\nan early example of convergence in the 'smart enterprise suite' market.\nWe spoke to Hummingbird's Marketing Director for Northern Europe\n", ['smart enterprise suite', 'Hummingbird Enterprise', 'information content', 'knowledge content', 'collaboration', 'management information systems', 'records management']), ('A distance between elliptical distributions based in an embedding into the\nSiegel group\nThis paper describes two different embeddings of the manifolds corresponding to\nmany elliptical probability distributions with the informative geometry\ninto the manifold of positive-definite matrices with the Siegel metric,\ngeneralizing a result published previously elsewhere. These new general\nembeddings are applicable to a wide class of elliptical probability\ndistributions, in which the normal, t-Student and Cauchy are specific\nexamples. A lower bound for the Rao distance is obtained, which is\nitself a distance, and, through these embeddings, a number of\nstatistical tests of hypothesis are derived\n', ['elliptical distributions', 'Siegel group', 'manifolds embeddings', 'informative geometry', 'positive-definite matrices', 'elliptical probability distributions', 'lower bound', 'matrix algebra', 'probability', 'statistical analysis']), ('International swinging: making Swing components locale-sensitive\nAlthough Java and its GUI library Swing provide software developers with a\nhighly customizable framework for creating truly "international"\napplications, the Swing library is not sensitive to locale switches: it\ncannot automatically change an application\'s appearance to conform to\nthe conventions of a specific locale at run time. Several types of\napplications benefit from the ability to easily switch the language at\nrun time. Training applications and other programs that run on\ncomputers in public spaces (such as libraries, airports, or government\noffices) may need to support multiple languages. Other applications\n(like travel dictionaries or translation programs) are inherently\nmultilingual and are specifically designed to support users of\ndissimilar tongues. Such applications would greatly benefit if the\nuser-interface language could be customized at run time. The article\nshows you how to customize Swing to support locale switching at run\ntime. The author has created a new look-and-feel called the\nMLMetalLookandFeel (where ML stands for multilingual). This new\nlook-and-feel extends the standard Metal look-and-feel but is\nlocale-sensitive at run time\n', ['Java', 'GUI library', 'Swing library', 'travel dictionaries', 'translation programs', 'user-interface language', 'locale switching', 'MLMetalLookandFeel', 'graphical user interfaces', 'Java', 'object-oriented programming', 'software architecture', 'software libraries']), ('Error resilient intra refresh scheme for H.26L stream\nRecently much attention has been focused on video streaming through IP-based\nnetworks. An error resilient RD intra macro-block refresh scheme for\nH.26L Internet video streaming is introduced. Various channel\nsimulations have proved that this scheme is more effective than those\ncurrently adopted in H.26L\n', ['H.26L video streaming', 'Internet', 'IP-based networks', 'error resilient scheme', 'intra macro-block refresh scheme', 'channel simulations', 'RD intra refresh scheme', 'video communication', 'RDerr scheme', 'RDall scheme', 'Internet', 'visual communication']), ('Novel denoising algorithm for obtaining a superresolved position estimation\nWe present a new algorithm that uses the randomness of the noise pattern to\nachieve high positioning accuracy by applying a modified averaging\noperation. Using the suggested approach, noise sensitivity of the\npositioning accuracy can be significantly reduced. This new improved\nalgorithm can improve the performances of tracking systems used for\nmilitary as well as civil applications. The concept is demonstrated\ntheoretically as well as by optical experiment\n', ['denoising algorithm', 'superresolved position estimation', 'noise pattern randomness', 'high positioning accuracy', 'modified averaging operation', 'noise sensitivity', 'tracking systems', 'military applications', 'civil applications', 'optical experiment', 'error statistics', 'image resolution', 'military computing', 'military systems', 'motion estimation', 'optical tracking', 'random noise', 'shot noise', 'target tracking']), ('Perceptual audio coding using adaptive pre- and post-filters and lossless\ncompression\nThis paper proposes a versatile perceptual audio coding method that achieves\nhigh compression ratios and is capable of low encoding/decoding delay.\nIt accommodates a variety of source signals (including both music and\nspeech) with different sampling rates. It is based on separating\nirrelevance and redundancy reductions into independent functional\nunits. This contrasts traditional audio coding where both are\nintegrated within the same subband decomposition. The separation allows\nfor the independent optimization of the irrelevance and redundancy\nreduction units. For both reductions, we rely on adaptive filtering and\npredictive coding as much as possible to minimize the delay. A\npsycho-acoustically controlled adaptive linear filter is used for the\nirrelevance reduction, and the redundancy reduction is carried out by a\npredictive lossless coding scheme, which is termed weighted cascaded\nleast mean squared (WCLMS) method. Experiments are carried out on a\ndatabase of moderate size which contains mono-signals of different\nsampling rates and varying nature (music, speech, or mixed). They show\nthat the proposed WCLMS lossless coder outperforms other competing\nlossless coders in terms of compression ratios and delay, as applied to\nthe pre-filtered signal. Moreover, a subjective listening test of the\ncombined pre-filter/lossless coder and a state-of-the-art perceptual\naudio coder (PAC) shows that the new method achieves a comparable\ncompression ratio and audio quality with a lower delay\n', ['perceptual audio coding', 'adaptive pre-filters', 'adaptive post-filters', 'lossless compression', 'high compression ratio', 'low encoding/decoding delay', 'source signals', 'music', 'sampling rates', 'redundancy reduction', 'adaptive filtering', 'predictive coding', 'psycho-acoustically controlled adaptive linear filter', 'irrelevance reduction', 'predictive lossless coding', 'weighted cascaded least mean squared', 'WCLMS lossless coder', 'subjective listening test', 'pre-filter/lossless coder', 'audio quality', 'adaptive filters', 'adaptive signal processing', 'audio coding', 'data compression', 'delays', 'filtering theory', 'hearing', 'least squares approximations', 'prediction theory']), ('Entanglement measures with asymptotic weak-monotonicity as lower (upper) bound\nfor the entanglement of cost (distillation)\nWe propose entanglement measures with asymptotic weak-monotonicity. We show\nthat a normalized form of entanglement measures with the asymptotic\nweak-monotonicity are lower (upper) bound for the entanglement of cost\n(distillation)\n', ['entanglement measures', 'asymptotic weak-monotonicity', 'entanglement of cost', 'distillation', 'quantum computing', 'quantum interference phenomena', 'quantum theory']), ("Eliminating recency with self-review: the case of auditors' 'going concern'\njudgments\nThis paper examines the use of self-review to debias recency. Recency is found\nin the 'going concern' judgments of staff auditors, but is successfully\neliminated by the auditor's use of a simple self-review technique that\nwould be extremely easy to implement in audit practice. Auditors who\nself-review are also less inclined to make audit report choices that\nare inconsistent with their going concern judgments. These results are\nimportant because the judgments of staff auditors often determine the\ntype and extent of documentation in audit workpapers and serve as\npreliminary inputs for senior auditors' judgments and choices. If staff\nauditors' judgments are affected by recency, the impact of this bias\nmay be impounded in the ultimate judgments and choices of senior\nauditors. Since biased judgments can expose auditors to significant\ncosts involving extended audit procedures, legal liability and\ndiminished reputation, simple debiasing techniques that reduce this\nexposure are valuable. The paper also explores some future research\nneeds and other important issues concerning judgment debiasing in\napplied professional settings\n", ['auditor going concern judgments', 'recency debiasing', 'self-review', 'staff auditors', 'audit report choices', 'documentation', 'audit workpapers', 'senior auditors', 'extended audit procedures', 'legal liability', 'diminished reputation', 'judgment debiasing', 'applied professional settings', 'accountability', 'probability judgments', 'auditing', 'behavioural sciences', 'decision theory', 'probability']), ('An analytic center cutting plane method for semidefinite feasibility problems\nSemidefinite feasibility problems arise in many areas of operations research.\nThe abstract form of these problems can be described as finding a point\nin a nonempty bounded convex body Gamma in the cone of symmetric\npositive semidefinite matrices. Assume that Gamma is defined by an\noracle, which for any given m * m symmetric positive semidefinite\nmatrix Gamma either confirms that Y epsilon Gamma or returns a cut,\ni.e., a symmetric matrix A such that Gamma is in the half-space {Y : A\n. Y <or= A . Y}. We study an analytic center cutting plane algorithm\nfor this problem. At each iteration, the algorithm computes an\napproximate analytic center of a working set defined by the cutting\nplane system generated in the previous iterations. If this approximate\nanalytic center is a solution, then the algorithm terminates; otherwise\nthe new cutting plane returned by the oracle is added into the system.\nAs the number of iterations increases, the working set shrinks and the\nalgorithm eventually finds a solution to the problem. All iterates\ngenerated by the algorithm are positive definite matrices. The\nalgorithm has a worst-case complexity of O*(m/sup 3// epsilon /sup 2/)\non the total number of cuts to be used, where epsilon is the maximum\nradius of a ball contained by Gamma\n', ['semidefinite feasibility problems', 'analytic center cutting plane method', 'operations research', 'nonempty bounded convex body', 'symmetric positive semidefinite matrices', 'oracle', 'iteration', 'approximate analytic center', 'working set', 'worst-case complexity', 'maximum ball radius', 'computational complexity', 'convergence of numerical methods', 'iterative methods', 'matrix algebra', 'Newton method', 'operations research']), ('Stability of Runge-Kutta methods for delay integro-differential equations\nWe study stability of Runge-Kutta (RK) methods for delay integro-differential\nequations with a constant delay on the basis of the linear equation\ndu/dt = Lu(t) + Mu(t- tau ) + K integral /sub t- tau //sup t/ u( theta\n)d theta , where L, M, K are constant complex matrices. In particular,\nwe show that the same result as in the case K = 0 (Koto, 1994) holds\nfor this test equation, i.e., every A-stable RK method preserves the\ndelay-independent stability of the exact solution whenever a step-size\nof the form h = tau /m is used, where m is a positive integer\n', ['Runge-Kutta methods', 'delay integro-differential equations', 'constant delay', 'stability', 'integro-differential equations', 'numerical stability', 'Runge-Kutta methods']), ('Verifying resonant grounding in distribution systems\nThe authors describe RESFAL, a software tool that can check on the behavior of\ndistribution network resonant grounding systems with regard to\ncompensation coil tuning and to fault detection\n', ['RESFAL software tool', 'resonant grounding systems', 'compensation coil tuning', 'fault detection', 'computer simulation', 'power distribution systems', 'coils', 'compensation', 'earthing', 'power distribution faults', 'power distribution protection', 'power system analysis computing', 'software packages']), ("Optimal and safe ship control as a multi-step matrix game\nThe paper describes the process of the safe ship control in a collision\nsituation using a differential game model with j participants. As an\napproximated model of the manoeuvring process, a model of a multi-step\nmatrix game is adopted here. RISKTRAJ computer program is designed in\nthe Matlab language in order to determine the ship's trajectory as a\ncertain sequence of manoeuvres executed by altering the course and\nspeed, in the online navigator decision support system. These\nconsiderations are illustrated with examples of a computer simulation\nof the safe ship's trajectories in real situation at sea when passing\ntwelve of the encountered objects\n", ['ship control', 'collision avoidance', 'RISKTRAJ computer program', 'multistep matrix game', 'trajectory tracking', 'differential game', 'optimal control', 'decision support system', 'online navigation', 'collision avoidance', 'computerised navigation', 'decision support systems', 'differential games', 'optimal control', 'path planning', 'real-time systems', 'ships']), ("Analyzing the potential of a firm: an operations research approach\nAn approach to analyzing the potential of a firm, which is understood as the\nfirm's ability to provide goods or (and) services to be supplied to a\nmarketplace under restrictions imposed by a business environment in\nwhich the firm functions, is proposed. The approach is based on using\nlinear inequalities and, generally, mixed variables in modelling this\nability for a broad spectrum of industrial, transportation,\nagricultural, and other types of firms and allows one to formulate\nproblems of analyzing the potential of a firm as linear programming\nproblems or mixed programming problems with linear constraints. This\napproach generalizes a previous one which was proposed for a more\nnarrow class of models, and allows one to effectively employ a widely\navailable software for solving practical problems of the considered\nkind, especially for firms described by large scale models of\nmathematical programming\n", ['firm potential analysis', 'operations research', 'OR', 'linear inequalities', 'industrial firms', 'transportation firms', 'agricultural firms', 'linear programming', 'mixed programming', 'large-scale models', 'mathematical programming', 'corporate modelling', 'mathematical programming', 'operations research']), ('Automatic extraction of eye and mouth fields from a face image using\neigenfeatures and ensemble networks\nThis paper presents a novel algorithm for the extraction of the eye and mouth\n(facial features) fields from 2D gray level images. Eigenfeatures are\nderived from the eigenvalues and eigenvectors of the binary edge data\nset constructed from eye and mouth fields. Such eigenfeatures are ideal\nfeatures for finely locating fields efficiently. The eigenfeatures are\nextracted from a set of the positive and negative training samples for\nfacial features and are used to train a multilayer perceptron (MLP)\nwhose output indicates the degree to which a particular image window\ncontains the eyes or the mouth within itself. An ensemble network\nconsisting of a multitude of independent MLPs was used to enhance the\ngeneralization performance of a single MLP. It was experimentally\nverified that the proposed algorithm is robust against facial size and\neven slight variations of the pose\n', ['eye field extraction', 'mouth field extraction', 'face feature extraction', '2D gray level images', 'eigenvalues', 'eigenvectors', 'binary edge data set', 'training samples', 'multilayer perceptron', 'generalization', 'experiment', 'eigenfeatures', 'ensemble neural networks', 'eigenvalues and eigenfunctions', 'face recognition', 'feature extraction', 'generalisation (artificial intelligence)', 'learning (artificial intelligence)', 'multilayer perceptrons']), ('Quantum universal variable-length source coding\nWe construct an optimal quantum universal variable-length code that achieves\nthe admissible minimum rate, i.e., our code is used for any probability\ndistribution of quantum states. Its probability of exceeding the\nadmissible minimum rate exponentially goes to 0. Our code is optimal in\nthe sense of its exponent. In addition, its average error\nasymptotically tends to 0\n', ['quantum universal variable-length source coding', 'optimal quantum universal variable-length code', 'admissible minimum rate', 'probability distribution', 'quantum states', 'quantum information theory', 'quantum cryptography', 'optimal code', 'exponent', 'average error', 'encoding', 'quantum communication', 'source coding']), ('Elastic constraint branching, the Wedelin/Carmen Lagrangian heuristic and\ninteger programming for personnel scheduling\nThe Wedelin algorithm is a Lagrangian based heuristic that is being\nsuccessfully used by Carmen Systems to solve large crew pairing\nproblems within the airline industry. We extend the Wedelin approach by\ndeveloping an implementation for personnel scheduling problems (also\ntermed staff rostering problems) that exploits the special structure of\nthese problems. We also introduce elastic constraint branching with the\ntwin aims of improving the performance of our new approach and making\nit more column generation friendly. Numerical results show that our\napproach can outperform the commercial solver CPLEX on difficult\ncommercial rostering problems\n', ['elastic constraint branching', 'integer programming', 'personnel scheduling', 'Wedelin algorithm', 'Lagrangian based heuristic', 'Carmen Systems', 'large crew pairing problems', 'airline industry', 'staff rostering problems', 'column generation friendly approach', 'duality (mathematics)', 'human resource management', 'integer programming', 'search problems', 'travel industry']), ('Ideal sliding mode in the problems of convex optimization\nThe characteristics of the sliding mode that appears with using continuous\nconvex-programming algorithms based on the exact penalty functions were\ndiscussed. For the case under study, the ideal sliding mode was shown\nto occur in the absence of infinite number of switchings\n', ['ideal sliding mode', 'convex optimization', 'continuous convex-programming algorithms', 'exact penalty functions', 'convex programming', 'differential equations', 'set theory', 'variable structure systems']), ("Public business libraries: the next chapter\nTraces the history of the provision of business information by Leeds Public\nLibraries, UK, from the opening of the Public Commercial and Technical\nLibrary in 1918 to the revolutionary impact of the Internet in the\n1990s. Describes how the Library came to terms with the need to\nintegrate the Internet into its mainstream business information\nservices, with particular reference to its limitations and to the\nprovision of company information, market research, British Standards\ninformation, press cuttings and articles from specialized trade and\nscientific journals, and patents information. Focuses on some of the\nreasons why the public business library is still needed as a service to\nbusinesses, even after the introduction of the Internet and considers\nthe Library's changing role and the need to impress on all concerned,\nespecially government, the continuing value of these services. Looks to\nthe partnerships formed by the Library over the years and the ways in\nwhich these are expected to assist in realizing future opportunities,\nin particular, the fact that all public libraries in England gained\nfree Internet access at the end of 2001. Offers some useful ideas about\nhow the Library could develop, noting that SINTO, a Sheffield based\ninformation network formed in 1938 and originally a partnership between\nthe public library, the two Sheffield universities and various leading\nsteel companies of the time, is being examined as a model for future\nservices in Leeds. Concludes that the way forward can be defined in\nterms of five actions: redefinition of priorities; marketing; budgets;\nresources; and the use of information technology (IT)\n", ['history', 'public business libraries', 'Leeds Public Libraries', 'Internet', 'Public Commercial and Technical Library', 'business information services', 'company information', 'market research', 'British Standards information', 'press cuttings', 'trade journal articles', 'scientific journal articles', 'patents information', 'government', 'SINTO', 'information network', 'Sheffield universities', 'steel companies', 'priority redefinition', 'marketing', 'budgets', 'resources', 'IT use', 'budgeting', 'history', 'information resources', 'Internet', 'library automation', 'management of change', 'marketing', 'public libraries', 'special libraries']), ('Learning spatial relations using an inductive logic programming system\nThe ability to learn spatial relations is a prerequisite for performing many\nrelevant tasks such as those associated with motion, orientation,\nnavigation, etc. This paper reports on using an Inductive Logic\nProgramming (ILP) system for learning function-free Horn-clause\ndescriptions of spatial knowledge. Its main contribution, however, is\nto show that an existing relation between two reference systems-the\nspeaker-relative and the absolute-can be automatically learned by an\nILP system, given the proper background knowledge and positive examples\n', ['spatial relations learning', 'inductive logic programming system', 'spatial relations', 'function-free Horn-clause descriptions', 'Horn clauses', 'inductive logic programming', 'knowledge representation', 'learning (artificial intelligence)']), ("Vibration control of the rotating flexible-shaft/multi-flexible-disk system\nwith the eddy-current damper\nIn this paper, the rotating flexible-Timoshenko-shaft/flexible-disk coupling\nsystem is formulated by applying the assumed-mode method into the\nkinetic and strain energies, and the virtual work done by the\neddy-current damper. From Lagrange's equations, the resulting\ndiscretized equations of motion can be simplified as a bilinear system\n(BLS). Introducing the control laws, including the quadratic, nonlinear\nand optimal feedback control laws, into the BLS, it is found that the\neddy-current damper can be used to suppress flexible and shear\nvibrations simultaneously, and the system is globally asymptotically\nstable. Numerical results are provided to validate the theoretical\nanalysis\n", ['rotating flexible-shaft/multi-flexible-disk system', 'eddy-current damper', 'rotating flexible-Timoshenko-shaft/flexible-disk coupling system', 'assumed-mode method', 'virtual work', "Lagrange's equations", 'discretized equations of motion', 'bilinear system', 'quadratic feedback control laws', 'nonlinear feedback control laws', 'optimal feedback control laws', 'shear vibrations', 'flexible vibrations', 'damping', 'feedback', 'nonlinear control systems', 'numerical analysis', 'optimal control', 'rotation', 'vibration control', 'vibrations']), ('Planning linear construction projects: automated method for the generation of\nearthwork activities\nEarthworks planning for road construction projects is a complex operation and\nthe planning rules used are usually intuitive and not well defined. An\napproach to automate the earthworks planning process is described and\nthe basic techniques that are used are outlined. A computer-based\nsystem has been developed, initially to help planners use existing\ntechniques more efficiently. With their input, the system has been\nextended to incorporate a knowledge base and a simulation of the\nearthworks processes. As well as creating activity sets in a much\nshorter time, the system has shown that for a real project, the model\nis able to generate activity sets that are comparable to those\ngenerated by a project planner\n', ['linear construction projects', 'earthwork activities', 'road construction projects', 'planning rules', 'earthworks planning process', 'computer-based system', 'knowledge base', 'civil engineering computing', 'intelligent design assistants']), ("Will new Palms win laurels.?\nPalmSource's latest operating system for mobile devices harnesses the ARM\narchitecture to support more powerful business software, but there are\nconcerns over compatibility with older applications\n", ['PalmSource', 'operating system', 'mobile devices', 'Palm OS 5.0', 'ARM architecture', 'compatibility', 'laptop computers', 'mobile computing', 'operating systems (computers)']), (".NET obfuscation and intellectual property\nThe author considers obfuscation options for protecting .NET code. Many\nprograms won't need obfuscation because the loss caused by reverse\nengineering will be nonexistent. Numerous obfuscators are already\navailable for the .NET platform, ranging from a basic renaming\nobfuscator to a fully functional obfuscator that handles mixed\nIL/native code assemblies created in any managed language, including\nMicrosoft's C++ with Managed Extensions. An obfuscator simply makes\nyour application harder to reverse engineer. It does not prevent\nreverse engineering. However, the cost of obfuscation is insignificant\nwhen compared to the cost of a typical software development project. If\nyou feel like an obfuscator provides you any benefit at all, it's\nprobably worth the price\n", ['.NET obfuscation', 'intellectual property', 'reverse engineering', 'industrial property', 'meta data', 'network operating systems', 'reverse engineering', 'software engineering']), ('Design and modeling of an interval-based ABR flow control protocol\nA novel flow control protocol is presented for availability bit rate (ABR)\nservice in asynchronous transfer mode (ATM) networks. This scheme\nfeatures periodic explicit rate feedback that enables precise\nallocation of link bandwidth and buffer space on a hop-by-hop basis to\nguarantee maximum throughput, minimum cell loss, and high resource\nefficiency. With the inclusion of resource management cell\nsynchronization and consolidation algorithms, this protocol is capable\nof controlling point-to-multipoint ABR services within a unified\nframework. The authors illustrate the modeling of single ABR\nconnection, the interaction between multiple ABR connections, and the\nconstraints applicable to flow control decisions. A loss-free flow\ncontrol mechanism is presented for high-speed ABR connections using a\nfluid traffic model. Supporting algorithms and ATM signaling procedures\nare specified, in company with linear system modeling, numerical\nanalysis, and simulation results, which demonstrate its performance and\ncost benefits in high-speed backbone networking scenarios\n', ['interval-based ABR flow control protocol', 'modeling', 'design', 'availability bit rate service', 'ATM networks', 'periodic explicit rate feedback', 'link bandwidth allocation', 'buffer space allocation', 'maximum throughput', 'minimum cell loss', 'high resource efficiency', 'resource management cell synchronization algorithms', 'resource management cell consolidation algorithms', 'point-to-multipoint services', 'flow control decisions', 'loss-free flow control mechanism', 'high-speed ABR connections', 'fluid traffic model', 'signaling', 'linear system modeling', 'numerical analysis', 'simulation', 'high-speed backbone networking scenarios', 'asynchronous transfer mode', 'bandwidth allocation', 'digital simulation', 'feedback', 'protocols', 'synchronisation', 'telecommunication computing', 'telecommunication congestion control', 'telecommunication signalling', 'telecommunication traffic']), ("Unlocking the clubhouse: the Carnegie Mellon experience\nIn the fall of 1995, just seven of 95 students entering the undergraduate\nprogram in computer science at Carnegie Mellon University were women.\nIn 2000, 54 of 130, or 42%, were women. What happened? This article\npresents a brief history of the transformation at Carnegie Mellon's\nSchool of Computer Science, and the research project that lay behind it\n", ['students', 'undergraduate program', 'computer science education', 'Carnegie Mellon University', 'women', 'history', 'research project', 'gender issues', 'computer science education', 'gender issues', 'social aspects of automation']), ('Information needs of the working journalists in Orissa: a study\nProvides an insight into the various information needs of working journalists\nin Orissa. Analyses data received from 226 working journalists\nrepresenting 40 newspaper organisations. Also depicts the\nspecialisation of working journalists, their frequency of information\nrequirement, mode of dissemination preferred, information sources\nexplored, mode of services opted, and their information privations. The\nstudy asserts that subjects primarily concerned with the professional\nwork and image of the working journalists are rated utmost significant\n', ['information needs', 'working journalists', 'data analysis', 'newspaper organisations', 'information requirement', 'information dissemination', 'information sources', 'professional work', 'information dissemination', 'information needs', 'information resources', 'publishing']), ('Four factors influencing the fair market value of out-of-print books.1\nFour factors (edition, condition, dust jacket, and autograph) that are\nhypothesized to influence the value of books are identified and linked\nto basic economic principles, which are explained. A sample of\nfifty-six titles is qualitatively examined to test the hypothesis\n', ['fair market value', 'out-of-print books', 'economic principles', 'pricing', 'costing', 'publishing']), ('A humane tool for aiding computer science advisors, computer science students,\nand parents\nOver the past few years, the computer science department faculty at Baylor has\nobserved that some students who perform adequately during the freshman\nand sophomore years have substantial difficulty during the junior and\nsenior years of study. Baylor University is an institution committed to\nbeing caring of its students. The objective for this study grew out of\nthese two realities. There are three objectives of this research. One\nobjective is to identify students, no later than the sophomore year,\nwho are less likely to succeed as computer science majors. A second\nobjective is to accomplish this identification by using data from\nseniors majoring in computer science. A third objective is to begin to\nuse this information at the end of their sophomore year when meeting\nwith a computer science faculty advisor. A regression study is\nconducted on the data from all students classified as seniors, majoring\nin computer science in May 2001, showing grades in six freshman and\nsophomore courses, and showing grades for at least five junior or\nsenior level computer science courses. These students and their course\nperformance data constituted the study sample\n', ['humane tool', 'computer science advisors', 'computer science students', 'parents', 'Baylor University', 'student care', 'sophomore year', 'computer science majors', 'regression study', 'course performance data', 'computer science education', 'statistical analysis', 'teaching']), ("Optimization of advertising expenses in the functioning of an insurance company\nWith the use of Pontryagin's maximum principle, a problem of optimal time\ndistribution of advertising expenses in the functioning of an insurance\ncompany is solved\n", ['optimization', 'advertising expenses', 'insurance company', 'Pontryagin maximum principle', 'optimal time distribution', 'differential equations', 'advertising', 'differential equations', 'economics', 'insurance', 'maximum principle', 'optimisation', 'statistical mechanics']), ('Complex dynamics in nearly symmetric three-cell cellular neural networks\nThe paper introduces a class of third-order nonsymmetric Cellular Neural\nNetworks (CNNs), and shows through computer simulations that they\nundergo a cascade of period doubling bifurcations which leads to the\nbirth of a large-size complex attractor. A major point is that these\nbifurcations and complex dynamics happen in a small neighborhood of a\nparticular CNN with a symmetric interconnection matrix\n', ['complex dynamics', 'nearly symmetric three-cell cellular neural networks', 'CNN', 'period doubling bifurcations', 'large-size complex attractor', 'symmetric interconnection matrix', 'robustness', 'complete stability', 'perturbations', 'stable limit cycles', 'differential equations', 'neuron interconnection matrix', 'bifurcation', 'cellular neural nets', 'limit cycles', 'matrix algebra', 'nonlinear control systems', 'nonlinear dynamical systems', 'perturbation theory', 'robust control']), ("Mining open answers in questionnaire data\nSurveys are important tools for marketing and for managing customer\nrelationships; the answers to open-ended questions, in particular,\noften contain valuable information and provide an important basis for\nbusiness decisions. The summaries that human analysts make of these\nopen answers, however, tend to rely too much on intuition and so aren't\nsatisfactorily reliable. Moreover, because the Web makes it so easy to\ntake surveys and solicit comments, companies are finding themselves\ninundated with data from questionnaires and other sources. Handling it\nall manually would be not only cumbersome but also costly. Thus,\ndevising a computer system that can automatically mine useful\ninformation from open answers has become an important issue. We have\ndeveloped a survey analysis system that works on these principles. The\nsystem mines open answers through two statistical learning techniques:\nrule learning (which we call rule analysis) and correspondence analysis\n", ['natural language response analysis', 'survey analysis', 'text mining system', 'questionnaire data', 'statistical learning techniques', 'rule analysis', 'correspondence analysis', 'open answer mining', 'data analysis', 'data mining', 'learning (artificial intelligence)', 'marketing data processing', 'text analysis']), ('Fast frequency acquisition phase-frequency detectors for Gsamples/s\nphase-locked loops\nThis paper describes two techniques for designing phase-frequency detectors\n(PFDs) with higher operating frequencies [periods of less than 8* the\ndelay of a fan-out-4 inverter (FO-4)] and faster frequency acquisition.\nPrototypes designed in 0.25- mu m CMOS process exhibit operating\nfrequencies of 1.25 GHz [=1/(8.FO-4)] and 1.5 GHz [=1/(6.7.FO-4)] for\ntwo techniques, respectively, whereas a conventional PFD operates at\n<1 GHz [=1/(10.FO-4)]. The two proposed PFDs achieve a capture range\nof 1.7* and 1.4* the conventional design, respectively\n', ['phase-frequency detectors', 'fast frequency acquisition', 'CMOS process', 'clock generator', 'latch-based PFD architecture', 'phase-locked loop', 'GSamples/s PLL', 'pass-transistor DFF PFD architecture', '1.25 GHz', '1.5 GHz', '0.25 micron', 'CMOS digital integrated circuits', 'detector circuits', 'digital phase locked loops', 'high-speed integrated circuits', 'timing circuits']), ('Color plane interpolation using alternating projections\nMost commercial digital cameras use color filter arrays to sample red, green,\nand blue colors according to a specific pattern. At the location of\neach pixel only one color sample is taken, and the values of the other\ncolors must be interpolated using neighboring samples. This color plane\ninterpolation is known as demosaicing; it is one of the important tasks\nin a digital camera pipeline. If demosaicing is not performed\nappropriately, images suffer from highly visible color artifacts. In\nthis paper we present a new demosaicing technique that uses\ninter-channel correlation effectively in an alternating-projections\nscheme. We have compared this technique with six state-of-the-art\ndemosaicing techniques, and it outperforms all of them, both visually\nand in terms of mean square error\n', ['color plane interpolation', 'alternating projections', 'digital cameras', 'demosaicing', 'color filter arrays', 'color artifacts', 'inter-channel correlation', 'cameras', 'correlation methods', 'image colour analysis', 'interpolation', 'optical filters']), ('How airlines and airports recover from schedule perturbations: a survey\nThe explosive growth in air traffic as well as the widespread adoption of\nOperations Research techniques in airline scheduling has given rise to\ntight flight schedules at major airports. An undesirable consequence of\nthis is that a minor incident such as a delay in the arrival of a small\nnumber of flights can result in a chain reaction of events involving\nseveral flights and airports, causing disruption throughout the system.\nThis paper reviews recent literature in the area of recovery from\nschedule disruptions. First we review how disturbances at a given\nairport could be handled, including the effects of runways and fixes.\nThen we study the papers on recovery from airline schedule\nperturbations, which involve adjustments in flight schedules, aircraft,\nand crew. The mathematical programming techniques used in ground\nholding are covered in some detail. We conclude the review with\nsuggestions on how singular perturbation theory could play a role in\nanalyzing disruptions to such highly sensitive schedules as those in\nthe civil aviation industry\n', ['air traffic management', 'schedule perturbation', 'operations research techniques', 'airline scheduling', 'tight flight schedules', 'airports', 'schedule disruptions', 'recovery', 'disturbance handling', 'runways', 'flight schedule adjustments', 'aircraft adjustments', 'crew adjustments', 'mathematical programming techniques', 'ground holding', 'singular perturbation theory', 'civil aviation industry', 'air traffic control', 'airports', 'mathematical programming', 'scheduling', 'singularly perturbed systems', 'travel industry']), ('CAD/CAE software aids converter design [DC/DC power conversion]\nTypically, power supply design involves electronic and magnetic components. In\nthis paper, the authors describe, using a flyback converter example,\nhow CAD/CAE tools can aid the power supply engineer in both areas,\nreducing prototyping costs and providing insights into system\nperformance\n', ['DC/DC power convertor design', 'power supply design', 'electronic components', 'magnetic components', 'CAD/CAE software', 'flyback power convertor topology', 'prototyping costs', 'circuit CAD', 'computer aided engineering', 'DC-DC power convertors', 'power engineering computing']), ('Design and implementation of a new sliding-mode observer for speed-sensorless\ncontrol of induction machine\nIn this letter, a new sliding-mode-sensorless control algorithm is proposed for\nthe field-oriented induction machine drive. In the proposed algorithm,\nthe terms containing flux, speed, and rotor time constant, which are\ncommon in both current and flux equations, in the current model of the\ninduction machine are estimated by a sliding function. The flux and\nspeed estimation accuracy is guaranteed when the error between the\nactual current and observed current converges to zero. Hence, the\nfourth-order system is reduced to two second-order systems, and the\nspeed estimation becomes very simple and robust to the parameter\nuncertainties. The new approach is verified by simulation and\nexperimental results\n', ['speed-sensorless control', 'induction machine', 'sliding-mode observer', 'induction motor drive', 'sensorless control', 'flux', 'speed', 'rotor time constant', 'flux equations', 'current equations', 'current model', 'sliding function', 'speed estimation accuracy', 'fourth-order system reduction', 'parameter uncertainties', 'induction motor drives', 'machine vector control', 'observers', 'parameter estimation', 'variable structure systems']), ('Self-organizing feature maps predicting sea levels\nIn this paper, a new method for predicting sea levels employing self-organizing\nfeature maps is introduced. For that purpose the maps are transformed\nfrom an unsupervised learning procedure to a supervised one. Two\nconcepts, originally developed to solve the problems of convergence of\nother network types, are proposed to be applied to Kohonen networks: a\nfunctional relationship between the number of neurons and the number of\nlearning examples and a criterion to break off learning. The latter one\ncan be shown to be conform with the process of self-organization by\nusing U-matrices for visualization of the learning procedure. The\npredictions made using these neural models are compared for accuracy\nwith observations and with the prognoses prepared using six models: two\nhydrodynamic models, a statistical model, a nearest neighbor model, the\npersistence model, and the verbal forecasts that are broadcast and kept\non record by the Sea Level Forecast Service of the Federal Maritime and\nHydrography Agency (BSH) in Hamburg. Before training the maps, the\nmeteorological and oceanographic situation has to be condensed as well\nas possible, and the weight and learning vectors have to be made as\nsmall as possible. The self-organizing feature maps predict sea levels\nbetter than all six models of comparison\n', ['self-organizing feature maps', 'sea level prediction', 'supervised learning', 'Kohonen networks', 'neurons', 'U-matrices', 'visualization', 'hydrodynamic models', 'statistical model', 'nearest neighbor model', 'persistence model', 'verbal forecasts', 'Sea Level Forecast Service', 'Federal Maritime and Hydrography Agency', 'oceanographic situation', 'meteorological situation', 'learning vectors', 'geophysics computing', 'learning (artificial intelligence)', 'oceanography', 'self-organising feature maps']), ("Waiting for the wave to crest [wavelength services]\nWavelength services have been hyped ad nauseam for years. But despite their\nquick turn-up time and impressive margins, such services have yet to\nlive up to the industry's expectations. The reasons for this lukewarm\nreception are many, not the least of which is the confusion that still\nsurrounds the technology, but most industry observers are still\nconvinced that wavelength services with ultimately flourish\n", ['wavelength services', 'fiber optic networks', 'Looking Glass Networks', 'PointEast Research', 'optical fibre networks', 'telecommunication']), ('From revenue management concepts to software systems\nIn 1999, after developing and installing over 170 revenue management (RM)\nsystems for more than 70 airlines, PROS Revenue Management, Inc. had\nthe opportunity to develop RM systems for three companies in nonairline\nindustries. PROS research and design department designed the\nopportunity analysis study (OAS), a mix of OR/MS, consulting, and\nsoftware development practices to determine the applicability of RM in\nnew business situations. PROS executed OASs with the three companies.\nIn all three cases, the OAS supported the value of RM and led to\ncontracts for implementation of RM systems\n', ['revenue management concepts', 'software systems', 'RM systems', 'PROS Revenue Management', 'Inc', 'opportunity analysis study', 'OAS', 'OR/MS', 'consulting practices', 'software development practices', 'management science', 'marketing data processing', 'software engineering', 'systems analysis']), ("Job rotation in an academic library: damned if you do and damned if you don't!\nThis article considers job rotation-the systematic movement of employees from\none job to another-as one of the many tools within the organizational\ndevelopment tool kit. There is a brief consideration of useful print\nand Internet literature on the subject as well as a discussion of the\npros and cons of job rotation. The application of job rotation methods\nin Ryerson University Library, a small academic library, concludes the\narticle in order to illustrate process and insights through example\n", ['job rotation', 'academic library', 'systematic employee movement', 'organizational development', 'Ryerson University Library', 'academic libraries', 'human resource management', 'personnel']), ('A heuristic approach to resource locations in broadband networks\nIn broadband networks, such as ATM, the importance of dynamic migration of data\nresources is increasing because of its potential to improve performance\nespecially for transaction processing. In environments with migratory\ndata resources, it is necessary to have mechanisms to manage the\nlocations of each data resource. In this paper, we present an algorithm\nthat makes use of system state information and heuristics to manage\nlocations of data resources in a distributed network. In the proposed\nalgorithm, each site maintains information about state of other sites\nwith respect to each data resource of the system and uses it to find:\n(1) a subset of sites likely to have the requested data resource; and\n(2) the site where the data resource is to be migrated from the current\nsite. The proposed algorithm enhances its effectiveness by continuously\nupdating system state information stored at each site. It focuses on\nreducing the overall average time delay needed by the transaction\nrequests to locate and access the migratory data resources. We\nevaluated the performance of the proposed algorithm and also compared\nit with one of the existing location management algorithms, by\nsimulation studies under several system parameters such as the\nfrequency of requests generation, frequency of data resource\nmigrations, network topology and scale of network. The experimental\nresults show the effectiveness of the proposed algorithm in all cases\n', ['broadband networks', 'ATM', 'resource locations', 'heuristics', 'distributed network', 'data resource migrations', 'network topology', 'asynchronous transfer mode', 'broadband networks', 'network topology', 'optimisation', 'resource allocation']), ("Women in computing around the world\nThis paper describes the participation of women in computing in more than 30\ncountries, by focussing on participation at undergraduate level. A\nbrief discussion covers how societal and cultural factors may affect\nwomen's participation. Statistics from many different sources are\npresented for comparison. Generally, participation is low - most\ncountries fall in the 10-40% range with a few below 10% and a few above\n40%\n", ['women', 'undergraduate computing', 'societal factors', 'cultural factors', 'statistics', 'computer science education', 'gender issues', 'social aspects of automation']), ('Efficient algorithms for stiff elliptic problems with large parameters\nWe consider a finite element approximation and iteration algorithms for solving\nstiff elliptic boundary value problems with large parameters in front\nof a higher derivative. The convergence rate of the algorithms is\nindependent of the spread in coefficients and a discretization\nparameter\n', ['finite element approximation', 'iteration algorithms', 'stiff elliptic boundary value problems', 'large parameters', 'higher derivative', 'efficient algorithms', 'convergence rate', 'boundary-value problems', 'convergence of numerical methods', 'elliptic equations', 'finite element analysis', 'iterative methods']), ('Efficient tracking of the cross-correlation coefficient\nIn many (audio) processing algorithms, involving manipulation of discrete-time\nsignals, the performance can vary strongly over the repertoire that is\nused. This may be the case when the signals from the various channels\nare allowed to be strongly positively or negatively correlated. We\npropose and analyze a general formula for tracking the (time-dependent)\ncorrelation between two signals. Some special cases of this formula\nlead to classical results known from the literature, others are new.\nThis formula is recursive in nature, and uses only the instantaneous\nvalues of the two signals, in a low-cost and low-complexity manner; in\nparticular, there is no need to take square roots or to carry out\ndivisions. Furthermore, this formula can be modified with respect to\nthe occurrence of the two signals so as to further decrease the\ncomplexity, and increase ease of implementation. The latter\nmodification comes at the expense that not the actual correlation is\ntracked, but, rather, a somewhat deformed version of it. To overcome\nthis problem, we propose, for a number of instances of the tracking\nformula, a simple warping operation on the deformed correlation. Now we\nobtain, at least for sinusoidal signals, the correct value of the\ncorrelation coefficient. Special attention is paid to the convergence\nbehavior of the algorithm for stationary signals and the dynamic\nbehavior if there is a transition to another stationary state; the\nlatter is considered to be important to study the tracking abilities to\nnonstationary signals. We illustrate tracking algorithm by using it for\nstereo music fragments, obtained from a number of digital audio\nrecordings\n', ['efficient tracking', 'cross-correlation coefficient', 'audio processing algorithms', 'discrete-time signals', 'time-dependent correlation', 'recursive formula', 'warping operation', 'deformed correlation', 'sinusoidal signals', 'convergence behavior', 'stationary signals', 'dynamic behavior', 'stationary state', 'nonstationary signals', 'tracking algorithm', 'stereo music fragments', 'digital audio recording', 'audio signal processing', 'convergence of numerical methods', 'correlation methods', 'music', 'tracking']), ('Maintaining e-commerce\nE-commerce over the Web has created a relatively new type of information\nsystem. So it is hardly surprising that little attention has been given\nto the maintenance of such systems-and even less to attempting to\ndevelop them with future maintenance in mind. But there are various\nways e-commerce systems can be developed to reduce future maintenance\n', ['e-commerce systems maintenance', 'Web systems', 'electronic commerce', 'Internet', 'software maintenance', 'systems analysis']), ('Superconvergence of discontinuous Galerkin method for nonstationary hyperbolic\nequation\nFor the first order nonstationary hyperbolic equation taking the piecewise\nlinear discontinuous Galerkin solver, we prove that under the uniform\nrectangular partition, such a discontinuous solver, after\npostprocessing, can have two and half approximative order which is half\norder higher than the optimal estimate by P. Lesaint and P. Raviart\n(1974) under the rectangular partition\n', ['superconvergence of discontinuous Galerkin method', 'nonstationary hyperbolic equation', 'piecewise linear discontinuous Galerkin solver', 'rectangular partition', 'approximative order', 'approximation theory', 'Galerkin method', 'piecewise linear techniques']), ('Web-based experiments controlled by JavaScript: an example from probability\nlearning\nJavaScript programs can be used to control Web experiments. This technique is\nillustrated by an experiment that tested the effects of advice on\nperformance in the classic probability-learning paradigm. Previous\nresearch reported that people tested via the Web or in the lab tended\nto match the probabilities of their responses to the probabilities that\nthose responses would be reinforced. The optimal strategy, however, is\nto consistently choose the more frequent event; probability matching\nproduces suboptimal performance. We investigated manipulations we\nreasoned should improve performance. A horse race scenario in which\nparticipants predicted the winner in each of a series of races between\ntwo horses was compared with an abstract scenario used previously. Ten\ngroups of learners received different amounts of advice, including all\ncombinations of (1) explicit instructions concerning the optimal\nstrategy, (2) explicit instructions concerning a monetary sum to\nmaximize, and (3) accurate information concerning the probabilities of\nevents. The results showed minimal effects of horse race versus\nabstract scenario. Both advice concerning the optimal strategy and\nprobability information contributed significantly to performance in the\ntask. This paper includes a brief tutorial on JavaScript, explaining\nwith simple examples how to assemble a browser-based experiment\n', ['Web-based experiments', 'JavaScript', 'probability learning', 'advice', 'explicit instructions', 'probability', 'browser-based experiment', 'Internet-based research', 'authoring languages', 'educational computing', 'information resources', 'Internet', 'Java', 'probability']), ('Stabilization of a linear object by frequency-modulated pulsed signals\nA control system consisting of an unstable continuous linear part and a\npulse-frequency modulator in the feedback circuit is studied.\nConditions for the boundedness of the solutions of the system under any\ninitial data are determined\n', ['discrete systems', 'stabilization', 'frequency-modulated pulsed signals', 'linear stationary object', 'control system', 'feedback circuit', 'solution boundedness', 'closed loop systems', 'continuous time systems', 'differential equations', 'discrete time systems', 'feedback', 'linear systems', 'pulse frequency modulation', 'stability']), ('Q-learning for risk-sensitive control\nWe propose for risk-sensitive control of finite Markov chains a counterpart of\nthe popular Q-learning algorithm for classical Markov decision\nprocesses. The algorithm is shown to converge with probability one to\nthe desired solution. The proof technique is an adaptation of the\no.d.e. approach for the analysis of stochastic approximation\nalgorithms, with most of the work involved used for the analysis of the\nspecific o.d.e.s that arise\n', ['finite Markov chains', 'risk-sensitive control', 'Q-learning algorithm', 'classical Markov decision processes', 'algorithm convergence', 'reinforcement learning algorithms', 'proof technique', 'stochastic approximation algorithms', 'dynamic programming', 'ordinary differential equations', 'decision theory', 'differential equations', 'dynamic programming', 'learning (artificial intelligence)', 'Markov processes']), ('Mission planning for regional surveillance\nThe regional surveillance problem discussed involves formulating a flight route\nfor an aircraft to scan a given geographical region. Aerial\nsurveillance is conducted using a synthetic aperture radar device\nmounted on the aircraft to compose a complete, high-resolution image of\nthe region. Two models for determining an optimised flight route are\ndescribed, the first employing integer programming and the second,\ngenetic algorithms. A comparison of the solution optimality in terms of\nthe total distance travelled, and model efficiency of the two\ntechniques in terms of their required CPU times, is made in order to\nidentify the conditions under which it is appropriate to apply each\nmodel\n', ['mission planning', 'regional surveillance', 'flight route', 'geographical region scanning', 'aerial surveillance', 'synthetic aperture radar device', 'high-resolution image', 'optimised flight route', 'integer programming', 'genetic algorithms', 'solution optimality', 'total distance travelled', 'genetic algorithms', 'integer programming', 'military systems', 'operations research', 'surveillance', 'synthetic aperture radar']), ('Making the MIS integration process work\nFocused, cross-functional teams that implement flexible and scalable\ninformation systems (IS) can deliver a smooth, lean manufacturing\nprocess. When integrating new technology into an existing facility, one\nshould always consider three things: the hard infrastructure, the soft\ninfrastructure, and information flow. Hard infrastructure includes\nclient and server hardware and network infrastructure. Soft\ninfrastructure includes operating systems, existing or legacy software,\nneeded code customizations, and the human resources to run/support the\nsystem. Information flow includes how data in the new system interacts\nwith legacy systems and what legacy data the new system will require,\nas well as who will want to receive/access the information that is held\nby the system\n', ['management information systems', 'scalable information systems', 'lean manufacturing process', 'client server hardware', 'network infrastructure', 'human resources', 'legacy software', 'information flow', 'client-server systems', 'management information systems', 'manufacturing data processing', 'software architecture']), ('Genetic algorithm guided selection: variable selection and subset selection\nA novel genetic algorithm guided selection method, GAS, has been described. The\nmethod utilizes a simple encoding scheme which can represent both\ncompounds and variables used to construct a QSAR/QSPR model. A genetic\nalgorithm is then utilized to simultaneously optimize the encoded\nvariables that include both descriptors and compound subsets. The GAS\nmethod generates multiple models each applying to a subset of the\ncompounds. Typically the subsets represent clusters with different\nchemotypes. Also a procedure based on molecular similarity is presented\nto determine which model should be applied to a given test set\ncompound. The variable selection method implemented in GAS has been\ntested and compared using the Selwood data set (n = 31 compounds; nu =\n53 descriptors). The results showed that the method is comparable to\nother published methods. The subset selection method implemented in GAS\nhas been first tested using an artificial data set (n = 100 points; nu\n= 1 descriptor) to examine its ability to subset data points and second\napplied to analyze the XLOGP data set (n = 1831 compounds; nu = 126\ndescriptors). The method is able to correctly identify artificial data\npoints belonging to various subsets. The analysis of the XLOGP data set\nshows that the subset selection method can be useful in improving a\nQSAR/QSPR model when the variable selection method fails\n', ['genetic algorithm guided selection method', 'encoding scheme', 'compounds', 'variables', 'variable selection', 'subset selection', 'QSAR/QSPR model', 'optimization', 'descriptors', 'compound subsets', 'multiple models', 'clusters', 'chemotypes', 'molecular similarity', 'Selwood data set', 'XLOGP data set', 'artificial data points', 'biology computing', 'chemical structure', 'chemistry computing', 'genetic algorithms']), ('Differential and integral calculus on discrete time series data\nIt has been found that discontinuity plays a crucial role in natural evolutions\n(Lin 1998). In this presentation, we will generalize the idea of\nintegration and differentiation, we developed in calculus, to the study\nof time series in the hope that the problem of outliers and\ndiscontinuities can be resolved more successfully than simply deleting\nthe outliers and avoiding discontinuities from the overall data\nanalysis. In general, appearances of outliers tend to mean existence of\ndiscontinuities, explosive growth or decline in the evolution. At the\nsame time, our approach can be employed to partially overcome the\nproblem of not having enough data values in any available time series.\nAt the end, we will look at some real-life problems of prediction in\norder to see the power of this new approach\n', ['natural evolutions', 'integration', 'differentiation', 'time series', 'outliers', 'prediction', 'calculus', 'time series']), ('A method for solution of systems of linear algebraic equations with\nm-dimensional lambda -matrices\nA system of linear algebraic equations with m-dimensional lambda -matrices is\nconsidered. The proposed method of searching for the solution of this\nsystem lies in reducing it to a numerical system of a special kind\n', ['linear algebraic equations', 'numerical system', 'm-dimensional lambda -matrices', 'matrix algebra', 'polynomials']), ("Controlled projective synchronization in nonpartially-linear chaotic systems\nProjective synchronization (PS), in which the state vectors synchronize up to a\nscaling factor, is usually observable only in partially linear systems.\nWe show that PS could, by means of control, be extended to general\nclasses of chaotic systems with nonpartial linearity. Performance of PS\nmay also be manipulated by controlling the scaling factor to any\ndesired value. In numerical experiments, we illustrate the applications\nto a Rossler system and a Chua's circuit. The feasibility of the\ncontrol for high dimensional systems is demonstrated in a hyperchaotic\nsystem\n", ['projective synchronization', 'nonpartially-linear chaotic systems', 'scaling factor', 'Rossler system', "Chua's circuit", 'control', 'hyperchaotic system', 'chaos', "Chua's circuit", 'nonlinear control systems', 'synchronisation'])]
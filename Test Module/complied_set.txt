[("The visible cement data set\nWith advances in x-ray microtomography, it is now possible to obtain\nthree-dimensional representations of a material's microstructure with a\nvoxel size of less than one micrometer. The Visible Cement Data Set\nrepresents a collection of 3-D data sets obtained using the European\nSynchrotron Radiation Facility in Grenoble, France in September 2000.\nMost of the images obtained are for hydrating portland cement pastes,\nwith a few data sets representing hydrating Plaster of Paris and a\ncommon building brick. All of these data sets are being made available\non the Visible Cement Data Set website at\nhttp://visiblecement.nist.gov. The website includes the raw 3-D\ndatafiles, a description of the material imaged for each data set,\nexample two-dimensional images and visualizations for each data set,\nand a collection of C language computer programs that will be of use in\nprocessing and analyzing the 3-D microstructural images. This paper\nprovides the details of the experiments performed at the ESRF, the\nanalysis procedures utilized in obtaining the data set files, and a few\nrepresentative example images for each of the three materials\ninvestigated\n", ['X-ray microtomography', '3D representations', 'microstructure', 'voxel size', 'European Synchrotron Radiation Facility', 'hydrating portland cement pastes', 'Plaster of Paris', 'building brick', 'cement hydration', 'two-dimensional images', 'microstructural images', 'ESRF', 'cements (building materials)', 'crystal microstructure', 'image processing', 'Internet', 'measurement standards', 'nondestructive testing', 'synchrotron radiation', 'X-ray imaging', 'X-ray topography']), ('Connecting the business without busting the budget\nThe "multi-channel content delivery" model (MCCD) might be a new concept to\nyou, but it is already beginning to replace traditional methods of\nbusiness communications, print and content delivery, argues Darren\nAtkinson, CTO, FormScape\n', ['multi-channel content delivery', 'FormScape', 'documents', 'distributed output management', 'business process management', 'archive', 'retrieval', 'content management', 'document handling', 'management information systems']), ('Approximate confidence intervals for one proportion and difference of two\nproportions\nConstructing a confidence interval for a binomial proportion or the difference\nof two proportions is a routine exercise in daily data analysis. The\nbest-known method is the Wald interval based on the asymptotic normal\napproximation to the distribution of the observed sample proportion,\nthough it is known to have bad performance for small to medium sample\nsizes. Agresti et al. (1998, 2000) proposed an Adding-4 method: 4\npseudo-observations are added with 2 successes and 2 failures and then\nthe resulting (pseudo-)sample proportion is used. The method is simple\nand performs extremely well. Here we propose an approximate method\nbased on a t-approximation that takes account of the uncertainty in\nestimating the variance of the observed (pseudo-)sample proportion. It\nfollows the same line of using a t-test, rather than z-test, in testing\nthe mean of a normal distribution with an unknown variance. For some\ncircumstances our proposed method has a higher coverage probability\nthan the Adding-4 method\n', ['approximate confidence intervals', 'binomial proportion', 'difference of two proportions', 'data analysis', 't-approximation', 'uncertainty', 'variance estimation', 't-test', 'normal distribution', 'coverage probability', 'pseudo-sample proportion', 'approximation theory', 'binomial distribution', 'data analysis', 'normal distribution', 'sampling methods']), ("Simple nonlinear dual-window operator for edge detection\nWe propose a nonlinear edge detection technique based on a\ntwo-concentric-circular-window operator. We perform a preliminary\nselection of edge candidates using a standard gradient and use the\ndual-window operator to reveal edges as zero-crossing points of a\nsimple difference function depending only on the minimum and maximum\nvalues in the two windows. Comparisons with other well-established\ntechniques are reported in terms of visual appearance and computational\nefficiency. They show that detected edges are surely comparable with\nCanny's and Laplacian of Gaussian algorithms, with a noteworthy\nreduction in terms of computational load\n", ['nonlinear dual-window operator', 'edge detection', 'nonlinear edge detection technique', 'two-concentric-circular-window operator', 'standard gradient', 'dual window operator', 'zero-crossing points', 'difference function', 'minimum values', 'maximum values', 'computational efficiency', 'detected edges', 'Laplacian algorithms', 'Gaussian algorithms', "Canny's algorithms", 'computational load', 'nonlinear processing', 'edge detection', 'Laplace equations', 'nonlinear optics']), ('Evolution complexity of the elementary cellular automaton rule 18\nCellular automata are classes of mathematical systems characterized by\ndiscreteness (in space, time, and state values), determinism, and local\ninteraction. Using symbolic dynamical theory, we coarse-grain the\ntemporal evolution orbits of cellular automata. By means of formal\nlanguages and automata theory, we study the evolution complexity of the\nelementary cellular automaton with local rule number 18 and prove that\nits width 1-evolution language is regular, but for every n >or= 2\nits width n-evolution language is not context free but context\nsensitive\n', ['cellular automata', 'symbolic dynamical theory', 'formal languages', 'complexity', 'evolution complexity', 'elementary cellular automaton', 'cellular automata', 'computational complexity', 'evolutionary computation']), ("Mustering motivation to enact decisions: how decision process characteristics\ninfluence goal realization\nDecision scientists tend to focus mainly on decision antecedents, studying how\npeople make decisions. Action psychologists, in contrast, study\npost-decision issues, investigating how decisions, once formed, are\nmaintained, protected, and enacted. Through the research presented\nhere, we seek to bridge these two disciplines, proposing that the\nprocess by which decisions are reached motivates subsequent pursuit and\nbenefits eventual realization. We identify three characteristics of the\ndecision process (DP) as having motivation-mustering potential: DP\neffort investment, DP importance, and DP confidence. Through two field\nstudies tracking participants' decision processes, pursuit and\nrealization, we find that after controlling for the influence of the\nmotivational mechanisms of goal intention and implementation intention,\nthe three decision process characteristics significantly influence the\nsuccessful enactment of the chosen decision directly. The theoretical\nand practical implications of these findings are considered and future\nresearch opportunities are identified\n", ['decision enactment', 'motivation', 'goal realization', 'decision process characteristics', 'action psychologists', 'post-decision issues', 'motivation-mustering potential', 'decision process investment', 'decision process importance', 'decision process confidence', 'goal intention', 'research opportunities', 'decision scientists', 'behavioural sciences', 'decision theory', 'human resource management']), ('The canonical dual frame of a wavelet frame\nWe show that there exist wavelet frames that have nice dual wavelet frames, but\nfor which the canonical dual frame does not consist of wavelets, i.e.,\ncannot be generated by the translates and dilates of a single function\n', ['canonical dual frame', 'wavelet frame', 'Gabor frames', 'multiresolution hierarchy', 'compact support', 'wavelet transforms']), ('Process planning for reliable high-speed machining of moulds\nA method of generating NC programs for the high-speed milling of moulds is\ninvestigated. Forging dies and injection moulds, whether plastic or\naluminium, have a complex surface geometry. In addition they are made\nof steels of hardness as much as 30 or even 50 HRC. Since 1995,\nhigh-speed machining has been much adopted by the die-making industry,\nwhich with this technology can reduce its use of Sinking\nElectrodischarge Machining (SEDM). EDM, in general, calls for longer\nmachining times. The use of high-speed machining makes it necessary to\nredefine the preliminary stages of the process. In addition, it affects\nthe methodology employed in the generation of NC programs, which\nrequires the use of high-level CAM software. The aim is to generate\nerror-free programs that make use of optimum cutting strategies in the\ninterest of productivity and surface quality. The final result is a\nmore reliable manufacturing process. There are two risks in the use of\nhigh-speed milling on hardened steels. One of these is tool breakage,\nwhich may be very costly and may furthermore entail marks on the\nworkpiece. The other is collisions between the tool and the workpiece\nor fixtures, the result of which may be damage to the ceramic bearings\nin the spindles. in order to minimize these risks it is necessary that\nnew control and optimization steps be included in the CAM methodology.\nThere are three things that the firm adopting high-speed methods should\ndo. It should redefine its process engineering, it should systematize\naccess by its CAM programmers to high-speed knowhow, and it should take\nup the use of process simulation tools. In the latter case, it will be\nvery advantageous to use tools for the estimation of cutting forces.\nThe new work methods proposed in this article have made it possible to\nintroduce high speed milling (HSM) into the die industry. Examples are\ngiven of how the technique has been applied with CAM programming\nre-engineered as here proposed, with an explanation of the novel\nfeatures and the results\n', ['moulds', 'reliable high-speed machining', 'process planning', 'NC programs', 'high-speed milling', 'forging dies', 'injection moulds', 'complex surface geometry', 'error-free programs', 'optimum cutting strategies', 'productivity', 'surface quality', 'hardened steels', 'tool breakage', 'tool workpiece collisions', 'ceramic bearings', 'CAM methodology', 'process engineering redefinition', 'process simulation tools', 'CAM programming re-engineering', 'cutting strategies', 'CAD/CAM', 'computer aided production planning', 'computerised numerical control', 'cutting', 'forging', 'machining', 'moulding', 'systems re-engineering']), ('Estimation of the Poisson stream intensity in a multilinear queue with an\nexponential job queue decay\nTimes the busy queue periods start are found for a multilinear queue with an\nexponential job queue decay and uniform resource allocation to\nindividual servers. The stream intensity and the average job are\nestimated from observations of the times the queue busy periods start\n', ['Poisson stream intensity', 'multilinear queue', 'exponential job queue decay', 'busy queue periods start', 'uniform resource allocation', 'stream intensity', 'individual servers', 'queueing theory', 'resource allocation', 'stochastic processes']), ('Evolution of litigation support systems\nFor original paper see ibid., vol. 12, no. 6: "The E-mail of the Species". The\nauthor responds to that paper and argues that printing, scanning and\nimaging E-mails or other electronic (rather than paper) documents prior\nto listing and disclosure seems to be unnecessary, not \'proportionate\'\n(from a costs point of view) and not particularly helpful, to either\nside. He asks how litigation support systems might evolve to help and\nsupport the legal team in their task\n', ['litigation support systems', 'E-mail', 'legal team', 'electronic mail', 'law administration']), ('Extracting straight road structure in urban environments using IKONOS satellite\nimagery\nWe discuss a fully automatic technique for extracting roads in urban\nenvironments. The method has its bases in a vegetation mask derived\nfrom multispectral IKONOS data and in texture derived from panchromatic\nIKONOS data. These two techniques together are used to distinguish road\npixels. We then move from individual pixels to an object-based\nrepresentation that allows reasoning on a higher level. Recognition of\nindividual segments and intersections and the relationships among them\nare used to determine underlying road structure and to then logically\nhypothesize the existence of additional road network components. We\nshow results on an image of San Diego, California. The object-based\nprocessing component may be adapted to utilize other basis techniques\nas well, and could be used to build a road network in any scene having\na straight-line structured topology\n', ['straight road structure', 'urban environments', 'IKONOS satellite imagery', 'fully automatic technique', 'vegetation mask', 'texture', 'panchromatic IKONOS data', 'road pixels', 'object-based representation', 'higher level reasoning', 'individual segment recognition', 'road network components', 'San Diego', 'object-based processing component', 'straight-line structured topology', 'high-resolution imagery', 'large-scale feature extraction', 'vectorized road network', 'civil engineering computing', 'feature extraction', 'image classification', 'image representation', 'image texture', 'remote sensing']), ('Transcripts: bane or boon? [law reporting]\nBecause judge-made law, by its very nature, is less immediately accessible than\nthe law of codified, statutory systems, it calls for an efficient\nsystem of law reporting. Of necessity, any such system will be\nselective, the majority of decisions going unreported. Considerable\npower thereby comes to repose in the hands of the law reporters. The\nauthor shares his invaluable perception and extensive research on the\ndifficulties which arise from the excess of access to judgments\n', ['transcripts', 'law reporting', 'judge-made law', 'judgments', 'information resources', 'law administration']), ('Identification of linear parameter varying models\nWe consider identification of a certain class of discrete-time nonlinear\nsystems known as linear parameter varying system. We assume that\ninputs, outputs and the scheduling parameters are directly measured,\nand a form of the functional dependence of the system coefficients on\nthe parameters is known. We show how this identification problem can be\nreduced to a linear regression, and provide compact formulae for the\ncorresponding least mean square and recursive least-squares algorithms.\nWe derive conditions on persistency of excitation in terms of the\ninputs and scheduling parameter trajectories when the functional\ndependence is of polynomial type. These conditions have a natural\npolynomial interpolation interpretation, and do not require the\nscheduling parameter trajectories to vary slowly. This method is\nillustrated with a simulation example using two different parameter\ntrajectories\n', ['linear parameter varying models', 'identification', 'discrete-time nonlinear systems', 'scheduling parameters', 'functional dependence', 'system coefficients', 'linear regression', 'least mean square algorithms', 'recursive least-squares algorithms', 'persistency of excitation conditions', 'scheduling parameter trajectories', 'polynomial interpolation interpretation', 'parameter trajectories', 'time-varying systems', 'discrete time systems', 'least mean squares methods', 'least squares approximations', 'nonlinear control systems', 'parameter estimation', 'polynomial approximation', 'recursive estimation', 'statistical analysis', 'time-varying systems']), ('Airline base schedule optimisation by flight network annealing\nA system for rigorous airline base schedule optimisation is described. The\narchitecture of the system reflects the underlying problem structure.\nThe architecture is hierarchical consisting of a master problem for\nlogical aircraft schedule optimisation and a sub-problem for schedule\nevaluation. The sub-problem is made up of a number of component\nsub-problems including connection generation, passenger choice\nmodelling, passenger traffic allocation by simulation and revenue and\ncost determination. Schedule optimisation is carried out by means of\nsimulated annealing of flight networks. The operators for the simulated\nannealing process are feasibility preserving and form a complete set of\noperators\n', ['airline base schedule optimisation', 'flight network annealing', 'system architecture', 'hierarchical architecture', 'master problem', 'logical aircraft schedule optimisation', 'schedule evaluation', 'connection generation', 'passenger choice modelling', 'passenger traffic allocation', 'cost determination', 'simulated annealing', 'operators', 'time complexity', 'air traffic', 'mathematical operators', 'scheduling', 'simulated annealing', 'travel industry']), ('The ultimate control group\nEmpirical research on the organization of firms requires that firms be\nclassified on the basis of their control structures. This should be\ndone in a way that can potentially be made operational. It is easy to\nidentify the ultimate controller of a hierarchical organization, and\nthe literature has largely focused on this case. However, many\norganizational structures mix hierarchy with collective choice\nprocedures such as voting, or use circular structures under which\nsuperiors are accountable to their subordinates. The author develops\nsome analytic machinery that can be used to map the authority\nstructures of such organizations, and show that under mild restrictions\nthere is a well-defined ultimate control group. The results are\nconsistent with intuitions about the nature of control in familiar\neconomic settings\n', ['ultimate control group', 'hierarchical organization', 'organizational structures', 'authority structures', 'committees', 'control rights', 'firm organization', 'management', 'management science', 'set theory']), ('Ethnography, customers, and negotiated interactions at the airport\nIn the late 1990s, tightly coordinated airline schedules unraveled owing to\nmassive delays resulting from inclement weather, overbooked flights,\nand airline operational difficulties. As schedules slipped, the delayed\ndepartures and late arrivals led to systemwide breakdowns, customers\nmissed their connections, and airline work activities fell further out\nof sync. In offering possible answers, we emphasize the need to\nconsider the customer as participant, following the human-centered\ncomputing model. Our study applied ethnographic methods to understand\nthe airline system domain and the nature of airline delays, and it\nrevealed the deficiencies of the airline production system model of\noperations. The research insights that led us to shift from a\nproduction and marketing system perspective to a\ncustomer-as-participant view might appear obvious to some readers.\nHowever, we do not know of any airline that designs its operations and\ntechnologies around any other model than the production and marketing\nsystem view. Our human-centered analysis used ethnographic methods to\ngather information, offering new insight into airline delays and\nsuggesting effective ways to improve operations reliability\n', ['human-centered computing model', 'customer trajectories', 'airports', 'employees', 'ethnography', 'negotiated interactions', 'airline delays', 'airline production system operations model', 'customer-as-participant view', 'operations reliability', 'airports', 'marketing data processing', 'negotiation support systems', 'personnel', 'reliability', 'travel industry', 'user centred design']), ("Ride quality evaluation of an actively-controlled stretcher for an ambulance\nThis study considers the subjective evaluation of ride quality during ambulance\ntransportation using an actively-controlled stretcher (ACS). The ride\nquality of a conventional stretcher and an assistant driver's seat is\nalso compared. Braking during ambulance transportation generates\nnegative foot-to-head acceleration in patients and causes blood\npressure to rise in the patient's head. The ACS absorbs the\nfoot-to-head acceleration by changing the angle of the stretcher, thus\nreducing the blood pressure variation. However, the ride quality of the\nACS should be investigated further because the movement of the ACS may\ncause motion sickness and nausea. Experiments of ambulance\ntransportation, including rapid acceleration and deceleration, are\nperformed to evaluate the effect of differences in posture of the\ntransported subject on the ride quality; the semantic differential\nmethod and factor analysis are used in the investigations. Subjects are\ntransported using a conventional stretcher with head forward, a\nconventional stretcher with head backward, the ACS, and an assistant\ndriver's seat for comparison with transportation using a stretcher.\nExperimental results show that the ACS gives the most comfortable\ntransportation when using a stretcher. Moreover, the reduction of the\nnegative foot-to-head acceleration at frequencies below 0.2 Hz and the\nsmall variation of the foot-to-head acceleration result in more\ncomfortable transportation. Conventional transportation with the head\nforward causes the worst transportation, although the characteristics\nof the vibration of the conventional stretcher seem to be superior to\nthat of the ACS\n", ['actively-controlled stretcher', 'ambulance', 'ride quality evaluation', 'subjective evaluation', 'ambulance transportation', 'conventional stretcher', 'assistant driver seat', 'braking', 'negative foot-to-head acceleration', 'blood pressure variation', 'patient head', 'stretcher angle', 'motion sickness', 'nausea', 'rapid acceleration', 'rapid deceleration', 'posture differences', 'transported subject', 'semantic differential method', 'factor analysis', 'head forward', 'head backward', 'comfortable transportation', 'vibration', 'braking', 'haemodynamics', 'patient care', 'position control', 'road vehicles']), ('Repeated games with lack of information on one side: the dual differential\napproach\nWe introduce the dual differential game of a repeated game with lack of\ninformation on one side as the natural continuous time version of the\ndual game introduced by De Meyer (1996). A traditional way to study the\nvalue of differential games is through discrete time approximations.\nHere, we follow the opposite approach: We identify the limit value of a\nrepeated game in discrete time as the value of a differential game.\nNamely, we use the recursive structure for the finitely repeated\nversion of the dual game to construct a differential game for which the\nupper values of the uniform discretization satisfy precisely the same\nproperty. The value of the dual differential game exists and is the\nunique viscosity solution of a first-order derivative equation with a\nlimit condition. We identify the solution by translating viscosity\nproperties in the primal\n', ['repeated games', 'dual differential game', 'repeated game', 'discrete time approximations', 'limit value', 'discrete time', 'viscosity solution', 'limit condition', 'differential games', 'duality (mathematics)']), ('Edison\'s direct current influenced "Broadway" show lighting\nDuring the early decades of the 20 th century, midtown Manhattan in New York\nCity developed an extensive underground direct current (DC) power\ndistribution system. This was a result of the original introduction of\ndirect current by Thomas Edison\'s pioneering Pearl Street Station in\n1882. The availability of DC power in the theater district, led to the\nperpetuation of an archaic form of stage lighting control through\nnearly three-quarters of the 20 th century. This control device was\nknown as a "resistance dimmer." It was essentially a series-connected\nrheostat, but it was wound with a special resistance "taper" so as to\nprovide a uniform change in the apparent light output of typical\nincandescent lamps throughout the travel of its manually operated arm.\nThe development and use of DC powered stage lighting is discussed in\nthis article\n', ['Broadway show lighting', 'Manhattan', 'New York City', 'underground direct current power distribution system', "Thomas Edison's Pearl Street Station", 'theater district', 'stage lighting control', 'resistance dimmer', 'series-connected rheostat', 'resistance taper', 'apparent light output', 'incandescent lamps', 'DC powered stage lighting', 'DC power transmission', 'history', 'lighting control']), ("Web services boost integration\nMicrosoft and IBM have announced products to help their database software\nco-exist with competitors' offerings. The products use web services\ntechnology allowing users to improve integration between databases and\napplication software from rival vendors\n", ['web services technology', 'Microsoft', 'IBM', 'database software', 'database management systems', 'information resources']), ('Limitations of delayed state feedback: a numerical study\nStabilization of a class of linear time-delay systems can be achieved by a\nnumerical procedure, called the continuous pole placement method\n[Michiels et al., 2000]. This method can be seen as an extension of the\nclassical pole placement algorithm for ordinary differential equations\nto a class of delay differential equations. In [Michiels et al., 2000]\nit was applied to the stabilization of a linear time-invariant system\nwith an input delay using static state feedback. In this paper we study\nthe limitations of such delayed state feedback laws. More precisely we\ncompletely characterize the class of stabilizable plants in the\n2D-case. For that purpose we make use of numerical continuation\ntechniques. The use of delayed state feedback in various control\napplications and the effect of its limitations are briefly discussed\n', ['linear time-delay systems', 'continuous pole placement method', 'delay differential equations', 'static state feedback', 'delayed state feedback', 'numerical continuation', 'delay-differential systems', 'differential equations', 'iterative methods', 'state feedback']), ('Dynamics of the firing probability of noisy integrate-and-fire neurons\nCortical neurons in vivo undergo a continuous bombardment due to synaptic\nactivity, which acts as a major source of noise. We investigate the\neffects of the noise filtering by synapses with various levels of\nrealism on integrate-and-fire neuron dynamics. The noise input is\nmodeled by white (for instantaneous synapses) or colored (for synapses\nwith a finite relaxation time) noise. Analytical results for the\nmodulation of firing probability in response to an oscillatory input\ncurrent are obtained by expanding a Fokker-Planck equation for small\nparameters of the problem-when both the amplitude of the modulation is\nsmall compared to the background firing rate and the synaptic time\nconstant is small compared to the membrane time constant. We report the\ndetailed calculations showing that if a synaptic decay time constant is\nincluded in the synaptic current model, the firing-rate modulation of\nthe neuron due to an oscillatory input remains finite in the\nhigh-frequency limit with no phase lag. In addition, we characterize\nthe low-frequency behavior and the behavior of the high-frequency limit\nfor intermediate decay times. We also characterize the effects of\nintroducing a rise time to the synaptic currents and the presence of\nseveral synaptic receptors with different kinetics. In both cases, we\ndetermine, using numerical simulations, an effective decay time\nconstant that describes the neuronal response completely\n', ['firing probability', 'noisy integrate-and-fire neurons', 'cortical neurons', 'synaptic activity', 'noise filtering', 'white noise', 'colored noise', 'Fokker-Planck equation', 'synaptic time constant', 'membrane time constant', 'phase lag', 'synaptic receptors', 'numerical simulation', 'bioelectric potentials', 'Fokker-Planck equation', 'neural nets', 'neurophysiology', 'noise', 'probability']), ('MPEG-4 video object-based rate allocation with variable temporal rates\nIn object-based coding, bit allocation is performed at the object level and\ntemporal rates of different objects may vary. The proposed algorithm\ndeals with these two issues when coding multiple video objects (MVOs).\nThe proposed algorithm is able to successfully achieve the target bit\nrate, effectively code arbitrarily shaped MVOs with different temporal\nrates, and maintain a stable buffer level\n', ['MPEG-4 video coding', 'bit allocation', 'multiple video objects', 'rate-distortion encoding', 'object-based rate allocation', 'variable temporal rates', 'rate distortion theory', 'video coding']), ('The art of the cross-sell [accounting software]\nWith the market for accounting software nearing saturation, vendors are\ntraining resellers in the subtleties of the cross-sell. The rewards can\nbe great. The key is knowing when to focus, and when to partner\n', ['accounting software', 'resellers', 'cross-selling', 'accounting']), ('Training multilayer perceptrons via minimization of sum of ridge functions\nMotivated by the problem of training multilayer perceptrons in neural networks,\nwe consider the problem of minimizing E(x)= Sigma /sub i=1//sup n/\nf/sub i/( xi /sub i/.x), where xi /sub i/ in R/sup S/,\n1<or=i<or=n, and each f/sub i/( xi /sub i/.x) is a ridge\nfunction. We show that when n is small the problem of minimizing E can\nbe treated as one of minimizing univariate functions, and we use the\ngradient algorithms for minimizing E when n is moderately large. For a\nlarge n, we present the online gradient algorithms and especially show\nthe monotonicity and weak convergence of the algorithms\n', ['multilayer perceptrons', 'online gradient algorithms', 'ridge functions', 'minimization', 'neural networks', 'gradient algorithms', 'univariate functions', 'monotonicity', 'weak convergence', 'convergence', 'gradient methods', 'learning (artificial intelligence)', 'minimisation', 'multilayer perceptrons']), ('Development of railway VR safety simulation system\nAbnormal conditions occur in railway transportation due to trouble or accidents\nand it affects a number of passengers. It is very important, therefore,\nto quickly recover and return to normal train operation. For this\npurpose we developed a system, "Computer VR Simulation System for the\nSafety of Railway Transportation." It is a new type simulation system\nto evaluate measures to be taken under abnormal conditions. Users of\nthis simulation system cooperate with one another to correct the\nabnormal conditions that have occurred in virtual reality. This paper\nreports the newly developed simulation system\n', ['railway transportation', 'accidents', 'normal train operation', 'Computer VR Simulation System', 'virtual reality simulation system', 'abnormal conditions correction', 'accidents', 'digital simulation', 'railways', 'traffic engineering computing', 'virtual reality']), ('Scheduling schemes for an integrated flight and propulsion control system\nWe describe two schemes for scheduling an integrated flight and propulsion\ncontrol system for an experimental vertical/short take-off and landing\n(V/STOL) aircraft concept in the acceleration from hover (0-120 kn)\nflight phase. Multivariable integrated flight and propulsion\ncontrollers are designed at several points over the V/STOL envelope and\nimplemented as exact plant observers with state feedback. In the first\nscheduling scheme, the values of the state feedback and observer gain\nmatrices are interpolated between the fixed-point designs as a function\nof aircraft speed. In the second approach, the control signals produced\nby the different fixed-point controllers are blended, allowing a\nsignificant reduction in the order of the scheduled controllers. Both\nscheduling schemes are shown in nonlinear simulation to provide\nexcellent handling qualities as the aircraft accelerates from the hover\n', ['vertical short take-off landing aircraft', 'VSTOL aircraft', 'propulsion control', 'flight control', 'scheduling', 'multivariable control systems', 'observers', 'state feedback', 'fixed-point controllers', 'aerospace propulsion', 'aircraft control', 'multivariable control systems', 'observers', 'scheduling', 'state feedback']), ('Generic simulation approach for multi-axis machining. Part 1: modeling\nmethodology\nThis paper presents a new methodology for analytically simulating multi-axis\nmachining of complex sculptured surfaces. A generalized approach is\ndeveloped for representing an arbitrary cutting edge design, and the\nlocal surface topology of a complex sculptured surface. A NURBS curve\nis used to represent the cutting edge profile. This approach offers the\nadvantages of representing any arbitrary cutting edge design in a\ngeneric way, as well as providing standardized techniques for\nmanipulating the location and orientation of the cutting edge. The\nlocal surface topology of the part is defined as those surfaces\ngenerated by previous tool paths in the vicinity of the current tool\nposition. The local surface topology of the part is represented without\nusing a computationally expensive CAD system. A systematic prediction\ntechnique is then developed to determine the instantaneous tool/part\ninteraction during machining. The methodology employed here determines\nthe cutting edge in-cut segments by determining the intersection\nbetween the NURBS curve representation of the cutting edge and the\ndefined local surface topology. These in-cut segments are then utilized\nfor predicting instantaneous chip load, static and dynamic cutting\nforces, and tool deflection. Part 1 of this paper details the modeling\nmethodology and demonstrates the capabilities of the simulation for\nmachining a complex surface\n', ['multiple axis machining', 'generic modeling', 'tool path specification', 'complex surface machining', 'complex sculptured surfaces', 'systematic prediction', 'cutting edge profile', 'surface topology', 'NURBS curve', 'digital simulation', 'machining', 'path planning', 'production engineering computing']), ('Hypothesis-based concept assignment in software maintenance\nSoftware maintenance accounts for a significant proportion of the lifetime cost\nof a software system. Software comprehension is required in many parts\nof the maintenance process and is one of the most expensive activities.\nMany tools have been developed to help the maintainer reduce the time\nand cost of this task, but of the numerous tools and methods available\none group has received relatively little attention: those using\nplausible reasoning to address the concept assignment problem. We\npresent a concept assignment method for COBOL II: hypothesis-based\nconcept assignment (HB-CA). An implementation of a prototype tool is\ndescribed, and results from a comprehensive evaluation using commercial\nCOBOL II sources are summarised. In particular, we identify areas of a\nstandard maintenance process where such methods would be appropriate,\nand discuss the potential cost savings that may result\n', ['hypothesis-based concept assignment', 'software maintenance', 'lifetime cost', 'COBOL II', 'scalability', 'COBOL', 'software engineering', 'software maintenance']), ('Scalable hybrid computation with spikes\nWe outline a hybrid analog-digital scheme for computing with three important\nfeatures that enable it to scale to systems of large complexity: First,\nlike digital computation, which uses several one-bit precise logical\nunits to collectively compute a precise answer to a computation, the\nhybrid scheme uses several moderate-precision analog units to\ncollectively compute a precise answer to a computation. Second,\nfrequent discrete signal restoration of the analog information prevents\nanalog noise and offset from degrading the computation. Third, a state\nmachine enables complex computations to be created using a sequence of\nelementary computations. A natural choice for implementing this hybrid\nscheme is one based on spikes because spike-count codes are digital,\nwhile spike-time codes are analog. We illustrate how spikes afford easy\nways to implement all three components of scalable hybrid computation.\nFirst, as an important example of distributed analog computation, we\nshow how spikes can create a distributed modular representation of an\nanalog number by implementing digital carry interactions between\nspiking analog neurons. Second, we show how signal restoration may be\nperformed by recursive spike-count quantization of spike-time codes.\nThird, we use spikes from an analog dynamical system to trigger state\ntransitions in a digital dynamical system, which reconfigures the\nanalog dynamical system using a binary control vector; such feedback\ninteractions between analog and digital dynamical systems create a\nhybrid state machine (HSM). The HSM extends and expands the concept of\na digital finite-state-machine to the hybrid domain. We present\nexperimental data from a two-neuron HSM on a chip that implements\nerror-correcting analog-to-digital conversion with the concurrent use\nof spike-time and spike-count codes. We also present experimental data\nfrom silicon circuits that implement HSM-based pattern recognition\nusing spike-time synchrony. We outline how HSMs may be used to perform\nlearning, vector quantization, spike pattern recognition and\ngeneration, and how they may be reconfigured\n', ['scalable hybrid computation', 'spikes', 'hybrid analog-digital scheme', 'moderate-precision analog units', 'frequent discrete signal restoration', 'analog noise', 'spike-count codes', 'finite-state-machine', 'distributed analog computation', 'spike-time codes', 'digital carry interactions', 'binary control vector', 'feedback interactions', 'two neuron hybrid state machine', 'error-correcting analog-to-digital conversion', 'silicon circuits', 'pattern recognition', 'learning', 'vector quantization', 'analogue-digital conversion', 'digital arithmetic', 'error correction', 'finite state machines', 'learning (artificial intelligence)', 'neural chips', 'pattern recognition', 'signal restoration', 'vector quantisation']), ("A Blog in every law firm?\nYou don't know today what you'll want to know next year. Rather than trying to\nsolve that problem, focus on providing simple tools to users that\ncreate valuable content across the firm. Individual contributions will\nbe more visible, and you will have a searchable archive of your\ninstitutional memory and a simplified process for ensuring everyone is\nup to speed. Whether you embrace weblogs for their individual or\ninstitutional benefits, one thing is certain: They will become powerful\ntools for those who seek ways to more efficiently and intelligently\nmanage information\n", ['law firm', 'weblogs', 'institutional memory', 'Web site', 'information resources', 'law administration']), ('Failures and successes: notes on the development of electronic cash\nBetween 1997 and 2001, two mid-sized communities in Canada hosted North\nAmerica\'s most comprehensive experiment to introduce electronic cash\nand, in the process, replace physical cash for casual, low-value\npayments. The technology used was Mondex, and its implementation was\nsupported by all the country\'s major banks. It was launched with an\nextensive publicity campaign to promote Mondex not only in the domestic\nbut also in the global market, for which the Canadian implementation\nwas to serve as a "showcase." However, soon after the start of the\nfirst field test it became apparent that the new technology did not\nwork smoothly. On the contrary, it created a host of controversies, in\nareas as varied as computer security, consumer privacy, and monetary\npolicy. In the following years, few of these controversies could be\nresolved and Mondex could not be established as a widely used payment\nmechanism. In 2001, the experiment was finally terminated. Using the\nconcepts developed in recent science and technology studies (STS), the\narticle analyzes these controversies as resulting from the difficulties\nof fitting electronic cash, a new sociotechnical system, into the\ncomplex setting of the existing payment system. The story of Mondex not\nonly offers lessons on why technologies fail, but also offers insight\ninto how short-term failures can contribute to long-term\ntransformations. This suggests the need to rethink the dichotomy of\nsuccess and failure\n', ['electronic cash', 'Canada', 'low-value payments', 'Mondex', 'major banks', 'publicity campaign', 'global market', 'Canadian implementation', 'computer security', 'consumer privacy', 'monetary policy', 'payment mechanism', 'science and technology studies', 'sociotechnical system', 'short-term failures', 'long-term transformations', 'bibliographies', 'electronic commerce', 'electronic money', 'government policies', 'management of change', 'socio-economic effects']), ('Super high definition image (WHD: Wide/Double HD) transmission system\nThis paper describes a WHD image transmission system constructed from a display\nprojector, CODECs, and a camera system imaging a super high definition\nimage (WHD: Wide/Double HD) corresponding to two screen portions of\ncommon high-vision images. This system was developed as a transmission\nsystem to communicate with or transmit information giving a\nreality-enhanced feeling to a remote location by using images of super\nhigh definition. In addition, the correction processing for the\ndistortions of images occurring due to the structure of the camera\nsystem, an outline of the transmission experiments using the proposed\nsystem, and subjective evaluation experiments using WHD images are\npresented\n', ['super high definition image transmission system', 'WHD image transmission system', 'CODECs', 'camera system imaging', 'reality-enhanced feeling', 'asynchronous transfer mode', 'high definition television', 'image processing']), ('Lossy SPICE models produce realistic averaged simulations\nIn previous averaged models, the state-space averaging technique or switch\nwaveforms analysis were usually applied over perfect elements,\nnon-inclusive of the ohmic losses. However, if these elements play an\nactive role in the DC transfer function, they affect the small-signal\nAC analysis by introducing various damping effects. A model is\nintroduced in a boost voltage-mode application\n', ['lossy SPICE models', 'realistic averaged simulations', 'state-space averaging technique', 'switch waveforms analysis', 'damping effects', 'boost voltage-mode application', 'ohmic losses', 'DC transfer function', 'circuit simulation', 'DC-DC power convertors', 'losses', 'SPICE', 'state-space methods', 'transfer functions', 'waveform analysis']), ('Relevance of Web documents: ghosts consensus method\nThe dominant method currently used to improve the quality of Internet search\nsystems is often called "digital democracy." Such an approach implies\nthe utilization of the majority opinion of Internet users to determine\nthe most relevant documents: for example, citation index usage for\nsorting of search results (google.com) or an enrichment of a query with\nterms that are asked frequently in relation with the query\'s theme.\n"Digital democracy" is an effective instrument in many cases, but it\nhas an unavoidable shortcoming, which is a matter of principle: the\naverage intellectual and cultural level of Internet users is very low;\neveryone knows what kind of information is dominant in Internet query\nstatistics. Therefore, when one searches the Internet by means of\n"digital democracy" systems, one gets answers that reflect an\nunderlying assumption that the user\'s mind potential is very low, and\nthat his cultural interests are not demanding. Thus, it is more correct\nto use the term "digital ochlocracy" to refer to Internet search\nsystems with "digital democracy." Based on the well-known mathematical\nmechanism of linear programming, we propose a method to solve the\nindicated problem\n', ['Internet search systems', 'digital democracy', 'majority opinion', 'citation index usage', 'search results', 'Internet query statistics', 'digital ochlocracy', 'linear programming', 'ghosts consensus method', 'World Wide Web', 'Internet', 'linear programming', 'relevance feedback', 'search engines']), ('Office essentials [stationery suppliers]\nMake purchasing stationery a relatively simple task through effective planning\nand management of stock, and identifying the right supplier\n', ['stationery suppliers', 'purchasing', 'planning', 'management of stock', 'computer stationery', 'paper', 'purchasing', 'stock control']), ('Disability-related special libraries\nOne of the ways that the federal government works to improve services to people\nwith disabilities is to fund disability-related information centers and\nclearinghouses that provide information resources and referrals to\ndisabled individuals, their family members, service providers, and the\ngeneral public. The Teaching Research Division of Western Oregon\nUniversity operates two federally funded information centers for people\nwith disabilities: OBIRN (the Oregon Brain Injury Resource Network) and\nDB-LINK (the National Information Clearinghouse on Children who are\nDeaf-Blind). Both have developed in-depth library collections and\nservices in addition to typical clearinghouse services. The authors\ndescribe how OBIRN and DB-LINK were designed and developed, and how\nthey are currently structured and maintained. Both information centers\nuse many of the same strategies and tools in day-to-day operations, but\ndiffer in a number of ways, including materials and clientele\n', ['disability-related special libraries', 'federal government', 'disability-related information centers', 'disability-related clearinghouses', 'information resources', 'information referrals', 'Western Oregon University', 'OBIRN', 'Oregon Brain Injury Resource Network', 'DB-LINK', 'National Information Clearinghouse on Children who are Deaf-Blind', 'library collections', 'handicapped aids', 'information centres', 'information needs', 'special libraries']), ("On the p-adic Birch, Swinnerton-Dyer Conjecture for non-semistable reduction\nIn this paper, we examine the Iwasawa theory of elliptic curves E with additive\nreduction at an odd prime p. By extending Perrin-Riou's theory to\ncertain nonsemistable representations, we are able to convert Kato's\nzeta-elements into p-adic L-functions. This allows us to deduce the\ncotorsion of the Selmer group over the cyclotomic Z/sub p/-extension of\nQ, and thus prove an inequality in the p-adic Birch and Swinnerton-Dyer\nconjecture at primes p whose square divides the conductor of E\n", ['p-adic Birch', 'Swinnerton-Dyer conjecture', 'nonsemistable reduction', 'lwasawa theory', 'elliptic curves', 'additive reduction', "Perrin-Riou's theory", 'p-adic L-functions', 'cotorsion', 'Selmer group', 'cyclotomic Z/sub p/-extension', 'interpolation', 'number theory']), ('Adaptive image enhancement for retinal blood vessel segmentation\nRetinal blood vessel images are enhanced by removing the nonstationary\nbackground, which is adaptively estimated based on local neighbourhood\ninformation. The result is a much better segmentation of the blood\nvessels with a simple algorithm and without the need to obtain a priori\nillumination knowledge of the imaging system\n', ['adaptive image enhancement', 'retinal blood vessel images', 'local neighbourhood information', 'nonstationary background removal', 'image segmentation', 'personal identification', 'security applications', 'blood vessels', 'eye', 'image enhancement', 'image segmentation']), ('Distribution software: ROI is king\nMiddle-market accounting software vendors are taking to the open road, by way\nof souped-up distribution suites that can track product as it wends its\nway from warehouse floor to customer site. Integration provides\nefficiencies, and cost savings\n', ['accounting software', 'warehouse management', 'distribution', 'accounting', 'goods distribution', 'stock control', 'warehouse automation']), ('Model intestinal microflora in computer simulation: a simulation and modeling\npackage for host-microflora interactions\nThe ecology of the human intestinal microflora and its interaction with the\nhost are poorly understood. Though more and more data are being\nacquired, in part using modern molecular methods, development of a\nquantitative theory has not kept pace with this increase in observing\npower. This is in part due to the complexity of the system and to the\nlack of simulation environments in which to test what the ecological\neffect of a hypothetical mechanism of interaction would be, before\nresorting to laboratory experiments. The MIMICS project attempts to\naddress this through the development of a cellular automaton for\nsimulation of the intestinal microflora. In this paper, the design and\nevaluation of this simulator is discussed\n', ['human intestines', 'intestinal microflora', 'molecular methods', 'observing power', 'system complexity', 'microbial ecology', 'parallel computing', 'host-microflora interactions', 'quantitative theory', 'MIMICS project', 'complex microbial ecosystem', 'biological organs', 'cellular automata', 'digital simulation', 'medical computing', 'microorganisms', 'physiological models', 'software packages']), ('ISCSI poised to lower SAN costs\nIT managers building storage area networks or expanding their capacity may be\nable to save money by using iSCSI and IP systems rather than Fibre\nChannel technologies\n', ['SAN costs', 'storage area networks', 'iSCSI', 'IP systems', 'computer network management', 'digital storage']), ('Ecological interface design: progress and challenges\nEcological interface design (EID) is a theoretical framework for designing\nhuman-computer interfaces for complex socio-technical systems. Its\nprimary aim is to support knowledge workers in adapting to change and\nnovelty. This literature review shows that in situations requiring\nproblem solving, EID improves performance when compared with current\ndesign approaches in industry. EID has been applied to industry-scale\nproblems in a broad variety of application domains (e.g., process\ncontrol, aviation, computer network management, software engineering,\nmedicine, command and control, and information retrieval) and has\nconsistently led to the identification of new information requirements.\nAn experimental evaluation of EID using a full-fidelity simulator with\nprofessional workers has yet to be conducted, although some are\nplanned. Several significant challenges remain as obstacles to the\nconfident use of EID in industry. Promising paths for addressing these\noutstanding issues are identified. Actual or potential applications of\nthis research include improving the safety and productivity of complex\nsocio-technical systems\n', ['ecological interface design', 'human-computer interfaces', 'complex social technical systems', 'industry', 'productivity', 'user interface', 'human factors', 'ergonomics', 'factory automation', 'human factors', 'large-scale systems', 'user interfaces']), ('A survey of interactive mesh-cutting techniques and a new method for\nimplementing generalized interactive mesh cutting using virtual tools\nIn our experience, mesh-cutting methods can be distinguished by how their\nsolutions address the following major issues: definition of the cut\npath, primitive removal and re-meshing, number of new primitives\ncreated, when re-meshing is performed, and representation of the\ncutting tool. Many researchers have developed schemes for interactive\nmesh cutting with the goals of reducing the number of new primitives\ncreated, creating new primitives with good aspect ratios, avoiding a\ndisconnected mesh structure between primitives in the cut path, and\nrepresenting the path traversed by the tool as accurately as possible.\nThe goal of this paper is to explain how, by using a very simple\nframework, one can build a generalized cutting scheme. This method\nallows for any arbitrary cut to be made within a virtual object, and\ncan simulate cutting surface, layered surface or tetrahedral objects\nusing a virtual scalpel, scissors, or loop cautery tool. This method\nhas been implemented in a real-time, haptic-rate surgical simulation\nsystem allowing arbitrary cuts to be made on high-resolution\npatient-specific models\n', ['generalized interactive mesh cutting', 'virtual tools', 'cut path definition', 're-meshing', 'cutting tool', 'disconnected mesh structure', 'virtual object', 'tetrahedral objects', 'layered surface', 'real-time system', 'haptic-rate surgical simulation system', 'high-resolution patient-specific models', 'rendering', 'haptic interfaces', 'cutting', 'digital simulation', 'haptic interfaces', 'medical computing', 'real-time systems', 'rendering (computer graphics)', 'surgery']), ('Four-terminal quantum resistor network for electron-wave computing\nInterconnected ultrathin conducting wires or, equivalently, interconnected\nquasi-one-dimensional electron waveguides, which form a quantum\nresistor network, are presented here in four-terminal configurations.\nThe transmission behaviors through such four-terminal networks are\nevaluated and classified. In addition, we show that such networks can\nbe used as the basic building blocks for a possible massive wave\ncomputing machine in the future. In a network, each interconnection, a\nnode point, is an elastic scatterer that routes the electron wave.\nRouting and rerouting of electron waves in a network is described in\nthe framework of quantum transport from Landauer-Buttiker theory in the\npresence of multiple elastic scatterers. Transmissions through various\ntypes of four-terminal generalized clean Aharonov-Bohm rings are\ninvestigated at zero temperature. Useful logic functions are gathered\nbased on the transmission probability to each terminal with the use of\nthe Buttiker symmetry rule. In the generalized rings, even and odd\nnumbers of terminals can possess some distinctly different transmission\ncharacteristics as we have shown here and earlier. Just as an even or\nodd number of atoms in a ring is an important quantity for classifying\nthe transmission behavior, we show here that whether the number of\nterminals is an even or an odd number is just as important in\nunderstanding the physics of transmission through such a ring.\nFurthermore, we show that there are three basic classes of\nfour-terminal rings and the scaling relation for each class is\nprovided. In particular, the existence of equitransmission among all\nfour terminals is shown here. This particular physical phenomena cannot\nexist in any three-terminal ring. Comparisons and discussions of\ntransmission characteristics between three-terminal and four-terminal\nrings are also presented. The node-equation approach by considering the\nKirchhoff current conservation law at each node point is used for this\nanalysis. Many useful logic functions for electron-wave computing are\nshown here. In particular, we show that a full adder can be constructed\nvery simply using the equitransmission property of the four-terminal\nring. This is in sharp contrast with circuits based on transistor logic\n', ['four-terminal quantum resistor network', 'electron-wave computing', 'interconnected ultrathin conducting wires', 'quasi1D electron waveguides', 'rerouting', 'Landauer-Buttiker theory', 'multiple elastic scatterers', 'Aharonov-Bohm rings', 'logic functions', 'transmission probability', 'Buttiker symmetry rule', 'transmission behavior', 'Kirchhoff current conservation law', 'equitransmission property', 'Aharonov-Bohm effect', 'electron waveguides', 'quantum computing', 'resistors']), ("Santera targets independents in major strategy overhaul [telecom]\nWith big carriers slashing capital expense budgets, Santera Systems is\nbroadening the reach of its next-generation switching platform to\ninclude independent telcos. This week, the vendor will announce that it\nhas signed a deal with Kerman, Calif-based Kerman Telephone Co.\nFurthermore, the company is angling for inclusion in the Rural\nUtilities Service's approved equipment list, hoping to sell its Class 5\nreplacement boxes to the smallest carriers. The move is almost a\ncomplete reversal for the Plano, Texas-based vendor, which previously\nfocused solely on large carriers, including the RBOCs\n", ['Santera Systems', 'switching', 'Kerman Telephone', 'Rural Utilities Service', 'telecommunication switching']), ('Modeling frequently accessed wireless data with weak consistency\nTo reduce the response times of wireless data access in a mobile network,\ncaches are utilized in wireless handheld devices. If the original data\nentry has been updated, the cached data in the handheld device becomes\nstale. Thus, a mechanism is required to predict when the cached copy\nwill expire. This paper studies a weakly consistent data access\nmechanism that computes the time-to-live (TTL) interval to predict the\nexpiration time. We propose an analytic model to investigate this\nTTL-based algorithm for frequently accessed data. The analytic model is\nvalidated against simulation experiments. Our study quantitatively\nindicates how the TTL-based algorithm reduces the wireless\ncommunication cost by increasing the probability of stale accesses.\nDepending on the requirements of the application, appropriate parameter\nvalues can be selected based on the guidelines provided\n', ['frequently accessed wireless data modeling', 'weak consistency', 'response time reduction', 'wireless data access', 'mobile network', 'caches', 'wireless handheld devices', 'data entry', 'time-to-live interval', 'expiration time prediction', 'analytic model', 'simulation experiments', 'wireless communication cost', 'stale access probability', 'cache storage', 'client-server systems', 'mobile communication', 'mobile computing', 'portable computers']), ('Median partitioning: a novel method for the selection of representative subsets\nfrom large compound pools\nA method termed median partitioning (MP) has been developed to select diverse\nsets of molecules from large compound pools. Unlike many other methods\nfor subset selection, the MP approach does not depend on pairwise\ncomparison of molecules and can therefore be applied to very large\ncompound collections. The only time limiting step is the calculation of\nmolecular descriptors for database compounds. MP employs arrays of\nproperty descriptors with little correlation to divide large compound\npools into partitions from which representative molecules can be\nselected. In each of n subsequent steps, a population of molecules is\ndivided into subpopulations above and below the median value of a\nproperty descriptor until a desired number of 2/sup n/ partitions are\nobtained. For descriptor evaluation and selection, an entropy\nformulation was embedded in a genetic algorithm. MP has been applied to\ngenerate a subset of the Available Chemicals Directory, and the results\nhave been compared with cell-based partitioning\n', ['median partitioning', 'large compound pools', 'representative subset selection', 'molecules', 'time limiting step', 'molecular descriptors', 'database compounds', 'property descriptor array', 'entropy formulation', 'genetic algorithm', 'Available Chemicals Directory', 'cell-based partitioning', 'chemistry computing', 'entropy', 'genetic algorithms', 'organic compounds', 'scientific information systems', 'very large databases']), ("Contentment management\nAndersen's William Yarker and Richard Young outline the route to a successful\ncontent management strategy\n", ['Andersen Consulting', 'content management strategy', 'document handling', 'information resources']), ('Reinventing broadband\nMany believe that broadband providers need to change their whole approach. The\nfuture, then, is in reinventing broadband. That means tiered pricing to\nmake broadband more competitive with dial-up access and livelier, more\ndistinct content: video on demand, MP3, and other features exclusive to\nthe fat-pipe superhighway\n', ['MP3', 'business plans', 'video on demand', 'tiered pricing', 'broadband', 'broadband networks', 'costing', 'economics', 'Internet', 'technological forecasting'])]
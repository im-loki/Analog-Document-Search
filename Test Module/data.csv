Initial Text,Ocr Generated Text,Initial Keyphrases,Generated Keyphrases,Init_No_Char,Init_No_Words,Init_No_Keyphrases,Pred_No_Char,Pred_No_Words,Pred_No_Keyphrases,mis_no_char,mis_no_words,mis_no_keyphrases
folder,folder,['folder'],['folder'],6,1,1,6,1,1,0,0,0
using tensorflow backend,using tensorflow backenc,"['Tensorflow', 'Tensorflow Backend', 'Backend']","['tensorflow backend', 'backend', 'tensorflow']",22,3,3,22,3,3,1,1,0
"waiting for the wave to crest [wavelength services] wavelength services have been hyped ad nauseam for years. but despite their quick turn-up time and impressive margins, such services have yet to live up to the industry's expectations. the reasons for this lukewarm reception are many, not the least of which is the confusion that still surrounds the technology, but most industry observers are still convinced that wavelength services with ultimately flourish ",waiting for the wawe to crest wavelength services wavelengt hsevices have been hyped ad nadseam fo years but despite their quick turneu ptime and impressive margins suc hsevice shave yet to live d pto the industrys expectations the eason sfo rthis ukewarm reception are many nd tthe east of whic his the confusion tha tstill surround sthe technology but mds tindustry observer sar estill convinced that wavelength senwce swith ultimately flourish,"['wavelength services', 'fiber optic networks', 'Looking Glass Networks', 'PointEast Research', 'optical fibre networks', 'telecommunication']","['wavelength', 'services', 'industry expectations', 'impressive margins', 'suc service', 'hyped', 'd pto', 'crest', 'industry', 'wave']",391,72,6,376,71,10,90,40,1
"marble cutting with single point cutting tool and diamond segments an investigation has been undertaken into the frame sawing with diamond blades. the kinematic behaviour of the frame sawing process is discussed. under different cutting conditions, cutting and indenting-cutting tests are carried out by single point cutting tools and single diamond segments. the results indicate that the depth of cut per diamond grit increases as the blades move forward. only a few grits per segment can remove the material in the cutting process. when the direction of the stroke changes, the cutting forces do not decrease to zero because of the residual plastic deformation beneath the diamond grits. the plastic deformation and fracture chipping of material are the dominant removal processes, which can be explained by the fracture theory of brittle material indentation ","Marble cutting with single point cutting tool and diamond segments An investigation has been undertaken into the frame sawing with diamond blades. The kinematic behaviour of the frame sawing process is discussed. Under different cutting conditions, cutting and indenting-cutting tests are carried out by single point cutting tools and single diamond segments. The results indicate that the depth of cut per diamond grit increases as the blades move forward. Only a few grits per segment can remove the material in the cutting process. When the direction of the stroke changes, the cutting forces do not decrease to zero because of the residual plastic deformation beneath the diamond grits. The plastic deformation and fracture chipping of material are the dominant removal processes, which can be explained by the fracture theory of brittle material indentation","['marble cutting', 'single point cutting tool', 'diamond segments', 'frame sawing', 'kinematic behaviour', 'cutting tests', 'indenting-cutting tests', 'residual plastic deformation', 'fracture chipping', 'removal processes', 'fracture theory', 'brittle material indentation', 'computerised numerical control', 'cutting', 'diamond', 'kinematics', 'machining', 'plastic deformation']","['single point', 'diamond grit', 'diamond', 'different cutting conditions', 'single diamond segments', 'diamond blades', 'tools', 'single', 'diamond segments', 'point']",730,134,18,730,133,10,0,0,9
"on trajectory and force tracking control of constrained mobile manipulators with parameter uncertainty studies the trajectory and force tracking control problem of mobile manipulators subject to holonomic and nonholonomic constraints with unknown inertia parameters. adaptive controllers are proposed based on a suitable reduced dynamic model, the defined reference signals and the mixed tracking errors. the proposed controllers not only ensure the entire state of the system to asymptotically converge to the desired trajectory but also ensure the constraint force to asymptotically converge to the desired force. a detailed numerical example is presented to illustrate the developed methods ","On trajectory and force tracking control of constrained mobile manipulators with parameter uncertainty Studies the trajectory and force tracking control problem of mobile manipulators subject to holonomic and nonholonomic constraints with unknown inertia parameters. Adaptive controllers are proposed based on a suitable reduced dynamic model, the defined reference signals and the mixed tracking errors. The proposed controllers not only ensure the entire state of the system to asymptotically converge to the desired trajectory but also ensure the constraint force to asymptotically converge to the desired force. A detailed numerical example is presented to illustrate the developed methods","['trajectory control', 'force tracking control', 'constrained mobile manipulators', 'parameter uncertainty', 'holonomic constraints', 'nonholonomic constraints', 'adaptive controllers', 'reduced dynamic model', 'mixed tracking errors', 'asymptotic convergence', 'position control', 'mobile robots', 'adaptive control', 'convergence', 'force control', 'manipulator dynamics', 'mobile robots', 'position control', 'reduced order systems', 'uncertain systems']","['trajectory', 'force tracking control', 'mobile manipulators', 'parameter uncertainty Studies', 'unknown inertia parameters', 'mixed tracking errors', 'Adaptive controllers', 'constraint force', 'force', 'controllers']",597,98,20,597,97,10,0,0,3
"model checking games for branching time logics this paper defines and examines model checking games for the branching time temporal logic ctl*. the games employ a technique called focus which enriches sets by picking out one distinguished element. this is necessary to avoid ambiguities in the regeneration of temporal operators. the correctness of these games is proved, and optimizations are considered to obtain model checking games for important fragments of ctl*. a game based model checking algorithm that matches the known lower and upper complexity bounds is sketched ","Model checking games for branching time logics This paper defines and examines model checking games for the branching time temporal logic CTL"". The games employ a technique called focus which enriches sets by picking out one distinguished element. This is necessary to avoid ambiguities in the regeneration of temporal operators. The correctness of these games is proved, and optimizations are considered to obtain model checking games for important fragments ‘of CTL"". Agame based model checking algorithm that matches the known lower and upper complexity bounds is sketched","['model checking games', 'branching time logics', 'temporal logic', 'temporal operators', 'complexity bounds', 'computability', 'computational complexity', 'temporal logic']","['model checking games', 'temporal logic col.', 'temporal operators', 'time logics', 'col. game', 'game', 'temporal', 'time', 'logics', 'model']",488,89,8,489,87,10,61,19,1
"multispectral color image capture using a liquid crystal tunable filter we describe the experimental setup of a multispectral color image acquisition system consisting of a professional monochrome ccd camera and a tunable filter in which the spectral transmittance can be controlled electronically. we perform a spectral characterization of the acquisition system taking into account the acquisition noise. to convert the camera output signals to device-independent color data, two main approaches are proposed and evaluated. one consists in applying regression methods to convert from the k camera outputs to a device-independent color space such as ciexyz or cielab. another method is based on a spectral model of the acquisition system. by inverting the model using a principal eigenvector approach, we estimate the spectral reflectance of each pixel of the imaged surface ","Multispectral color image capture using a liquid crystal tunable filter We describe the experimental setup of a multispectral color image acquisition system consisting of a professional monochrome CCD camera and a tunable filter in which the spectral transmittance can be controlled electronically. We perform a spectral characterization of the acquisition system taking into account the acquisition noise. To convert the camera output signals to device-independent color data, two main approaches are proposed and evaluated. One consists in applying regression methods to convert from the K camera outputs to a device-independent color space such as CIEXYZ or CIELAB. Another method is based on a spectral model of the acquisition system. By inverting the model using a principal eigenvector approach, we estimate the spectral reflectance of each pixel of the imaged surface","['multispectral color image capture', 'liquid crystal tunable filter', 'multispectral color image acquisition system', 'monochrome CCD camera', 'tunable filter', 'spectral transmittance', 'spectral characterization', 'acquisition system', 'acquisition noise', 'camera output signals', 'device-independent color data', 'regression methods', 'camera outputs', 'independent color space', 'CIEXYZ', 'CIELAB', 'spectral model', 'principal eigenvector approach', 'spectral reflectance', 'imaged surface', 'pixel', 'CCD image sensors', 'data acquisition', 'eigenvalues and eigenfunctions', 'image colour analysis', 'optical filters', 'optical tuning', 'reflectivity']","['acquisition system', 'tunable filter', 'device-independent color space', 'device-independent color data', 'spectral transmittance', 'camera output signals', 'acquisition noise', 'K camera outputs', 'spectral model', 'color']",746,131,28,746,130,10,0,0,6
system embedding. polynomial equations the class of solutions of the polynomial equations including their generalizations in the form of the bezout matrix identities was constructed analytically using the technology of constructive system embedding. the structure of a solution depends on the number of steps of the euclidean algorithm and is obtained explicitly by appropriate substitutions. illustrative and descriptive examples are presented ,"System embedding. Polynomial equations The class of solutions of the polynomial equations including their generalizations in the form of the Bezout matrix identities was, constructed analytically using the technology of constructive system embedding. The structure of a solution depends on the number of steps of the Euclidean algorithm and is obtained explicitly by appropriate substitutions. Illustrative and descriptive examples are presented","['determinate systems', 'polynomial equations', 'Bezout matrix identities', 'constructive system embedding', 'Euclidean algorithm', 'differential equations', 'matrix algebra', 'polynomials']","['solution', 'beirut matrix identities', 'constructive system', 'class', 'polynomial', 'Polynomial', 'System', 'equations', 'polynomial equations', 'Polynomial equations']",384,62,8,385,61,10,0,1,1
"a geometric process equivalent model for a multistate degenerative system in this paper, a monotone process model for a one-component degenerative system with k+1 states (k failure states and one working state) is studied. we show that this model is equivalent to a geometric process (gp) model for a two-state one component system such that both systems have the same long-run average cost per unit time and the same optimal policy. furthermore, an explicit expression for the determination of an optimal policy is derived ","A geometric process equivalent model for a multistate degenerative system In this paper, a monotone process model for a one-component degenerative system with k+1 states (k failure states and one working state) is studied. We show that this model is equivalent to a geometric process (GP) model for a two-state one component system such that both systems have the same long-run average cost per unit time and the same optimal policy. Furthermore, an explicit expression for the determination of an optimal policy is derived","['multistate degenerative system', 'geometric process equivalent model', 'monotone process model', 'one-component degenerative system', 'failure states', 'working state', 'two-state one component system', 'long-run average cost', 'optimal policy', 'replacement policy', 'renewal reward process', 'maintenance engineering', 'optimisation', 'reliability theory']","['geometric process', 'one-component degenerative system', 'multi-stage degenerative system', 'monotone process model', 'equivalent model', 'component system', 'failure states', 'k1 states', 'model', 'process']",440,85,14,440,84,10,0,0,3
"open hypermedia for product support as industrial systems become increasingly more complex, the maintenance and operating information increases both in volume and complexity. with the current pressures on manufacturing, the management of information resources has become a critical issue. in particular, ensuring that personnel can access current information quickly and effectively when undertaking a specific task. this paper discusses some of the issues involved in, and the benefits of using, open hypermedia to manage and deliver a diverse range of information. while the paper concentrates on the problems specifically associated with manufacturing organizations, the problems are generic across other business sectors such as healthcare, defence and finance. the open hypermedia approach to information management and delivery allows a multimedia resource base to be used for a range of applications and it permits a user to have controlled access to the required information in an easily accessible and structured manner. recent advancement in hypermedia also permits just-in-time support in the most appropriate format for all users. our approach is illustrated by the discussion of a case study in which an open hypermedia system delivers maintenance and process information to factory-floor users to support the maintenance and operation of a very large manufacturing cell ","Open hypermedia for product support As industrial systems become increasingly more complex, the maintenance and ‘operating information increases both in volume and complexity. With the current pressures on manufacturing, the management of information resources has become a critical issue. In particular, ensuring that personnel can access current information quickly and effectively when undertaking a specific task. This paper discusses some of the issues involved in, and the benefits of using, open hypermedia to manage and deliver a diverse range of information. While the paper concentrates on the problems specifically associated with manufacturing organizations, the problems are generic across other business sectors such as healthcare, defence and finance. The open hypermedia approach to information management and delivery allows a multimedia resource base to be used for a range of applications and it permits a user to have controlled access to the required information in an easily accessible and structured manner. Recent advancement in hypermedia also permits just-in-time support in the most appropriate format for all users. Our approach is illustrated by the discussion of a case study in which an ‘open hypermedia system delivers maintenance and process information to factory-floor users to support the maintenance and operation of a very large manufacturing cell","['open hypermedia', 'maintenance', 'operating information', 'information resources', 'just-in-time support', 'product support', 'hypermedia', 'information resources', 'multimedia systems', 'production control']","['information', 'open hypermedia approach', 'open hypermedia system', 'information management', 'hypermedia', 'information resources', 'process information', 'current information', 'open hypermedia', 'product support']",1182,203,10,1184,202,10,13,2,1
"numerical representation of binary relations with a multiplicative error function this paper studies the case of the representation of a binary relation via a numerical function with threshold (error) depending on both compared alternatives. the error is considered to be multiplicative, its value being either directly or inversely proportional to the values of the numerical function. for the first case, it is proved that a binary relation is a semiorder. moreover, any semiorder can be represented in this form. in the second case, the corresponding binary relation is an interval order ","Numerical representation of binary relations with a multiplicative error function This paper studies the case of the representation of a binary relation via a numerical function with threshold (error) depending on both compared alternatives. The error is considered to be multiplicative, its value being either directly or inversely proportional to the values of the numerical function. For the first case, it is proved that a binary relation is a semiorder. Moreover, any semiorder can be represented in this form. In the second case, the corresponding binary relation is an interval order","['numerical representation', 'binary relations', 'multiplicative error function', 'numerical function', 'threshold', 'error', 'semiorder', 'interval order', 'decision theory', 'error analysis']","['numerical function', 'multiplicative error function', 'corresponding binary relation', 'Numerical representation', 'binary relations', 'threshold error', 'second case', 'first case', 'relation', 'binary']",500,92,10,500,91,10,0,0,2
"valuing corporate debt: the effect of cross-holdings of stock and debt we have developed a simple approach to valuing risky corporate debt when corporations own securities issued by other corporations. we assume that corporate debt can be valued as an option on corporate business asset value, and derive payoff functions when there exist cross-holdings of stock or debt between two firms. next we show that payoff functions with multiple cross-holdings can be solved by the contraction principle. the payoff functions which we derive provide a number of insights about the risk structure of company cross-holdings. first, the modigliani-miller theorem can obtain when there exist cross-holdings between firms. second, by establishing cross-shareholdings each of stock holders distributes a part of its payoff values to the bond holder of the other's firm, so that both firms can decrease credit risks by cross-shareholdings. in the numerical examples, we show that the correlation in firms can be a critical condition for reducing credit risk by cross-holdings of stock using monte carlo simulation. moreover, we show we can calculate the default spread easily when complicated cross-holdings exist, and find which shares are beneficial or disadvantageous ","Valuing corporate debt: the effect of cross-holdings of stock and debt We have developed a simple approach to valuing risky corporate debt when corporations own securities issued by other corporations. We assume that corporate debt can be valued as an option on corporate business asset value, and derive payoff functions when there exist cross-holdings of stock or debt between two firms. Next we show that payoff functions with multiple cross-holdings can be solved by the contraction principle. The payoff functions which we derive provide a number of insights about the risk structure of company cross-holdings. First, the Modigliani-Miller theorem can obtain when there exist cross-holdings between firms. Second, by establishing cross-shareholdings each of stock holders distributes a part of its payoff values to the bond holder of the other's firm, so that both firms can decrease credit risks by cross-shareholdings. In the numerical examples, we show that the correlation in firms can be a critical condition for reducing credit risk by cross-holdings of stock using Monte Carlo simulation. Moreover, we show we can calculate the default spread easily when complicated cross-holdings exist, and find which shares are beneficial or disadvantageous,","['risky corporate debt valuation', 'stock holdings', 'debt holdings', 'securities', 'option', 'corporate business asset value', 'payoff functions', 'multiple cross-holdings', 'Modigliani-Miller theorem', 'cross-shareholdings', 'bond holder', 'credit risks', 'correlation', 'Monte Carlo simulation', 'corporate modelling', 'correlation methods', 'Monte Carlo methods', 'stock markets']","['payoff functions', 'cross-holdings', 'multiple cross-holdings', 'company cross-holdings', 'risky corporate debt', 'other corporations', 'others firm', 'debt', 'corporate', 'corporate debt']",1067,191,18,1068,190,10,0,1,8
"modeling self-consistent multi-class dynamic traffic flow in this study, we present a systematic self-consistent multiclass multilane traffic model derived from the vehicular boltzmann equation and the traffic dispersion model. the multilane domain is considered as a two-dimensional space and the interaction among vehicles in the domain is described by a dispersion model. the reason we consider a multilane domain as a two-dimensional space is that the driving behavior of road users may not be restricted by lanes, especially motorcyclists. the dispersion model, which is a nonlinear poisson equation, is derived from the car-following theory and the equilibrium assumption. under the concept that all kinds of users share the finite section, the density is distributed on a road by the dispersion model. in addition, the dynamic evolution of the traffic flow is determined by the systematic gas-kinetic model derived from the boltzmann equation. multiplying boltzmann equation by the zeroth, first- and second-order moment functions, integrating both side of the equation and using chain rules, we can derive continuity, motion and variance equation, respectively. however, the second-order moment function, which is the square of the individual velocity, is employed by previous researches does not have physical meaning in traffic flow ","Modeling self-consistent multi-class dynamic traffic flow In this study, we present a systematic self-consistent multiclass multilane traffic model derived from the vehicular Boltzmann equation and the traffic dispersion model. The multiane domain is considered as a two-dimensional space and the interaction among vehicles in the domain is described by a dispersion model. The reason we consider a multilane domain as a two-dimensional space is that the driving behavior of road users may not be restricted by lanes, especially motorcyclists. The dispersion model, which is a nonlinear Poisson equation, is derived from the car-following theory and the equilibrium assumption. Under the concept that all kinds of users share the finite section, the density is distributed on a road by the dispersion model. In addition, the ‘dynamic evolution of the traffic flow is determined by the systematic gas-kinetic model derived from the Boltzmann equation. Multiplying Boltzmann equation by the zeroth, first- and second-order moment functions, integrating both side of the equation and using chain rules, we can derive continuity, motion and variance equation, respectively. However, the second-order moment function, which is the square of the individual velocity, is employed by previous researches does not have physical meaning in traffic flow","['self-consistent multiclass dynamic traffic flow modeling', 'multilane traffic model', 'vehicular Boltzmann equation', 'traffic dispersion model', 'road users', 'nonlinear Poisson equation', 'car-following theory', 'dynamic evolution', 'variance equation', 'motion equation', 'Poisson equation', 'Boltzmann equation', 'flow simulation', 'multiphase flow', 'nonlinear differential equations', 'Poisson equation', 'road traffic']","['traffic flow', 'model', 'systematic self-consistent multiclass', 'vehicular Boltzmann equation', 'systematic gas-kinetic model', 'traffic', 'traffic dispersion model', 'traffic model', 'dispersion model', 'Boltzmann equation']",1145,199,17,1145,198,10,10,2,3
"a fuzzy logic approach to accommodate thermal stress and improve the start-up phase in combined cycle power plants use of combined cycle power generation plant has increased dramatically over the last decade. a supervisory control approach based on a dynamic model is developed, which makes use of proportional-integral-derivative (pid), fuzzy logic and fuzzy pid schemes. the aim is to minimize the steam turbine plant start-up time, without violating maximum thermal stress limits. an existing start-up schedule provides the benchmark by which the performance of candidate controllers is assessed. improvements regarding possible reduced start-up times and satisfaction of maximum thermal stress restrictions have been realized using the proposed control scheme ","Atuzzy logic approach to accommodate thermal stress and improve the start-up phase in combined cycle power plants Use of combined cycle power generation plant has increased dramatically over the last decade. A supervisory control approach based on a dynamic model is developed, which makes use of proportional-integral-derivative (PID), fuzzy logic and fuzzy PID schemes. The aim is to minimize the steam turbine plant start-up time, without violating maximum thermal stress limits. An existing start-up schedule provides the benchmark by which the performance of candidate controllers is assessed. Improvements regarding possible reduced start-up times and satisfaction ‘of maximum thermal stress restrictions have been realized using the proposed control scheme","['combined cycle power plants', 'supervisory control', 'fuzzy logic approach', 'dynamic model', 'PID control', 'fuzzy PID schemes', 'steam turbine plant start-up time minimization', 'maximum thermal stress limits', 'start-up schedule', 'combined cycle power stations', 'control system synthesis', 'fuzzy control', 'power station control', 'steam turbines', 'thermal stresses', 'three-term control']","['supervisory control approach', 'thermal stress restrictions', 'thermal stress limits', 'fuzzy logic approach', 'steam turbine plant', 'start-up times', 'start-up phase', 'thermal stress', 'fuzzy', 'fuzzy logic']",655,110,16,656,108,10,419,108,2
"enhancing the reliability of modular medium-voltage drives a method to increase the reliability of modular medium-voltage induction motor drives is discussed, by providing means to bypass a failed module. the impact on reliability is shown. a control, which maximizes the output voltage available after bypass, is described, and experimental results are given ","Enhancing the reliability of modular medium-voltage drives A method to increase the reliability of modular medium-voltage induction motor drives is discussed, by providing means to bypass a failed module. The impact on reliability is shown. A control, which maximizes the output voltage available after bypass, is described, and experimental results are given","['modular medium-voltage induction motor drives', 'reliability enhancement', 'failed module bypass', 'available output voltage control', 'failure analysis', 'fault tolerance', 'induction motor drives', 'machine control', 'reliability', 'voltage control']","['reliability', 'modular medium-voltage drives', 'output voltage', 'induction', 'Enhancing', 'method', 'motor', 'drives', 'modular', 'medium-voltage']",308,53,10,308,52,10,0,0,0
"hybrid fuzzy modeling of chemical processes fuzzy models have been proved to have the ability of modeling all plants without any priori information. however, the performance of conventional fuzzy models can be very poor in the case of insufficient training data due to their poor extrapolation capacity. in order to overcome this problem, a hybrid grey-box fuzzy modeling approach is proposed in this paper to combine expert experience, local linear models and historical data into a uniform framework. it consists of two layers. the expert fuzzy model constructed from linguistic information, the local linear model and the t-s type fuzzy model constructed from data are all put in the first layer. layer 2 is a fuzzy decision module that is used to decide which model in the first layer should be employed to make the final prediction. the output of the second layer is the output of the hybrid fuzzy model. with the help of the linguistic information, the poor extrapolation capacity problem caused by sparse training data for conventional fuzzy models can be overcome. simulation result for ph neutralization process demonstrates its modeling ability over the linear models, the expert fuzzy model and the conventional fuzzy model ","Hybrid fuzzy modeling of chemical processes Fuzzy models have been proved to have the ability of modeling all plants without any priori information. However, the performance of conventional fuzzy models can be very poor in the case of insufficient training data due to their poor extrapolation capacity. In order to ‘overcome this problem, a hybrid grey-box fuzzy modeling approach is proposed in this paper to combine expert experience, local linear models and historical data into a uniform framework. It consists of two layers. The expert fuzzy model constructed from linguistic information, the local linear model and the T-S type fuzzy model constructed from data are all put in the first layer. Layer 2 is a fuzzy decision module that is used to decide which model in the first layer should be ‘employed to make the final prediction. The output of the second layer is the output of the hybrid fuzzy model. With the help of the linguistic information, the poor extrapolation capacity problem caused by Sparse training data for conventional fuzzy models can be overcome. Simulation result for pH neutralization process demonstrates its modeling ability over the linear models, the expert fuzzy model and the conventional fuzzy model","['fuzzy modeling', 'chemical processes', 'expert fuzzy model', 'fuzzy decision module', 'process modeling', 'chemical engineering computing', 'fuzzy set theory', 'modelling', 'process control']","['conventional fuzzy model', 'Fuzzy models', 'fuzzy', 'fuzzy model', 'insufficient training data', 'fuzzy decision module', 'local linear models', 'modelling ability', 'models', 'linear models']",1037,199,9,1039,198,10,16,2,1
"the heat is on [building automation systems] integrating building automation systems (bass) can result in systems that have the ability to sense changes in the air temperature through a building's heating, ventilation, and air conditioning (hvac) systems. taking advantages of the internet, using remote monitoring, and building interoperability through open protocol systems are some of the issues discussed throughout the bas/hvac community. by putting information over the internet, facility managers get real-time data on energy usage and performance issues ","The heat is on [building automation systems] Integrating building automation systems (BASs) can result in systems that have the ability to sense changes in the air temperature through a building's heating, ventilation, and air conditioning (HVAC) systems. Taking advantages of the Internet, using remote monitoring, and building interoperability through open protocol systems are some of the issues discussed throughout the BAS/HVAC community. By putting information over the Internet, facility managers get real-time data on energy usage and performance issues","['building automation systems', 'HVAC', 'heating', 'ventilation', 'air conditioning', 'remote monitoring', 'interoperability', 'Internet', 'real-time data', 'building management systems', 'computerised monitoring', 'HVAC', 'Internet', 'open systems']","['automation systems', 'buildings heating ventilation', 'building interoperability', 'open protocol systems', 'air conditioning', 'air temperature', 'building', 'systems', 'heat', 'automation']",483,80,14,483,79,10,0,0,6
"revisiting hardy's paradox: counterfactual statements, real measurements, entanglement and weak values hardy's (1992) paradox is revisited. usually the paradox is dismissed on grounds of counterfactuality, i.e., because the paradoxical effects appear only when one considers results of experiments which do not actually take place. we suggest a new set of measurements in connection with hardy's scheme, and show that when they are actually performed, they yield strange and surprising outcomes. more generally, we claim that counterfactual paradoxes point to a deeper structure inherent to quantum mechanics ","Revisiting Hardy's paradox: Counterfactual statements, real measurements, entanglement and weak values Hardy's (1992) paradox is revisited. Usually the paradox is dismissed on grounds of counterfactuality, i.e., because the paradoxical effects appear only when one considers results of experiments which do not actually take place. We suggest a new set of measurements in connection with Hardy's scheme, and show that when they are actually performed, they yield strange and surprising outcomes. More generally, we claim that counterfactual paradoxes point to a deeper structure inherent to quantum mechanics","['Hardy paradox', 'counterfactual statements', 'real measurements', 'entanglement', 'gedanken-experiments', 'weak values', 'paradoxical effects', 'quantum mechanics', 'bound states', 'information theory', 'measurement theory', 'probability', 'quantum theory']","['paradox', 'real measurements entanglement', 'Counterfactual statements', 'counterfactual paradoxes', 'paradoxical effects', 'hardy paradox', 'hardy scheme', 'weak values', 'hardy', 'measurements']",523,87,13,523,86,10,0,0,1
"open courseware and shared knowledge in higher education most college and university campuses in the united states and much of the developed world today maintain one, two, or several learning management systems (lmss), which are courseware products that provide students and faculty with web-based tools to manage course-related applications. since the mid-1990s, two predominant models of web courseware management systems have emerged: commercial and noncommercial. some of the commercial products available today were created in academia as noncommercial but have since become commercially encumbered. other products remain noncommercial but are struggling to survive in a world of fierce commercial competition. this article argues for an ethics of pedagogy in higher education that would be based on the guiding assumptions of the non-proprietary, peer-to-peer, open-source software movement ","Open courseware and shared knowledge in higher education Most college and university campuses in the United States and much of the developed world today maintain one, two, or several learning management systems (LMSs), which are courseware products that provide students and faculty with Web-based tools to manage course-related applications. Since the mid-1990s, two predominant models of Web courseware management systems have emerged: commercial and noncommercial. Some of the commercial products available today were created in academia as noncommercial but have since become commercially encumbered. Other products remain noncommercial but are struggling to survive in a world of fierce commercial competition. This article argues for an ethics of pedagogy in higher education that would be based on the guiding assumptions of the non-proprietary, peer-to-peer, open-source software movement","['open courseware', 'shared knowledge', 'higher education', 'learning management systems', 'college', 'university', 'Web courseware management systems', 'commercial products', 'ethics', 'Internet', 'open-source software', 'courseware', 'information resources', 'Internet', 'public domain software']","['management systems', 'higher education', 'fierce commercial competition', 'developed world today', 'courseware products', 'commercial products', 'Other products', 'Most college', 'courseware', 'education']",771,127,15,771,126,10,0,0,4
"prospective on computer applications in power the so-called ""deregulation"" and restructuring of the electric power industry have made it very difficult to keep up with industry changes and have made it much more difficult to envision the future. in this article, current key issues and major developments of the past few years are reviewed to provide perspective, and prospects for future computer applications in power are suggested. technology changes are occurring at an exponential rate. the interconnected bulk electric systems are becoming integrated with vast networked information systems. this article discusses the skills that will be needed by future power engineers to keep pace with these developments and trends ","Prospective on computer applications in power The so-called ""deregulation"" and restructuring of the electric power industry have made it very difficult to keep up with industry changes and have made it much more difficult to envision the future. In this article, current key issues and major developments of the past few years are reviewed to provide perspective, and prospects for future computer applications in power are suggested. Technology changes are occurring at an exponential rate. The interconnected bulk electric systems are becoming integrated with vast networked information systems. This article discusses the skills that will be needed by future power engineers to keep pace with these developments and trends","['electric power industry deregulation', 'computer applications', 'electricity industry restructuring', 'technology changes', 'interconnected bulk electric systems', 'networked information systems', 'electricity supply industry', 'management information systems', 'power engineering computing']","['future computer applications', 'electric power industry', 'future power engineers', 'Technology changes', 'industry changes', 'electric systems', 'power', 'computer', 'applications', 'computer applications']",617,110,9,617,109,10,0,0,0
"learning nonregular languages: a comparison of simple recurrent networks and lstm rodriguez (2001) examined the learning ability of simple recurrent nets (srns) (elman, 1990) on simple context-sensitive and context-free languages. in response to rodriguez's (2001) article, we compare the performance of simple recurrent nets and long short-term memory recurrent nets on context-free and context-sensitive languages ","Learning nonregular languages: a comparison of simple recurrent networks and LSTM Rodriguez (2001) examined the learning ability of simple recurrent nets (SRNs) (Elman, 1990) on simple context-sensitive and context-free languages. In response to Rodriguez's (2001) article, we compare the performance of simple recurrent nets and long short-term memory recurrent nets on context-free and context-sensitive languages","['nonregular language learning', 'recurrent neural networks', 'LSTM', 'context-sensitive languages', 'context-free languages', 'performance', 'short-term memory recurrent nets', 'context-free languages', 'context-sensitive languages', 'learning (artificial intelligence)', 'performance evaluation', 'recurrent neural nets']","['simple recurrent nets', 'context-sensitive languages', 'simple recurrent networks', 'context-free languages', 'nonregular languages', 'languages', 'recurrent', 'simple', 'nets', 'recurrent nets']",361,56,12,361,55,10,0,0,2
"aim for the enterprise: microsoft project 2002 a long-time favorite of project managers, microsoft project 2002 is making its enterprise debut. its new web-based collaboration tools and improved scalability with olap support make it much easier to manage multiple web projects with disparate workgroups and budgets ","Aim for the enterprise: Microsoft Project 2002 A long-time favorite of project managers, Microsoft Project 2002 is making its enterprise debut. Its new Web-based collaboration tools and improved scalability with OLAP support make it much easier to manage multiple Web projects with disparate workgroups and budgets","['Microsoft Project 2002', 'Web-based collaboration tools', 'scalability', 'OLAP support', 'multiple Web project management', 'workgroups', 'budgets', 'business data processing', 'data mining', 'Internet', 'project management', 'software reviews']","['enterprise Microsoft Project', 'multiple Web projects', 'long-time favorite', 'enterprise debut', 'project', 'Project', 'Aim', 'enterprise', 'long-time', 'Microsoft']",269,47,12,269,46,10,0,0,5
"ventilation-perfusion ratio of signal intensity in human lung using oxygen-enhanced and arterial spin labeling techniques this study investigates the distribution of ventilation-perfusion (v/q) signal intensity (si) ratios using oxygen-enhanced and arterial spin labeling (asl) techniques in the lungs of 10 healthy volunteers. ventilation and perfusion images were simultaneously acquired using the flow-sensitive alternating inversion recovery (fair) method as volunteers alternately inhaled room air and 100% oxygen. images of the t/sub 1/ distribution were calculated for five volunteers for both selective (t/sub 1f/) and nonselective (t/sub 1/) inversion. the average t/sub 1/ was 1360 ms+or-116 ms, and the average t/sub 1f/ was 1012 ms+or-112 ms, yielding a difference that is statistically significant (p<0.002). excluding large pulmonary vessels, the average v/q si ratios were 0.355+or-0.073 for the left lung and 0.371+or-0.093 for the right lung, which are in agreement with the theoretical v/q si ratio. plots of the wo si ratio are similar to the logarithmic normal distribution obtained by multiple inert gas elimination techniques, with a range of ratios matching ventilation and perfusion. this mri v/q technique is completely noninvasive and does not involve ionized radiation. a limitation of this method is the nonsimultaneous acquisition of perfusion and ventilation data, with oxygen administered only for the ventilation data ","Ventilation-pertusion ratio of signal intensity in human lung using ‘oxygen-enhanced and arterial spin labeling techniques This study investigates the distribution of ventilation-pertusion (V/Q) signal intensity (SI) ratios using oxygen-enhanced and arterial spin labeling (ASL) techniques in the lungs of 10 healthy volunteers. Ventilation and perfusion images were simultaneously acquired using the flow-sensitive alternating inversion recovery (FAIR) method as volunteers alternately inhaled room air and 100% oxygen. Images of the T/sub 1/ distribution were calculated for five volunteers for both selective (T/sub 1f/) and nonselective (T/sub 1/) inversion. The average T/sub 1/ was 1360 mstor-116 ms, and the average T/sub 1f/ was 1012 ms+or-112 ms, yielding a difference that is statistically significant (P<0.002). Excluding large pulmonary vessels, the average V/Q SI ratios were 0.355+0r-0.073 for the left lung and 0.371+01-0.093 for the right lung, which are in agreement with the theoretical V/Q SI ratio. Plots of the WO SI ratio are similar to the logarithmic normal distribution obtained by multiple inert gas elimination techniques, with a range of ratios matching ventilation and perfusion. This MRI V/Q technique is completely noninvasive and does not involve ionized radiation. A limitation of this method is the nonsimultaneous acquisition of perfusion and ventilation data, with oxygen administered only for the ventilation data","['human lung', 'ventilation-perfusion ratio', 'signal intensity', 'oxygen-enhanced techniques', 'arterial spin labeling techniques', 'ventilation images', 'perfusion images', 'flow-sensitive alternating inversion recovery', 'logarithmic normal distribution', 'nonsimultaneous acquisition', 'gas exchange efficiency', 'pathomechanisms', 'MRI', 'time delay', 'pixel-by-pixel maps', 'multiple inert gas elimination', 'pulmonary embolism', 'chronic obstructive pulmonary disease', 'biomedical MRI', 'log normal distribution', 'lung', 'medical image processing', 'pneumodynamics']","['signal intensity', 'ratios', 'Ventilation-pertusion ratio', 'MRI v/v technique', 'WO SI ratio', 'lung', 'right lung', 'human lung', 'left lung', 'intensity']",1243,208,23,1244,207,10,21,6,6
"quantitative speed control for srm drive using fuzzy adapted inverse model quantitative and robust speed control for a switched reluctance motor (srm) drive is considered to be rather difficult and challenging owing to its highly nonlinear dynamic behavior. a speed control scheme having two-degree-of-freedom (2dof) structure is developed here to improve the speed dynamic response of an srm drive. in the proposed control scheme, the feedback controller is quantitatively designed to meet the desired regulation control requirements first. then a reference model and a command feedforward controller based on an inverse plant model are employed to yield the desired tracking response at nominal case. as the variations of system parameters and operating conditions occur, the prescribed control specifications may not be satisfied any more. to improve this, the inverse model is adaptively tuned by a fuzzy control scheme so that the model-following tracking error is significantly reduced. in addition, a simple disturbance cancellation robust controller is added to improve the tracking and regulation control performances further ","Quantitative speed control for SRM drive using fuzzy adapted inverse model Quantitative and robust speed control for a switched reluctance motor (SRM) drive is considered to be rather difficult and challenging owing to its highly nonlinear dynamic behavior. A speed control scheme having two-degree-of-freedom (2DOF) structure is developed here to improve the speed dynamic response of an SRM drive. In the proposed control scheme, the feedback controller is quantitatively designed to meet the desired regulation control requirements first. Then a reference model and a command feedforward controller based on an inverse plant model are ‘employed to yield the desired tracking response at nominal case. As the variations of system parameters and operating conditions occur, the prescribed control specifications may not be satisfied any more. To improve this, the inverse model is adaptively tuned by a fuzzy control scheme so that the model-following tracking error is significantly reduced. In addition, a simple disturbance cancellation robust controller is added to improve the tracking and regulation control performances further","['quantitative speed control', 'SRM drive', 'fuzzy adapted inverse model', 'switched reluctance motor', 'nonlinear dynamic behavior', 'two-degree-of-freedom structure', 'speed dynamic response', 'regulation control requirements', 'reference model', 'command feedforward controller', 'inverse plant model', 'tracking response', 'system parameters', 'operating conditions', 'control specifications', 'fuzzy control scheme', 'model-following tracking error', 'disturbance cancellation controller', 'feedforward', 'fuzzy control', 'model reference adaptive control systems', 'reluctance motor drives', 'velocity control']","['regulation control performances', 'Quantitative speed control', 'feedforward controller', 'control specifications', 'speed control scheme', 'robust speed control', 'fuzzy control scheme', 'feedback controller', 'robust controller', 'control scheme']",969,167,23,970,166,10,8,1,9
"a partial converse to hadamard's theorem on homeomorphisms a theorem by hadamard gives a two-part condition under which a map from one banach space to another is a homeomorphism. the theorem, while often very useful, is incomplete in the sense that it does not explicitly specify the family of maps for which the condition is met. here, under a typically weak additional assumption on the map, we show that hadamard's condition is met if, and only if, the map is a homeomorphism with a lipschitz continuous inverse. an application is given concerning the relation between the stability of a nonlinear system and the stability of related linear systems ","A partial converse to Hadamard's theorem on homeomorphisms A theorem by Hadamard gives a two-part condition under which a map from one Banach space to another is a homeomorphism. The theorem, while often very useful, is incomplete in the sense that it does not explicitly specify the family of maps for which the condition is met. Here, under a typically weak additional assumption on the map, we show that Hadamard's condition is met if, and only if, the map is a homeomorphism with a Lipschitz continuous inverse. An application is given concerning the relation between the stability of a nonlinear system and the stability of related linear systems","['partial converse', 'Hadamard theorem', 'homeomorphisms', 'Banach space', 'Lipschitz continuous inverse', 'nonlinear system stability', 'linear system stability', 'linearization', 'nonlinear feedback systems', 'nonlinear networks', 'Banach spaces', 'feedback', 'linear systems', 'linearisation techniques', 'nonlinear network analysis', 'nonlinear systems', 'numerical stability']","['theorem', 'Hadamard', 'weak additional assumption', 'two-part condition', 'partial converse', 'homeomorphisms', 'branch space', 'maps', 'partial', 'condition']",544,109,17,544,108,10,0,0,2
"flexible air-jet tooling for vibratory bowl feeder systems vibratory bowl feeders (vbfs) are machines that feed various small parts in large volume automatic assembly systems. their shortcomings, like inflexibility and the propensity to jam, stem from the use of mechanical orienting devices. air jet based orienting devices can be implemented to overcome these limitations. applications of passive and active air jet based orienting devices that replace conventional devices for the vbf are discussed. passive devices, which reject incorrectly oriented parts, are discussed first. active air jet based orienting devices are then introduced to further improve the flexibility of vbfs. since active devices reorient parts into a desired orientation, the part motion under their influence is analyzed. a number of tests demonstrate the feasibility and advantages of these new orienting devices ","Flexible air-jet tooling for vibratory bowl feeder systems Vibratory bowl feeders (VBFs) are machines that feed various small parts in large volume automatic assembly systems. Their shortcomings, like inflexibility and the propensity to jam, stem from the use of mechanical orienting devices. Air jet based orienting devices can be implemented to overcome these limitations. Applications of passive and active air jet based orienting devices that replace conventional devices for the VBF are discussed. Passive devices, which reject incorrectly oriented parts, are discussed first. Active air jet based orienting devices are then introduced to further improve the flexibility of VBFS. Since active devices reorient parts into a desired orientation, the part motion under their influence is analyzed. A number of tests demonstrate the feasibility and advantages of these new orienting devices","['vibratory bowl feeders', 'automatic assembly systems', 'passive air jet', 'active air jet', 'orienting devices', 'parts feeding', 'assembling', 'computerised control', 'materials handling']","['automatic assembly systems', 'Flexible air-vent tooling', 'Vibratory bowl feeders', 'various small parts', 'devices Air jet', 'active devices', 'bowl', 'feeders', 'Flexible', 'vibratory']",762,131,9,762,130,10,0,0,0
"modeling dynamic objects in distributed systems with nested petri nets nested petri nets (np-nets) is a petri net extension, allowing tokens in a net marking to be represented by marked nets themselves. the paper discusses applicability of np-nets for modeling task planning systems, multi-agent systems and recursive-parallel systems. a comparison of np-nets with some other formalisms, such as opns of r. valk (2000), recursive parallel programs of o. kushnarenko and ph. schnoebelen (1997) and process algebras is given. some aspects of decidability for object-oriented petri net extensions are also discussed ","Modeling dynamic objects in distributed systems with nested Petri nets Nested Petri nets (NP-nets) is a Petri net extension, allowing tokens in a net marking to be represented by marked nets themselves. The paper discusses applicability of NP-nets for modeling task planning systems, multi-agent systems and recursive-parallel systems. A comparison of NP-nets with some other formalisms, such as OPNs of R. Valk (2000), recursive parallel programs of O. Kushnarenko and Ph. Schnoebelen (1997) and process algebras is given. Some aspects of decidability for object-oriented Petri net extensions are also discussed","['dynamic objects modelling', 'distributed systems', 'nested Petri nets', 'multi-agent systems', 'recursive-parallel systems', 'process algebras', 'decidability', 'object-oriented Petri net', 'decidability', 'formal specification', 'multi-agent systems', 'parallel programming', 'Petri nets', 'process algebra', 'synchronisation']","['Petri net extension', 'net', 'recursive-parallel systems', 'systems', 'task planning systems', 'multi-agency systems', 'dynamic objects', 'marked nets', 'dynamic', 'Petri']",523,91,15,523,90,10,0,0,4
"toward a formalism for conversation protocols using joint intention theory conversation protocols are used to achieve certain goals or to bring about certain states in the world. therefore, one may identify the landmarks or the states that must be brought about during the goal-directed execution of a protocol. accordingly, the landmarks, characterized by propositions that are true in the state represented by that landmark, are the most important aspect of a protocol. families of conversation protocols can be expressed formally as partially ordered landmarks after the landmarks necessary to achieve a goal have been identified. concrete protocols represented as joint action expressions can, then, be derived from the partially ordered landmarks and executed directly by joint intention interpreters. this approach of applying joint intention theory to protocols also supports flexibility in the actions used to get to landmarks, shortcutting protocol execution, automatic exception handling, and correctness criterion for protocols and protocol compositions ","Toward a formalism for conversation protocols using joint intention theory Conversation protocols are used to achieve certain goals or to bring about certain states in the world. Therefore, one may identify the landmarks or the states that must be brought about during the goal-directed execution of a protocol. Accordingly, the landmarks, characterized by propositions that are true in the state represented by that landmark, are the most important aspect of a protocol. Families of conversation protocols can be expressed formally as partially ordered landmarks after the landmarks necessary to achieve a goal have been identified Concrete protocols represented as joint action expressions can, then, be derived from the partially ordered landmarks and executed directly by joint intention interpreters. This approach of applying Joint Intention theory to protocols also supports flexibility in the actions used to get to landmarks, shortcutting protocol execution, automatic exception handling, and correctness criterion for protocols and protocol compositions","['conversation protocols', 'protocol execution', 'automatic exception handling', 'correctness criterion', 'multi-agent interactions', 'finite state machines', 'finite state machines', 'multi-agent systems', 'protocols']","['protocol', 'landmarks', 'conversation protocols', 'joint intention interpreters', 'joint action expressions', 'protocol compositions', 'protocol execution', 'Concrete protocols', 'protocol Families', 'certain states']",913,153,9,912,152,10,0,1,2
playing for time [3g networks] the delays in rolling out 3g networks across europe should not always be seen with a negative slant ,Playing for time [3G networks] The delays in rolling out 3G networks across Europe should not always be seen with a negative slant,"['3G networks', 'delays', 'Europe', 'mobile operators', 'cellular radio', 'mobile computing']","['negative slant', 'big networks', 'delays', 'Europe', 'time', 'g', 'networks', 'big', 'negative', 'g networks']",108,24,6,108,23,10,0,0,0
"the impact and implementation of xml on business-to-business commerce this paper discusses the impact analysis of the extensible markup language (xml). each business partner within a supply chain will be allowed to generate its own data exchange format by adopting an xml meta-data management system in the local side. followed after a brief introduction of the information technology for business to customer (b2c) and business to business (b2b) electronic commerce (ec), the impact of xml on the tomorrow business world is discussed. a real case study for impact analysis on information exchange platform, microsoft's biztalk platform which is actually an xml schema builder and the implementation of xml commerce application will provide an interest insight for users' future implementation ","The impact and implementation of XML on business-to-business commerce This paper discusses the impact analysis of the Extensible Markup Language (XML). Each business partner within a supply chain will be allowed to generate its own data exchange format by adopting an XML meta-data management system in the local side. Followed after a brief introduction of the information technology for Business to Customer (B2C) and Business to Business (B2B) Electronic Commerce (EC), the impact of XML on the tomorrow business world is discussed. A real case study for impact analysis on information exchange platform, Microsoft's BizTalk platform which is actually an XML schema builder and the implementation of XML commerce application will provide an interest insight for users' future implementation","['Extensible Markup Language', 'XML', 'Business to Customer', 'BizTalk', 'XML schema builder', 'Business to Business', 'electronic commerce', 'Electronic Data Interchange', 'Enterprise Resources Planning', 'electronic commerce', 'electronic data interchange', 'hypermedia markup languages']","['impact analysis', 'business-to-business commerce', 'Extensible Markup Language', 'tomorrow business world', 'ml commerce application', 'future implementation', 'ml schema builder', 'ml', 'implementation', 'impact']",675,120,12,675,119,10,0,0,2
"innovative phase unwrapping algorithm: hybrid approach we present a novel algorithm based on a hybrid of the global and local treatment of a wrapped map. the proposed algorithm is especially effective for the unwrapping of speckle-coded interferogram contour maps. in contrast to earlier unwrapping algorithms by region, we propose a local discontinuity-restoring criterion to serve as the preprocessor or postprocessor of our hybrid algorithm, which makes the unwrapping by region much easier and more efficient. with this hybrid algorithm, a robust, stable, and especially time effective phase unwrapping can be achieved. additionally, the criterion and limitation of this hybrid algorithm are fully described. the robustness, stability, and speed of this hybrid algorithm are also studied. the proposed algorithm can be easily upgraded with minor modifications to solve the unwrapping problem of maps with phase inconsistency. both numerical simulation and experimental applications demonstrate the effectiveness of the proposed algorithm ","Innovative phase unwrapping algorithm: hybrid approach We present a novel algorithm based on a hybrid of the global and local treatment of a wrapped map. The proposed algorithm is especially effective for the unwrapping of speckle-coded interferogram contour maps. In contrast to earlier unwrapping algorithms by region, we propose a local discontinuity-restoring criterion to serve as the preprocessor or postprocessor of our hybrid algorithm, which makes the unwrapping by region much easier and more efficient. With this hybrid algorithm, a robust, stable, and especially time effective phase unwrapping can be achieved. Additionally, the criterion and limitation of this hybrid algorithm are fully described. The robustness, stability, and speed of this hybrid algorithm are also studied. The proposed algorithm can be easily upgraded with minor modifications to solve the unwrapping problem of maps with phase inconsistency. Both numerical simulation and experimental applications demonstrate the effectiveness of the proposed algorithm","['phase unwrapping algorithm', 'global treatment', 'local treatment', 'wrapped map', 'speckle-coded interferogram contour maps', 'unwrapping algorithms', 'local discontinuity-restoring criterion', 'postprocessor', 'hybrid algorithm', 'robust stable time effective phase unwrapping', 'unwrapping problem', 'phase inconsistency', 'numerical simulation', 'interferogram analysis', 'light interferometry', 'light interferometry', 'optical information processing', 'speckle', 'stability']","['algorithm', 'hybrid algorithm', 'local discontinuity-restoring criterion', 'earlier unwrapping algorithms', 'effective phase unwrapping', 'algorithm hybrid approach', 'phase inconsistency', 'Innovative phase', 'novel algorithm', 'hybrid']",894,149,19,894,148,10,0,0,6
"mrp in a job shop environment using a resource constrained project scheduling model one of the most difficult tasks in a job shop manufacturing environment is to balance schedule and capacity in an ongoing basis. mrp systems are commonly used for scheduling, although their inability to deal with capacity constraints adequately is a severe drawback. in this study, we show that material requirements planning can be done more effectively in a job shop environment using a resource constrained project scheduling model. the proposed model augments mrp models by incorporating capacity constraints and using variable lead time lengths. the efficacy of this approach is tested on mrp systems by comparing the inventory carrying costs and resource allocation of the solutions obtained by the proposed model to those obtained by using a traditional mrp model. in general, it is concluded that the proposed model provides improved schedules with considerable reductions in inventory carrying costs ","MRP in a job shop environment using a resource constrained project scheduling model One of the most difficult tasks in a job shop manufacturing environment is to balance schedule and capacity in an ongoing basis. MRP systems are ‘commonly used for scheduling, although their inability to deal with capacity constraints adequately is a severe drawback. In this study, we show that material requirements planning can be done more effectively in a job shop environment using a resource constrained project scheduling model. The proposed model augments MRP models by incorporating capacity constraints and using variable lead time lengths. The efficacy of this approach is tested on MRP systems by ‘comparing the inventory carrying costs and resource allocation of the solutions obtained by the proposed model to those obtained by using a traditional MRP model. In general, it is concluded that the proposed model provides improved schedules with considerable reductions in inventory carrying costs","['job shop environment', 'MRP', 'resource constrained project scheduling model', 'material requirements planning', 'scheduling', 'capacity constraints', 'variable lead time lengths', 'inventory carrying costs', 'resource allocation', 'project management', 'manufacturing resources planning', 'production control', 'project management', 'resource allocation', 'stock control']","['project scheduling model', 'job shop environment', 'capacity constraints', 'model', 'MRP systems', 'traditional MRP model', 'resource allocation', 'MRP models', 'MRP', 'schedules']",841,153,15,843,152,10,16,2,3
"production capacity of flexible manufacturing systems with fixed production ratios determining the production capacity of flexible manufacturing systems is a very important issue in the design of such systems. we propose an approach for determining the production capacity (i.e. the maximum production rate) of a flexible manufacturing system with several part types, dedicated pallets, and fixed production ratios among the different part types. we show that the problem reduces to the determination of a single parameter for which we propose an iterative procedure. simulation or approximate analytical techniques can be used as the building block performance evaluation technique in the iterative procedure ","Production capacity of flexible manufacturing systems with fixed production ratios Determining the production capacity of flexible manufacturing systems is a very important issue in the design of such systems. We propose an approach for determining the production capacity (i.e. the maximum production rate) of a flexible manufacturing system with several part types, dedicated pallets, and fixed production ratios among the different part types. We show that the problem reduces to the determination of a single parameter for which we propose an iterative procedure. Simulation or approximate analytical techniques can be used as the building block performance evaluation technique in the iterative procedure","['flexible manufacturing systems', 'fixed production ratios', 'production capacity', 'maximum production rate', 'multiple part type', 'dedicated pallets', 'single parameter determination', 'iterative procedure', 'simulation', 'approximate analytical techniques', 'building block performance evaluation technique', 'stability condition', 'numerical experiments', 'flexible manufacturing systems', 'iterative methods', 'production control']","['flexible manufacturing systems', 'fixed production ratios', 'production', 'maximum production rate', 'several part types', 'such systems', 'systems', 'capacity', 'production capacity', 'Production capacity']",608,103,16,608,102,10,0,0,5
an entanglement measure based on the capacity of dense coding an asymptotic entanglement measure for any bipartite states is derived in the light of the dense coding capacity optimized with respect to local quantum operations and classical communications. general properties and some examples with explicit forms of this entanglement measure are investigated ,‘An entanglement measure based on the capacity of dense coding An asymptotic entanglement measure for any bipartite states is derived in the light of the dense coding capacity optimized with respect to local quantum operations and classical communications. General properties and some examples with explicit forms of this entanglement measure are investigated,"['entanglement measure', 'dense coding capacity', 'asymptotic entanglement measure', 'bipartite states', 'local quantum operations', 'classical communications', 'optimization', 'encoding', 'optimisation', 'quantum communication']","['asymptotic entanglement measure', 'local quantum operations', 'dense coding capacity', 'bipartite states', 'explicit forms', 'measure', 'entanglement', 'dense', 'capacity', 'entanglement measure']",307,53,10,308,52,10,2,1,1
"the plot thins: thin-client computer systems and academic libraries the few libraries that have tried thin client architectures have noted a number of compelling reasons to do so. for starters, thin client devices are far less expensive than most pcs. more importantly, thin client computing devices are believed to be far less expensive to manage and support than traditional pcs ","The plot thins: thin-client computer systems and academic libraries The few libraries that have tried thin client architectures have noted a number ‘of compelling reasons to do so. For starters, thin client devices are far less expensive than most PCs. More importantly, thin client computing devices are believed to be far less expensive to manage and support than traditional PCs","['academic libraries', 'thin-client computer systems', 'academic libraries', 'library automation', 'network computers']","['thin-client computer systems', 'thin', 'thin client architectures', 'client', 'thin client devices', 'academic libraries', 'few libraries', 'libraries', 'systems', 'computer']",321,61,5,322,60,10,2,1,0
"women in computing: what brings them to it, what keeps them in it? career stereotyping and misperceptions about the nature of computing are substantive reasons for the under representation of women in professional computing careers. in this study, 15 women who have work experience in several aspects of computing were asked about their reasons for entering computing, what they liked about working in computing, and what they disliked. while there are many common threads, there are also individual differences. common reasons for choosing computing as a career included: exposure to computing in a setting which enabled them to see the versatility of computers; the influence of someone close to them; personal abilities which they perceived to be appropriate for a career in computing; and characteristics of such careers which appealed to them. generally, women working in the field enjoy the work they are doing. dislikes arising from their work experiences are more likely to be associated with people and politics than with the work they do-and they would like to have more female colleagues ","Women in computing: what brings them to it, what keeps them in it? Career stereotyping and misperceptions about the nature of computing are substantive reasons for the under representation of women in professional computing careers. In this study, 15 women who have work experience in several aspects of computing were asked about their reasons for entering computing, what they liked about working in ‘computing, and what they disliked. While there are many common threads, there are also individual differences. Common reasons for choosing ‘computing as a career included: exposure to computing in a setting which enabled them to see the versatility of computers; the influence ‘of someone close to them; personal abilities which they perceived to be appropriate for a career in computing; and characteristics of such careers which appealed to them. Generally, women working in the field enjoy the work they are doing. Dislikes arising from their work experiences are more likely to be associated with people and politics than with the work they do-and they would like to have more female colleagues","['career stereotyping', 'personal abilities', 'politics', 'misperceptions', 'women', 'professional computing careers', 'computer science education', 'employment', 'gender issues', 'professional aspects']","['Women', 'work experience', 'professional computing careers', 'substantive reasons', 'career', 'Common reasons', 'such careers', 'nature', 'reasons', 'computing']",925,175,10,928,174,10,21,3,3
"use of extra degrees of freedom in multilevel drives multilevel converters with series connection of semiconductors allow power electronics to reach medium voltages (1-10 kv) with relatively standard components. the increase of the number of semiconductors provides extra degrees of freedom, which can be used to improve different characteristics. this paper is focused on variable-speed drives and it is shown that with the proposed multilevel direct torque control strategy (dicoif) the tradeoff between the performances of the drive (harmonic distortions, torque dynamics, voltage step gradients, etc.) and the switching frequency of the semiconductors is improved. then, a slightly modified strategy reducing common-mode voltage and bearing currents is presented ","Use of extra degrees of freedom in multilevel drives Multilevel converters with series connection of semiconductors allow power electronics to reach medium voltages (1-10 kV) with relatively standard ‘components. The increase of the number of semiconductors provides extra degrees of freedom, which can be used to improve different characteristics. This paper is focused on variable-speed drives and it is shown that with the proposed multilevel direct torque control strategy (DiCoIF) the tradeoff between the performances of the drive (harmonic distortions, torque dynamics, voltage step gradients, etc.) and the switching frequency of the semiconductors is improved. Then, a slightly modified strategy reducing common-mode voltage and bearing currents is presented","['degrees of freedom', 'series connection', 'semiconductors', 'power electronics', 'medium voltages', 'variable-speed drives', 'multilevel direct torque control strategy', 'harmonic distortions', 'torque dynamics', 'voltage step gradients', 'switching frequency', 'common-mode voltage reduction', 'bearing currents', 'delay estimation', 'industrial power systems', 'insulated gate bipolar transistors', 'state estimation', 'multilevel drives', 'fixed-frequency dynamic control', '1 to 10 kV', 'harmonic distortion', 'induction motor drives', 'insulated gate bipolar transistors', 'machine bearings', 'machine control', 'power convertors', 'state estimation', 'switching circuits', 'torque control']","['semiconductors', 'extra degrees', 'freedom', 'Use', 'different characteristics', 'variable-speed drives', 'multilevel drives', 'drive', 'extra', 'degrees']",659,109,29,660,108,10,11,1,10
"utilizing web-based case studies for cutting-edge information services issues this article reports on a pilot study conducted by the academic libraries of the 21st century project team to determine whether the benefits of the case study method as a training framework for change initiatives could successfully transfer from the traditional face-to-face format to a virtual format. methods of developing the training framework, as well as the benefits, challenges, and recommendations for future strategies gained from participant feedback are outlined. the results of a survey administered to chat session registrants are presented in three sections: (1) evaluation of the training framework; (2) evaluation of participants' experiences in the virtual environment; and (3) a comparison of participants' preference of format. the overall participant feedback regarding the utilization of the case study method in a virtual environment for professional development and collaborative problem solving is very positive ","Utilizing Web-based case studies for cutting-edge information services issues This article reports on a pilot study conducted by the Academic Libraries of the 21st Century project team to determine whether the benefits of the case study method as a training framework for change initiatives could successfully transfer from the traditional face-to-face format to a virtual format. Methods of developing the training framework, as well as the benefits, challenges, and recommendations for future strategies gained from participant feedback are outlined. The results of a survey administered to chat session registrants are presented in three sections: (1) evaluation of the training framework; (2) evaluation of participants’ experiences in the virtual environment, and (3) a ‘comparison of participants! preference of format. The overall participant feedback regarding the utilization of the case study method ina virtual environment for professional development and collaborative problem solving is very positive","['Web-based case studies', 'cutting-edge information services', 'academic libraries', 'training', 'change initiatives', 'survey', 'virtual environment', 'professional development', 'Internet', 'collaborative problem solving', 'academic libraries', 'computer based training', 'information resources', 'Internet', 'library automation']","['training framework', 'case study method', 'overall participant feedback', 'virtual format Methods', 'pen-based case studies', 'Century project team', 'pilot study', 'case', 'studies', 'participant feedback']",870,145,15,871,143,10,65,17,7
"e-learning on the college campus: a help or hindrance to students learning objectives: a case study if you know how to surf the world wide web, have used email before, and can learn how to send an email attachment, then learning how to interact in an online course should not be difficult at all. in a way to find out, i decided to offer two identical courses, one of which would be offered online and the other the ""traditional way"". i wanted to see how students would fare with identical material provided in each course. i wanted their anonymous feedback, when the course was over ","E-learning on the college campus: a help or hindrance to students learning objectives: a case study If you know how to surf the World Wide Web, have used email before, and can learn how to send an email attachment, then learning how to interact in an online course should not be difficult at all. In a way to find out, I decided to offer two identical courses, one of which would be offered online and the other the ""traditional way"". | wanted to see how students would fare with identical material provided in each course. | wanted their anonymous feedback, when the course was over","['distance education', 'William Paterson University', 'e-learning', 'computer aided instruction', 'distance learning']","['students', 'identical courses', 'World Wide webb', 'college campus', 'online course', 'case study', 'hindrance', 'course', 'help', 'college']",479,106,5,479,105,10,2,2,1
"advanced aerostatic stability analysis of cable-stayed bridges using finite-element method based on the concept of limit point instability, an advanced nonlinear finite-element method that can be used to analyze the aerostatic stability of cable-stayed bridges is proposed. both geometric nonlinearity and three components of wind loads are considered in this method. the example bridge is the second santou bay cable-stayed bridge with a main span length of 518 m built in china. aerostatic stability of the example bridge is investigated using linear and proposed methods. the effect of pitch moment coefficient on the aerostatic stability of the bridge has been studied. the results show that the aerostatic instability analyses of cable-stayed bridges based on the linear method considerably overestimate the wind-resisting capacity of cable-stayed bridges. the proposed method is highly accurate and efficient. pitch moment coefficient has a major effect on the aerostatic stability of cable-stayed bridges. finally, the aerostatic failure mechanism of cable-stayed bridges is explained by tracing the aerostatic instability path ","Advanced aerostatic stability analysis of cable-stayed bridges using finite-element method Based on the concept of limit point instability, an advanced nonlinear finite-element method that can be used to analyze the aerostatic stability of cable-stayed bridges is proposed. Both geometric nonlinearity and three components of wind loads are considered in this, method. The example bridge is the second Santou Bay cable-stayed bridge with a main span length of 518 m built in China. Aerostatic stability of the example bridge is investigated using linear and proposed methods. The effect of pitch moment coefficient on the aerostatic stability of the bridge has been studied. The results show that the aerostatic instability analyses of cable-stayed bridges based on the linear method considerably overestimate the wind-resisting capacity of cable-stayed bridges. The proposed method is highly accurate and efficient. Pitch moment coefficient has a major effect on the aerostatic stability of cable-stayed bridges. Finally, the aerostatic failure mechanism of cable-stayed bridges is explained by tracing the aerostatic instability path","['limit point instability', 'advanced nonlinear finite element method', 'advanced aerostatic stability analysis', 'cable-stayed bridges', 'geometric nonlinearity', 'wind loads', 'Santou Bay cable-stayed bridge', 'China', 'pitch moment coefficient', 'aerostatic failure mechanism', 'civil engineering computing', 'finite element analysis', 'mechanical stability', 'wind']","['cable-stayed bridges', 'aerobatic', 'finite-element method', 'example bridge', 'aerobatic instability analyses', 'aerobatic stability analysis', 'aerobatic failure mechanism', 'aerobatic instability path', 'bridge', 'aerobatic stability']",972,164,14,973,163,10,0,1,5
"an efficient dipie algorithm for cad of electrostatically actuated mems devices pull-in parameters are important properties of electrostatic actuators. efficient and accurate analysis tools that can capture these parameters for different design geometries, are therefore essential. current simulation tools approach the pull-in state by iteratively adjusting the voltage applied across the actuator electrodes. the convergence rate of this scheme gradually deteriorates as the pull-in state is approached. moreover, the convergence is inconsistent and requires many mesh and accuracy refinements to assure reliable predictions. as a result, the design procedure of electrostatically actuated mems devices can be time-consuming. in this paper a novel displacement iteration pull-in extraction (dipie) scheme is presented. the dipie scheme is shown to converge consistently and far more rapidly than the voltage iterations (vi) scheme (>100 times faster!). the dipie scheme requires separate mechanical and electrostatic field solvers. therefore, it can be easily implemented in existing moems cad packages. moreover, using the dipie scheme, the pull-in parameters extraction can be performed in a fully automated mode, and no user input for search bounds is required ","An efficient DIPIE algorithm for CAD of electrostatically actuated MEMS devices Pullin parameters are important properties of electrostatic actuators. Efficient and accurate analysis tools that can capture these parameters for different design geometries, are therefore essential. Current simulation tools approach the pullin state by iteratively adjusting the voltage applied across the actuator electrodes. The convergence rate of this scheme gradually deteriorates as the pull-in state is approached. Moreover, the convergence is inconsistent and requires many mesh and accuracy refinements to assure reliable predictions. As a result, the design procedure of electrostatically actuated MEMS devices can be time-consuming. In this paper a novel Displacement Iteration Pullin Extraction (DIPIE) scheme is presented. The DIPIE scheme is shown to converge consistently and far more rapidly than the Voltage Iterations (VI) scheme (>100 times faster!). The DIPIE scheme requires separate mechanical and electrostatic field solvers. Therefore, it can be easily implemented in existing MOEMS CAD packages. Moreover, using the DIPIE scheme, the pullin parameters extraction can be performed in a fully automated mode, and no user input for search bounds is required","['DIPIE algorithm', 'MOEMS CAD packages', 'electrostatically actuated MEMS devices', 'pull-in parameters', 'electrostatic actuators', 'design geometries', 'convergence rate', 'displacement iteration pull-in extraction scheme', 'mechanical field solver', 'electrostatic field solver', 'computer-aided design', 'displacement iteration', 'convergence', 'electronic design automation', 'electrostatic actuators', 'iterative methods', 'mechanical engineering computing', 'micro-optics', 'micromechanical devices', 'relaxation theory']","['dixie scheme', 'pullin state', 'mess devices', 'pullin parameters extraction', 'efficient dixie algorithm', 'electrostatic actuators', 'accurate analysis tools', 'poems CAD packages', 'dixie', 'scheme']",1088,179,20,1084,178,10,8,4,4
"monoids all polygons over which are omega -stable: proof of the mustafin-poizat conjecture a monoid s is called an omega -stabilizer (superstabilizer, or stabilizer) if every s-polygon has an omega -stable (superstable, or stable) theory. it is proved that every omega -stabilizer is a regular monoid. this confirms the mustafin-poizat conjecture and allows us to end up the description of omega -stabilizers ","Monoids all polygons over which are omega -stable: proof of the Mustafin-Poizat conjecture A monoid S is called an omega -stabilizer (superstabllizer, or stabilizer) if every S-polygon has an omega -stable (superstable, or stable) theory. It is proved that every omega -stabilizer is a regular monoid. This confirms the Mustafin-Poizat conjecture and allows us to end up the description of omega -stabilizers","['monoids all polygons', 'omega -stabilizer', 'S-polygon', 'regular monoid', 'Mustafin-Poizat conjecture', 'computational geometry', 'formal logic']","['Mustafin-Poizat conjecture', 'omega stable proof', 'omega stabilizers', 'stable theory', 'omega', 'polygons', 'monod S', 'stable', 'stabilizer', 'Mustafin-Poizat']",347,63,7,347,62,10,1,1,2
"chaotic phenomena and fractional-order dynamics in the trajectory control of redundant manipulators redundant manipulators have some advantages when compared with classical arms because they allow the trajectory optimization, both on the free space and on the presence of obstacles, and the resolution of singularities. for this type of arms the proposed kinematic control algorithms adopt generalized inverse matrices but, in general, the corresponding trajectory planning schemes show important limitations. motivated by these problems this paper studies the chaos revealed by the pseudoinverse-based trajectory planning algorithms, using the theory of fractional calculus ","Chaotic phenomena and fractional-order dynamics in the trajectory control of redundant manipulators Redundant manipulators have some advantages when compared with classical arms because they allow the trajectory optimization, both on the free space and on the presence of obstacles, and the resolution of singularities. For this type of arms the proposed kinematic control algorithms adopt generalized inverse matrices but, in general, the corresponding trajectory planning schemes show important limitations. Motivated by these problems this paper studies the chaos revealed by the pseudoinverse-based trajectory planning algorithms, using the theory of fractional calculus","['chaotic phenomena', 'fractional-order dynamics', 'trajectory control', 'redundant manipulators', 'classical arms', 'trajectory optimization', 'kinematic control algorithms', 'generalized inverse matrices', 'trajectory planning schemes', 'fractional calculus', 'calculus', 'chaos', 'inverse problems', 'matrix algebra', 'nonlinear dynamical systems', 'optimisation', 'position control', 'redundant manipulators']","['trajectory', 'kinematic control algorithms', 'fractional-order dynamics', 'corresponding trajectory', 'trajectory optimization', 'trajectory control', 'Chaotic phenomena', 'Chaotic', 'redundant manipulators', 'Redundant manipulators']",584,92,18,584,91,10,0,0,5
"quantitative analysis of reconstructed 3-d coronary arterial tree and intracoronary devices traditional quantitative coronary angiography is performed on two-dimensional (2-d) projection views. these views are chosen by the angiographer to minimize vessel overlap and foreshortening. with 2-d projection views that are acquired in this nonstandardized fashion, however, there is no way to know or estimate how much error occurs in the qca process. furthermore, coronary arteries possess a curvilinear shape and undergo a cyclical deformation due to their attachment to the myocardium. therefore, it is necessary to obtain three-dimensional (3-d) information to best describe and quantify the dynamic curvilinear nature of the human coronary artery. using a patient-specific 3-d coronary reconstruction algorithm and routine angiographic images, a new technique is proposed to describe: (1) the curvilinear nature of 3-d coronary arteries and intracoronary devices; (2) the magnitude of the arterial deformation caused by intracoronary devices and due to heart motion; and (3) optimal view(s) with respect to the desired ""pathway"" for delivering intracoronary devices ","Quantitative analysis of reconstructed 3-D coronary arterial tree and intracoronary devices Traditional quantitative coronary angiography is performed on two-dimensional (2-D) projection views. These views are chosen by the angiographer to minimize vessel overlap and foreshortening. With 2-D projection views that are acquired in this nonstandardized fashion, however, there is no way to know or estimate how much error occurs in the QCA process. Furthermore, coronary arteries possess a curvilinear shape and undergo a cyclical deformation due to their attachment to the myocardium Therefore, it is necessary to obtain three-dimensional (3-D) information to best describe and quantify the dynamic curvilinear nature of the human coronary artery. Using a patient-specific 3-D coronary reconstruction algorithm and routine angiographic images, a new technique is proposed to describe: (1) the curvilinear nature of 3-D coronary arteries and intracoronary devices; (2) the magnitude of the arterial deformation caused by intracoronary devices and due to heart motion; and (3) optimal view(s) with respect to the desired ""pathway"" for delivering intracoronary devices","['medical diagnostic imaging', 'cyclical deformation', 'myocardium', 'dynamic curvilinear nature quantification', 'patient-specific 3-D coronary reconstruction algorithm', 'routine angiographic images', 'arterial deformation magnitude', 'intracoronary devices delivery pathway', 'human coronary artery', 'angiocardiography', 'image reconstruction', 'medical image processing', 'prosthetics']","['intracoronary devices', 'coronary', 'human coronary artery', 'd-d coronary arteries', 'Quantitative analysis', 'd-d projection views', 'arterial deformation', 'd-d', 'projection views', 'coronary arteries']",1003,165,13,1002,164,10,0,1,2
"the changing landscape for multi access portals discusses the factors that have made life difficult for consumer portal operators in recent years causing them, like others in the telecommunications, media and technology sector, to take a close look at their business models following the dot.com crash and the consequent reassessment of internet-related project financing by the venture capital community. while the pressure is on to generate income from existing customers and users, portal operators must reach new markets and find realistic revenue streams. this search for real revenues has led to a move towards charging for content, a strategy being pursued by a large number of horizontal portal players, including msn and terra lycos. this trend is particularly noticeable in china, where chinadotcom operates a mainland portal and plans a range of fee-based services, including electronic mail. the nature of advertising itself is changing, with portals seeking blue-chip sponsorship and marketing deals that span a number of years. players are struggling to redefine and reinvent themselves as a result of the changing environment and even the term ""portal"" is believed to be obsolete, partly due to its dot.com crash associations. multi-access portals are expected to dominate the consumer sector, becoming bigger and better overall than their predecessors and playing a more powerful role in the consumer environment ","The changing landscape for multi access portals Discusses the factors that have made life difficult for consumer portal operators in recent years causing them, like others in the telecommunications, media and technology sector, to take a close look at their business models following the dot.com crash and the consequent reassessment of Internet-related project financing by the venture capital community. While the pressure is on to generate income from existing customers and users, portal operators must reach new markets and find realistic revenue streams. This search for real revenues has led to a move towards charging for content, a strategy being pursued by a large number of horizontal portal players, including MSN and Terra Lycos. This trend is particularly noticeable in China, where Chinadotcom operates a mainiand portal and plans a range of fee-based services, including electronic mail. The nature of advertising itself is changing, with portals seeking blue-chip sponsorship and marketing deals that span a number of years. Players are struggling to redefine and reinvent themselves as a result of the changing environment and even the term ""portal"" is believed to be obsolete, partly due to its dot.com crash associations. Multi-access portals are expected to dominate the consumer sector, becoming bigger and better overall than their predecessors and playing a more powerful role in the consumer environment","['multi-access portals', 'consumer portal operators', 'revenue streams', 'fee-based services', 'advertising', 'blue-chip sponsorship', 'information industry', 'information resources', 'Internet', 'management of change', 'online front-ends', 'search engines']","['portal', 'portal operators', 'horizontal portal players', 'multi access portals', 'consumer environment', 'Multi-access portals', 'mainland portal', 'consumer sector', 'term portal', 'landscape']",1212,218,12,1212,217,10,1,1,5
"effects of white space in learning via the web this study measured the effect of specific white space features on learning from instructional web materials. the study also measured learners' beliefs regarding web-based instruction. prior research indicated that small changes in the handling of presentation elements can affect learning. achievement results from this study indicated that in on-line materials, when content and overall structure are sound, minor differences regarding table borders and vertical spacing in text do not hinder learning. beliefs regarding web-based instruction and instructors who use it did not differ significantly between treatment groups. implications of the study and cautions regarding generalizing from the results are discussed ","Effects of white space in learning via the Web This study measured the effect of specific white space features on learning from instructional Web materials. The study also measured learners’ beliefs regarding Web-based instruction. Prior research indicated that small changes in the handling of presentation elements can affect learning. Achievement results from this study indicated that in on-line materials, when content and overall structure are sound, minor differences regarding table borders and vertical spacing in text do not hinder learning. Beliefs regarding Web-based instruction and instructors who use it did not differ significantly between treatment groups. Implications of the study and cautions regarding generalizing from the results are discussed","['white space features', 'Web-based instruction', 'presentation', 'online educational materials', 'table borders', 'text vertical spacing', 'Internet', 'educational computing', 'human factors', 'information resources', 'Internet', 'user interfaces']","['study', 'pen-based instruction', 'white space', 'instructional Web materials', 'on-line materials', 'vertical spacing', 'instructions', 'Web', 'spacing', 'white']",658,110,12,658,109,10,1,1,2
"there is no optimal routing policy for the torus a routing policy is the method used to select a specific output channel for a message from among a number of acceptable output channels. an optimal routing policy is a policy that maximizes the probability of a message reaching its destination without delays. optimal routing policies have been proposed for several regular networks, including the mesh and the hypercube. an open problem in interconnection network research has been the identification of an optimal routing policy for the torus. in this paper, we show that there is no optimal routing policy for the torus. our result is demonstrated by presenting a detailed example in which the best choice of output channel is dependent on the probability of each channel being available. this result settles, in the negative, a conjecture by j. wu (1996) concerning an optimal routing policy for the torus ","There is no optimal routing policy for the torus A routing policy is the method used to select a specific output channel for a message from among a number of acceptable output channels. An optimal routing policy is a policy that maximizes the probability of a message reaching its destination without delays. Optimal routing policies have been proposed for several regular networks, including the mesh and the hypercube. An open problem in interconnection network research has been the identification of an optimal routing policy for the torus. In this Paper, we show that there is no optimal routing policy for the torus. Our result is demonstrated by presenting a detailed example in which the best choice of output channel is dependent on the probability of each channel being available. This result settles, in the negative, a conjecture by J. Wu (1996) concerning an optimal routing policy for the, torus","['optimal routing policy', 'torus', 'hypercube', 'hypercube networks', 'network routing']","['policy', 'optimal', 'interconnection network research', 'acceptable output channels', 'several regular networks', 'specific output channel', 'torus A', 'output channel', 'torus', 'channels']",760,150,5,761,149,10,0,1,1
"human face detection in visual scenes using neural networks this paper presents a neural network based face detection system. our objective is to design a system that can detect human faces in visual scenes at high searching speed and accuracy. we used a neural network with a simple structure but trained using face and non-face samples preprocessed by several methods (position normalization, histogram equalization, etc.) to attain high accuracy, then pruned the size of the neural network so that it could run faster and reduced the total search area of a target visual scene using the skin color detector. skin color detection assumes that faces reside only in skin color regions. the system design is made up of two parts: the face detecting system that detects the faces, and the searching speed improving system. speed improvement is achieved by reducing the face locator network size using the structural learning with knowledge and by reducing the face search area using the skin color detection system. faster training of the neural networks was also achieved using variable step sizes ","Human face detection in visual scenes using neural networks This paper presents a neural network based face detection system. Our objective is to design a system that can detect human faces in visual scenes at high searching speed and accuracy. We used a neural network with a simple structure but trained using face and non-face samples preprocessed by several methods (position normalization, histogram equalization, etc.) to attain high accuracy, then pruned the size of the neural network so that it could run faster and reduced the total search area of a target visual scene using the skin color detector. Skin color detection assumes that faces reside only in skin color regions. The system design is made up of two parts: the face detecting system that detects the faces, and the searching speed improving system. Speed improvement is achieved by reducing the face locator network size using the structural learning with knowledge and by reducing the face search area using the skin color detection system. Faster training of the neural networks was also achieved using variable step sizes","['backpropagation', 'self organizing maps', 'down-sampling', 'neural network', 'human face recognition', 'visual scene', 'skin color detector', 'face detecting system', 'merging overlapping detections', 'backpropagation', 'computer vision', 'face recognition', 'image colour analysis', 'neural nets']","['neural network', 'visual scenes', 'system Speed improvement', 'face detection system', 'Human face detection', 'skin color regions', 'face search area', 'system design', 'human faces', 'faces']",920,178,14,920,177,10,0,0,0
updating systems for monitoring and controlling power equipment on the basis of the firmware system sargon the economic difficulties experienced by the power industry of russia has considerably retarded the speed of commissioning new capacities and reconstructing equipment in service. the increasing deterioration of the equipment at power stations makes the problem of its updating very acute. the main efforts of organizations working in the power industry are now focused on updating all kinds of equipment installed at power installations. the necessary condition for the efficient operation of power equipment is to carry out serious modernization of systems for monitoring and control (smc) of technological processes. the specialists at zao nvt-avtomatika have developed efficient technology for updating the smc on the basis of the firmware system sargon which ensures the fast introduction of high-quality systems of automation with a minimal payback time of the capital outlay. this paper discusses the updating of equipment using sargon ,Updating systems for monitoring and controlling power equipment on the basis of the firmware system SARGON The economic difficulties experienced by the power industry of Russia has considerably retarded the speed of commissioning new capacities and reconstructing equipment in service. The increasing deterioration of the equipment at power stations makes the problem of its updating very acute. The main efforts of organizations working in the power industry are now focused on updating all kinds of equipment installed at power installations. The necessary condition for the efficient operation of power equipment is to carry out serious modernization of systems for monitoring and control (SMC) of technological processes. The specialists at ZAO NVT-Avtomatika have developed efficient technology for updating the SMC on the basis of the firmware system SARGON which ensures the fast introduction of high-quality systems of automation with a minimal payback time of the capital outlay. This paper discusses the updating of equipment using SARGON,"['power industry', 'Russia', 'SARGON firmware system', 'ZAO NVT-Avtomatika', 'monitoring systems', 'control systems', 'power equipment monitoring', 'power equipment control', 'computerised control', 'computerised monitoring', 'firmware', 'power apparatus', 'power engineering computing']","['firmware system jargon', 'power equipment', 'power industry', 'power', 'high-quality systems', 'efficient technology', 'power installations', 'power stations', 'systems', 'equipment']",894,156,13,894,155,10,0,0,2
"an adaptive sphere-fitting method for sequential tolerance control the machining of complex parts typically involves a logical and chronological sequence of n operations on m machine tools. because manufacturing datums cannot always match design constraints, some of the design specifications imposed on the part are usually satisfied by distinct subsets of the n operations prescribed in the process plan. conventional tolerance control specifies a fixed set point for each operation and a permissible variation about this set point to insure compliance with the specifications, whereas sequential tolerance control (stc) uses real-time measurement information at the completion of one stage to reposition the set point for subsequent operations. however, it has been shown that earlier sphere-fitting methods for stc can lead to inferior solutions when the process distributions are skewed. this paper introduces an extension of stc that uses an adaptive sphere-fitting method that significantly improves the yield in the presence of skewed distributions as well as significantly reducing the computational effort required by earlier probabilistic search methods ","An adaptive sphere-fitting method for sequential tolerance control The machining of complex parts typically involves a logical and chronological sequence of n operations on m machine tools. Because manufacturing datums cannot always match design constraints, some of the design specifications imposed on the part are usually satisfied by distinct subsets of the n operations prescribed in the process plan. Conventional tolerance control specifies a fixed set point for each operation and a permissible variation about this set point to insure compliance with the specifications, whereas sequential tolerance control (STC) uses real-time measurement information at the completion of one stage to reposition the set point for subsequent operations. However, it has been shown that earlier sphere-fitting methods for STC can lead to inferior solutions when the process distributions are skewed. This paper introduces an extension of STC that uses an adaptive sphere-fitting method that significantly improves the yield in the presence of skewed distributions as well as significantly reducing the computational effort required by earlier probabilistic search methods","['machine tools', 'sequential tolerance control', 'adaptive sphere-fitting method', 'design constraints', 'compliance', 'real-time measurement information', 'yield improvement', 'skewed distributions', 'computational effort', 'constraint theory', 'machine tools', 'optimisation', 'production control', 'tolerance analysis']","['adaptive sphere-fitting method', 'sequential tolerance control', 'n operations', 'earlier sphere-fitting methods', 'Conventional tolerance control', 'subsequent operations', 'm machine tools', 'fixed set point', 'methods', 'set point']",998,168,14,998,167,10,0,0,5
"a universal decomposition of the integration range for exponential functions the problem of determining the independent constants for decomposition of the integration range of exponential functions was solved on the basis of a similar approach to polynomials. the constants obtained enable one to decompose the integration range in two so that the integrals over them are equal independently of the function parameters. for the nontrigonometrical polynomials of even functions, an alternative approach was presented ","A universal decomposition of the integration range for exponential functions The problem of determining the independent constants for decomposition of the. integration range of exponential functions was solved on the basis of a similar approach to polynomials. The constants obtained enable one to decompose the integration range in two so that the integrals over them are equal independently of the function parameters. For the nontrigonometrical polynomials of even functions, an alternative approach was presented","['integration range universal decomposition', 'exponential functions', 'polynomials', 'integration range decomposition', 'nontrigonometrical polynomials', 'even functions', 'exponential distribution', 'integration']","['integration range', 'exponential functions', 'nontrigonometrical polynomials', 'universal decomposition', 'independent constants', 'function parameters', 'decomposition', 'functions', 'range', 'integration']",442,75,8,443,74,10,0,1,0
"product development: using a 3d computer model to optimize the stability of the rocket tm powered wheelchair a three-dimensional (3d) lumped-parameter model of a powered wheelchair was created to aid the development of the rocket prototype wheelchair and to help explore the effect of innovative design features on its stability. the model was developed using simulation software, specifically working model 3d. the accuracy of the model was determined by comparing both its static stability angles and dynamic behavior as it passed down a 4.8-cm (1.9"") road curb at a heading of 45 degrees with the performance of the actual wheelchair. the model's predictions of the static stability angles in the forward, rearward, and lateral directions were within 9.3, 7.1, and 3.8% of the measured values, respectively. the average absolute error in the predicted position of the wheelchair as it moved down the curb was 2.2 cm/m (0.9"" per 3'3"") traveled. the accuracy was limited by the inability to model soft bodies, the inherent difficulties in modeling a statically indeterminate system, and the computing time. nevertheless, it was found to be useful in investigating the effect of eight design alterations on the lateral stability of the wheelchair. stability was quantified by determining the static lateral stability angles and the maximum height of a road curb over which the wheelchair could successfully drive on a diagonal heading. the model predicted that the stability was more dependent on the configuration of the suspension system than on the dimensions and weight distribution of the wheelchair. furthermore, for the situations and design alterations studied, predicted improvements in static stability were not correlated with improvements in dynamic stability ","Product development: using a 3D computer model to optimize the stability of the. Rocket TM powered wheelchair A three-dimensional (3D) lumped-parameter model of a powered wheelchair was created to aid the development of the Rocket prototype wheelchair and to help explore the effect of innovative design features on its stability. The model was developed using simulation software, specifically Working Model 3D. The accuracy of the model was determined by comparing both its static stability angles and dynamic behavior as it passed down a 4.8-cm (1.9"") road curb at a heading of 45 degrees with the performance of the actual wheelchair. The model's predictions of the static stability angles in the forward, rearward, and lateral directions were within 9.3, 7.1, and 3.8% of the measured values, respectively. The average absolute error in the predicted position of the wheelchair as it moved down the curb was 2.2 cm/m (0.9"" per 3'3"") traveled. The accuracy was limited by the inability to model soft bodies, the inherent difficulties in modeling a statically indeterminate system, and the computing time. Nevertheless, it was found to be useful in investigating the effect of eight design alterations on the lateral stability of the wheelchair. Stability was quantified by determining the static lateral stability angles and the maximum height of a road curb over which the wheelchair could successfully drive on a diagonal heading. The model predicted that the stability was more dependent on the configuration of the suspension system than on the dimensions and weight distribution of the wheelchair. Furthermore, for the situations and design alterations studied, predicted improvements in static stability were not correlated with improvements in dynamic stability","['3D computer model', 'product development', 'innovative design features', 'suspension system configuration', 'dynamic stability improvements', 'average absolute error', 'predicted position', 'soft bodies modeling', 'statically indeterminate system', 'computing time', 'design alterations effect', 'diagonal heading', 'weight distribution', 'Rocket TM powered wheelchair', '4.8 cm', 'digital simulation', 'handicapped aids', 'mechanical stability', 'medical computing']","['static stability angles', 'lateral stability', 'Rocket prototype wheelchair', 'lumped-parameter model', 'wheelchair Stability', 'models predictions', 'dynamic stability', 'actual wheelchair', 'd computer model', 'static stability']",1500,273,19,1501,272,10,0,1,8
"using latent semantic analysis to assess reader strategies we tested a computer-based procedure for assessing reader strategies that was based on verbal protocols that utilized latent semantic analysis (lsa). students were given self-explanation-reading training (sert), which teaches strategies that facilitate self-explanation during reading, such as elaboration based on world knowledge and bridging between text sentences. during a computerized version of sert practice, students read texts and typed self-explanations into a computer after each sentence. the use of sert strategies during this practice was assessed by determining the extent to which students used the information in the current sentence versus the prior text or world knowledge in their self-explanations. this assessment was made on the basis of human judgments and lsa. both human judgments and lsa were remarkably similar and indicated that students who were not complying with sert tended to paraphrase the text sentences, whereas students who were compliant with sert tended to explain the sentences in terms of what they knew about the world and of information provided in the prior text context. the similarity between human judgments and lsa indicates that lsa will be useful in accounting for reading strategies in a web-based version of sert ","Using latent semantic analysis to assess reader strategies We tested a computer-based procedure for assessing reader strategies that was based on verbal protocols that utilized latent semantic analysis (LSA). Students were given self-explanation-reading training (SERT), which teaches strategies that facilitate self-explanation during reading, such as elaboration based on world knowledge and bridging between text sentences. During a computerized version of SERT practice, students read texts and typed self-explanations into a computer after each sentence. The use of SERT strategies during this practice was assessed by determining the extent to which students used the information in the current sentence versus the prior text or world knowledge in their self-explanations. This assessment was made on the basis of human judgments and LSA. Both human judgments and LSA were remarkably similar and indicated that students who were not complying with SERT tended to paraphrase the text sentences, whereas students who were compliant with SERT tended to explain the sentences in terms of what they knew about the world and of information provided in the prior text context. The similarity between human judgments and LSA indicates that LSA will be useful in accounting for reading strategies in a Web-based version of SERT","['latent semantic analysis', 'reader strategy assessment', 'computer-based procedure', 'verbal protocols', 'self-explanation-reading training', 'elaboration', 'world knowledge', 'text sentence bridging', 'human judgments', 'computer aided instruction', 'psychology']","['latent semantic analysis', 'reader strategies', 'text sentences', 'set practice students', 'verbal protocols', 'set strategies', 'strategies', 'latent', 'semantic', 'analysis']",1127,199,11,1127,198,10,0,0,5
"even unimodular gaussian lattices of rank 12 we classify even unimodular gaussian lattices of rank 12, that is, even unimodular integral lattices of rank 12 over the ring of gaussian integers. this is equivalent to the classification of the automorphisms tau with tau /sup 2/=-1 in the automorphism groups of all the niemeier lattices, which are even unimodular (real) integral lattices of rank 24. there are 28 even unimodular gaussian lattices of rank 12 up to equivalence ","Even unimodular Gaussian lattices of rank 12 We classify even unimodular Gaussian lattices of rank 12, that is, even unimodular integral lattices of rank 12 over the ring of Gaussian integers. This is equivalent to the classification of the automorphisms, tau with tau /sup 2/=-1 in the automorphism groups of all the Niemeier lattices, which are even unimodular (real) integral lattices of rank 24. There are 28 even unimodular Gaussian lattices of rank 12 up to equivalence","['even unimodular Gaussian lattices', 'even unimodular integral lattices', 'Gaussian integers', 'automorphisms', 'Niemeier lattices', 'equivalence classes', 'number theory']","['unimodular Gaussian lattices', 'lattices', 'unimodular integral lattices', 'automorphism groups', 'Niemeier lattices', 'Gaussian integers', 'rank u2', 'unimodular', 'Gaussian', 'rank']",398,78,7,399,77,10,0,1,1
"attribute generation based on association rules a decision tree is considered to be appropriate (1) if the tree can classify the unseen data accurately, and (2) if the size of the tree is small. one of the approaches to induce such a good decision tree is to add new attributes and their values to enhance the expressiveness of the training data at the data pre-processing stage. there are many existing methods for attribute extraction and construction, but constructing new attributes is still an art. these methods are very time consuming, and some of them need a priori knowledge of the data domain. they are not suitable for data mining dealing with large volumes of data. we propose a novel approach that the knowledge on attributes relevant to the class is extracted as association rules from the training data. the new attributes and the values are generated from the association rules among the originally given attributes. we elaborate on the method and investigate its feature. the effectiveness of our approach is demonstrated through some experiments ","Attribute generation based on association rules A decision tree is considered to be appropriate (1) if the tree can classify the unseen data accurately, and (2) if the size of the tree is small One of the approaches to induce such a good decision tree is to add new attributes and their values to enhance the expressiveness of the training data at the data pre-processing stage. There are many existing methods for attribute extraction and construction, but constructing new attributes is still an art. These methods are very time consuming, and some of them need a priori knowledge of the data domain. They are not suitable for data mining dealing with large volumes of data. We propose a novel approach that the knowledge on attributes relevant to the class is extracted as association rules from the training data. The new attributes and the values are generated from the association rules among the originally given attributes. We elaborate on the method and investigate its feature. The effectiveness of our approach is demonstrated through some experiments","['attribute generation', 'association rules', 'decision tree', 'training data', 'attribute extraction', 'data mining', 'large database', 'experiments', 'data mining', 'decision trees', 'learning (artificial intelligence)', 'very large databases']","['association rules', 'new attributes', 'data', 'training data', 'attribute extraction', 'good decision tree', 'unseen data', 'data mining', 'data domain', 'decision tree']",890,175,12,889,174,10,0,1,1
"os porting and application development for soc to deliver improved usability in high-end portable consumer products, the use of an appropriate consumer operating system (os) is becoming far more widespread. using a commercially supported os also vastly increases the availability of supported applications. for the device developer, this trend adds major complexity to the problem of system implementation. porting a complete operating system to a new hardware design adds significantly to the development burden, increasing both time-to-market and expense. even for those familiar with the integration of a real-time os, the porting, validation and support of a complex platform os is a formidable task ","OS porting and application development for SoC To deliver improved usability in high-end portable consumer products, the use of an appropriate consumer operating system (OS) is becoming far more widespread. Using a commercially supported OS also vastly increases the availability of supported applications. For the device developer, this trend adds major complexity to the problem of system implementation. Porting a complete operating system to a new hardware design adds significantly to the development burden, increasing both time-to-market and expense. Even for those familiar with the integration of a realtime OS, the porting, validation and support of a complex platform OS is a formidable task","['OS porting', 'application development', 'consumer operating system', 'hardware design', 'operating systems (computers)', 'software portability']","['complete operating system', 'application development', 'system implementation', 'complex platform OS', 'development burden', 'device developers', 'applications', 'OS porting', 'OS', 'developers']",600,105,6,599,104,10,4,1,1
"the xml typechecking problem when an xml document conforms to a given type (e.g. a dtd or an xml schema type) it is called a valid document. checking if a given xml document is valid is called the validation problem, and is typically performed by a parser (hence, validating parser), more precisely it is performed right after parsing, by the same program module. in practice however, xml documents are often generated dynamically, by some program: checking whether all xml documents generated by the program are valid wrt a given type is called the typechecking problem. while a validation analyzes an xml document, a type checker analyzes a program, and the problem's difficulty is a function of the language in which that program is expressed. the xml typechecking problem has been investigated recently and the xquery working group adopted some of these techniques for typechecking xquery. all these techniques, however, have limitations which need to be understood and further explored and investigated. we define the xml typechecking problem, and present current approaches to typechecking, discussing their limitations ","The XML typechecking problem When an XML document conforms to a given type (e.g. a DTD or an XML schema type) it is called a valid document. Checking if a given XML document is valid is called the validation problem, and is typically performed by a parser (hence, validating parser), more precisely it is performed right after parsing, by the same program module. In practice however, XML documents are often generated dynamically, by some program: checking whether all XML documents generated by the program are valid WRT a given type is called the typechecking problem. While a validation analyzes an XML document, a type checker analyzes a program, and the problem's difficulty is a function of the language in which that program is expressed. The XML typechecking problem has been investigated recently and the XQuery Working Group adopted some of these techniques for typechecking XQuery. All these techniques, however, have limitations which need to be understood and further explored and investigated. We define the XML typechecking problem, and present current approaches to typechecking, discussing their limitations.","['XML typechecking problem', 'XML document', 'validation', 'XQuery', 'hypermedia markup languages']","['ml document', 'ml typechecking problem', 'valid document Checking', 'type checker analyses', 'validation analyses', 'problems difficulty', 'validation problem', 'ml schema type', 'ml', 'typechecking problem']",950,177,5,951,176,10,0,1,1
control of combustion processes in an internal combustion engine by low-temperature plasma a new method of operation of internal combustion engines enhances power and reduces fuel consumption and exhaust toxicity. low-temperature plasma control combines working processes of thermal engines and steam machines into a single process ,Control of combustion processes in an internal combustion engine by low-temperature plasma ‘Anew method of operation of internal combustion engines enhances power and reduces fuel consumption and exhaust toxicity. Low-temperature plasma control combines working processes of thermal engines and steam machines into a single process,"['combustion processes', 'internal combustion engine', 'low-temperature plasma', 'fuel consumption', 'exhaust toxicity', 'working processes', 'thermal engines', 'steam machines', 'heat transfer', 'internal combustion engines']","['Low-temperature plasma control', 'internal combustion engines', 'combustion processes', 'thermal engines', 'single process', 'combustion', 'process', 'engine', 'internal', 'low-temperature plasma']",286,47,10,287,45,10,127,33,3
"too much middleware the movement from client-server computing to multi-tier computing has created a potpourri of so-called middleware systems, including application servers, workflow products, eai systems, etl systems and federated data systems. we argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. the world would be well served by considerable consolidation, and we present some of the ways this might happen. some of the points covered in the article have been previously explored by p. bernstein (1996) ","Too much middleware ‘The movement from client-server computing to multi-tier computing has created a potpourri of so-called middleware systems, including application servers, workflow products, EAI systems, ETL systems and federated data systems. We argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. The world would be well served by considerable consolidation, and we present some of the ways this might happen. Some of the points covered in the article have been previously explored by P. Bernstein (1996)","['client-server computing', 'multi-tier computing', 'middleware systems', 'application servers', 'workflow products', 'EAI systems', 'ETL systems', 'federated data systems', 'poorly integrated systems', 'overlapping functionality', 'application program interfaces', 'client-server systems', 'distributed databases', 'distributed object management', 'workflow management software']","['so-called middleware systems', 'client-server computing', 'federated data systems', 'multiplier computing', 'systems', 'much middleware', 'middleware', 'computing', 'much', 'client-server']",493,86,15,494,85,10,3,1,3
the year of the racehorse [china telecom] does china really offer the telecoms industry a route out of the telecoms slump? according to the chinese government it has yet to receive a single application from foreign companies looking to invest in the country's domestic telecoms sector since the country joined the world trade organisation ,The year of the racehorse [China Telecom] Does China really offer the telecoms industry a route out of the telecoms slump? According to the Chinese government it has yet to receive a single application from foreign companies looking to invest in the country's domestic telecoms sector since the country joined the World Trade Organisation,"['China', 'telecoms industry', 'foreign investment', 'China Telecom', 'China Netcom', 'China Unicorn', 'telecommunication']","['domestic telecoms sector', 'racehorse china telecom', 'single application', 'Chinese government', 'telecoms industry', 'foreign companies', 'year', 'telecoms', 'china', 'racehorse']",285,55,7,285,54,10,0,0,0
"stem: secure telephony enabled middlebox dynamic applications, including ip telephony, have not seen wide acceptance within enterprises because of problems caused by the existing network infrastructure. static elements, including firewalls and network address translation devices, are not capable of allowing dynamic applications to operate properly. the secure telephony enabled middlebox (stem) architecture is an enhancement of the existing network design to remove the issues surrounding static devices. the architecture incorporates an improved firewall that can interpret and utilize information in the application layer of packets to ensure proper functionality. in addition to allowing dynamic applications to function normally, the stem architecture also incorporates several detection and response mechanisms for well-known network-based vulnerabilities. this article describes the key components of the architecture with respect to the sip protocol ","STEM: Secure Telephony Enabled Middlebox Dynamic applications, including IP telephony, have not seen wide acceptance within enterprises because of problems caused by the existing network infrastructure. Static elements, including firewalls and network address translation devices, are not capable of allowing dynamic applications to operate properly. The Secure Telephony Enabled Middlebox (STEM) architecture is an enhancement of the existing network design to remove the issues surrounding static devices. The architecture incorporates an improved firewall that can interpret and utilize information in the application layer of packets to ensure proper functionality. In addition to allowing dynamic applications to function normally, the STEM architecture also incorporates several detection and response mechanisms for well-known network-based vulnerabilities. This article describes the key components of the architecture with respect to the SIP protocol","['Secure Telephony Enabled Middlebox', 'STEM', 'IP telephony', 'network infrastructure', 'firewalls', 'network address translation devices', 'dynamic applications', 'STEM architecture', 'network design', 'static devices', 'application layer', 'detection mechanisms', 'response mechanisms', 'network-based vulnerabilities', 'SIP protocol', 'authorisation', 'Internet telephony', 'protocols', 'telecommunication security']","['middlesex Dynamic applications', 'middlesex stem architecture', 'network infrastructure', 'stem Secure Telephony', 'application layer', 'stem', 'applications', 'Secure Telephony', 'STEM architecture', 'dynamic applications']",833,128,19,833,127,10,0,0,5
"optical recognition of three-dimensional objects with scale invariance using a classical convergent correlator we present a real-time method for recognizing three-dimensional (3-d) objects with scale invariance. the 3-d information of the objects is codified in deformed fringe patterns using the fourier transform profilometry technique and is correlated using a classical convergent correlator. the scale invariance property is achieved using two different approaches: the mellin radial harmonic decomposition and the logarithmic radial harmonic filter. thus, the method is invariant for changes in the scale of the 3-d target within a defined interval of scale factors. experimental results show the utility of the proposed method ","Optical recognition of three-dimensional objects with scale invariance using a classical convergent correlator We present a real-time method for recognizing three-dimensional (3-D) objects with scale invariance. The 3-D information of the objects is codified in deformed fringe patterns using the Fourier transform profilometry technique and is correlated using a classical convergent correlator. The scale invariance property is achieved using two different approaches: the Mellin radial harmonic decomposition and the logarithmic radial harmonic filter. Thus, the method is invariant for changes in the scale of the 3-D target within a defined interval of scale factors. Experimental results show the utility of the proposed method","['optical recognition', '3D object recognition', 'scale invariance', 'classical convergent correlator', 'real-time method', '3-D information', 'deformed fringe patterns', 'Fourier transform profilometry technique', 'scale invariance property', 'Mellin radial harmonic decomposition', 'logarithmic radial harmonic filter', 'invariant', 'scale factors', 'CCD image sensors', 'Fourier transform optics', 'invariance', 'object recognition', 'optical correlation', 'spatial filters', 'spatial light modulators', 'surface topography measurement']","['classical convergent correlator', 'three-dimensional objects', 'scale invariance property', 'Optical recognition', 'real-time method', 'scale', 'scale factors', 'invariance', 'objects', 'scale invariance']",631,104,21,631,103,10,0,0,6
"in medias res [dvd formats] four years in the making, the dvd format war rages on, no winner insight. meanwhile, the spoils of war abound, and dvd media manufacturers stand poised to profit ","In medias res [DVD formats] Four years in the making, the DVD format war rages on, no winner insight. meanwhile, the spoils of war abound, and DVD media manufacturers stand poised to profit","['DVD media manufacturers', 'DVD format war', 'DVD-RAM', 'DVD+RW', 'DVD+R', 'DVD-RW', 'DVD-R', 'compatibility', 'writable DVD', 'optical disc storage', 'video discs']","['media manufacturers', 'winner insight', 'format war', 'media res', 'formats', 'spoils', 'years', 'media', 'war', 'insight']",157,34,11,157,33,10,0,0,0
"binocular model for figure-ground segmentation in translucent and occluding images a fourier-based solution to the problem of figure-ground segmentation in short baseline binocular image pairs is presented. each image is modeled as an additive composite of two component images that exhibit a spatial shift due to the binocular parallax. the segmentation is accomplished by decoupling each fourier component in one of the resultant additive images into its two constituent phasors, allocating each to its appropriate object-specific spectrum, and then reconstructing the foreground and background using the inverse fourier transform. it is shown that the foreground and background shifts can be computed from the differences of the magnitudes and phases of the fourier transform of the binocular image pair. while the model is based on translucent objects, it also works with occluding objects ","Binocular model for figure-ground segmentation in translucent and occluding images A Fourier-based solution to the problem of figure-ground segmentation in short baseline binocular image pairs is presented. Each image is modeled as an additive composite of two component images that exhibit a spatial shift due to the binocular parallax. The segmentation is accomplished by decoupling each Fourier component in one of the resultant additive images into its two constituent phasors, allocating each to its appropriate object-specific spectrum, and then reconstructing the foreground and background using the inverse Fourier transform. It is, shown that the foreground and background shifts can be computed from the differences of the magnitudes and phases of the Fourier transform of the binocular image pair. While the model is based on translucent objects, it also works with occluding objects","['binocular model', 'figure-ground segmentation', 'translucent images', 'occluding images', 'images', 'image segmentation', 'Fourier-based solution', 'short baseline binocular image pairs', 'component images', 'spatial shift', 'binocular parallax', 'Fourier component decoupling', 'phasors', 'object-specific spectrum', 'foreground', 'background', 'inverse Fourier transform', 'binocular image pair', 'translucent objects', 'occluding objects', 'Fourier transform optics', 'image segmentation', 'inverse problems']","['figure-ground segmentation', 'binocular image pair', 'image', 'resultant additive images', 'translucent objects', 'binocular parallax', 'Fourier component', 'component images', 'Binocular model', 'segmentation']",762,133,23,763,132,10,0,1,6
"processing of complexly shaped multiply connected domains in finite element mesh generation large number of finite element models in modern materials science and engineering is defined on complexly shaped domains, quite often multiply connected. generation of quality finite element meshes on such domains, especially in cases when the mesh must be 100% quadrilateral, is highly problematic. this paper describes mathematical fundamentals and practical -implementation of a powerful method and algorithm allowing transformation of multiply connected domains of arbitrary geometrical complexity into a set of simple domains; the latter can then be processed by broadly available finite element mesh generators. the developed method was applied to a number of complex geometries, including those arising in analysis of parasitic inductances and capacitances in printed circuit boards. the quality of practical results produced by the method and its programming implementation provide evidence that the algorithm can be applied to other finite element models with various physical backgrounds ","Processing of complexly shaped multiply connected domains in finite element mesh generation Large number of finite element models in modern materials science and engineering is defined on complexly shaped domains, quite often multiply connected. Generation of quality finite element meshes on such domains, especially in cases when the mesh must be 100% quadrilateral, is highly problematic. This paper describes mathematical fundamentals and practical -implementation of a powerful method and algorithm, allowing transformation of multiply connected domains of arbitrary geometrical complexity into a set of simple domains; the latter can then be processed by broadly available finite element mesh generators. The developed method was applied to a number of complex geometries, including those arising in analysis of parasitic inductances and capacitances in printed circuit boards. The quality of practical results produced by the method and its programming implementation provide evidence that the algorithm can be applied to other finite element models with various physical backgrounds,","['finite element mesh generation', 'complexly shaped multiply connected domains', 'finite element models', 'quadrilateral mesh', 'domains transformation', 'set of simple domains', 'parasitic inductances', 'parasitic capacitances', 'printed circuit boards', 'programming implementation', 'arbitrary geometrical complexity', 'metal forming processes', 'structural engineering models', 'iterative basis', 'general domain subdivision algorithm', 'artificial cut', 'automatic step calculation', 'computational complexity', 'iterative methods', 'mesh generation', 'modelling']","['finite element models', 'connected domains', 'domains', 'connected Generation', 'simple domains', 'shaped domains', 'such domains', 'finite', 'element', 'finite element']",936,155,21,938,154,10,0,2,4
"optical encoding of color three-dimensional correlation three-dimensional (3d) correlation of color images, considering the color distribution as the third dimension, has been shown to be useful for color pattern recognition tasks. nevertheless, 3d correlation cannot be directly performed on an optical correlator, that can only process two-dimensional (2d) signals. we propose a method to encode 3d functions onto 2d ones in such a way that the fourier transform and correlation of these signals, that can be optically performed, encode the 3d fourier transform and correlation of the 3d signals. the theory for the encoding is given and experimental results obtained in an optical correlator are shown ","Optical encoding of color three-dimensional correlation Three-dimensional (3D) correlation of color images, considering the color distribution as the third dimension, has been shown to be useful for color pattern recognition tasks. Nevertheless, 3D correlation cannot be directly performed on an optical correlator, that can only process ‘two-dimensional (2D) signals. We propose a method to encode 3D functions onto 2D ones in such a way that the Fourier transform and correlation of these signals, that can be optically performed, encode the 3D Fourier transform and correlation of the 3D signals. The theory for the encoding is given and experimental results obtained in an optical correlator are shown","['optical encoding', 'color three-dimensional correlation', '3D correlation', 'color images', 'color distribution', 'color pattern recognition tasks', 'optical correlator', '3D function encoding', 'Fourier transform', '3D Fourier transform', 'encoding', 'Fourier transform optics', 'image coding', 'image colour analysis', 'image recognition', 'optical correlation']","['correlation', 'three-dimensional correlation', 'd correlation cannon', 'optical correlator', 'color distribution', 'Optical encoding', 'color images', 'd signals', 'd Fourier', 'color']",599,107,16,600,106,10,15,1,0
current-mode fully-programmable piece-wise-linear block for neuro-fuzzy applications a new method to implement an arbitrary piece-wise-linear characteristic in current mode is presented. each of the breaking points and each slope is separately controllable. as an example a block that implements an n-shaped piece-wise-linearity has been designed. the n-shaped block operates in the subthreshold region and uses only ten transistors. these characteristics make it especially suitable for large arrays of neuro-fuzzy systems where the number of transistors and power consumption per cell is an important concern. a prototype of this block has been fabricated in a 0.35 mu m cmos technology. the functionality and programmability of this circuit has been verified through experimental results ,Current-mode fully-programmable piece-wise-linear block for neuro-fuzzy applications Anew method to implement an arbitrary piece-wise-linear characteristic in current mode is presented. Each of the breaking points and each slope is separately controllable. As an example a block that implements an N-shaped piece-wise-linearity has been designed. The N-shaped block operates in the subthreshold region and uses only ten transistors. These characteristics make it especially suitable for large arrays of neuro-fuzzy systems where the number of transistors and power consumption per cell is an important concern. A prototype of this block has been fabricated in a 0.35 mu m CMOS technology. The functionality and programmability of this circuit has been verified through experimental results,"['arbitrary piece-wise-linear characteristic', 'current mode', 'breaking points', 'separately controllable', 'N-shaped piece-wise-linearity', 'VLSI', 'subthreshold region', 'neuro-fuzzy systems', 'power consumption', 'CMOS', '0.35 micron', 'CMOS analogue integrated circuits', 'current-mode circuits', 'fuzzy neural nets', 'low-power electronics', 'neural chips', 'piecewise linear techniques', 'VLSI']","['l-shaped piece-wise-linearity', 'neuro-fuzzy applications', 'neuro-fuzzy systems', 'l-shaped block', 'current mode', 'Current-mode', 'Anew method', 'neuro-fuzzy', 'block', 'piece-wise-linear']",679,113,18,679,111,10,326,104,5
"on deciding stability of constrained homogeneous random walks and queueing systems we investigate stability of scheduling policies in queueing systems. to this day no algorithmic characterization exists for checking stability of a given policy in a given queueing system. in this paper we introduce a certain generalized priority policy and prove that the stability of this policy is algorithmically undecidable. we also prove that stability of a homogeneous random walk in l/sub +//sup d/ is undecidable. finally, we show that the problem of computing a fluid limit of a queueing system or of a constrained homogeneous random walk is undecidable. to the best of our knowledge these are the first undecidability results in the area of stability of queueing systems and random walks in l/sub +//sup d/. we conjecture that stability of common policies like first-in-first-out and priority policy is also an undecidable problem ","On deciding stability of constrained homogeneous random walks and queueing systems We investigate stability of scheduling policies in queueing systems. To this, day no algorithmic characterization exists for checking stability of a given policy in a given queueing system. In this paper we introduce a certain generalized priority policy and prove that the stability of this policy is algorithmically undecidable. We also prove that stability of a homogeneous random walk in Lisub #/sup d/ is. undecidable. Finally, we show that the problem of computing a fluid limit of a queueing system or of a constrained homogeneous random walk is undecidable. To the best of our knowledge these are the first, undecidability results in the area of stability of queueing systems and random walks in L/sub +//sup d/. We conjecture that stability of common Policies like First-In-First-Out and priority policy is also an undecidable problem","['queueing systems', 'scheduling policy stability', 'constrained homogeneous random walks', 'generalized priority policy', 'homogeneous random walk stability', 'fluid limit computation', 'undecidability results', 'first-in-first-out policy', 'priority policy', 'undecidable problem', 'decidability', 'queueing theory', 'scheduling']","['stability', 'queueing system', 'homogeneous random walk', 'systems', 'priority policy', 'first undecidability', 'scheduling policies', 'list sup d', 'policies', 'random']",781,145,13,783,144,10,5,5,2
"acquisitions in the james ford bell library this article presents basic acquisitions philosophy and approaches in a noted special collection, with commentary on ""just saying no"" and on how the electronic revolution has changed the acquisition of special collections materials ","Acquisitions in the James Ford Bell Library This article presents basic acquisitions philosophy and approaches in a noted special collection, with commentary on ""just saying no"" and on how the electronic revolution has changed the acquisition of special collections materials","['James Ford Bell Library', 'library acquisitions philosophy', 'out-of-print books', 'University library', 'special collections', 'electronic revolution', 'academic libraries', 'library automation']","['special collections materials', 'basic acquisitions philosophy', 'article', 'Library', 'James', 'Ford', 'Bell', 'acquisitions', 'Acquisitions', 'special collection']",236,41,8,236,40,10,0,0,1
"application of ultrasonic sensors in the process industry continuous process monitoring in gaseous, liquid or molten media is a fundamental requirement for process control. besides temperature and pressure other process parameters such as level, flow, concentration and conversion are of special interest. more qualified information obtained from new or better sensors can significantly enhance the process quality and thereby product properties. ultrasonic sensors or sensor systems can contribute to this development. the state of the art of ultrasonic sensors and their advantages and disadvantages will be discussed. commercial examples will be presented. among others, applications in the food, chemical and pharmaceutical industries are described. possibilities and limitations of ultrasonic process sensors are discussed ","Application of ultrasonic sensors in the process industry Continuous process monitoring in gaseous, liquid or molten media is a fundamental requirement for process control. Besides temperature and pressure other process parameters such as level, flow, concentration and conversion are of special interest. More qualified information obtained from new or better sensors can significantly enhance the process quality and thereby product properties. Ultrasonic sensors or sensor systems can contribute to this development. The state of the art of ultrasonic sensors and their advantages and disadvantages will be discussed. Commercial examples will be presented. Among others, applications in the food, chemical and pharmaceutical industries are described. Possibilities and limitations of ultrasonic process sensors are discussed","['ultrasonic sensors application', 'process industry', 'continuous process monitoring', 'process control', 'process quality', 'food industries', 'chemical industries', 'pharmaceutical industries', 'acoustic microsensors', 'ultrasonic measurements', 'ultrasonic attenuation', 'acoustic impedance', 'temperature measurement', 'pressure measurement', 'level measurement', 'distance measurement', 'flow measurement', 'chemical industry', 'distance measurement', 'flow measurement', 'flowmeters', 'food processing industry', 'level measurement', 'microsensors', 'pharmaceutical industry', 'pressure measurement', 'process control', 'process monitoring', 'reviews', 'temperature measurement', 'ultrasonic applications', 'ultrasonic measurement']","['Continuous process monitoring', 'ultrasonic process sensors', 'other process parameters', 'process industry', 'process quality', 'process control', 'sensor systems', 'better sensors', 'ultrasonic sensors', 'Ultrasonic sensors']",715,114,32,715,113,10,0,0,1
resonant controllers for smart structures in this paper we propose a special type of colocated feedback controller for smart structures. the controller is a parallel combination of high-q resonant circuits. each of the resonant circuits is tuned to a pole (or the resonant frequency) of the smart structure. it is proven that the parallel combination of resonant controllers is stable with an infinite gain margin. only one set of actuator-sensor can damp multiple resonant modes with the resonant controllers. experimental results are presented to show the robustness of the proposed controller in damping multimode resonances ,Resonant controllers for smart structures In this paper we propose a special type of colocated feedback controller for smart structures. The controller is a parallel combination of high-Q resonant circuits. Each of the resonant circuits is tuned to a pole (or the resonant frequency) of the smart structure. It is proven that the parallel combination of resonant controllers is stable with an infinite gain margin. Only one set of actuator-sensor can damp multiple resonant modes with the resonant controllers. Experimental results are presented to show the robustness of the proposed controller in damping multimode resonances,"['feedback controller', 'smart structures', 'high-Q resonant circuits', 'resonant frequency', 'smart structure', 'actuator-sensor', 'multiple resonant modes', 'damping', 'multimode resonances', 'laminate beam', 'circuit feedback', 'closed loop systems', 'control system synthesis', 'controllers', 'damping', 'intelligent actuators', 'intelligent control', 'intelligent structures', 'piezoelectric actuators', 'resonators', 'stability']","['resonant circuits', 'smart structures', 'located feedback controller', 'multiple resonant modes', 'multimode resonances', 'proposed controller', 'resonant frequency', 'controllers', 'resonant controllers', 'Resonant controllers']",533,96,21,533,95,10,0,0,3
a stochastic averaging approach for feedback control design of nonlinear systems under random excitations this paper presents a method for designing and quantifying the performance of feedback stochastic controls for nonlinear systems. the design makes use of the method of stochastic averaging to reduce the dimension of the state space and to derive the ito stochastic differential equation for the response amplitude process. the moment equation of the amplitude process closed by the rayleigh approximation is used as a means to characterize the transient performance of the feedback control. the steady state and transient response of the amplitude process are used as the design criteria for choosing the feedback control gains. numerical examples are studied to demonstrate the performance of the control ,A stochastic averaging approach for feedback control design of nonlinear systems under random excitations This paper presents a method for designing and quantifying the performance of feedback stochastic controls for nonlinear systems. The design makes use of the method of stochastic averaging to reduce the dimension of the state space and to derive the Ito stochastic differential equation for the response amplitude process. The moment equation of the amplitude process closed by the Rayleigh approximation is used as a means to characterize the transient performance of the feedback control. The steady state and transient response of the amplitude process are used as the design criteria for choosing the feedback control gains. Numerical examples are studied to demonstrate the performance of the control,"['stochastic averaging', 'feedback control', 'nonlinear systems', 'steady state', 'transient response', 'random excitations', 'feedback stochastic controls', 'Ito stochastic differential equation', 'Rayleigh approximation', 'nonlinear control systems', 'state feedback', 'stochastic systems']","['nonlinear systems', 'control', 'stochastic', 'stochastic differential equation', 'stochastic averaging approach', 'response amplitude process', 'feedback control design', 'feedback control gains', 'stochastic controls', 'feedback control']",690,123,12,690,122,10,0,0,3
supersampling multiframe blind deconvolution resolution enhancement of adaptive optics compensated imagery of low earth orbit satellites we describe a postprocessing methodology for reconstructing undersampled image sequences with randomly varying blur that can provide image enhancement beyond the sampling resolution of the sensor. this method is demonstrated on simulated imagery and on adaptive-optics-(ao)-compensated imagery taken by the starfire optical range 3.5-m telescope that has been artificially undersampled. also shown are the results of multiframe blind deconvolution of some of the highest quality optical imagery of low earth orbit satellites collected with a ground-based telescope to date. the algorithm used is a generalization of multiframe blind deconvolution techniques that include a representation of spatial sampling by the focal plane array elements based on a forward stochastic model. this generalization enables the random shifts and shape of the ao-compensated point spread function (psf) to be used to partially eliminate the aliasing effects associated with sub-nyquist sampling of the image by the focal plane array. the method could be used to reduce resolution loss that occurs when imaging in wide-field-of-view (fov) modes ,Supersampling muttiframe blind deconvolution resolution enhancement of adaptive optics compensated imagery of low earth orbit satellites We describe a postprocessing methodology for reconstructing undersampled image sequences with randomly varying blur that can provide image enhancement beyond the sampling resolution of the sensor. This method is demonstrated on simulated imagery and on adaptive-optics-(AO)-compensated imagery taken by the Starfire Optical Range 3.5-m telescope that has been artificially undersampled. Also shown are the results of multiframe blind deconvolution of some of the highest quality optical imagery of low earth orbit satellites collected with a ground-based telescope to date. The algorithm used is a generalization of multiframe blind deconvolution techniques that include a representation of spatial sampling by the focal plane array elements based on a forward stochastic model. This generalization enables the random shifts and shape of the AO-compensated point spread function (PSF) to be used to partially eliminate the aliasing effects associated with sub-Nyquist sampling of the image by the focal plane array. The method could be used to reduce resolution loss that occurs when imaging in wide-field-of-view (FOV) modes,"['supersampling multiframe blind deconvolution resolution enhancement', 'adaptive optics compensated imagery', 'low earth orbit satellites', 'postprocessing methodology', 'undersampled image sequence reconstruction', 'randomly varying blur', 'image enhancement', 'sensor sampling resolution', 'simulated imagery', 'Starfire Optical Range telescope', 'multiframe blind deconvolution', 'ground-based telescope', 'spatial sampling', 'focal plane array elements', 'forward stochastic model', 'random shifts', 'AO-compensated point spread function', 'aliasing effects', 'sub-Nyquist sampling', 'resolution loss', 'wide-field-of-view modes', '3.5 m', 'adaptive optics', 'astronomical techniques', 'astronomical telescopes', 'astronomy computing', 'deconvolution', 'focal planes', 'image enhancement', 'image reconstruction', 'image resolution', 'image sampling', 'image sequences', 'optical transfer function']","['multiframe blind convolution', 'focal plane array', 'undersampled image sequences', 'sampling resolution', 'simulated imagery', 'image enhancement', 'resolution loss', 'optical imagery', 'blind', 'convolution']",1085,179,34,1085,178,10,1,1,10
"multidimensional data visualization historically, data visualization has been limited primarily to two dimensions (e.g., histograms or scatter plots). available software packages (e.g., data desk 6.1, matlab 6.1, sas-jmp 4.04, spss 10.0) are capable of producing three-dimensional scatter plots with (varying degrees of) user interactivity. we constructed our own data visualization application with the visualization toolkit (schroeder et al., 1998) and tcl/tk to display multivariate data through the application of glyphs (ware, 2000). a glyph is a visual object onto which many data parameters may be mapped, each with a different visual attribute (e.g., size or color). we used our multi-dimensional data viewer to explore data from several psycholinguistic experiments. the graphical interface provides flexibility when users dynamically explore the multidimensional image rendered from raw experimental data. we highlight advantages of multidimensional data visualization and consider some potential limitations ","Multidimensional data visualization Historically, data visualization has been limited primarily to two dimensions (eg., histograms or scatter plots). Available software packages (e.9., Data Desk 6.1, MatLab 6.1, SAS-JMP 4.04, SPSS 10.0) are capable of producing three-dimensional scatter plots with (varying degrees of) user interactivity. We constructed our own data visualization application with the Visualization Toolkit (Schroeder et al., 1998) and TeVTk to display multivariate data through the application of glyphs (Ware, 2000). A glyph is a visual object onto which many data parameters may be mapped, each with a different visual attribute (e.9., size or color). We used our multi-dimensional data viewer to explore data from several psycholinguistic experiments. The graphical interface provides flexibility when users dynamically explore the multidimensional image rendered from raw experimental data. We highlight advantages of multidimensional data visualization and consider some potential limitations","['multidimensional data visualization', '3D scatter plots', 'user interactivity', 'Visualization Toolkit', 'Tcl/Tk', 'multivariate data display', 'glyphs', 'visual object', 'data parameters', 'visual attribute', 'multi-dimensional data viewer', 'psycholinguistic experiments', 'graphical interface', 'multidimensional image rendering', 'data visualisation', 'psychology', 'rendering (computer graphics)']","['data', 'multi-dimensional data viewer', 'different visual attribute', 'raw experimental data', 'many data parameters', 'multivariate data', 'visual object', 'data visualization', 'multidimensional data visualization', 'Multidimensional data visualization']",881,139,17,879,138,10,9,4,5
"a scalable model of cerebellar adaptive timing and sequencing: the recurrent slide and latch (rsl) model from the dawn of modern neural network theory, the mammalian cerebellum has been a favored object of mathematical modeling studies. early studies focused on the fanout, convergence, thresholding, and learned weighting of perceptual-motor signals within the cerebellar cortex. this led to the still viable idea that the granule cell stage in the cerebellar cortex performs a sparse expansive recoding of the time-varying input vector. this recoding reveals and emphasizes combinations in a distributed representation that serves as a basis for the learned, state-dependent control actions engendered by cerebellar outputs to movement related centers. to make optimal use of available signals, the cerebellum must be able to sift the evolving state representation for the most reliable predictors of the need for control actions, and to use those predictors even if they appear only transiently and well in advance of the optimal time for initiating the control action. the paper proposes a modification to prior, population, models for cerebellar adaptive timing and sequencing. since it replaces a population with a single element, the proposed rsl model is in one sense maximally efficient, and therefore optimal from the perspective of scalability ","A scalable model of cerebellar adaptive timing and sequencing: the recurrent slide and latch (RSL) model From the dawn of modern neural network theory, the mammalian cerebellum has been a favored object of mathematical modeling studies. Early studies focused on the fanout, convergence, thresholding, and learned weighting of perceptual-motor signals within the cerebellar cortex. This led to the stil viable idea that the granule cell stage in the cerebellar cortex performs a sparse expansive recoding of the time-varying input vector. This recoding reveals and emphasizes combinations in a distributed representation that serves as a basis for the learned, state-dependent control actions engendered by cerebellar outputs to movement related centers. To make optimal use of available signals, the cerebellum must be able to sift the evolving state representation for the most reliable predictors of the need for control actions, and to use those predictors even if they appear only transiently and well in advance of the optimal time for initiating the control action. The Paper proposes a modification to prior, population, models for cerebellar adaptive timing and sequencing. Since it replaces a population with a single element, the proposed RSL model is in one sense maximally efficient, and therefore optimal from the perspective of scalability","['scalable model', 'cerebellar adaptive timing', 'cerebellar sequencing', 'recurrent slide and latch model', 'neural network theory', 'mammalian cerebellum', 'granule cell stage', 'sparse expansive recoding', 'time-varying input vector', 'distributed representation', 'recurrent network', 'brain models', 'recurrent neural nets', 'timing']","['cerebellar cortex', 'adaptive timing', 'model', 'mathematical modelling studies', 'prior population models', 'cerebellar outputs', 'scalable model', 'optimal time', 'cerebellar', 'usl model']",1151,205,14,1150,204,10,0,1,6
"establishing the discipline of physics-based cmp modeling for the past decade, a physically based comprehensive process model for chemical mechanical polishing has eluded the semiconductor industry. however, a long-term collaborative effort has now resulted in a workable version of that approach. the highly fundamental model is based on advanced finite element analysis and is beginning to show promise in cmp process development ","Establishing the discipline of physics-based CMP modeling For the past decade, a physically based comprehensive process model for ‘chemical mechanical polishing has eluded the semiconductor industry. However, a long-term collaborative effort has now resulted ina workable version of that approach. The highly fundamental model is based on advanced finite element analysis and is beginning to show promise in CMP process development","['chemical mechanical polishing', 'CMP', 'physically based process model', 'finite element analysis', 'CMP process development', 'chemical mechanical polishing', 'finite element analysis', 'semiconductor process modelling']","['long-term collaborative effort', 'physics-based cup modelling', 'comprehensive process model', 'cup process development', 'fundamental model', 'past decade', 'discipline', 'process', 'cup', 'modelling']",370,63,8,371,61,10,93,28,5
mount sinai hospital uses integer programming to allocate operating room time an integer-programming model and a post-solution heuristic allocates operating room time to the five surgical divisions at toronto's mount sinai hospital. the hospital has used this approach for several years and credits it with both administrative savings and the ability to produce quickly an equitable master surgical schedule ,Mount Sinai Hospital uses integer programming to allocate operating room time An integer-programming model and a post-solution heuristic allocates operating room time to the five surgical divisions at Toronto's Mount Sinai Hospital. The hospital has used this approach for several years and credits it with both administrative savings and the ability to produce quickly an equitable master surgical schedule,"['Mount Sinai Hospital', 'integer programming', 'operating room time allocation', 'Toronto', 'Ontario', 'Canada', 'post-solution heuristic', 'health care', 'heuristic programming', 'integer programming', 'resource allocation', 'surgery']","['Mount Sinai Hospital', 'room time', 'integer-programming model', 'integer programming', 'surgical divisions', 'time', 'room', 'Sinai', 'Mount', 'hospital']",349,60,12,349,59,10,0,0,2
"efficient transitive closure reasoning in a combined class/part/containment hierarchy class hierarchies form the backbone of many implemented knowledge representation and reasoning systems. they are used for inheritance, classification and transitive closure reasoning. part hierarchies are also important in artificial intelligence. other hierarchies, e.g. containment hierarchies, have received less attention in artificial intelligence. this paper presents an architecture and an implementation of a hierarchy reasoner that integrates a class hierarchy, a part hierarchy, and a containment hierarchy into one structure. in order to make an implemented reasoner useful, it needs to operate at least at speeds comparable to human reasoning. as real-world hierarchies are always large, special techniques need to be used to achieve this. we have developed a set of parallel algorithms and a data representation called maximally reduced tree cover for that purpose. the maximally reduced tree cover is an improvement of a materialized transitive closure representation which has appeared in the literature. our experiments with a medical vocabulary show that transitive closure reasoning for combined class/part/containment hierarchies in near constant time is possible for a fixed hardware configuration ","Efficient transitive closure reasoning in a combined class/part/containment hierarchy Class hierarchies form the backbone of many implemented knowledge representation and reasoning systems. They are used for inheritance, classification and transitive closure reasoning. Part hierarchies are also important in artificial intelligence. Other hierarchies, e.g. containment hierarchies, have received less attention in artificial intelligence. This paper presents an architecture and an implementation of a hierarchy reasoner that integrates a class hierarchy, a part hierarchy, and a containment hierarchy into one structure. In order to make an implemented reasoner usetul, it needs to operate at least at speeds comparable to human reasoning. As real-world hierarchies are always large, special techniques need to be used to achieve this. We have developed a set of parallel algorithms and a data representation called maximally reduced tree cover for that purpose. The maximally reduced tree cover is an improvement of a materialized transitive closure representation which has appeared in the literature. Our experiments with a medical vocabulary show that transitive closure reasoning for combined class/part/containment hierarchies in near constant time is possible for a fixed hardware configuration","['transitive closure reasoning', 'knowledge representation', 'class hierarchy', 'part hierarchy', 'containment hierarchy', 'parallel algorithms', 'data representation', 'maximally reduced tree cover', 'materialized transitive closure representation', 'experiments', 'medical vocabulary', 'fixed hardware configuration', 'parallel reasoning', 'inheritance', 'part hierarchies', 'artificial intelligence', 'classification', 'inference mechanisms', 'knowledge representation', 'medical computing', 'parallel algorithms', 'tree data structures', 'vocabulary']","['transitive closure reasoning', 'hierarchies', 'containment hierarchy', 'combined class/part/containment hierarchies', 'transitive closure representation', 'real-world hierarchies', 'hierarchy reasoner', 'Other hierarchies', 'class hierarchy', 'part hierarchy']",1124,181,23,1124,180,10,1,1,10
"practice management goes remote [accounting] there's a lot of life in accounting practice management software, a valuable category that has been subject to much change in the last few years. web-based time tracking grows in popularity. looks at cch prosystem fx practice, cms open solutions 6, creative solutions practice, time matters, cpasoftware visual practice management, and abak ","Practice management goes remote [accounting] There's a lot of life in accounting practice management software, a valuable category that has been subject to much change in the last few years. Web-based time tracking grows in popularity. Looks at CCH ProSystem fx Practice, CMS Open Solutions 6, Creative Solutions Practice, Time Matters, CPASoftware Visual Practice Management, and Abak","['accounting practice management software', 'Web-based time tracking', 'CCH ProSystem fx Practice', 'CMS Open Solutions 6', 'Creative Solutions Practice', 'Time Matters', 'CPASoftware Visual Practice Management', 'Abak', 'accounting', ""buyer's guides"", 'invoicing']","['practice', 'management', 'valuable category', 'remote accounting', 'last few years', 'much change', 'life', 'lot', 'remote', 'accounting']",329,58,11,329,57,10,0,0,4
"collective action in the age of the internet: mass communication and online mobilization this article examines how the internet transforms collective action. current practices on the web bear witness to thriving collective action ranging from persuasive to confrontational, individual to collective, undertakings. even more influential than direct calls for action is the indirect mobilizing influence of the internet's powers of mass communication, which is boosted by an antiauthoritarian ideology on the web. theoretically, collective action through the otherwise socially isolating computer is possible because people rely on internalized group memberships and social identities to achieve social involvement. empirical evidence from an online survey among environmental activists and nonactivists confirms that online action is considered an equivalent alternative to offline action by activists and nonactivists alike. however, the internet may slightly alter the motives underlying collective action and thereby alter the nature of collective action and social movements. perhaps more fundamental is the reverse influence that successful collective action will have on the nature and function of the internet ","Collective action in the age of the Internet: mass communication and online mobilization This article examines how the Internet transforms collective action. Current practices on the Web bear witness to thriving collective action ranging from persuasive to confrontational, individual to collective, undertakings. Even more influential than direct calls for action is the indirect mobilizing influence of the Internet's powers of mass communication, which is boosted by an antiauthoritarian ideology on the Web. Theoretically, collective action through the otherwise socially isolating computer is possible because people rely on internalized group memberships and social identities to achieve social involvement. Empirical evidence from an online survey among environmental activists and nonactivists confirms that online action is considered an equivalent alternative to offline action by activists and nonactivists alike. However, the Internet may slightly alter the motives underlying collective action and thereby alter the nature of collective action and social movements. Perhaps more fundamental is the reverse influence that successful collective action will have on the nature and function of the Internet,","['Internet', 'mass communication', 'online mobilization', 'collective action', 'World Wide Web', 'antiauthoritarian ideology', 'group memberships', 'social identities', 'online survey', 'anonymity', 'politics', 'human factors', 'information resources', 'Internet', 'politics', 'psychology', 'social aspects of automation']","['action', 'collective', 'successful collective action', 'internet mass communication', 'collective undertakings', 'offline action', 'online action', 'collective action', 'Collective action', 'mass communication']",1048,169,17,1049,168,10,0,1,3
"to commit or not to commit: modeling agent conversations for action conversations are sequences of messages exchanged among interacting agents. for conversations to be meaningful, agents ought to follow commonly known specifications limiting the types of messages that can be exchanged at any point in the conversation. these specifications are usually implemented using conversation policies (which are rules of inference) or conversation protocols (which are predefined conversation templates). in this article we present a semantic model for specifying conversations using conversation policies. this model is based on the principles that the negotiation and uptake of shared social commitments entail the adoption of obligations to action, which indicate the actions that agents have agreed to perform. in the same way, obligations are retracted based on the negotiation to discharge their corresponding shared social commitments. based on these principles, conversations are specified as interaction specifications that model the ideal sequencing of agent participations negotiating the execution of actions in a joint activity. these specifications not only specify the adoption and discharge of shared commitments and obligations during an activity, but also indicate the commitments and obligations that are required (as preconditions) or that outlive a joint activity (as postconditions). we model the contract net protocol as an example of the specification of conversations in a joint activity ","To commit or not to commit: modeling agent conversations for action Conversations are sequences of messages exchanged among interacting agents. For conversations to be meaningful, agents ought to follow commonly known specifications limiting the types of messages that can be exchanged at any point in the conversation. These specifications are usually implemented using conversation policies (which are rules of inference) ‘or conversation protocols (which are predefined conversation templates). In this article we present a semantic model for specifying conversations using conversation policies. This model is based on the principles that the negotiation and uptake of shared social commitments entail the adoption of obligations to action, which indicate the actions that agents have agreed to perform. In the same way, obligations are retracted based on the negotiation to discharge their corresponding shared social commitments. Based on these principles, conversations are specified as interaction specifications that model the ideal sequencing of agent participations negotiating the execution of actions in a joint activity. These specifications not only specity the adoption and discharge of shared commitments and obligations during an activity, but also indicate the commitments and obligations that are required (as preconditions) or that outlive a joint activity (as postconditions). We model the Contract Net Protocol as an example of the specification of conversations in a joint activity","['interacting agents', 'specifications', 'rules of inference', 'conversation protocols', 'autonomous agents', 'social commitments', 'speech acts', 'software agents', 'conversation templates', 'formal languages', 'software agents']","['conversation policies', 'conversations', 'modelling agent conversations', 'principles conversations', 'conversation templates', 'conversation protocols', 'agent participation', 'meaningful agents', 'semantic model', 'agents']",1290,216,11,1291,215,10,3,2,3
"intelligent optimal sieving method for facts device control in multi-machine systems a multi-target oriented optimal control strategy for facts devices installed in multi-machine power systems is presented in this paper, which is named the intelligent optimal sieving control (iosc) method. this new method divides the facts device output region into several parts and selects one typical value from each part, which is called output candidate. then, an intelligent optimal sieve is constructed, which predicts the impacts of each output candidate on a power system and sieves out an optimal output from all of the candidates. the artificial neural network technologies and fuzzy methods are applied to build the intelligent sieve. finally, the real control signal of facts devices is calculated according to the selected optimal output through inverse system method. simulation has been done on a three-machine power system and the results show that the proposed iosc controller can effectively attenuate system oscillations and enhance the power system transient stability ","Intelligent optimal sieving method for FACTS device control in multi-machine systems ‘A mult-target oriented optimal control strategy for FACTS devices installed in multi-machine power systems is presented in this paper, which is named the intelligent optimal sieving control (IOSC) method. This new method divides the FACTS device output region into several parts and selects ‘one typical value from each part, which is called output candidate. Then, an intelligent optimal sieve is constructed, which predicts the impacts of each output candidate on a power system and sieves out an ‘optimal output from all of the candidates. The artificial neural network technologies and fuzzy methods are applied to build the inteligent sieve. Finally, the real control signal of FACTS devices is calculated according to the selected optimal output through inverse system method. Simulation has been done on a three-machine power system and the results show that the proposed IOSC controller can effectively attenuate system oscillations and enhance the power system transient stability","['FACTS', 'intelligent control', 'intelligent optimal sieving method', 'FACTS device control', 'multi-machine systems', 'multi-target oriented optimal control strategy', 'intelligent optimal sieve', 'artificial neural network technologies', 'fuzzy methods', 'control signal', 'selected optimal output', 'inverse system method', 'three-machine power system', 'system oscillations attenuation', 'power system transient stability enhancement', 'control system synthesis', 'flexible AC transmission systems', 'fuzzy set theory', 'intelligent control', 'neural nets', 'optimal control', 'power system control', 'predictive control']","['optimal output', 'FACTS devices', 'multi-machine power systems', 'three-machine power system', 'intelligent optimal sieve', 'optimal control strategy', 'multi-machine systems', 'FACTS device control', 'intelligent sieve', 'power system']",915,161,23,916,160,10,23,5,2
trading exchanges: online marketplaces evolve looks at how trading exchanges are evolving rapidly to help manufacturers keep up with customer demand ,Trading exchanges: online marketplaces evolve Looks at how trading exchanges are evolving rapidly to help manufacturers keep up with customer demand,"['online marketplaces', 'trading exchanges', 'manufacturers', 'customer demand', 'enterprise platforms', 'supply chain management', 'enterprise resource planning', 'core software platform', 'private exchanges', 'integration technology', 'middleware', 'XML standards', 'content management capabilities', 'manufacturing industries', 'marketing', 'workflow management software']","['online marketplace', 'customer demand', 'Looks', 'trading', 'Trading', 'online', 'exchanges', 'marketplace', 'trading exchanges', 'Trading exchanges']",128,22,16,128,21,10,0,0,1
"cane railway scheduling via constraint logic programming: labelling order and constraints in a real-life application in australia, cane transport is the largest unit cost in the manufacturing of raw sugar, making up around 35% of the total manufacturing costs. producing efficient schedules for the cane railways can result in significant cost savings. the paper presents a study using constraint logic programming (clp) to solve the cane transport scheduling problem. tailored heuristic labelling order and constraints strategies are proposed and encouraging results of application to several test problems and one real-life case are presented. the preliminary results demonstrate that clp can be used as an effective tool for solving the cane transport scheduling problem, with a potential decrease in development costs of the scheduling system. it can also be used as an efficient tool for rescheduling tasks which the existing cane transport scheduling system cannot perform well ","Cane railway scheduling via constraint logic programming: labelling order and constraints in a real-life application In Australia, cane transport is the largest unit cost in the manufacturing of raw sugar, making up around 35% of the total manufacturing costs. Producing efficient schedules for the cane railways can result in significant cost savings. The paper presents a study using constraint logic programming (CLP) to solve the cane transport scheduling problem Tailored heuristic labelling order and constraints strategies are proposed and encouraging results of application to several test problems and one reablife case are presented. The preliminary results demonstrate that CLP can be used as an effective tool for solving the cane transport scheduling problem, with a potential decrease in development costs of the scheduling system. It can also be used as an efficient tool for rescheduling tasks which the existing cane transport scheduling system cannot perform well","['cane railway scheduling', 'constraint logic programming', 'cane transport', 'raw sugar', 'total manufacturing costs', 'cost savings', 'heuristic labelling order', 'constraints strategies', 'agriculture', 'constraint handling', 'constraint theory', 'goods distribution', 'locomotives', 'scheduling']","['scheduling system', 'scheduling', 'heuristic labelling order', 'significant cost savings', 'australia cane transport', 'Cane railway scheduling', 'constraints strategies', 'efficient schedules', 'largest unit cost', 'cane railways']",838,147,14,836,146,10,5,2,3
"hilbert modular threefolds of arithmetic genus one d. weisser (1981) proved that there are exactly four galois cubic number fields with hilbert modular threefolds of arithmetic genus one. in this paper, we extend weisser's work to cover all cubic number fields. our main result is that there are exactly 33 fields with hilbert modular threefolds of arithmetic genus one. these fields are enumerated explicitly ","Hilbert modular threefolds of arithmetic genus one D. Weisser (1981) proved that there are exactly four Galois cubic number fields with Hilbert modular threefolds of arithmetic genus one. In this paper, we extend Weisser's work to cover all cubic number fields. Our main result is that there are exactly 33 fields with Hilbert modular threefolds of arithmetic genus one. These fields are enumerated explicitly","['Hilbert modular threefolds', 'arithmetic genus one', 'Galois cubic number fields', 'Hilbert transforms', 'number theory']","['arithmetic', 'cubic number fields', 'modular', 'Hilbert', 'D.', 'main result', 'D. neisser', 'cubic', 'number', 'fields']",346,65,5,346,64,10,0,0,0
"recognition of finite simple groups s/sub 4/(q) by their element orders it is proved that among simple groups s/sub 4/(q) in the class of finite-groups, only the groups s/sub 4/(3/sup n/), where n is an odd number greater than unity, are recognizable by a set of their element orders. it is also shown that simple groups u/sub 3/(9), /sup 3/d/sub 4/(2), g/sub 2/(4), s/sub 6/(3), f/sub 4/(2), and /sup 2/e/sub 6/(2) are recognizable, but l/sub 3/(3) is not ","Recognition of finite simple groups S/sub 4/(q) by their element orders It is proved that among simple groups S/sub 4/(q) in the class of finite-groups, only the groups S/sub 4/(3/sup n/), where n is an odd number greater than unity, are recognizable by a set of their element orders. It is also shown that simple groups U/sub 3/(9), /Sup 3/D/sub 4/(2), Gisub 2/(4), S/sub 6/(3), F/sub 4/(2), and /sup 2/E/sub 6/(2) are recognizable, but L/sub 3/(3) is not","['finite simple groups recognition', 'divisibility relation', 'element orders', 'group theory', 'process algebra']","['element orders', 'finite simple groups', 'sup 3/D/sub', 'sup 2/E/sub', 'groups', 'q', 'simple', 'simple groups', 'orders', 'element']",379,79,5,379,78,10,1,1,0
"trusted...or...trustworthy: the search for a new paradigm for computer and network security this paper sets out a number of major questions and challenges which include: (a) just what is meant by `trusted' or `trustworthy' systems after 20 years of experience, or more likely, lack of business level experience, with the 'trusted computer system' criteria anyway; (b) does anyone really care about the adoption of international standards for computer system security evaluation by it product and system manufacturers and suppliers (is 15408) and, if so, how does it all relate to business risk management anyway (is 17799); (c) with the explosion of adoption of the microcomputer and personal computer some 20 years ago, has the industry abandoned all that it learnt about security during the `mainframe era'; or - `whatever happened to multics' and its lessons; (d) has education kept up with security requirements by industry and government alike in the need for safe and secure operation of large scale and networked information systems on national and international bases, particularly where web or internet-based information services are being proposed as the major `next best thing' in the it industry; (e) has the `fourth generation' of computer professionals inherited the spirit of information systems management and control that resided by necessity with the last `generation', the professionals who developed and created the applications for shared mainframe and minicomputer systems? ","Trusted...or...trustworthy: the search for a new paradigm for computer and network security This paper sets out a number of major questions and challenges which include (a) just what is meant by ‘trusted’ or ‘trustworthy’ systems after 20 years of experience, or more likely, lack of business level experience, with the ‘trusted computer system’ criteria anyway; (b) does anyone really care about the adoption of international standards for computer system security evaluation by IT product and system manufacturers and suppliers (IS 15408) and, if so, how does it all relate to business risk management anyway (IS 17799); (c) with the explosion of adoption of the microcomputer and personal computer some 20 years ago, has the industry abandoned all that it learnt about security during the “mainframe era’; or - ‘whatever happened to MULTICS' and its lessons; (d) has education kept up with security requirements by industry and government alike in the need for safe and secure operation of large scale and networked information systems on national and international bases, particularly where Web or Internet-based information services are being proposed as the major ‘next best thing’ in the IT industry; (e) has the “fourth generation’ of computer professionals inherited the spirit of information systems management and control that resided by necessity with the last ‘generation’, the professionals who developed and created the applications for shared mainframe and minicomputer systems?","['computer security', 'network security', 'trusted systems', 'trustworthy systems', 'international standards', 'IS 15408', 'business risk management', 'IS 17799', 'IT manufacturers', 'microcomputer', 'personal computer', 'MULTICS', 'education', 'large scale information systems', 'Web', 'Internet-based information services', 'fourth generation computer professionals', 'information systems management', 'information systems control', 'computer network management', 'computer science education', 'information systems', 'Internet', 'microcomputers', 'risk management', 'security of data', 'standards', 'telecommunication security']","['systems', 'information systems management', 'networked information systems', 'computer system criteria', 'computer professionals', 'system manufacturers', 'minicomputer systems', 'trustworthy systems', 'personal computer', 'network security']",1269,228,28,1268,227,10,15,13,9
"non-nested multi-level solvers for finite element discretisations of mixed problems we consider a general framework for analysing the convergence of multi-grid solvers applied to finite element discretisations of mixed problems, both of conforming and nonconforming type. as a basic new feature. our approach allows to use different finite element discretisations on each level of the multi-grid hierarchy. thus, in our multi-level approach, accurate higher order finite element discretisations can be combined with fast multi-level solvers based on lower order (nonconforming) finite element discretisations. this leads to the design of efficient multi-level solvers for higher order finite element discretisations ","Non-nested multi-level solvers for finite element discretisations of mixed problems We consider a general framework for analysing the convergence of multi-grid solvers applied to finite element discretisations of mixed problems, both of conforming and nonconforming type. As a basic new feature. our approach allows to use different finite element discretisations on each level of the multi-grid hierarchy. Thus, in our multi-level approach, accurate higher order finite element discretisations can be combined with fast multi-level solvers based on lower order (nonconforming) finite element discretisations. This leads to the design of efficient multi-level solvers for higher order finite element discretisation","['non-nested multi-level solvers', 'finite element discretisations', 'mixed problems', 'multi-grid solvers', 'higher order finite element discretisations', 'multi-level solvers', 'differential equations', 'finite element analysis']","['finite element discretisation', 'discretisations', 'Non-nested multi-level solvers', 'efficient multi-level solvers', 'fast multi-level solvers', 'accurate higher order', 'multi-level approach', 'multi-grid solvers', 'finite', 'element']",618,99,8,617,98,10,0,1,1
"erp systems implementation: best practices in canadian government organizations erp (enterprise resource planning) systems implementation is a complex exercise in organizational innovation and change management. government organizations are increasing their adoption of these systems for various benefits such as integrated real-time information, better administration, and result-based management. government organizations, due to their social obligations, higher legislative and public accountability, and unique culture face many specific challenges in the transition to enterprise systems. this motivated the authors to explore the key considerations and typical activities in government organizations adopting erp systems. the article adopts the innovation process theory framework as well as the (markus & tanis, 2000) model as a basis to delineate the erp adoption process. although, each adopting organization has a distinct set of objectives for its systems, the study found many similarities in motivations, concerns, and strategies across organizations ","ERP systems implementation: Best practices in Canadian government organizations ERP (Enterprise resource planning) systems implementation is a complex exercise in organizational innovation and change management. Government ‘organizations are increasing their adoption of these systems for various benefits such as integrated real-time information, better administration, and result-based management. Government organizations, due to their social obligations, higher legislative and public accountability, and unique culture face many specific challenges in the transition to enterprise systems. This motivated the authors to explore the key considerations and typical activities in government organizations adopting ERP systems. The article adopts the innovation process theory framework as well as the (Markus & Tanis, 2000) model as a basis to delineate the ERP adoption process. Although, each adopting organization has a distinct set of objectives for its systems, the study found many similarities in motivations, concerns, and strategies across organizations","['ERP systems implementation', 'Canadian government organizations', 'best practices', 'enterprise resource planning', 'integrated real-time information', 'administration', 'result-based management', 'social obligations', 'higher legislative accountability', 'public accountability', 'innovation process theory framework', 'client-server systems', 'government data processing', 'relational databases', 'systems re-engineering']","['government organizations', 'systems', 'ERP', 'ERP systems implementation', 'ERP adoption process', 'enterprise systems', 'Best practices', 'ERP systems', 'implementation', 'organizations']",923,142,15,924,141,10,13,1,5
"modeling cutting temperatures for turning inserts with various tool geometries and materials temperatures are of interest in machining because cutting tools often fail by thermal softening or temperature-activated wear. many models for cutting temperatures have been developed, but these models consider only simple tool geometries such as a rectangular slab with a sharp corner. this report describes a finite element study of tool temperatures in cutting that accounts for tool nose radius and included angle effects. a temperature correction factor model that can be used in the design and selection of inserts is developed to account for these effects. a parametric mesh generator is used to generate the finite element models of tool and inserts of varying geometries. the steady-state temperature response is calculated using nastran solver. several finite element analysis (fea) runs are performed to quantify the effects of inserts included angle, nose radius, and materials for the insert and the tool holder on the cutting temperature at the insert rake face. the fea results are then utilized to develop a temperature correction factor model that accounts for these effects. the temperature correction factor model is integrated with an analytical temperature model for rectangular inserts to predict cutting temperatures for contour turning with inserts of various shapes and nose radii. finally, experimental measurements of cutting temperature using the tool-work thermocouple technique are performed and compared with the predictions of the new temperature model. the comparisons show good agreement ","Modeling cutting temperatures for turning inserts with various tool geometries and materials ‘Temperatures are of interest in machining because cutting tools often fail by thermal softening or temperature-activated wear. Many models for cutting temperatures have been developed, but these models consider only simple tool geometries such as a rectangular slab with a sharp corner. This report describes a finite element study of tool temperatures in cutting that accounts for tool nose radius and included angle effects. A temperature correction factor model that can be used in the design and selection of inserts is developed to account for these effects. A parametric mesh generator is used to generate the finite element models of tool and inserts of varying geometries. The steady-state temperature response is calculated using NASTRAN solver. Several finite element analysis (FEA) runs are performed to quantity the effects of inserts included angle, nose radius, and materials for the insert and the too! holder on the cutting temperature at the insert rake face. The FEA results are then utilized to develop a temperature correction factor model that accounts for these effects. The temperature correction factor model is integrated with an analytical temperature model for rectangular inserts to predict cutting temperatures for contour turning with inserts of various shapes and nose radii. Finally, experimental measurements of cutting temperature using the tool-work thermocouple technique are performed and compared with the predictions of the new temperature model. The comparisons show good agreement","['cutting temperature model', 'turning inserts', 'machining', 'tool nose radius', 'parametric mesh generator', 'finite element models', 'temperature correction factor', 'insert shape effects', 'tool geometries', 'cutting', 'machine tools', 'machining', 'mesh generation', 'temperature distribution']","['temperatures', 'steady-state temperature response', 'analytical temperature model', 'various tool geometries', 'simple tool geometries', 'materials temperatures', 'new temperature model', 'finite element models', 'tool temperatures', 'tool nose radius']",1376,240,14,1377,239,10,14,3,5
"crone control: principles and extension to time-variant plants with asymptotically constant coefficients the principles of crone control, a frequency-domain robust control design methodology based on fractional differentiation, are presented. continuous time-variant plants with asymptotically constant coefficients are analysed in the frequency domain, through their representation using time-variant frequency responses. a stability theorem for feedback systems including time-variant plants with asymptotically constant coefficients is proposed. finally, crone control is extended to robust control of these plants ","CRONE control: principles and extension to time-variant piants with asymptotically constant coefficients The principles of CRONE control, a frequency-domain robust control design methodology based on fractional differentiation, are presented Continuous time-variant plants with asymptotically constant coefficients are analysed in the frequency domain, through their representation using time-variant frequency responses. A stability theorem for feedback systems including time-variant plants with asymptotically constant coefficients is proposed. Finally, CRONE control is extended to robust control of these plants","['CRONE control', 'time-variant plants', 'asymptotically constant coefficients', 'frequency-domain robust control design', 'fractional differentiation', 'time-variant frequency responses', 'stability theorem', 'feedback systems', 'robust control', 'automatic control', 'asymptotic stability', 'differentiation', 'frequency response', 'robust control', 'time-varying systems']","['constant coefficients', 'robust control', 'time-variant frequency responses', 'Continuous time-variant plants', 'CRONE control principles', 'control', 'time-variant', 'CRONE', 'CRONE control', 'time-variant plants']",543,76,15,542,75,10,1,2,4
"a building block approach to automated engineering shenandoah valley electric cooperative (svec, mt. crawford, virginia, us) recognized the need to automate engineering functions and create an interactive model of its distribution system in the early 1990s. it had used milsoft's da software for more than 10 years to make engineering studies, and had a landis and gyr scada system and a hybrid load management system for controlling water heater switches. with the development of gis and facilities management (fm) applications, svec decided this should be the basis for an information system that would model its physical plant and interface with its accounting and billing systems. it could add applications such as outage management, staking, line design and metering to use this information and interface with these databases. however, based on svec's size it was not feasible to implement a sophisticated and expensive gis/fm system. over the past nine years, svec has had success with a building block approach, and its customers and employees are realizing the benefits of the automated applications. this building block approach is discussed in this article including the gis, outage management system, mapviewer and a staking package. the lessons learned and future expansion are discussed ","A building block approach to automated engineering Shenandoah Valley Electric Cooperative (SVEC, Mt. Crawford, Virginia, US) recognized the need to automate engineering functions and create an interactive model of its distribution system in the early 1990s. It had used Milsoft's DA software for more than 10 years to make engineering studies, and had a Landis and Gyr SCADA system and a hybrid load management system for controlling water heater switches. With the development of GIS and facilities management (FM) applications, SVEC decided this should be the basis for an information system that would model its physical plant and interface with its accounting and billing systems. It could add applications such as outage management, staking, line design and metering to use this information and interface with these databases. However, based on SVEC's size it was not feasible to implement a sophisticated and expensive GIS/FM system. Over the past nine years, SVEC has had success with a building block approach, and its customers and employees are realizing the benefits of the automated applications. This building block approach is discussed in this article including the GIS, outage management system, MapViewer and a staking package. The lessons learned and future expansion are discussed","['Shenandoah Valley Electric Cooperative', 'engineering functions automation', 'interactive model', 'distribution system', 'building block approach', 'billing systems', 'outage management', 'staking', 'line design', 'metering', 'databases', 'MapViewer', 'GIS', 'geographic information systems', 'management', 'power distribution control', 'power engineering computing']","['building block approach', 'expensive GIS/FM system', 'engineering functions', 'engineering studies', 'distribution system', 'information system', 'gym scala system', 'billing systems', 'block', 'building']",1101,200,17,1101,199,10,0,0,10
"factors contributing to preservice teachers' discomfort in a web-based course structured as an inquiry a report is given of a qualitative emergent design study of a science, technology, society interaction (sts) web-enhanced course. students' discomfort during the pilot test provided insight into the intellectual scaffolding that preservice secondary science teachers needed to optimize their performance when required to develop understanding through open-ended inquiry in a web environment. eight factors identified contributed to student discomfort: computer skills, paradigm shifts, trust, time management, thinking about their own thinking, systematic inquiry, self-assessment, and scientific discourse. these factors suggested developing understanding through inquiry by conducting a self-designed, open-ended, systematic inquiry required autonomous learning involving metacognitive skills and time management skills. to the extent in which students either came into the course with this scaffolding, or developed it during the course, they were successful in learning about sts and its relationship to science teaching. changes in the web site made to accommodate learners' needs as they surfaced are described ","Factors contributing to preservice teachers’ discomfort in a Web-based course structured as an inquiry A report is given of a qualitative emergent design study of a Science, Technology, Society Interaction (STS) Web-enhanced course. Students’ discomfort during the pilot test provided insight into the intellectual scaffolding that preservice secondary science teachers needed to optimize their performance when required to develop understanding through open-ended inquiry in a Web environment. Eight factors identified contributed to student discomfort: computer skills, paradigm shifts, trust, time management, thinking about their own thinking, systematic inquiry, self-assessment, and scientific discourse. These factors suggested developing understanding through inquiry by conducting a self-designed, open-ended, systematic inquiry required autonomous learning involving metacognitive skills and time management skills. To the extent in which students either came into the course with this scaffolding, or developed it during the course, they were successful in learning about STS and its relationship to science teaching. Changes in the Web site made to accommodate learners' needs as they surfaced are described","['preservice teacher discomfort', 'Web-based course', 'qualitative emergent design study', 'science technology society interaction course', 'Web-enhanced course', 'student discomfort', 'intellectual scaffolding', 'preservice secondary science teachers', 'open-ended inquiry', 'Web environment', 'computer skills', 'paradigm shifts', 'trust', 'time management', 'thinking', 'systematic inquiry', 'self-assessment', 'scientific discourse', 'autonomous learning', 'metacognitive skills', 'time management skills', 'STS', 'science teaching', 'educational computing', 'educational courses', 'human factors', 'information resources', 'natural sciences computing', 'teacher training', 'teaching', 'user interfaces']","['inquiry', 'systematic inquiry self-assessment', 'pre-service teachers discomfort', 'open-ended systematic inquiry', 'science teaching Changes', 'time management skills', 'open-ended inquiry', 'pen-based course', 'inquiry A report', 'discomfort']",1056,165,31,1056,164,10,2,2,9
"autofocus system for microscope a technique is developed for microscope autofocusing, which is called the eccentric light beam approach with high resolution, wide focusing range, and compact construction. the principle is described. the theoretical formula of the eccentric light beam approach deduced can be applied not only to an object lens whose objective plane is just at the focal plane, but also to an object lens whose objective plane is not at the focal plane. the experimental setup uses a semiconductor laser device as the light source. the laser beam that enters into the microscope is eccentric with the main light axis. a defocused signal is acquired by a symmetrical silicon photocell for the change of the reflected light position caused by differential amplification and processed by a microprocessor. then the electric signal is power-amplified and drives a dc motor, which moves a fine working platform to an automatic focus of the microscope. the result of the experiments shows a +or-0.1- mu m precision of autofocusing for a range of +or-500- mu m defocusing. the system has high reliability and can meet the requirements of various accurate micro measurement systems ","Autofocus system for microscope A technique is developed for microscope autofocusing, which is called the eccentric light beam approach with high resolution, wide focusing range, and compact construction. The principle is described. The theoretical formula of the eccentric light beam approach deduced can be applied not only to an object lens whose objective plane is just at the focal plane, but also to an object lens whose objective plane is not at the focal plane. The experimental setup uses a semiconductor laser device as the light source. The laser beam that enters into the microscope is eccentric with the main light axis. A defocused signal is acquired by a symmetrical silicon photocell for the change of the, reflected light position caused by differential amplification and processed by a microprocessor. Then the electric signal is power-amplified and drives a dc motor, which moves a fine working platform to an automatic focus of the microscope. The result of the experiments shows a +or-0.1- mu m precision of autofocusing for a range of +0r-500- mu m defocusing. The system has high reliability and can meet the requirements of various accurate micro measurement systems","['autofocus system', 'microscope autofocusing', 'eccentric light beam approach', 'object lens', 'objective plane', 'semiconductor laser', 'main light axis', 'defocused signal', 'symmetrical silicon photocell', 'reflected light position', 'differential amplification', 'microprocessor', 'power-amplified electric signal', 'dc motor', 'fine working platform', 'high reliability', 'micro measurement systems', 'lenses', 'micropositioning', 'optical focusing', 'optical microscopes']","['objective plane', 'microscope autofocusing', 'microscope A technique', 'Autofocus system', 'light position', 'light source', 'light', 'laser beam', 'microscope', 'systems']",1000,191,21,1001,190,10,1,2,8
"automated breath detection on long-duration signals using feedforward backpropagation artificial neural networks a new breath-detection algorithm is presented, intended to automate the analysis of respiratory data acquired during sleep. the algorithm is based on two independent artificial neural networks (ann/sub insp/ and ann/sub expi/) that recognize, in the original signal, windows of interest where the onset of inspiration and expiration occurs. postprocessing consists in finding inside each of these windows of interest minimum and maximum corresponding to each inspiration and expiration. the ann/sub insp/ and ann/sub expi/ correctly determine respectively 98.0% and 98.7% of the desired windows, when compared with 29 820 inspirations and 29 819 expirations detected by a human expert, obtained from three entire-night recordings. postprocessing allowed determination of inspiration and expiration onsets with a mean difference with respect to the same human expert of (mean +or- sd) 34 +or- 71 ms for inspiration and 5 +or- 46 ms for expiration. the method proved to be effective in detecting the onset of inspiration and expiration in full night continuous recordings. a comparison of five human experts performing the same classification task yielded that the automated algorithm was undifferentiable from these human experts, failing within the distribution of human expert results. besides being applicable to adult respiratory volume data, the presented algorithm was also successfully applied to infant sleep data, consisting of uncalibrated rib cage and abdominal movement recordings. a comparison with two previously published algorithms for breath detection in respiratory volume signal shows that the presented algorithm has a higher specificity, while presenting similar or higher positive predictive values ","Automated breath detection on long-duration signals using feedforward backpropagation artificial neural networks Anew breath-detection algorithm is presented, intended to automate the analysis of respiratory data acquired during sleep. The algorithm is based on two independent artificial neural networks (ANN/sub insp/ and ANN/sub expi/) that recognize, in the original signal, windows of interest where the onset of inspiration and expiration occurs. Postprocessing consists in finding inside each of these windows of interest minimum and maximum corresponding to each inspiration and expiration. The ANN/sub insp/ and ANN/sub expi/ correctly determine respectively 98.0% and 98.7% of the desired windows, when compared with 29 820 inspirations and 29 819 expirations detected by a human expert, obtained from three entire-night recordings. Postprocessing allowed determination of inspiration and expiration onsets with a mean difference with respect to the same human expert of (mean +or- SD) 34 +or- 71 ms for inspiration and 5 +or- 46 ms for expiration. The method proved to be effective in detecting the onset of inspiration and expiration in full night continuous recordings. A comparison of five human experts performing the same classification task yielded that the, automated algorithm was undifferentiable from these human experts, failing within the distribution of human expert results. Besides being applicable to adult respiratory volume data, the presented algorithm was also successfully applied to infant sleep data, consisting of uncalibrated rib cage and abdominal movement recordings. A comparison with two previously published algorithms for breath detection in respiratory volume signal shows that the presented algorithm has a higher specificity, while presenting similar or higher positive predictive values","['respiratory movements', 'automated breath detection', 'postprocessing', 'inspiration', 'expiration', 'automated algorithm', 'human experts', 'entire-night recordings', 'uncalibrated rib cage', 'abdominal movement recordings', 'infant sleep data', 'adult respiratory volume data', 'long-duration signals', 'feedforward backpropagation artificial neural networks', '34 ms', '5 ms', 'backpropagation', 'feedforward neural nets', 'medical signal detection', 'pneumodynamics', 'sleep']","['artificial neural networks', 'breath detection', 'breath-detection algorithm', 'respiratory volume signal', 'original signal windows', 'long-duration signals', 'same human expert', 'respiratory data', 'algorithm', 'human expert']",1572,262,21,1573,260,10,887,248,8
"tax forms: cd or not cd? the move from cd to the web looks unstoppable. besides counting how many thousands of electronic tax forms they offer, vendors are rapidly moving those documents to the web ","Tax forms: CD or not CD? The move from CD to the Web looks unstoppable. Besides counting how many thousands of electronic tax forms they offer, vendors are rapidly moving those documents to the Web","['electronic tax forms', 'Web', 'ATX Forms Zillion Forms', 'CCH Perform Plus H', 'Kleinrock Forms Library Plus', 'Nelco LaserLibrarian II', 'RIA eForm', 'STF Services Superform', 'Universal Tax Systems Forms Complete', 'accounting', 'tax preparation']","['electronic tax forms', 'many thousands', 'Tax forms CD', 'unstoppable', 'Web', 'move', 'forms', 'Tax', 'cd', 'CD']",163,36,11,163,35,10,0,0,0
"ant colony optimization and stochastic gradient descent we study the relationship between the two techniques known as ant colony optimization (aco) and stochastic gradient descent. more precisely, we show that some empirical aco algorithms approximate stochastic gradient descent in the space of pheromones, and we propose an implementation of stochastic gradient descent that belongs to the family of aco algorithms. we then use this insight to explore the mutual contributions of the two techniques ","Ant colony optimization and stochastic gradient descent We study the relationship between the two techniques known as ant colony ‘optimization (ACO) and stochastic gradient descent. More precisely, we show that some empirical ACO algorithms approximate stochastic gradient descent in the space of pheromones, and we propose an implementation of stochastic gradient descent that belongs to the family of ACO algorithms. We then use this insight to explore the mutual contributions of the two techniques","['ant colony optimization', 'stochastic gradient descent', 'empirical ACO algorithms', 'pheromones', 'combinatorial optimization', 'heuristic', 'reinforcement learning', 'social insects', 'swarm intelligence', 'artificial life', 'local search algorithms', 'artificial life', 'heuristic programming', 'learning (artificial intelligence)', 'optimisation', 'search problems']","['stochastic gradient descent', 'Ant colony optimization', 'empirical act algorithms', 'Ant', 'act algorithms', 'colony', 'optimization', 'descent', 'gradient', 'stochastic']",427,75,16,428,74,10,12,1,1
"on fuzzy and probabilistic control charts in this article, different procedures of constructing control charts for linguistic data, based on fuzzy and probability theory, are discussed. three sets of membership functions, with different degrees of fuzziness, are proposed for fuzzy approaches. a comparison between fuzzy and probability approaches, based on the average run length and samples under control, is conducted for real data. contrary to the conclusions of raz and wang (1990) the choice of degree of fuzziness affected the sensitivity of control charts ","On fuzzy and probabilistic control charts In this article, different procedures of constructing control charts for linguistic data, based on fuzzy and probability theory, are discussed. Three sets of membership functions, with different degrees of fuzziness, are proposed for fuzzy approaches. A comparison between fuzzy and probability approaches, based on the Average Run Length and samples under control, is conducted for real data. Contrary to the conclusions of Raz and Wang (1990) the choice of degree of fuzziness affected the sensitivity of control charts","['linguistic data', 'fuzzy control charts', 'probabilistic control charts', 'control chart construction', 'membership functions', 'fuzziness degree', 'average run length', 'sensitivity', 'fuzzy subsets', 'porcelain products', 'fuzzy control', 'fuzzy set theory', 'probability', 'production control', 'sensitivity analysis']","['probabilistic control charts', 'probability approaches', 'different procedures', 'different degrees', 'fuzzy approaches', 'linguistic data', 'control', 'charts', 'fuzzy', 'control charts']",480,85,15,480,84,10,0,0,3
"computational capacity of an odorant discriminator: the linear separability of curves we introduce and study an artificial neural network inspired by the probabilistic receptor affinity distribution model of olfaction. our system consists of n sensory neurons whose outputs converge on a single processing linear threshold element. the system's aim is to model discrimination of a single target odorant from a large number p of background odorants within a range of odorant concentrations. we show that this is possible provided p does not exceed a critical value p/sub c/ and calculate the critical capacity alpha c=p/sub c//n. the critical capacity depends on the range of concentrations in which the discrimination is to be accomplished. if the olfactory bulb may be thought of as a collection of such processing elements, each responsible for the discrimination of a single odorant, our study provides a quantitative analysis of the potential computational properties of the olfactory bulb. the mathematical formulation of the problem we consider is one of determining the capacity for linear separability of continuous curves, embedded in a large-dimensional space. this is accomplished here by a numerical study, using a method that signals whether the discrimination task is realizable, together with a finite-size scaling analysis ","Computational capacity of an odorant discriminator: the linear separability of curves We introduce and study an artificial neural network inspired by the probabilistic receptor affinity distribution model of olfaction. Our system consists of N sensory neurons whose outputs converge on a single processing linear threshold element. The system's aim is to model discrimination of a single target odorant from a large number p of background odorants within a range of odorant concentrations. We show that this is possible provided p does not exceed a critical value p/sub / and calculate the critical capacity alpha c=p/sub c//N. The critical capacity depends on the range of concentrations in which the discrimination is to be accomplished. If the olfactory bulb may be thought of as a collection of such processing elements, each responsible for the discrimination of a single odorant, our study provides a quantitative analysis of the potential computational properties of the olfactory bulb. The mathematical formulation of the problem we consider is one of determining the capacity for linear separability of continuous curves, embedded in a large-dimensional space. This is accomplished here by a numerical study, using a method that signals whether the discrimination task is realizable, together with a finite-size scaling analysis","['artificial neural network', 'receptor affinity distribution', 'olfaction', 'linear threshold element', 'sensory neurons', 'linear separability', 'odorant discriminator', 'chemioception', 'neural nets']","['linear inseparability', 'critical capacity', 'odorant', 'linear threshold elements', 'artificial neural network', 'odorant concentrations', 'Computational capacity', 'single target odorant', 'background odorants', 'capacity']",1137,203,9,1136,202,10,1,1,3
"crm: approaching zenith looks at how manufacturers are starting to warm up to the concept of customer relationship management. crm has matured into what is expected to be big business. as crm software evolves to its second, some say third, generation, it's likely to be more valuable to holdouts in manufacturing and other sectors ","CRM: approaching zenith Looks at how manufacturers are starting to warm up to the concept of customer relationship management. CRM has matured into what is expected to be big business. As CRM software evolves to its second, some say third, generation, it's likely to be more valuable to holdouts in Manufacturing and other sectors","['manufacturers', 'customer relationship management', 'CRM', 'manufacturing', 'management']","['manufacturers', 'relationship', 'erm software', 'big business', 'management', 'customer', 'concept', 'zenith', 'crmf', 'erm']",277,55,5,277,54,10,0,0,2
"robust output-feedback control for linear continuous uncertain state delayed systems with unknown time delay the state-delayed time often is unknown and independent of other variables in most real physical systems. a new stability criterion for uncertain systems with a state time-varying delay is proposed. then, a robust observer-based control law based on this criterion is constructed via the sequential quadratic programming method. we also develop a separation property so that the state feedback control law and observer can be independently designed and maintain closed-loop system stability. an example illustrates the availability of the proposed design method ","Robust output-feedback control for linear continuous uncertain state delayed systems with unknown time delay The state-delayed time often is unknown and independent of other variables in most real physical systems. A new stability criterion for uncertain systems with a state time-varying delay is proposed. Then, a robust observer-based control law based on this criterion is constructed via the sequential quadratic programming method. We also develop a separation property so that the state feedback control law and observer can be independently designed and maintain closed-loop system stability. An example illustrates the availabilty of the proposed design method","['robust control', 'output-feedback control', 'linear continuous systems', 'uncertain systems', 'state delayed systems', 'time delay', 'state time-varying delay', 'observer-based control law', 'sequential quadratic programming', 'state feedback control law', 'closed-loop system stability', 'closed loop systems', 'continuous time systems', 'control theory', 'delays', 'linear systems', 'observers', 'quadratic programming', 'robust control', 'state feedback', 'time-varying systems', 'uncertain systems']","['Robust output-feedback control', 'closed-loop system stability', 'new stability criterion', 'unknown time delay', 'state-delayed time', 'uncertain systems', 'state', 'Robust', 'systems', 'control']",575,97,22,574,96,10,2,1,2
"chemical information based scaling of molecular descriptors: a universal chemical scale for library design and analysis scaling is a difficult issue for any analysis of chemical properties or molecular topology when disparate descriptors are involved. to compare properties across different data sets, a common scale must be defined. using several publicly available databases (acd, cmc, mddr, and nci) as a basis, we propose to define chemically meaningful scales for a number of molecular properties and topology descriptors. these chemically derived scaling functions have several advantages. first, it is possible to define chemically relevant scales, greatly simplifying similarity and diversity analyses across data sets. second, this approach provides a convenient method for setting descriptor boundaries that define chemically reasonable topology spaces. for example, descriptors can be scaled so that compounds with little potential for biological activity, bioavailability, or other drug-like characteristics are easily identified as outliers. we have compiled scaling values for 314 molecular descriptors. in addition the 10th and 90th percentile values for each descriptor have been calculated for use in outlier filtering ","Chemical information based scaling of molecular descriptors: a universal chemical scale for library design and analysis Scaling is a difficult issue for any analysis of chemical properties or molecular topology when disparate descriptors are involved. To compare properties across different data sets, a common scale must be defined. Using several publicly available databases (ACD, CMC, MDDR, and NCI) as a basis, we propose to define chemically meaningful scales for a number of molecular properties and topology descriptors. These chemically derived scaling functions have several advantages. First, it is possible to define chemically relevant scales, greatly simplifying similarity and diversity analyses across data sets. Second, this approach provides a convenient method for setting descriptor boundaries that define chemically reasonable topology spaces. For example, descriptors can be scaled so that compounds with little potential for biological activity, bioavailability, or other drug-like characteristics are easily identified as outliers. We have compiled scaling values for 314 molecular descriptors. In addition the 10th and 90th percentile values for each descriptor have been calculated for use in outler fittering","['universal chemical scale', 'library design', 'library analysis', 'chemical information based scaling', 'molecular descriptors', 'molecular topology', 'chemical properties', 'databases', 'diversity analyses', 'similarity analyses', 'data sets', 'descriptor boundaries', 'drug-like characteristics', 'biological activity', 'bioavailability', 'outliers', 'biology computing', 'chemistry computing', 'medical computing', 'medical information systems', 'molecular biophysics', 'pharmaceutical industry', 'scientific information systems']","['molecular descriptors', 'universal chemical scale', 'disparate descriptors', 'descriptor boundaries', 'topology descriptors', 'molecular properties', 'molecular topology', 'meaningful scales', 'relevant scales', 'common scale']",1063,174,23,1062,173,10,3,2,8
"model predictive control helps to regulate slow processes-robust barrel temperature control slow temperature control is a challenging control problem. the problem becomes even more challenging when multiple zones are involved, such as in barrel temperature control for extruders. often, strict closed-loop performance requirements (such as fast startup with no overshoot and maintaining tight temperature control during production) are given for such applications. when characteristics of the system are examined, it becomes clear that a commonly used proportional plus integral plus derivative (pid) controller cannot meet such performance specifications for this kind of system. the system either will overshoot or not maintain the temperature within the specified range during the production run. in order to achieve the required performance, a control strategy that utilizes techniques such as model predictive control, autotuning, and multiple parameter pid is formulated. this control strategy proves to be very effective in achieving the desired specifications, and is very robust ","Model predictive contro! helps to regulate slow processes-robust barrel temperature control Slow temperature control is a challenging control problem. The problem becomes. even more challenging when multiple zones are involved, such as in barrel temperature control for extruders. Often, strict closed-loop performance requirements (such as fast startup with no overshoot and maintaining tight temperature control during production) are given for such applications. When characteristics of the system are examined, it becomes clear that a commonly used proportional plus integral plus derivative (PID) controller cannot meet such performance specifications for this kind of system. The system either will overshoot or not maintain the temperature within the specified range during the production run. In order to achieve the required performance, a control strategy that utilizes techniques such as model predictive control, autotuning, and multiple parameter PID is formulated. This control strategy proves to be very effective in achieving the desired specifications, and is very robust","['model predictive control', 'slow processes regulation', 'robust barrel temperature control', 'extruders', 'autotuning', 'multiple parameter PID', 'extrusion', 'moulding', 'predictive control', 'robust control', 'temperature control', 'three-term control', 'tuning']","['control', 'barrel temperature control', 'control strategy', 'such performance specifications', 'predictive control autotuning', 'challenging control problem', 'tight temperature control', 'controller cannon', 'temperature', 'predictive control']",935,154,13,936,153,10,1,2,3
variable structure intelligent control for pm synchronous servo motor drive the variable structure control (vsc) of discrete time systems based on intelligent control is presented in this paper. a novel approach is proposed for the state estimation. a linear observer is firstly designed. then a neural network is used for compensating uncertainty. the parameter of the vsc scheme is adjusted online by a neural network. practical operating results from a pm synchronous motor (pmsm) illustrate the effectiveness and practicability of the proposed approach ,Variable structure intelligent control for PM synchronous servo motor drive The variable structure control (VSC) of discrete time systems based on intelligent control is presented in this paper. A novel approach is proposed for the state estimation. A linear observer is firstly designed. Then a neural network is used for compensating uncertainty. The parameter of the VSC scheme is adjusted online by a neural network. Practical operating results from a PM synchronous motor (PMSM) illustrate the effectiveness and practicability of the proposed approach,"['PM synchronous servo motor drive', 'variable structure intelligent control', 'control design', 'discrete time systems', 'state estimation', 'linear observer', 'neural network', 'uncertainty compensation', 'control performance', 'compensation', 'control system synthesis', 'discrete time systems', 'intelligent control', 'machine control', 'machine testing', 'neurocontrollers', 'permanent magnet motors', 'servomotors', 'state estimation', 'synchronous motor drives', 'uncertain systems', 'variable structure systems']","['intelligent control', 'variable structure control', 'neural network Practical', 'discrete time systems', 'state estimation A', 'synchronous motor', 'control', 'structure', 'Variable structure', 'intelligent']",474,84,22,474,83,10,0,0,1
excess energy [cooling system] the designers retrofitting a comfort cooling system to offices in hertfordshire have been able to make use of the waste heat rejected. what's more they're now making it a standard solution for much larger projects ,Excess energy [cooling system] The designers retrofitting a comfort cooling system to offices in Hertfordshire have been able to make use of the waste heat rejected. what's more they're now making it a standard solution for much larger projects,"['comfort cooling system', 'waste heat', 'Nationwide Trust', 'air conditioning', 'air conditioning', 'energy conservation']","['designers retrofitting', 'comfort cooling system', 'waste heat', 'energy', 'Excess', 'system', 'comfort', 'cooling', 'designers', 'retrofitting']",206,40,6,206,39,10,0,0,0
"embeddings of planar graphs that minimize the number of long-face cycles we consider the problem of finding embeddings of planar graphs that minimize the number of long-face cycles. we prove that for any k >or= 4, it is np-complete to find an embedding that minimizes the number of face cycles of length at least k ","Embeddings of planar graphs that minimize the number of long-face cycles We consider the problem of finding embeddings of planar graphs that minimize the number of long-face cycles. We prove that for any k >or= 4, it is NP-complete to find an embedding that minimizes the number of face cycles of length at least k","['embeddings', 'planar graphs', 'long-face cycles', 'NP-complete problem', 'graph drawing', 'computational complexity', 'graph theory', 'minimisation']","['embedding', 'long-wave cycles', 'number', 'planar graphs', 'face cycles', 'least k', 'cycles', 'planar', 'graphs', 'long-wave']",260,56,8,260,55,10,0,0,1
"taming the paper tiger [paperwork organization] generally acknowledged as a critical problem for many information professionals, the massive flow of documents, paper trails, and information needs efficient and dependable approaches for processing and storing and finding items and information ","Taming the paper tiger [paperwork organization] Generally acknowledged as a critical problem for many information professionals, the massive flow of documents, paper trails, and information needs efficient and dependable approaches for processing and storing and finding items and information","['paperwork organization', 'information professionals', 'information processing', 'information storage', 'information retrieval', 'information retrieval', 'information storage', 'library automation']","['many information professionals', 'dependable approaches', 'critical problem', 'documents paper', 'organization', 'massive flow', 'paperwork', 'paper', 'critical', 'information']",254,40,8,254,39,10,0,0,0
"mobile banking's tough sell banks are having to put their mobile-commerce projects on hold because the essential technology to make the services usable, in particular gprs (general packet radio service) hasn't become widely available. it is estimated that by the end of 2002, only 5 per cent of adults will have gprs phones. this will have a knock-on effect for other technologies such as clickable icons and multimedia messaging. in fact banking via wap (wireless application protocol) has proved to be a frustrating and time-consuming process for the customer. financial firms' hopes for higher mobile usage are stymied by the fact that improvements to the systems won't happen as fast as they want and the inadequacies of the system go beyond immature technology. financial services institutions should not wait for customers to become au fait with their wap. instead they should be the ones ""driving the traffic"" ","Mobile banking's tough sell Banks are having to put their mobile-commerce projects on hold because the essential technology to make the services usable, in particular GPRS (general packet radio service) hasn't become widely available. It is estimated that by the end of 2002, only 5 per cent of adults will have GPRS phones. This will have a knock-on effect for other technologies such as clickable icons and multimedia messaging. In fact banking via WAP (wireless application protocol) has proved to be a frustrating and time-consuming process for the customer. Financial firms' hopes for higher mobile usage are stymied by the fact that improvements to the systems won't happen as fast as they want and the inadequacies of the system go beyond immature technology. Financial services institutions, should not wait for customers to become au fait with their WAP. Instead they should be the ones “driving the traffic”","['banking', 'mobile-commerce', 'GPRS', 'wireless application protocol', 'banking', 'electronic commerce', 'mobile computing']","['Financial services institutions', 'mobile-commerce projects', 'essential technology', 'other technologies', 'tough sell Banks', 'Mobile banking', 'fact banking', 'Mobile', 'banking', 'sell']",770,148,7,771,147,10,2,3,2
"business school research: bridging the gap between producers and consumers there has been a great deal of continuing discussion concerning the seemingly unbridgeable gap between so much of the research produced by business school professors and the needs of the business people who, ideally, would use it. here, we examine this gap and suggest a model for bridging it. we sample four groups of people, business school academics (professors), deans of business schools, executive mba students/recent graduates, and senior business executives. each group rates 44 different (potential) properties of exemplary research. we analyze within-group differences, and more meaningfully, between-group differences. we then offer commentary on the results and use the results to develop the aforementioned suggestions for bridging the gap we find ","Business school research: bridging the gap between producers and consumers There has been a great deal of continuing discussion concerning the seemingly unbridgeable gap between so much of the research produced by business school professors and the needs of the business people who, ideally, would use it. Here, we examine this gap and suggest a model for bridging it. We sample four groups of people, business school academics (professors), deans of business schools, executive MBA students/recent graduates, and senior business executives. Each group rates 44 different (potential) properties of exemplary research. We analyze within-group differences, and more meaningfully, between-group differences. We then offer commentary on the results and use the results to develop the aforementioned suggestions for bridging the gap we find","['business school', 'producers', 'consumers', 'professors', 'business people', 'academics', 'deans', 'students', 'recent graduates', 'senior business executives', 'exemplary research', 'within-group differences', 'between-group differences', 'ANOVA', 'coefficient of concordance', 'multiple comparison testing', 'education', 'management science', 'statistical analysis']","['business', 'senior business executives', 'business school professors', 'Business school research', 'school', 'exemplary research', 'unbridgeable gap', 'business schools', 'business people', 'gap']",714,123,19,714,122,10,0,0,8
"writing the fulfillment rfp [publishing] for the uninitiated, writing a request for proposal can seem both mysterious and daunting. here's a format that will make you look like a pro the first time out ","Writing the fulfilment RFP [publishing] For the uninitiated, writing a request for proposal can seem both mysterious. and daunting. Here's a format that will make you look like a pro the first time out","['request for proposal', 'fulfillment', 'publisher', 'publishing']","['fulfilment rip publishing', 'daunting heres', 'uninitiated', 'mysterious', 'first time', 'proposal', 'request', 'rip', 'publishing', 'fulfilment']",168,35,4,168,34,10,4,2,1
technology cad of sige-heterojunction field effect transistors a 2d virtual wafer fabrication simulation suite has been employed for the technology cad of sige channel heterojunction field effect transistors (hfets). complete fabrication process of sige p-hfets has been simulated. the sige material parameters and mobility model were incorporated to simulate si/sige p-hfets with a uniform germanium channel having an l/sub eff/ of 0.5 mu m. a significant improvement in linear transconductance is observed when compared to control-silicon p-mosfets ,Technology CAD of SiGe-heterojunction field effect transistors A 2D virtual water fabrication simulation suite has been employed for the technology CAD of SiGe channel heterojunction field effect transistors (HFETs). Complete fabrication process of SiGe p-HFETs has been simulated. The SiGe material parameters and mobility model were incorporated to simulate S/SiGe p-HFETS with a uniform germanium channel having an L/sub eff/ of 0.5 mu m. A significant improvement in linear transconductance is observed when compared to control-silicon p-MOSFETS,"['technology CAD', 'heterojunction field effect transistors', 'SiGe', 'fabrication process', 'material parameters', 'mobility model', 'uniform channel', 'linear transconductance', '0.5 micron', 'carrier mobility', 'field effect transistors', 'Ge-Si alloys', 'semiconductor device models', 'semiconductor materials', 'technology CAD (electronics)']","['effect', 'Complete fabrication process', 'uniform germanium channel', 'field', 'side material parameters', 'side p-HFETs', 'side', 'CAD', 'technology CAD', 'Technology CAD']",474,78,15,473,77,10,6,2,3
a model for choosing an electronic reserves system: a pre-implementation study at the library of long island university's brooklyn campus this study explores the nature of electronic reserves (e-reserves) and investigates the possibilities of implementing the e-reserves at the long island university/brooklyn campus library (liu/bcl) ,‘A model for choosing an electronic reserves system: a pre-implementation study at the library of Long Island University's Brooklyn campus. This study explores the nature of electronic reserves (e-reserves) and investigates the possibilities of implementing the e-reserves at the Long Island University/Brooklyn Campus Library (LIU/BCL),"['electronic reserves system', 'Long Island University Brooklyn Campus Library', 'academic libraries', 'library automation', 'software selection']","['Island', 'electronic reserves system', 'pre-implementation study', 'Long', 'model', 'reserves', 'electronic', 'study', 'system', 'electronic reserves']",290,46,5,292,45,10,1,2,0
"new voice over internet protocol technique with hierarchical data security protection the authors propose a voice over internet protocol (voip) technique with a new hierarchical data security protection (hdsp) scheme. the proposed hdsp scheme can maintain the voice quality degraded from packet loss and preserve high data security. it performs both the data inter-leaving on the inter-frame of voice for achieving better error recovery of voices suffering from continuous packet loss, and the data encryption on the intra-frame of voice for achieving high data security, which are controlled by a random bit-string sequence generated from a chaotic system. to demonstrate the performance of the proposed hdsp scheme, we have successfully verified and analysed the proposed approach through software simulation and statistical measures on several test voices ","New voice over Internet protocol technique with hierarchical data security protection The authors propose a voice over Internet protocol (VoIP) technique with a new hierarchical data security protection (HDSP) scheme. The proposed HDSP scheme can maintain the voice quality degraded from packet loss and preserve high data security. It performs both the data inter-leaving on the inter-frame of voice for achieving better error recovery of voices suffering from continuous packet loss, and the data encryption on the intra-trame of voice for achieving high data security, which are controlled by a random bit-string sequence generated from a chaotic system. To demonstrate the performance of the proposed HDSP scheme, we have successfully verified and analysed the proposed approach through software simulation and statistical measures on several test voices","['voice over Internet protocol', 'hierarchical data security protection', 'VoIP', 'HDSP scheme', 'packet loss', 'high data security', 'data interleaving', 'data encryption', 'random bit-string sequence', 'chaotic system', 'software simulation', 'statistical measures', 'packet voice communications', 'cryptography', 'Internet telephony', 'protocols', 'security of data']","['voice', 'high data security', 'Internet protocol technique', 'continuous packet loss', 'several test voices', 'data encryption', 'voice quality', 'New voice', 'data', 'Internet protocol']",733,127,17,733,126,10,1,1,6
"computing the frequency response of systems affinely depending on uncertain parameters the computation of the frequency response of systems depending affinely on uncertain parameters can be reduced to that of all its one-dimensional edge plants while the image of such an edge plant at a fixed frequency is an arc or a line segment in the complex plane. based on this conclusion, four computational formulas of the maximal and minimal (maxi-mini) magnitudes and phases of an edge plant at a fixed frequency are given. the formulas, besides sharing a simpler form of expression, concretely display how the extrema of the frequency response of the edge plant relate to the typical characteristics of the arc and line segment such as the centre, radius and tangent points of the arc, the distance from the origin to the line segment etc. the direct application of the results is to compute the bode-, nichols- and nyquist-plot collections of the systems which are needed in robustness analysis and design ","Computing the frequency response of systems affinely depending on uncertain parameters The computation of the frequency response of systems depending affinely on uncertain parameters can be reduced to that of all its one-dimensional edge plants while the image of such an edge plant at a fixed frequency is an arc or a line segment in the complex plane. Based on this conclusion, four computational formulas of the maximal and minimal (maxi-mini) magnitudes and phases of an edge plant at a fixed frequency are given. The formulas, besides sharing a simpler form of expression, concretely display how the extrema of the frequency response of the edge plant relate to the typical characteristics of the arc and line segment such as the centre, radius and tangent points of the arc, the distance from the origin to the line segment etc. The direct, application of the results is to compute the Bode-, Nichols- and Nyquist-plot collections of the systems which are needed in robustness analysis and design","['frequency response', 'uncertain parameters', 'affine systems', 'one-dimensional edge plants', 'arc', 'line segment', 'Bode-plot', 'Nichols-plot', 'Nyquist-plot', 'robustness analysis', 'robustness design', 'frequency-domain design methods', 'Bode diagrams', 'frequency response', 'frequency-domain analysis', 'frequency-domain synthesis', 'Nyquist diagrams', 'polynomials', 'robust control', 'uncertain systems']","['frequency response', 'uncertain parameters', 'systems', 'fixed frequency', 'one-dimensional edge plants', 'computational formulas', 'line segment etc', 'frequency', 'edge plant', 'response']",838,165,20,839,164,10,0,1,3
"integrate-and-fire neurons driven by correlated stochastic input neurons are sensitive to correlations among synaptic inputs. however, analytical models that explicitly include correlations are hard to solve analytically, so their influence on a neuron's response has been difficult to ascertain. to gain some intuition on this problem, we studied the firing times of two simple integrate-and-fire model neurons driven by a correlated binary variable that represents the total input current. analytic expressions were obtained for the average firing rate and coefficient of variation (a measure of spike-train variability) as functions of the mean, variance, and correlation time of the stochastic input. the results of computer simulations were in excellent agreement with these expressions. in these models, an increase in correlation time in general produces an increase in both the average firing rate and the variability of the output spike trains. however, the magnitude of the changes depends differentially on the relative values of the input mean and variance: the increase in firing rate is higher when the variance is large relative to the mean, whereas the increase in variability is higher when the variance is relatively small. in addition, the firing rate always tends to a finite limit value as the correlation time increases toward infinity, whereas the coefficient of variation typically diverges. these results suggest that temporal correlations may play a major role in determining the variability as well as the intensity of neuronal spike trains ","Integrate-and-fire neurons driven by correlated stochastic input Neurons are sensitive to correlations among synaptic inputs. However, analytical models that explicitly include correlations are hard to solve analytically, so their influence on a neuron's response has been difficult to ascertain. To gain some intuition on this problem, we studied the firing times of two simple integrate-and-fire model neurons driven by a correlated binary variable that represents the total input current. Analytic expressions were obtained for the average firing rate and coefficient of variation (a measure of spike-train variability) as functions of the mean, variance, and correlation time of the stochastic input. The results of computer simulations were in excellent agreement with these expressions. In these models, an increase in correlation time in general produces an increase in both the average firing rate and the variability of the output spike trains. However, the magnitude of the changes depends differentially on the relative values of the input mean and variance: the increase in firing rate is higher when the variance is large relative to the mean, whereas the increase in variability is higher when the variance is relatively small. In addition, the firing rate always tends to a finite limit value as the correlation time increases toward infinity, whereas the coefficient of variation typically diverges. These results suggest that temporal correlations may play a major role in determining the variability as well as the intensity of neuronal spike trains","['integrate-and-fire neurons', 'correlated stochastic input', 'synaptic input correlations', 'firing times', 'correlated binary variable', 'coefficient of variation', 'spike-train variability', 'computer simulation', 'output spike trains', 'temporal correlations', 'bioelectric potentials', 'neural nets', 'neurophysiology']","['correlation time', 'average firing rate', 'Integrate-and-fire neurons', 'stochastic input Neurons', 'neuronal spike trains', 'neurons response', 'synaptic inputs', 'total input', 'input mean', 'stochastic input']",1332,237,13,1332,236,10,0,0,5
"cyberethics bibliography 2002: a select list of recent works included in the 2002 annual bibliography update is a select list of recent books and conference proceedings that have been published since 2000. also included is a select list of special issues of journals and periodicals that were recently published. for additional lists of recently published books and articles, see ibid. (june 2000, june 2001) ","CyberEthics bibliography 2002: a select list of recent works Included in the 2002 annual bibliography update is a select list of recent books and conference proceedings that have been published since 2000. Also included is a select list of special issues of journals and periodicals that were recently published. For additional lists of recently published books and articles, see ibid. (June 2000, June 2001)","['CyberEthics bibliography', '2002 annual bibliography', 'recent books', 'conference proceedings', 'special issues', 'journals', 'periodicals', 'bibliographies', 'social aspects of automation']","['select list', 'cybernetics bibliography', 'annual bibliography', 'additional lists', 'recent works', 'recent books', 'recent', 'list', 'bibliography', 'select']",345,65,9,345,64,10,0,0,4
"meeting of minds technical specialists need to think about their role in it projects and how they communicate with end-users and other participants to ensure they contribute fully as team members. it is especially important to communicate and document trade-offs that may have to be made, including the rationale behind them, so that if requirements change, the impact and decisions can be readily communicated to the stakeholders ","Meeting of minds Technical specialists need to think about their role in IT projects and how they communicate with end-users and other participants to ensure they contribute fully as team members. It is especially important to. ‘communicate and document trade-offs that may have to be made, including the rationale behind them, so that if requirements change, the impact and decisions can be readily communicated to the stakeholders","['technical specialists', 'IT projects', 'communication', 'end-users', 'DP management', 'professional communication', 'project management']","['minds Technical specialists', 'other participants', 'team members', 'end-users', 'projects', 'role', 'other', 'minds', 'Technical', 'specialists']",364,68,7,366,67,10,10,2,0
"neighborhood operator systems and approximations this paper presents a framework for the study of generalizing the standard notion of equivalence relation in rough set approximation space with various categories of k-step neighborhood systems. based on a binary relation on a finite universe, six families of binary relations are obtained, and the corresponding six classes of k-step neighborhood systems are derived. extensions of pawlak's (1982) rough set approximation operators based on such neighborhood systems are proposed. properties of neighborhood operator systems and rough set approximation operators are investigated, and their connections are examined ","Neighborhood operator systems and approximations This paper presents a framework for the study of generalizing the standard notion of equivalence relation in rough set approximation space with various categories of k-step neighborhood systems. Based on a binary relation on a finite universe, six families of binary relations are obtained, and the corresponding six classes of k-step neighborhood systems are derived. Extensions of Pawlak's (1982) rough set approximation operators based on such neighborhood systems are proposed. Properties of neighborhood operator systems and rough set approximation operators are investigated, and their connections are examined","['neighborhood operator systems', 'equivalence relation', 'rough set approximation space', 'k-step neighborhood systems', 'binary relation', 'finite universe', 'approximation theory', 'mathematical operators', 'rough set theory']","['step neighborhood systems', 'neighborhood', 'systems', 'such neighborhood systems', 'equivalence relation', 'binary relations', 'approximations', 'operator', 'neighborhood operator systems', 'Neighborhood operator systems']",574,93,9,574,92,10,0,0,2
records role in e-business records management standards are now playing a key role in e-business strategy ,Records role in e-business Records management standards are now playing a key role in e-business strategy,"['e-business strategy', 'records management', 'electronic commerce', 'records management']","['business strategy', 'Records role', 'management', 'standards', 'key role', 'role', 'key', 'Records', 'strategy', 'business']",90,17,4,90,16,10,0,0,0
"heuristics for single-pass welding task sequencing welding task sequencing is a prerequisite in the offline programming of robot arc welding. single-pass welding task sequencing can be modelled as a modified travelling salesman problem. owing to the difficulty of the resulting arc-routing problems, effective local search heuristics are developed. computational speed becomes important because robot arc welding is often part of an automated process-planning procedure. generating a reasonable solution in an acceptable time is necessary for effective automated process planning. several different heuristics are proposed for solving the welding task-sequencing problem considering both productivity and the potential for welding distortion. constructive heuristics based on the nearest neighbour concept and tabu search heuristics are developed and enhanced using improvement procedures. the effectiveness of the heuristics developed is tested and verified on actual welded structure problems and random problems ","Heuristics for single-pass welding task sequencing Welding task sequencing is a prerequisite in the offline programming of robot arc welding. Single-pass welding task sequencing can be modelled as a modified travelling salesman problem. Owing to the difficulty of the, resulting arc-routing problems, effective local search heuristics are developed. Computational speed becomes important because robot arc welding is often part of an automated process-planning procedure. Generating a reasonable solution in an acceptable time is necessary for effective automated process planning. Several different heuristics are proposed for solving the welding task-sequencing problem considering both productivity and the potential for welding distortion. Constructive heuristics based on the nearest neighbour concept and tabu search heuristics are developed and enhanced using improvement procedures. The effectiveness of the heuristics developed is tested and verified on actual welded structure problems and random problems","['single-pass welding task sequencing', 'constructive heuristics', 'offline programming', 'robot arc welding', 'modified travelling salesman problem', 'local search heuristics', 'computational speed', 'automated process-planning procedure', 'productivity', 'welding distortion', 'nearest neighbour concept', 'tabu search heuristics', 'random problems', 'welded structure problems', 'arc welding', 'assembly planning', 'computer aided production planning', 'heuristic programming', 'industrial robots', 'robot programming', 'search problems', 'travelling salesman problems']","['welding', 'robot arc welding', 'welding task-sequencing problem', 'Several different heuristics', 'abu search heuristics', 're-routing problems', 'welding distortion', 'salesman problem', 'task', 'sequencing']",879,137,22,880,136,10,0,1,5
"control of a heavy-duty robotic excavator using time delay control with integral sliding surface the control of a robotic excavator is difficult from the standpoint of the following problems: parameter variations in mechanical structures, various nonlinearities in hydraulic actuators and disturbance due to the contact with the ground. in addition, the more the size of robotic excavators increases, the more the length and mass of the excavator links; the more the parameters of a heavy-duty excavator vary. a time-delay control with switching action (tdcsa) using an integral sliding surface is proposed in this paper for the control of a 21-ton robotic excavator. through analysis and experiments, we show that using an integral sliding surface for the switching action of tdcsa is better than using a pd-type sliding surface. the proposed controller is applied to straight-line motions of a 21-ton robotic excavator with a speed level at which skillful operators work. experiments, which were designed for surfaces with various inclinations and over broad ranges of joint motions, show that the proposed controller exhibits good performance ","Control of a heavy-duty robotic excavator using time delay control with integral sliding surface The control of a robotic excavator is difficult from the standpoint of the following problems: parameter variations in mechanical structures, various nonlinearities in hydraulic actuators and disturbance due to the contact with the ground. In addition, the more the size of robotic excavators increases, the more the length and mass of the excavator links; the more the parameters of a heavy-duty excavator vary. A time-delay control with switching action (TDCSA) using an integral sliding surface is proposed in this paper for the control of a 21-ton robotic excavator. Through analysis and experiments, we show that using an integral sliding surface for the switching action of TDCSA is better than using a PD-type sliding surface. The proposed controller is applied to straight-line motions of a 21-ton robotic excavator with a speed level at which skillful operators work. Experiments, which were designed for surfaces with various inclinations and over broad ranges of joint motions, show that the proposed controller exhibits good performance","['time-delay control', 'robust control', 'robotic excavator', 'integral sliding surface', 'motion control', 'trajectory control', 'dynamics', 'tracking', 'pressure control', 'delay systems', 'dynamics', 'excavators', 'industrial robots', 'motion control', 'pressure control', 'robust control', 'tracking', 'variable structure systems']","['excavator', 'control', '21-ton robotic excavator', 'proposed controller', 'robotic', 'heavy-duty robotic excavator', 'heavy-duty excavator', 'time-delay control', 'time delay control', 'robotic excavators']",972,175,18,972,174,10,0,0,1
"robust speech recognition using probabilistic union models this paper introduces a new statistical approach, namely the probabilistic union model, for speech recognition involving partial, unknown frequency-band corruption. partial frequency-band corruption accounts for the effect of a family of real-world noises. previous methods based on the missing feature theory usually require the identity of the noisy bands. this identification can be difficult for unexpected noise with unknown, time-varying band characteristics. the new model combines the local frequency-band information based on the union of random events, to reduce the dependence of the model on information about the noise. this model partially accomplishes the target: offering robustness to partial frequency-band corruption, while requiring no information about the noise. this paper introduces the theory and implementation of the union model, and is focused on several important advances. these new developments include a new algorithm for automatic order selection, a generalization of the modeling principle to accommodate partial feature stream corruption, and a combination of the union model with conventional noise reduction techniques to deal with a mixture of stationary noise and unknown, nonstationary noise. for the evaluation, we used the tidigits database for speaker-independent connected digit recognition. the utterances were corrupted by various types of additive noise, stationary or time-varying, assuming no knowledge about the noise characteristics. the results indicate that the new model offers significantly improved robustness in comparison to other models ","Robust speech recognition using probabilistic union models This paper introduces a new statistical approach, namely the probabilistic union model, for speech recognition involving partial, unknown frequency-band corruption. Partial frequency-band corruption accounts for the effect of a family of real-world noises. Previous methods based ‘on the missing feature theory usually require the identity of the noisy bands. This identification can be difficult for unexpected noise with unknown, time-varying band characteristics. The new model combines the local frequency-band information based on the union of random events, to reduce the dependence of the model on information about the noise. This model partially accomplishes the target: offering robustness to partial frequency-band corruption, while requiring no information about the noise. This paper introduces the theory and implementation of the union model, and is focused on several important advances. These new developments include a new algorithm for automatic order selection, a generalization of the modeling principle to accommodate partial feature stream corruption, and a combination of the union model with conventional noise reduction techniques to deal with a mixture of stationary noise and unknown, nonstationary noise. For the evaluation, we used the TIDIGITS database for speaker-independent connected digit recognition. The utterances were corrupted by various types of additive noise, stationary or time-varying, assuming no knowledge about the noise characteristics. The results indicate that the new model offers significantly improved robustness in comparison to other models","['robust speech recognition', 'probabilistic union models', 'partial real-world noise', 'automatic order selection', 'modeling', 'partial feature stream corruption', 'noise reduction techniques', 'stationary noise', 'nonstationary noise', 'TIDIGITS database', 'speaker-independent connected digit recognition', 'additive noise', 'noise characteristics', 'missing feature theory', 'noisy bands', 'time-varying band characteristics', 'local frequency-band information', 'partial frequency-band corruption', 'noise', 'probability', 'speech recognition']","['model', 'partial frequency-band corruption', 'new model', 'unknown nonstationary noise', 'probabilistic union models', 'Robust speech recognition', 'modelling principle', 'other models', 'union model', 'speech recognition']",1426,230,21,1427,229,10,2,1,6
"impact of aviation highway-in-the-sky displays on pilot situation awareness thirty-six pilots (31 men, 5 women) were tested in a flight simulator on their ability to intercept a pathway depicted on a highway-in-the-sky (hits) display. while intercepting and flying the pathway, pilots were required to watch for traffic outside the cockpit. additionally, pilots were tested on their awareness of speed, altitude, and heading during the flight. results indicated that the presence of a flight guidance cue significantly improved flight path awareness while intercepting the pathway, but significant practice effects suggest that a guidance cue might be unnecessary if pilots are given proper training. the amount of time spent looking outside the cockpit while using the hits display was significantly less than when using conventional aircraft instruments. additionally, awareness of flight information present on the hits display was poor. actual or potential applications of this research include guidance for the development of perspective flight display standards and as a basis for flight training requirements ","Impact of aviation highway-in-the-sky displays on pilot situation awareness Thirty-six pilots (31 men, 5 women) were tested in a flight simulator on their ability to intercept a pathway depicted on a highway-in-the-sky (HITS) display. While intercepting and flying the pathway, pilots were required to watch for traffic outside the cockpit. Additionally, pilots were tested on their awareness of speed, altitude, and heading during the flight. Results indicated that the presence of a fight guidance cue significantly improved flight path awareness while intercepting the pathway, but significant practice effects suggest that a guidance cue might be unnecessary if pilots are given proper training. The amount of time spent looking outside the cockpit while using the HITS display was significantly less than when using conventional aircraft instruments. Additionally, awareness of flight information present on the HITS display was poor. Actual or potential applications of this research include guidance for the development of perspective flight display standards and as a basis for flight training requirements","['flight simulator', 'pilots', 'highway-in-the-sky display', 'cockpit', 'flight guidance', 'human factors', 'situation awareness', 'flight path awareness', 'aircraft display', 'aircraft displays', 'human factors', 'man-machine systems']","['HITS display', 'flight training requirements', 'highway-in-the-sky displays', 'pilot situation awareness', 'highway-in-the-sky hits', 'flight path awareness', 'flight information', 'flight simulator', 'pathway pilots', 'pilots']",954,163,12,953,162,10,4,1,1
recording quantum properties of light in a long-lived atomic spin state: towards quantum memory we report an experiment on mapping a quantum state of light onto the ground state spin of an ensemble of cs atoms with the lifetime of 2 ms. recording of one of the two quadrature phase operators of light is demonstrated with vacuum and squeezed states of light. the sensitivity of the mapping procedure at the level of approximately 1 photon/sec per hz is shown. the results pave the road towards complete (storing both quadrature phase observables) quantum memory for gaussian states of light. the experiment also sheds new light on fundamental limits of sensitivity of the magneto-optical resonance method ,Recording quantum properties of light in a long-lived atomic spin state: towards quantum memory We report an experiment on mapping a quantum state of light onto the ground state spin of an ensemble of Cs atoms with the lifetime of 2 ms. Recording of one of the two quadrature phase operators of light is demonstrated with vacuum and squeezed states of light. The sensitivity of the mapping procedure at the level of approximately 1 photon/sec per Hz is shown. The results pave the road towards complete (storing both quadrature phase observables) quantum memory for Gaussian states of light. The experiment also sheds new light on fundamental limits of sensitivity of the magneto-optical resonance method,"['light quantum properties recording', 'long-lived atomic spin state', 'quantum memory', 'ground state spin', 'ensemble', 'two quadrature phase operators', 'vacuum states', 'squeezed states', 'mapping procedure', 'magnetooptical resonance method', '2 ms', 'Cs', 'ground states', 'magneto-optical effects', 'optical squeezing', 'quantum computing']","['quantum memory', 'state', 'Recording quantum properties', 'quadrature phase operators', 'ground state spin', 'Gaussian states', 'quantum state', 'ms. Recording', 'new light', 'light']",591,115,16,591,114,10,0,0,4
"conceptual modeling and specification generation for b2b business processes based on ebxml in order to support dynamic setup of business processes among independent organizations, a formal standard schema for describing the business processes is basically required. the ebxml framework provides such a specification schema called bpss (business process specification schema) which is available in two standalone representations: a uml version, and an xml version. the former, however, is not intended for the direct creation of business process specifications, but for defining specification elements and their relationships required for creating an ebxml-compliant business process specification. for this reason, it is very important to support conceptual modeling that is well organized and directly matched with major modeling concepts. this paper deals with how to represent and manage b2b business processes using uml-compliant diagrams. the major challenge is to organize uml diagrams in a natural way that is well suited to the business process meta-model and then to transform the diagrams into an xml version. this paper demonstrates the usefulness of conceptually modeling business processes by prototyping a business process editor tool called ebdesigner ","Conceptual modeling and specification generation for B2B business processes based on ebXML In order to support dynamic setup of business processes among independent organizations, a formal standard schema for describing the business processes is basically required. The ebXML framework provides such a specification schema called BPSS (Business Process Specification Schema) which is available in two standalone representations: a UML version, and an XML version. The former, however, is not intended for the direct creation of business process specifications, but for defining specification elements and their relationships required for creating an ebXML-compliant business process specification. For this reason, it is very important to support conceptual modeling that is well organized and directly matched with major modeling concepts. This paper deals with how to represent and manage B2B business processes using UML-compliant diagrams. The major challenge is to organize UML diagrams in a natural way that is well suited to the business process meta-model and then to transform the diagrams into an XML version. This paper demonstrates the usefulness of conceptually modeling business processes by prototyping a business process editor tool called ebDesigner","['B2B business processes', 'ebXML', 'conceptual modeling', 'specification generation', 'formal standard schema', 'Business Process Specification Schema', 'UML-compliant diagrams', 'meta model', 'ebDesigner', 'business process editor', 'electronic commerce', 'formal specification', 'hypermedia markup languages', 'specification languages']","['business', 'bob business processes', 'specification schema', 'Conceptual modelling', 'business process specifications', 'business process meta-model', 'specification generation', 'boss business', 'processes', 'business processes']",1086,182,14,1086,181,10,0,0,3
"it: utilities a look at five utilities to make your pcs more, efficient, effective, and efficacious ","IT: Utilities Alook at five utilities to make your PCs more, efficient, effective, and efficacious","['utilities', 'PCs', 'MobileMessenger', 'Post-it software', 'EasyNotes', 'Print Shop Pro', 'Download Accelerator Plus', 'microcomputers', 'software packages']","['efficacious', 'efficient', 'effective', 'PCs', 'utilities', 'Utilities']",84,17,9,84,15,6,38,13,0
"computational complexity of probabilistic disambiguation recent models of natural language processing employ statistical reasoning for dealing with the ambiguity of formal grammars. in this approach, statistics, concerning the various linguistic phenomena of interest, are gathered from actual linguistic data and used to estimate the probabilities of the various entities that are generated by a given grammar, e.g., derivations, parse-trees and sentences. the extension of grammars with probabilities makes it possible to state ambiguity resolution as a constrained optimization formula, which aims at maximizing the probability of some entity that the grammar generates given the input (e.g., maximum probability parse-tree given some input sentence). the implementation of these optimization formulae in efficient algorithms, however, does not always proceed smoothly. in this paper, we address the computational complexity of ambiguity resolution under various kinds of probabilistic models. we provide proofs that some, frequently occurring problems of ambiguity resolution are np-complete. these problems are encountered in various applications, e.g., language understanding for textand speech-based applications. assuming the common model of computation, this result implies that, for many existing probabilistic models it is not possible to devise tractable algorithms for solving these optimization problems ","Computational complexity of probabilistic disambiguation Recent models of natural language processing employ statistical reasoning for dealing with the ambiguity of formal grammars. In this approach, statistics, concerning the various linguistic phenomena of interest, are gathered from actual linguistic data and used to estimate the probabilities of the various entities that are generated by a given grammar, €.g., derivations, parse-trees and sentences. The extension of grammars with probabilities makes it possible to state ambiguity resolution as a constrained optimization formula, which aims at maximizing the probability of some entity that the grammar generates given the input (¢.g., maximum probability parse-tree given some input sentence). The implementation of these optimization formulae in efficient algorithms, however, does not always proceed smoothly. In this paper, we address the computational complexity of ambiguity resolution under various kinds of probabilistic models. We provide proofs that some, frequently occurring problems of ambiguity resolution are NP-complete. These problems are encountered in various applications, e.g., language understanding for textand speech-based applications. Assuming the common model of computation, this result implies that, for many existing probabilistic models it is not possible to devise tractable algorithms for solving these optimization problems","['natural language processing', 'statistical reasoning', 'formal grammars', 'statistics', 'computational complexity', 'probabilistic disambiguation', 'NP-completeness results', 'parsing problems', 'speech processing', 'state ambiguity resolution', 'constrained optimization formula', 'probabilistic models', 'language understanding', 'computational complexity', 'grammars']","['probabilistic models', 'various linguistic phenomena', 'probabilistic disambiguation', 'state ambiguity resolution', 'various applications', 'Recent models', 'common model', 'ambiguity resolution', 'computational complexity', 'Computational complexity']",1228,191,15,1228,190,10,2,2,7
"dynamics and control of initialized fractional-order systems due to the importance of historical effects in fractional-order systems, this paper presents a general fractional-order system and control theory that includes the time-varying initialization response. previous studies have not properly accounted for these historical effects. the initialization response, along with the forced response, for fractional-order systems is determined. the scalar fractional-order impulse response is determined, and is a generalization of the exponential function. stability properties of fractional-order systems are presented in the complex w-plane, which is a transformation of the s-plane. time responses are discussed with respect to pole positions in the complex w-plane and frequency response behavior is included. a fractional-order vector space representation, which is a generalization of the state space concept, is presented including the initialization response. control methods for vector representations of initialized fractional-order systems are shown. finally, the fractional-order differintegral is generalized to continuous order-distributions which have the possibility of including all fractional orders in a transfer function ","Dynamics and control of initialized fractional-order systems Due to the importance of historical effects in fractional-order systems, this paper presents a general fractional-order system and control theory that includes the time-varying initialization response. Previous studies have not properly accounted for these historical effects. The initialization response, along with the forced response, for fractional-order systems is determined. The scalar fractional-order impulse response is determined, and is a generalization of the exponential function. Stability properties of fractional-order systems are presented in the complex w-plane, which is a transformation of the, s-plane. Time responses are discussed with respect to pole positions in the complex w-plane and frequency response behavior is included. A fractional-order vector space representation, which is a generalization of the state space concept, is presented including the initialization response. Control methods for vector representations of initialized fractional-order systems are shown. Finally, the fractional-order differintegral is generalized to continuous order-distributions which have the possibilty of including all fractional orders in a transfer function","['initialized fractional-order systems', 'dynamics', 'control', 'initialization response', 'forced response', 'impulse response', 'exponential function', 'vector space representation', 'state space concept', 'fractional-order differintegral', 'transfer function', 'differential equations', 'integral equations', 'nonlinear control systems', 'nonlinear dynamical systems', 'numerical stability', 'transfer functions', 'transient response']","['fractional-order', 'initialised fractional-order systems', 'time-varying initialisation response', 'general fractional-order system', 'fractional-order differintegral', 'frequency response behavior', 's-plane Time responses', 'forced response', 'initialisation response', 'fractional-order systems']",1079,162,18,1079,161,10,2,2,6
"oxygen-enhanced mri of the brain blood oxygenation level-dependent (bold) contrast mri is a potential method for a physiological characterization of tissue beyond mere morphological representation. the purpose of this study was to develop evaluation techniques for such examinations using a hyperoxia challenge. administration of pure oxygen was applied to test these techniques, as pure oxygen can be expected to induce relatively small signal intensity (si) changes compared to co/sub 2/-containing gases and thus requires very sensitive evaluation methods. fourteen volunteers were investigated by alternating between breathing 100% o/sub 2/ and normal air, using two different paradigms of administration. changes ranged from >30% in large veins to 1.71%+or-0.14% in basal ganglia and 0.82%+or-0.08% in white matter. to account for a slow physiological response function, a reference for correlation analysis was derived from the venous reaction. an objective method is presented that allows the adaptation of the significance threshold to the complexity of the paradigm used. reference signal characteristics in representative brain tissue regions were established. as the presented evaluation scheme proved its applicability to small si changes induced by pure oxygen, it can readily be used for similar experiments with other gases ","Oxygen-enhanced MRI of the brain Blood oxygenation level-dependent (BOLD) contrast MRI is a potential method for a physiological characterization of tissue beyond mere morphological representation. The purpose of this study was to develop evaluation techniques for such examinations using a hyperoxia challenge. Administration of pure oxygen was applied to test these techniques, as pure oxygen can be expected to induce relatively small signal intensity (SI) changes compared to CO/sub 2/-containing gases and thus requires very sensitive evaluation methods. Fourteen volunteers were investigated by alternating between breathing 100% O/sub 2/ and normal air, using two different paradigms of administration. Changes ranged from >30% in large veins to 1.71%+or-0.14% in basal ganglia and 0.82%+0r-0.08% in white matter. To account for a slow physiological response function, a reference for correlation analysis was derived from the venous reaction. An objective method is presented that allows the adaptation of the significance threshold to the complexity of the paradigm used. Reference signal characteristics in representative brain tissue regions were established. As the presented evaluation scheme proved its applicability to small SI changes induced by pure oxygen, it can readily be used for similar experiments with other gases","['BOLD contrast MRI', 'oxygen-enhanced MRI', 'hyperoxia', 'brain', 'oxygen breathing', 'normal air breathing', 'physiological response function', 'correlation analysis', 'venous reaction', 'significance threshold', 'paradigm complexity', 'MRI contrast agent', 'functional imaging', 'Fourier transform Analysis', 'biomedical MRI', 'brain', 'haemodynamics', 'medical image processing']","['pure oxygen', 'sensitive evaluation methods', 'brain Blood oxygenation', 'small signal intensity', 'evaluation techniques', 'oxygen', 'Oxygen-enhanced MRI', 'potential method', 'MRI', 'brain']",1148,192,18,1148,191,10,1,1,5
"geometrically invariant watermarking using feature points this paper presents a new approach for watermarking of digital images providing robustness to geometrical distortions. the weaknesses of classical watermarking methods to geometrical distortions are outlined first. geometrical distortions can be decomposed into two classes: global transformations such as rotations and translations and local transformations such as the stirmark attack. an overview of existing self-synchronizing schemes is then presented. theses schemes can use periodical properties of the mark, invariant properties of transforms, template insertion, or information provided by the original image to counter geometrical distortions. thereafter, a new class of watermarking schemes using the image content is presented. we propose an embedding and detection scheme where the mark is bound with a content descriptor defined by salient points. three different types of feature points are studied and their robustness to geometrical transformations is evaluated to develop an enhanced detector. the embedding of the signature is done by extracting feature points of the image and performing a delaunay tessellation on the set of points. the mark is embedded using a classical additive scheme inside each triangle of the tessellation. the detection is done using correlation properties on the different triangles. the performance of the presented scheme is evaluated after jpeg compression, geometrical attack and transformations. results show that the fact that the scheme is robust to these different manipulations. finally, in our concluding remarks, we analyze the different perspectives of such content-based watermarking scheme ","Geometrically invariant watermarking using feature points This paper presents a new approach for watermarking of digital images providing robustness to geometrical distortions. The weaknesses of classical watermarking methods to geometrical distortions are outlined first. Geometrical distortions can be decomposed into two classes: global transformations such as rotations and translations and local transformations such as the StirMark attack. An overview of existing self-synchronizing schemes is then presented. Theses schemes can use periodical properties of the mark, invariant properties of transforms, template insertion, or information provided by the original image to counter geometrical distortions. Thereafter, a new class of watermarking schemes using the image content is presented. We propose an embedding and detection scheme where the mark is bound with a content descriptor defined by salient points. Three different types of feature points are studied and their robustness to geometrical transformations is evaluated to develop an enhanced detector. The ‘embedding of the signature is done by extracting feature points of the, image and performing a Delaunay tessellation on the set of points. The mark is embedded using a classical additive scheme inside each triangle of the tessellation. The detection is done using correlation properties on the different triangles. The performance of the presented scheme is evaluated after JPEG compression, geometrical attack and transformations. Results show that the fact that the scheme is robust, to these different manipulations. Finally, in our concluding remarks, we analyze the different perspectives of such content-based watermarking scheme","['geometrically invariant watermarking', 'feature points', 'digital images', 'geometrical distortions', 'global transformations', 'rotations', 'translations', 'local transformations', 'StirMark attack', 'self-synchronizing schemes', 'periodical properties', 'invariant properties', 'transforms', 'template insertion', 'image content', 'embedding', 'detection scheme', 'content descriptor', 'feature extraction', 'Delaunay tessellation', 'additive scheme', 'correlation properties', 'JPEG compression', 'geometrical attack', 'copy protection', 'data encapsulation', 'feature extraction', 'image coding', 'image recognition', 'mesh generation', 'transforms']","['geometrical distortions', 'feature points', 'classical watermarking methods', 'geometrical transformations', 'self-synchronizing schemes', 'classical additive scheme', 'invariant watermarking', 'invariant properties', 'detection scheme', 'Theses schemes']",1468,241,31,1471,240,10,8,3,12
"development of visual design steering as an aid in large-scale multidisciplinary design optimization. i. method development a modified paradigm of computational steering (cs), termed visual design steering (vds), is developed in this paper. the vds paradigm is applied to optimal design problems to provide a means for capturing and enabling designer insights. vds allows a designer to make decisions before, during or after an analysis or optimization via a visual environment, in order to effectively steer the solution process. the objective of vds is to obtain a better solution in less time through the use of designer knowledge and expertise. using visual representations of complex systems in this manner enables human experience and judgement to be incorporated into the optimal design process at appropriate steps, rather than having traditional black box solvers return solutions from a prescribed input set. part i of this paper focuses on the research issues pertaining to the graph morphing visualization method created to represent an n-dimensional optimization problem using 2-dimensional and 3-dimensional visualizations. part ii investigates the implementation of the vds paradigm, using the graph morphing approach, to improve an optimal design process. specifically, the following issues are addressed: impact of design variable changes on the optimal design space; identification of possible constraint redundancies; impact of constraint tolerances on the optimal solution: and smoothness of the objective function contours. it is demonstrated that graph morphing can effectively reduce the complexity and computational time associated with some optimization problems ","Development of visual design steering as an aid in large-scale multidisciplinary design optimization. |. Method development A modified paradigm of computational steering (CS), termed visual design steering (VDS), is developed in this paper. The VDS paradigm is applied to optimal design problems to provide a means for capturing and enabling designer insights. VDS allows a designer to make decisions before, during or after an analysis or optimization via a visual environment, in order to effectively steer the solution process. The objective of VDS is to obtain a better solution in less time through the use of designer knowledge and expertise. Using visual representations of complex systems in this manner enables human experience and judgement to be incorporated into the optimal design process at appropriate steps, rather than having traditional black box solvers return solutions from a prescribed input set. Part | of this paper focuses on the research issues pertaining to the Graph Morphing visualization method created to represent an n-dimensional optimization problem using 2-dimensional and 3-dimensional visualizations. Part Il investigates the implementation of the VDS paradigm, using the graph morphing approach, to improve an optimal design process. Specifically, the following issues are addressed: impact of design variable changes ‘on the optimal design space; identification of possible constraint redundancies; impact of constraint tolerances on the optimal solution: and smoothness of the objective function contours. It is demonstrated that graph morphing can effectively reduce the complexity and computational time associated with some optimization problems","['visual design steering', 'large-scale multidisciplinary design optimization', 'computational steering', 'optimal design problems', 'designer decision making', 'visual representations', 'complex systems', 'graph morphing visualization method', '3D visualizations', '2D visualizations', 'n-dimensional optimization', 'computational time', 'complexity', 'design variable changes', 'constraint redundancies', 'constraint tolerances', 'objective function contour smoothness', 'CAD', 'computational complexity', 'data visualisation', 'engineering graphics', 'graph theory', 'image morphing', 'optimisation']","['design', 'visual design steering', 'optimal design process', 'n-dimensional optimization problem', 'optimal design problems', 'optimization problems', 'designer insights ads', 'visual environment', 'designer knowledge', 'optimal solution']",1445,244,24,1446,243,10,5,4,6
"pool halls, chips, and war games: women in the culture of computing computers are becoming ubiquitous in our society and they offer superb opportunities for people in jobs and everyday life. but there is a noticeable sex difference in use of computers among children. this article asks why computers are more attractive to boys than to girls and offers a cultural framework for explaining the apparent sex differences. although the data are fragmentary, the world of computing seems to be more consistent with male adolescent culture than with feminine values and goals. furthermore, both arcade and educational software is designed with boys in mind. these observations lead us to speculate that computing is neither inherently difficult nor uninteresting to girls, but rather that computer games and other software might have to be designed differently for girls. programs to help teachers instill computer efficacy in all children also need to be developed ","Pool halls, chips, and war games: women in the culture of computing Computers are becoming ubiquitous in our society and they offer superb ‘opportunities for people in jobs and everyday life. But there is a noticeable sex difference in use of computers among children. This article asks why computers are more attractive to boys than to girls and offers a cultural framework for explaining the apparent sex differences. Although the data are fragmentary, the world of computing seems to be more consistent with male adolescent culture than with feminine values and goals. Furthermore, both arcade and educational software is designed with boys in mind. These observations lead us to speculate that computing is neither inherently difficult nor uninteresting to girls, but rather that computer games and other software might have to be designed differently for girls. Programs to help teachers instill computer efficacy in all children also need to be developed","['women', 'culture of computing', 'sex difference', 'children', 'male adolescent culture', 'educational software', 'computer games', 'teachers', 'computer games', 'computer literacy', 'educational computing', 'gender issues', 'social aspects of automation', 'teaching']","['computers', 'noticeable sex difference', 'male adolescent culture', 'cultural framework', 'computer efficacy', 'Pool halls chips', 'war games women', 'computer games', 'war', 'Pool']",809,152,14,810,151,10,12,1,3
finally! some sensible european legislation on software the european commission has formally tabled a draft directive on the protection by patents of computer-implemented inventions. the aim of this very important directive is to harmonise national patent laws relating to inventions using software. it follows an extensive consultation launched by the commission in october 2000. the impetus behind the directive was the recognition at eu level of a total lack of unity between the european patent office and european national courts in deciding what was or was not deemed patentable when it came to the subject of computer programs ,Finally! some sensible European legislation on software The European Commission has formally tabled a draft Directive on the Protection by Patents of Computer-Implemented Inventions. The aim of this very important Directive is to harmonise national patent laws relating to inventions using software. It follows an extensive consultation launched by the Commission in October 2000. The impetus behind the Directive was the recognition at EU level of a total lack of unity between the European Patent Office and European national courts in deciding what was or was not deemed patentable when it came to the subject of computer programs,"['European Commission', 'Directive on the Protection by Patents of Computer-Implemented Inventions', 'national patent laws', 'law harmonisation', 'EU', 'European Patent Office', 'national courts', 'computer programs', 'computer software', 'government policies', 'legislation', 'patents']","['software', 'European', 'sensible European legislation', 'European national courts', 'European Patent Office', 'national patent laws', 'important Directive', 'European Commission', 'draft Directive', 'Directive']",536,99,12,536,98,10,0,0,3
"examining children's reading performance and preference for different computer-displayed text this study investigated how common online text affects reading performance of elementary school-age children by examining the actual and perceived readability of four computer-displayed typefaces at 12- and 14-point sizes. twenty-seven children, ages 9 to 11, were asked to read eight children's passages and identify erroneous/substituted words while reading. comic sans ms, arial and times new roman typefaces, regardless of size, were found to be more readable (as measured by a reading efficiency score) than courier new. no differences in reading speed were found for any of the typeface combinations. in general, the 14-point size and the examined sans serif typefaces were perceived as being the easiest to read, fastest, most attractive, and most desirable for school-related material. in addition, participants significantly preferred comic sans ms and 14-point arial to 12-point courier. recommendations for appropriate typeface combinations for children reading on computers are discussed ","Examining children's reading performance and preference for different computer-displayed text This study investigated how common online text affects reading performance of elementary school-age children by examining the actual and perceived readability of four computer-displayed typefaces at 12- and 14-point sizes. Twenty-seven children, ages 9 to 11, were asked to read eight children's passages and identify erroneous/substituted words while reading. Comic Sans MS, Arial and Times New Roman typefaces, regardless of size, were found to be more readable (as measured by a reading efficiency score) than Courier New. No differences in reading speed were found for any of the typeface combinations. In general, the 14-point size and the examined sans serif typefaces were perceived as being the easiest to read, fastest, most attractive, and most desirable for school-related material. In addition, participants significantly preferred Comic Sans MS and 14-point Arial to 12-point Courier. Recommendations for appropriate typeface combinations for children reading on computers are discussed","['child reading performance', 'computer-displayed text', 'online text', 'elementary school-age children', 'computer-displayed typefaces', 'fonts', 'user interface', 'human factors', 'educational computing', 'character sets', 'educational computing', 'human factors', 'text analysis', 'user interfaces']","['different computer-displayed text', 'appropriate typeface combinations', 'elementary school-age children', 'computer-displayed typefaces', 'common online text', 'childrens passages', '14-point sizes', 'children ages', 'childrens', 'typeface combinations']",940,155,14,940,154,10,0,0,0
"lan-based building maintenance and surveillance robot the building and construction industry is the major industry of hong kong as in many developed countries around the world. after the commissioning of a high-rise building or a large estate, substantial manpower, both inside the management centre under a standby manner, as well as surveillance for security purposes around the whole building, is required for daily operation to ensure a quality environment for the occupants. if the surveillance job can be done by robots, the efficiency can be highly enhanced, resulting in a great saving of manpower and the improved safety of the management staff as a by-product. furthermore, if the robot can retrieve commands from the building management system via a local area network (lan), further savings in manpower can be achieved in terms of first-line fault attendance by human management staff. this paper describes the development of a robot prototype here in hong kong, which can handle some daily routine maintenance works and surveillance responsibilities. the hardware structure of the robot and its on-board devices are described. real-time images captured by a camera on the robot with pan/tilt/zoom functions can be transmitted back to the central management office via a local area network. the interface between the robot and the building automation system (bas) of the building is discussed. this is the first key achievement of this project with a strong implication on reducing the number of human staff to manage a modem building. teleoperation of the robot via the internet or intranet is also possible, which is the second achievement of this project. finally, the robot can identify its physical position inside the building by a landmark recognition method based on standard cad drawings, which is the third achievement of this project. the main goal of this paper is not the description of some groundbreaking technology in robotic development. it is mainly intended to convince building designers and managers to incorporate robotic systems when they are managing modem buildings to save manpower and improve efficiency ","LAN-based building maintenance and surveillance robot The building and construction industry is the major industry of Hong Kong as in many developed countries around the world. After the commissioning of a high-rise building or a large estate, substantial manpower, both inside the management centre under a standby manner, as well as surveillance for security purposes around the whole building, is required for daily operation to ensure a quality environment for the occupants. If the surveillance job can be done by robots, the efficiency can be highly enhanced, resulting in a great saving of manpower and the improved safety of the management staff as a by-product. Furthermore, if the robot can retrieve commands from the building management system via a local area network (LAN), further savings in manpower can be achieved in terms of first-line fault attendance by human management staff. This paper describes the development of a robot prototype here in Hong Kong, which can handle some daily routine maintenance works and surveillance responsibilities. The hardware structure of the robot and its on-board devices are described. Real-time images captured by a camera on the robot with panstit’zoom functions can be transmitted back to the central management office via a local area network. The interface between the robot and the building automation system (BAS) of the building is discussed. This is the first key achievement of this project with a strong implication on reducing the number of human staff to manage a modem building. Teleoperation of the robot via the Internet or intranet is also possible, which is the second achievement of this project. Finally, the robot can identity its physical position inside the building by a landmark recognition method based on standard CAD drawings, which is the third achievement of this project. The main goal of this paper is not the description of some groundbreaking technology in robotic development. It is mainly intended to convince building designers and managers to incorporate robotic systems when they are managing modem buildings to save manpower and improve efficiency","['LAN-based building maintenance and surveillance robot', 'high-rise building', 'security purposes', 'building management system', 'local area network', 'first-line fault attendance', 'hardware structure', 'pan/tilt/zoom functions', 'teleoperation', 'landmark recognition method', 'building management systems', 'local area networks', 'mobile robots', 'security', 'surveillance']","['building', 'land-based building maintenance', 'modem building Teleoperation', 'building management system', 'robotic development', 'surveillance robot', 'high-rise building', 'building designers', 'modem buildings', 'whole building']",1808,336,15,1807,335,10,7,2,7
how to avoid merger pitfalls paul diamond of consultancy kpmg explains why careful it asset management is crucial to the success of mergers ,How to avoid merger pitfalls Paul Diamond of consultancy KPMG explains why careful IT asset management is crucial to the success of mergers,"['consultancy', 'KPMG', 'IT asset management', 'mergers', 'DP management']","['consultancy KPMG', 'asset management', 'pitfalls', 'mergers', 'careful', 'Diamond', 'Paul', 'KPMG', 'management', 'consultancy']",117,24,5,117,23,10,0,0,0
"a winning combination [wireless health care] three years ago, the institute of medicine (iom) reported that medical errors result in at least 44,000 deaths each year-more than deaths from highway accidents, breast cancer or aids. that report, and others which placed serious errors as high as 98,000 annually, served as a wake-up call for healthcare providers such as the caregroup healthcare system inc., a boston-area healthcare network that is the second largest integrated delivery system in the northeastern united states. with annual revenues of $1.2b, caregroup provides primary care and specialty services to more than 1,000,000 patients. caregroup combined wireless technology with the web to create a provider order entry (poe) system designed to reduce the frequency of costly medical mistakes. the poe infrastructure includes intersystems corporation's cache database, dell computer c600 laptops and cisco systems' aironet 350 wireless networks ","winning combination [wireless health care] Three years ago, the Institute of Medicine (IOM) reported that medical errors result in at least 44,000 deaths each year-more than deaths from highway accidents, breast cancer or AIDS. That report, and others which placed serious errors as high as 98,000 annually, served as a wake-up call for healthcare providers such as the CareGroup Healthcare System Inc., a Boston-area healthcare network that is the second largest integrated delivery system in the northeastern United States. With annual revenues of $1.2B, CareGroup provides primary care and specialty services to more than 1,000,000 patients. CareGroup combined wireless technology with the Web to create a provider order entry (POE) system designed to reduce the frequency of costly medical mistakes. The POE infrastructure includes InterSystems Corporation's CACHE database, Dell Computer C600 laptops and Cisco Systems’ Aironet 350 wireless networks","['CareGroup Healthcare System', 'healthcare network', 'wireless', 'medical errors', 'provider order entry', 'InterSystems Corporation CACHE database', 'Cisco Systems Aironet 350 wireless networks', 'Dell Computer C600 laptops', 'health care', 'mobile computing']","['costly medical mistakes', 'wireless health care', 'wireless technology', 'wireless networks', 'medical errors', 'primary care', 'Medicine iom', 'combination', 'wireless', 'care']",817,141,10,816,139,10,566,139,4
"source/channel coding of still images using lapped transforms and block classification a novel scheme for joint source/channel coding of still images is proposed. by using efficient lapped transforms, channel-optimised robust quantisers and classification methods it is shown that significant improvements over traditional source/channel coding of images can be obtained while keeping the complexity low ","Source/channel coding of stil images using lapped transforms and block classification Anovel scheme for joint source/channel coding of still mages is proposed. By using efficient lapped transforms, channel-optimised robust quantisers and classification methods it is shown that significant improvements over traditional source/channel coding of images can be obtained while keeping the complexity low","['joint source-channel coding', 'still images', 'lapped transforms', 'block classification', 'image coding', 'channel-optimised robust quantisers', 'low complexity', 'combined source-channel coding', 'image classification', 'image coding', 'transform coding', 'transforms']","['images', 'channel-optimised robust quantifiers', 'traditional source/channel coding', 'joint source/channel coding', 'classification methods', 'novel scheme', 'Source/channel coding', 'source/channel', 'coding', 'classification']",350,55,12,348,53,10,156,43,2
"sliding mode control of chaos in the cubic chua's circuit system in this paper, a sliding mode controller is applied to control the cubic chua's circuit system. the sliding surface of this paper used is one dimension higher than the traditional surface and guarantees its passage through the initial states of the controlled system. therefore, using the characteristic of this sliding mode we aim to design a controller that can meet the desired specification and use less control energy by comparing with the result in the current existing literature. the results show that the proposed controller can steer chua's circuit system to the desired state without the chattering phenomenon and abrupt state change ","Sliding mode control of chaos in the cubic Chua's circuit system In this paper, a sliding mode controller is applied to control the cubic Chua's circuit system. The sliding surface of this paper used is one dimension higher than the traditional surface and guarantees its passage through the initial states of the controlled system. Therefore, using the characteristic of this sliding mode we aim to design a controller that can meet the desired specification and use less control energy by comparing with the result in the current existing literature. The results show that the proposed controller can steer Chua's circuit system to the desired state without the chattering phenomenon and abrupt state change","['sliding mode control', 'chaos', 'cubic Chua circuit system', 'sliding surface', 'chattering', 'state change', 'match disturbance', 'mismatch disturbance', 'bifurcation', 'chaos', ""Chua's circuit"", 'nonlinear control systems', 'nonlinear dynamical systems', 'state feedback', 'variable structure systems']","['chaos circuit system', 'mode control', 'proposed controller', 'less control energy', 'controlled system', 'system', 'controller', 'mode', 'chaos', 'circuit']",597,114,15,597,113,10,0,0,3
"approximation theory of fuzzy systems based upon genuine many-valued implications - mimo cases it is constructively proved that the multi-input-multi-output fuzzy systems based upon genuine many-valued implications are universal approximators (they are called boolean type fuzzy systems in this paper). the general approach to construct such fuzzy systems is given, that is, through the partition of the output region (by the given accuracy). two examples are provided to demonstrate the way in which fuzzy systems are designed to approximate given functions with a given required approximation accuracy ","Approximation theory of fuzzy systems based upon genuine many-valued implications - MIMO cases Itis constructively proved that the multi-input-multi-output fuzzy systems based upon genuine many-valued implications are universal approximators (they are called Boolean type fuzzy systems in this paper). The general approach to construct such fuzzy systems is given, that is, through the Partition of the output region (by the given accuracy). Two examples are provided to demonstrate the way in which fuzzy systems are designed to approximate given functions with a given required approximation accuracy","['multi-input-multi-output fuzzy systems', 'Boolean type fuzzy systems', 'fuzzy systems', 'many-valued implication', 'universal approximator', 'approximation theory', 'fuzzy systems', 'MIMO systems']","['genuine many-valued implications', 'fuzzy systems', 'multi-input-multi-output fuzzy systems', 'universal approximations', 'approximation accuracy', 'Approximation theory', 'such fuzzy systems', 'systems', 'fuzzy', 'genuine']",517,88,8,517,86,10,257,73,0
"power electronics spark new simulation challenges this article discusses some of the changes that have taken place in power systems and explores some of the inherent requirements for simulation technologies in order to keep up with this rapidly changing environment. the authors describe how energy utilities are realizing that, with the appropriate tools, they can train and sustain engineers who can maintain a great insight into system dynamics ","Power electronics spark new simulation challenges This article discusses some of the changes that have taken place in power systems and explores some of the inherent requirements for simulation technologies in order to keep up with this rapidly changing environment. The authors describe how energy utilities are realizing that, with the appropriate tools, they can train and sustain engineers who can maintain a great insight into system dynamics","['power system computer simulation', 'power electronics', 'simulation challenges', 'simulation technologies', 'electric utilities', 'circuit simulation', 'electricity supply industry', 'power electronics', 'power system simulation']","['new simulation challenges', 'simulation technologies', 'inherent requirements', 'Power electronics', 'system dynamics', 'power systems', 'simulation', 'Power', 'new', 'electronics']",380,69,9,380,68,10,0,0,0
"contrast sensitivity in a dynamic environment: effects of target conditions and visual impairment contrast sensitivity was determined as a function of target velocity (0 degrees -120 degrees /s) over a variety of viewing conditions. in experiment 1, measurements of dynamic contrast sensitivity were determined for observers as a function of target velocity for letter stimuli. significant main effects were found for target velocity, target size, and target duration, but significant interactions among the variables indicated especially pronounced adverse effects of increasing target velocity for small targets and brief durations. in experiment 2, the effects of simulated cataracts were determined. although the simulated impairment had no effect on traditional acuity scores, dynamic contrast sensitivity was markedly reduced. results are discussed in terms of dynamic contrast sensitivity as a useful composite measure of visual functioning that may provide a better overall picture of an individual's visual functioning than does traditional static acuity, dynamic acuity, or contrast sensitivity alone. the measure of dynamic contrast sensitivity may increase understanding of the practical effects of various conditions, such as aging or disease, on the visual system, or it may allow improved prediction of individuals' performance in visually dynamic situations ","Contrast sensitivity in a dynamic environment: effects of target conditions and visual impairment Contrast sensitivity was determined as a function of target velocity (0 degrees -120 degrees /s) over a variety of viewing conditions. in Experiment 1, measurements of dynamic contrast sensitivity were determined for observers as a function of target velocity for letter stimuli Significant main effects were found for target velocity, target size, and target duration, but significant interactions among the variables indicated especially pronounced adverse effects of increasing target velocity for small targets and brief durations. In Experiment 2, the effects of simulated cataracts were determined. Although the simulated impairment had no effect on traditional acuity scores, dynamic contrast sensitivity was markedly reduced. Results are discussed in terms of ‘dynamic contrast sensitivity as a useful composite measure of visual functioning that may provide a better overall picture of an individual's visual functioning than does traditional static acuity, ‘dynamic acuity, or contrast sensitivity alone. The measure of dynamic contrast sensitivity may increase understanding of the practical effects of various conditions, such as aging or disease, on the visual system, or it may allow improved prediction of individuals' performance in visually dynamic situations","['contrast sensitivity', 'dynamic environment', 'target conditions', 'visual impairment', 'dynamic contrast sensitivity', 'target velocity', 'target size', 'target duration', 'acuity scores', 'aging', 'disease', 'human factors', 'vision defects', 'visual perception']","['dynamic contrast sensitivity', 'target velocity', 'dynamic environment effects', 'target conditions', 'target duration', 'dynamic acuity', 'small targets', 'sensitivity', 'contrast sensitivity', 'Contrast sensitivity']",1180,195,14,1181,194,10,14,3,3
"generalized spatio-chromatic diffusion a framework for diffusion of color images is presented. the method is based on the theory of thermodynamics of irreversible transformations which provides a suitable basis for designing correlations between the different color channels. more precisely, we derive an equation for color evolution which comprises a purely spatial diffusive term and a nonlinear term that depends on the interactions among color channels over space. we apply the proposed equation to images represented in several color spaces, such as rgb, cielab, opponent colors, and ihs ","Generalized spatio-chromatic diffusion A framework for diffusion of color images is presented. The method is based on the theory of thermodynamics of irreversible transformations which provides a suitable basis for designing correlations between the different color channels. More precisely, we derive an equation for color evolution which comprises a purely spatial diffusive term and a nonlinear term that depends on the interactions among color channels ‘over space. We apply the proposed equation to images represented in several color spaces, such as RGB, CIELAB, Opponent colors, and IHS","['generalized spatio-chromatic diffusion', 'color images', 'diffusion', 'thermodynamics', 'irreversible transformations', 'color channels', 'color evolution', 'spatial diffusive term', 'nonlinear term', 'vector-valued diffusion', 'scale-space', 'RGB', 'CIELAB', 'Opponent colors', 'IHS', 'diffusion', 'image colour analysis', 'image representation', 'thermodynamics']","['color', 'spatio-chromatic diffusion', 'different color channels', 'spatial diffusive term', 'several color spaces', 'color evolution', 'Opponent colors', 'color images', 'diffusion', 'color channels']",506,88,19,507,87,10,4,1,6
"edit distance of run-length encoded strings let x and y be two run-length encoded strings, of encoded lengths k and l, respectively. we present a simple o(|x|l+|y|k) time algorithm that computes their edit distance ","Edit distance of run-length encoded strings Let X and Y be two run-length encoded strings, of encoded lengths k and |, respectively. We present a simple O(|X|I+[YIk) time algorithm that computes their edit distance","['run-length encoded strings', 'encoded lengths', 'algorithm', 'edit distance', 'computation time', 'computational complexity', 'string matching']","['edit distance', 'run-length', 'strings', 'time algorithm', 'lengths k', 'simple O', 'Y', 'X', 'time', 'distance']",181,35,7,181,34,10,4,2,0
"reply to carreira-perpinan and goodhill [mathematics in biology] in a paper by carreira-perpinan and goodhill (see ibid., vol.14, no.7, p.1545-60, 2002) the authors apply mathematical arguments to biology. swindale et al. think it is inappropriate to apply the standards of proof required in mathematics to the acceptance or rejection of scientific hypotheses. to give some examples, showing that data are well described by a linear model does not rule out an infinity of other possible models that might give better descriptions of the data. proving in a mathematical sense that the linear model was correct would require ruling out all other possible models, a hopeless task. similarly, to demonstrate that two dna samples come from the same individual, it is sufficient to show a match between only a few regions of the genome, even though there remains a very large number of additional comparisons that could be done, any one of which might potentially disprove the match. this is unacceptable in mathematics, but in the real world, it is a perfectly reasonable basis for belief ","Reply to Carreira-Perpinan and Goodhill [mathematics in biology] Ina paper by Carreira-Perpinan and Goodhill (see ibid., vol.14, no.7, p.1545-60, 2002) the authors apply mathematical arguments to biology. Swindale et al. think itis inappropriate to apply the standards of proof required in mathematics to the acceptance or rejection of scientific hypotheses. To give some examples, showing that data are well described by a linear model does not rule out an infinity of other possible models that might give better descriptions of the data Proving in a mathematical sense that the linear model was correct would require ruling out all other possible models, a hopeless task. Similarly, to demonstrate that two DNA samples come from the same individual, itis sufficient to show a match between only a few regions of the genome, even though there remains a very large number of additional comparisons that could be done, any one of which might potentially disprove the match. This is unacceptable in mathematics, but in the real world, it is a perfectly reasonable basis for belief","['mathematical arguments', 'biology', 'scientific hypotheses', 'linear model', 'DNA', 'genome', 'hypothesis testing', 'cortical maps', 'neural nets', 'biology', 'neural nets']","['other possible models', 'Carreira-Perpinan', 'mathematical arguments', 'goodwill mathematics', 'mathematical sense', 'biology Ina paper', 'biology swindle', 'mathematical', 'biology', 'goodwill']",909,176,11,908,172,10,504,163,4
"a dataflow computer which accelerates execution of sequential programs by precedent firing instructions in the dataflow machine, it is important to avoid degradation of performance in sequential processing, and it is important from the viewpoint of hardware scale to reduce the number of waiting operands. this paper demonstrates that processing performance is degraded by sequential processing in the switching process, and presents a method of remedy. precedent firing control is proposed as a means of remedy, and it is shown by a simulation that the execution time and the total number of waiting operands can be reduced by the precedent firing control. then the hardware scale is examined as an evaluation of precedent firing control ","A dataflow computer which accelerates execution of sequential programs by precedent firing instructions In the dataflow machine, it is important to avoid degradation of performance in sequential processing, and it is important from the viewpoint of hardware scale to reduce the number of waiting operands. This paper demonstrates that processing performance is degraded by sequential processing in the switching process, and presents a method of remedy. Precedent firing control is proposed as a means of remedy, and it is shown by a simulation that the execution time and the total number of waiting operands can be reduced by the precedent firing control. Then the hardware scale is examined as an evaluation of precedent firing control","['dataflow computer', 'execution acceleration', 'sequential programs', 'precedent firing instructions', 'precedent firing control', 'execution time', 'waiting operands', 'hardware scale', 'parallel processing', 'computer architecture', 'processing performance', 'switching process', 'data flow computing', 'parallel architectures', 'performance evaluation']","['precedent firing control', 'sequential processing', 'precedent firing instructions', 'sequential programs', 'switching process', 'dataflow computer', 'dataflow machine', 'sequential', 'firing', 'dataflow']",624,116,15,624,115,10,0,0,3
server safeguards tax service peterborough-based tax consultancy ie taxguard wanted real-time failover protection for important windows-based applications. its solution was to implement a powerful failover server from uk supplier neverfail in order to provide real-time backup for three core production servers ,Server safeguards tax service Peterborough-based tax consultancy IE Taxguard wanted real-time failover protection for important Windows-based applications. Its solution was to implement a powerful failover server from UK supplier Neverfail in order to provide real-time backup for three core production servers,"['tax consultancy', 'IE Taxguard', 'failover server', 'Neverfail', 'backup', 'back-up procedures', 'network servers', 'system recovery', 'tax preparation']","['Peterborough-based tax consultancy', 'real-time allover protection', 'powerful allover server', 'real-time backup', 'IE vanguard', 'safeguards', 'service', 'tax', 'Server', 'Peterborough-based']",270,42,9,270,41,10,0,0,1
"application of normal possibility decision rule to silence the paper presents the way of combining two decision problems concerning a single (or a common) dimension, so that an effective fuzzy decision rule can be obtained. normality of the possibility distribution is assumed, leading to possibility of fusing the respective functions related to the two decision problems and their characteristics (decisions, states of nature, utility functions, etc.). the approach proposed can be applied in cases when the statement of the problem requires making of more refined distinctions rather than considering simply a bi-criterion or bi-utility two-decision problem ","Application of normal possibility decision rule to silence The paper presents the way of combining two decision problems concerning a single (or a common) dimension, so that an effective fuzzy decision rule can be obtained. Normality of the possibility distribution is assumed, leading to possibility of fusing the respective functions related to the two decision problems and their characteristics (decisions, states of nature, utlity functions, etc.). The approach proposed can be applied in cases when the statement of the problem requires making of more refined distinctions rather than considering simply a bi-criterion or bi-utilty two-decision problem","['normal possibility decision rule', 'silence', 'conflicting objectives', 'conflicting utilities', 'cool head', 'warm heart', 'decision problems', 'two-dimensional fuzzy events', 'decision theory', 'fuzzy set theory', 'possibility theory']","['decision problems', 'decision', 'characteristics decisions states', 'bi-utilty co-decision problem', 'possibility distribution', 'respective functions', 'rule', 'Application', 'problem', 'possibility']",565,97,11,563,96,10,6,2,1
"evolving robust asynchronous cellular automata for the density task in this paper the evolution of three kinds of asynchronous cellular automata are studied for the density task. results are compared with those obtained for synchronous automata and the influence of various asynchronous update policies on the computational strategy is described. how synchronous and asynchronous cellular automata behave is investigated when the update policy is gradually changed, showing that asynchronous cellular automata are more adaptable. the behavior of synchronous and asynchronous evolved automata are studied under the presence of random noise of two kinds and it is shown that asynchronous cellular automata implicitly offer superior fault tolerance ","Evolving robust asynchronous cellular automata for the density task In this paper the evolution of three kinds of asynchronous cellular automata are studied for the density task. Results are compared with those obtained for synchronous automata and the influence of various asynchronous update policies on the computational strategy is described. How synchronous and asynchronous cellular automata behave is investigated when the update policy is gradually changed, showing that asynchronous cellular automata are more adaptable. The behavior of synchronous and asynchronous evolved automata are studied under the presence of random noise of two kinds and it is shown that asynchronous cellular automata implicitly offer superior fault tolerance","['asynchronous cellular automata', 'cellular automata', 'fault tolerance', 'discrete dynamical systems', 'random noise', 'synchronous automata', 'cellular automata']","['asynchronous cellular automata', 'synchronous', 'synchronous automata', 'density task Results', 'automata', 'task', 'density', 'density task', 'cellular', 'asynchronous']",640,107,7,640,106,10,0,0,2
"using fractional order adjustment rules and fractional order reference models in model-reference adaptive control this paper investigates the use of fractional order calculus (foc) in conventional model reference adaptive control (mrac) systems. two modifications to the conventional mrac are presented, i.e., the use of fractional order parameter adjustment rule and the employment of fractional order reference model. through examples, benefits from the use of foc are illustrated together with some remarks for further research ","Using fractional order adjustment rules and fractional order reference models in modeL-reference adaptive control This paper investigates the use of Fractional Order Calculus (FOC) in conventional Model Reference Adaptive Control (MRAC) systems. Two modifications to the conventional MRAC are presented, e., the use of fractional order parameter adjustment rule and the employment of fractional order reference model. Through examples, benefits from the use of FOC are illustrated together with ome remarks for further research","['fractional order adjustment rules', 'fractional order reference models', 'model-reference adaptive control', 'MRAC', 'FOC', 'fractional calculus', 'model reference adaptive control systems', 'nonlinear control systems', 'nonlinear dynamical systems', 'parameter estimation', 'variational techniques']","['fractional', 'reference', 'order', 'adaptive control', 'adjustment', 'use', 'examples benefits', 'rule', 'conventional mac', 'models']",457,75,11,454,74,10,5,2,2
"why your web strategy is, err, wrong an awkward look at a few standard views from the author, who thinks that most people have got it, err, wrong. like every other investment, when the time comes to sign the contract, the question that should be asked is not whether it is a good investment, but whether it is the best investment the firm can make with the money. the author argues that he would be surprised if any law firm web site he has seen yet would jump that particular hurdle ","Why your Web strategy is, err, wrong ‘An awkward look at a few standard views from the author, who thinks that most people have got it, err, wrong. Like every other investment, when the time comes to sign the contract, the question that should be asked is, not whether it is a good investment, but whether it is the best investment the firm can make with the money. the author argues that he would be surprised if any law firm Web site he has seen yet would jump that particular hurdle","['Web strategy', 'law firm Web site', 'DP management', 'information resources', 'Internet', 'legislation']","['author', 'wrong', 'few standard views', 'other investment', 'good investment', 'best investment', 'awkward look', 'Web strategy', 'investment', 'Web']",393,92,6,395,91,10,2,2,0
"information interaction: providing a framework for information architecture information interaction is the process that people use in interacting with the content of an information system. information architecture is a blueprint and navigational aid to the content of information-rich systems. as such information architecture performs an important supporting role in information interactivity. this article elaborates on a model of information interactivity that crosses the ""no-man's land"" between user and computer articulating a model that includes user, content and system, illustrating the context for information architecture ","Information interaction: providing a framework for information architecture Information interaction is the process that people use in interacting with the content of an information system. Information architecture is a blueprint and navigational aid to the content of information-rich systems. As such information architecture performs an important supporting role in information interactivity. This article elaborates ‘on a model of information interactivity that crosses the ""no-man's land” between user and computer articulating a model that includes user, content and system, illustrating the context for information architecture","['information interaction', 'navigational aid', 'information-rich systems', 'information interactivity', 'electronic publishing', 'hypermedia', 'information resources', 'information retrieval', 'Internet']","['information', 'Information', 'information interactivity', 'Information interaction', 'such information architecture', 'information-rich systems', 'users content', 'architecture', 'information architecture', 'interaction']",549,85,9,550,84,10,3,2,1
"natural language from artificial life this article aims to show that linguistics, in particular the study of the lexico-syntactic aspects of language, provides fertile ground for artificial life modeling. a survey of the models that have been developed over the last decade and a half is presented to demonstrate that alife techniques have a lot to offer an explanatory theory of language. it is argued that this is because much of the structure of language is determined by the interaction of three complex adaptive systems: learning, culture, and biological evolution. computational simulation, informed by theoretical linguistics, is an appropriate response to the challenge of explaining real linguistic data in terms of the processes that underpin human language ","Natural language from artificial life This article aims to show that linguistics, in particular the study of the lexico-syntactic aspects of language, provides fertile ground for artificial life modeling. A survey of the models that have been developed over the last decade and a half is presented to demonstrate that ALife techniques have a lot to offer an explanatory theory of language. It is argued that this is because much of the structure of language is determined by the interaction of three complex adaptive systems: learning, culture, and biological evolution. Computational simulation, informed by theoretical linguistics, is an appropriate response to the challenge of explaining real linguistic data in terms of the processes that underpin human language","['natural language', 'linguistics', 'lexico-syntactic aspects', 'ALife', 'adaptive systems', 'learning', 'culture', 'biological evolution', 'computational simulation', 'artificial life', 'artificial life', 'computational linguistics', 'natural languages']","['artificial life', 'theoretical linguistics', 'language', 'real linguistic data', 'Natural language', 'life techniques', 'human language', 'life', 'linguistics', 'artificial']",651,118,13,651,117,10,0,0,7
"development of computer-mediated teaching resources for tourism distance education: the university of otago model this article presents a qualitative account of the development of computer-mediated tourism distance learning resources. a distance learning model was developed at the centre for tourism, university of otago (new zealand) in 1998-1999. the article reviews the development of this internet-based learning resource explaining the design and development of programme links (providing study information for students) and paper links (course material and learning features). the design of course material is reviewed with emphasis given to consistency of presentation between papers. the template for course material is described and illustrated and the article concludes with an overview of important design considerations ","Development of computer-mediated teaching resources for tourism distance education: the University of Otago model This article presents a qualitative account of the development of ‘computer-mediated tourism distance learning resources. A distance learning model was developed at the Centre for Tourism, University of Otago (New Zealand) in 1998-1999. The article reviews the development of this Internet-based learning resource explaining the design and development of programme links (providing study information for students) and paper links (course material and learning features). The design of course material is reviewed with emphasis given to consistency of presentation between papers. The template for course material is described and illustrated and the article concludes with an ‘overview of important design considerations","['computer-mediated tourism distance learning resources', 'University of Otago', 'Internet-based learning resource', 'paper links', 'programme links', 'computer aided instruction', 'distance learning', 'Internet', 'teaching', 'travel industry']","['computer-mediated teaching resources', 'computer-mediated tourism distance', 'Internet-based learning resource', 'tourism distance education', 'tourism University', 'Otago model', 'tourism', 'distance', 'computer-mediated', 'resources']",719,115,10,721,114,10,25,2,3
quantum market games we propose a quantum-like description of markets and economics. the approach has roots in the recently developed quantum game theory ,Quantum market games We propose a quantum-like description of markets and economics. The approach has roots in the recently developed quantum game theory,"['quantum market games', 'economics', 'quantum game theory', 'quantum strategies', 'financial markets', 'economics', 'game theory', 'quantum statistical mechanics', 'quantum theory']","['quantum-like description', 'Quantum market games', 'quantum game theory', 'Quantum', 'economics', 'approach', 'markets', 'games', 'description', 'quantum-like']",131,24,9,131,23,10,0,0,0
"a web-accessible database of characteristics of the 1,945 basic japanese kanji in 1981, the japanese government published a list of the 1,945 basic japanese kanji (jooyoo kanji-hyo), including specifications of pronunciation. this list was established as the standard for kanji usage in print. the database for 1,945 basic japanese kanji provides 30 cells that explain in detail the various characteristics of kanji. means, standard deviations, distributions, and information related to previous research concerning these kanji are provided in this paper. the database is saved as a microsoft excel 2000 file for windows. this kanji database is accessible on the web site of the oxford text archive, oxford university (http://ota.ahds.ac.uk). using this database, researchers and educators will be able to conduct planned experiments and organize classroom instruction on the basis of the known characteristics of selected kanji ","‘AWeb-accessible database of characteristics of the 1,945 basic Japanese kanji In 1981, the Japanese government published a list of the 1,945 basic Japanese kanji (Jooyoo Kanji-hyo), including specifications of pronunciation. This list was established as the standard for kanji usage in print. The database for 1,945 basic Japanese kanji provides 30 cells that explain in detail the various characteristics of kanji. Means, standard deviations, distributions, and information related to previous research concerning these kanji are provided in this paper. The database is saved as a Microsoft Excel 2000 file for Windows. This kanji database is accessible on the Web site of the Oxford Text Archive, Oxford University (http://ota.ahds.ac.uk). Using this database, researchers and educators will be able to conduct planned experiments and organize classroom instruction on the basis of the known characteristics of selected kanji","['Web-accessible database', 'basic Japanese kanji', 'Jooyoo Kanji-hyo', 'pronunciation', 'kanji usage print', 'cells', 'means', 'standard deviations', 'distributions', 'Microsoft Excel 2000 file for Windows', 'Oxford Text Archive Web site', 'classroom instruction', 'behavioural sciences computing', 'character sets', 'information resources']","['basic', 'standard deviations distributions', 'AWeb-accessible database', 'various characteristics', 'database', 'known characteristics', 'database researchers', 'Japanese government', 'characteristics', 'Japanese']",793,137,15,794,135,10,484,135,6
"stabilization of global invariant sets for chaotic systems: an energy based control approach this paper presents a new control approach for steering trajectories of three-dimensional nonlinear chaotic systems towards stable stationary states or time-periodic orbits. the proposed method mainly consists in a sliding mode-based control design that is extended by an explicit consideration of system energy as basis for both controller design and system stabilization. the control objective is then to regulate the energy with respect to a shaped nominal representation implicitly related to system trajectories. in this paper, we establish some theoretical results to introduce the control design approach referred to as energy based sliding mode control. then, some capabilities of the proposed approach are illustrated through examples related to the chaotic circuit of chua ","Stabilization of global invariant sets for chaotic systems: an energy based control approach This paper presents a new control approach for steering trajectories of three-dimensional nonlinear chaotic systems towards stable stationary states or time-periodic orbits. The proposed method mainly consists in a sliding mode-based control design that is extended by an explicit consideration of system energy as basis for both controller design and system stabilization. The control objective is then to regulate the energy with respect to a shaped nominal representation implicitly related to system trajectories. In this paper, we establish some theoretical results to introduce the control design approach referred to as energy based sliding mode control. Then, some capabilities of the proposed approach are illustrated through examples related to the chaotic circuit of Chua","['three-dimensional nonlinear chaotic systems', 'stable stationary states', 'time-periodic orbits', 'sliding mode-based control', 'energy based sliding mode control', ""Chua's circuit"", 'global invariant sets', 'chaos', ""Chua's circuit"", 'set theory', 'variable structure systems']","['chaotic systems', 'home-based control design', 'control design approach', 'global invariant sets', 'new control approach', 'system trajectories', 'controller design', 'system energy', 'mode control', 'control approach']",750,127,11,750,126,10,0,0,2
"moving into the mainstream [product lifecycle management] product lifecycle management (plm) is widely recognised by most manufacturing companies, as manufacturers begin to identify and implement targeted projects intended to deliver return-on investment in a timely fashion. vendors are also releasing second-generation plm products that are packaged, out-of-the-box solutions ","Moving into the mainstream [product lifecycle management] Product lifecycle management (PLM) is widely recognised by most manufacturing ‘companies, as manufacturers begin to identify and implement targeted projects intended to deliver return-on investment in a timely fashion. Vendors are also releasing second-generation PLM products that are packaged, out-of-the-box solutions,","['product lifecycle management', 'manufacturing companies', 'product data management', 'product development', 'enterprise resource planning', 'manufacturing data processing', 'manufacturing resources planning', 'product development']","['management', 'lifecycle', 'second-generation PLM products', 'most manufacturing companies', 'timely fashion Vendors', 'targeted projects', 'mainstream', 'Product', 'products', 'manufacturing']",330,49,8,332,48,10,10,2,0
two-layer model for the formation of states of the hidden markov chains procedures for the formation of states of the hidden markov models are described. formant amplitudes and frequencies are used as state features. the training strategy is presented that allows one to calculate the parameters of conditional probabilities of the generation of a given formant set by a given hidden state with the help of the maximum likelihood method ,‘Two-layer model for the formation of states of the hidden Markov chains Procedures for the formation of states of the hidden Markov models are described. Formant amplitudes and frequencies are used as state features. The training strategy is presented that allows one to calculate the parameters of conditional probabilities of the generation of a given formant set by a given hidden state with the help of the maximum likelihood method,"['hidden Markov models', 'formant amplitudes', 'formant frequencies', 'state features', 'conditional probabilities', 'hidden state', 'maximum likelihood method', 'hidden Markov models', 'maximum likelihood estimation', 'speech recognition']","['formation', 'states', 'hidden Markov models', 'format amplitudes', 'two-layer model', 'state features', 'hidden state', 'hidden', 'model', 'Markov']",367,71,10,368,70,10,9,1,2
how to drive strategic innovation [law firms] innovation. it has everything to do with organization and attitude. marginal improvement isn't enough anymore. convert your problem-solving skills into a new value for the entire firm. 10 initiatives ,"How to drive strategic innovation [law firms] Innovation. It has everything to do with organization and attitude. Marginal improvement isn't enough anymore. Convert your problem-solving skills into a new value for the entire firm. 10 initiatives,","['law firms', 'strategic innovation', 'management', 'change', 'clients', 'experiments', 'law administration']","['innovation', 'problem-solving skills', 'Marginal improvement', 'organization', 'entire firm', 'everything', 'strategic', 'new value', 'law', 'firm']",210,37,7,211,36,10,0,1,0
"exploring the sabbatical or other leave as a means of energizing a career this article challenges librarians to create leaves that will not only inspire professional growth but also renewal. it presents a framework for developing a successful leave, incorporating useful advice from librarians at concordia university (montreal). as food for thought, the article offers examples of specific options meant to encourage professionals to explore their own creative ideas. finally, a central theme of this article is that a midlife leave provides one with the perfect opportunity to take stock of oneself in order to define future career directions. midlife is a time when rebel forces, feisty protestors from within, often insist on being heard. it is a time, in other words, when professionals often long to break loose from the stress ""to do far more, in less time"" (barner, 1994). escaping from current job constraints into a world of creative endeavor, when well-executed, is a superb means of invigorating a career stuck in gear and discovering a fresh perspective from which to view one's profession. to ignite renewal, midcareer is the perfect time to grant one's imagination free reign ","Exploring the sabbatical or other leave as a means of energizing a career This article challenges librarians to create leaves that will not only inspire professional growth but also renewal. It presents a framework for developing a successful leave, incorporating useful advice from librarians at Concordia University (Montreal). As food for thought, the, article offers examples of specific options meant to encourage professionals to explore their own creative ideas. Finally, a central theme of this article is that a midlife leave provides one with the perfect opportunity to take stock of oneseff in order to define future career directions. Midlife is a time when rebel forces, feisty protestors from within, often insist on being heard. It is a time, in ‘other words, when professionals often long to break loose from the stress ""to do far more, in less time"" (Barner, 1994). Escaping from current job constraints into a world of creative endeavor, when well-executed, is a superb means of invigorating a career stuck in gear and discovering a fresh perspective from which to view one's profession. To ignite renewal, midcareer is the perfect time to grant ‘one's imagination free reign","['sabbatical leave', 'career', 'librarians', 'professional growth', 'library staff', 'midlife leave', 'employment', 'human resource management', 'information science', 'libraries', 'personnel', 'professional aspects']","['professional growth', 'successful leave', 'midlife leave', 'superb means', 'perfect time', 'other words', 'other leave', 'less time', 'leaves', 'other']",1001,191,12,1004,190,10,11,4,2
"more constructions for boolean algebras we construct boolean algebras with prescribed behaviour concerning depth for the free product of two boolean algebras over a third, in zfc using pcf; assuming squares we get results on ultraproducts. we also deal with the family of cardinalities and topological density of homomorphic images of boolean algebras (you can translate it to topology-on the cardinalities of closed subspaces); and lastly we deal with inequalities between cardinal invariants, mainly d(b)/sup kappa /<|b| implies ind(b)>/sup kappa /v depth(b)>or=log(|b|) ","More constructions for Boolean algebras We construct Boolean algebras with prescribed behaviour concerning depth for the free product of two Boolean algebras over a third, in ZFC using cf; assuming squares we get results on ultraproducts. We also deal with the family of cardinalities and topological density of homomorphic images of Boolean algebras (you can translate it to topology-on the cardinalities of closed subspaces): and lastly we deal with inequalities between cardinal invariants, mainly d(B)/sup kappa J<|B| implies ind(B)>/sup kappa /V Depth(B)>or=og(|B])","['Boolean algebras', 'prescribed behaviour', 'free product', 'ZFC', 'ultraproducts', 'homomorphic images', 'cardinal invariants', 'Boolean algebra', 'formal logic']","['| B', 'Boolean algebras', 'cardinalities', 'topological density', 'cardinal invariants', 'monomorphic images', 'constructions', 'free product', 'Boolean', 'algebras']",491,83,9,489,82,10,12,4,3
"a feature-preserving volumetric technique to merge surface triangulations several extensions and improvements to surface merging procedures based on the extraction of isosurfaces from a distance map defined on an adaptive background grid are presented. the main objective is to extend the application of these algorithms to surfaces with sharp edges and comers. in order to deal with objects of different length scales, the initial background grids are created using a delaunay triangulation method and local voxelizations. a point enrichment technique that introduces points into the background grid along detected surface features such as ridges is used to ensure that these features are preserved in the final merged surface. the surface merging methodology is extended to include other boolean operations between surface triangulations. the iso-surface extraction algorithms are modified to obtain the correct iso-surface for multi-component objects. the procedures are demonstrated with various examples, ranging from simple geometrical entities to complex engineering applications. the present algorithms allow realistic modelling of a large number of complex engineering geometries using overlapping components defined discretely, i.e. via surface triangulations. this capability is very useful for grid generation starting from data originated in measurements or images ","A feature-preserving volumetric technique to merge surface triangulations Several extensions and improvements to surface merging procedures based on the extraction of isosurfaces from a distance map defined on an adaptive background grid are presented. The main objective is to extend the application of these algorithms to surfaces with sharp edges and comers. In order to deal with objects of different length scales, the initial background grids are created using a Delaunay triangulation method and local voxelizations. A point enrichment technique that introduces points into the background grid along detected surface features such as ridges is used to ensure that these features are preserved in the final merged surface. The surface merging methodology is extended to include other Boolean operations between surface triangulations. The iso-surface extraction algorithms are modified to obtain the correct iso-surface for multi-component objects. The procedures are demonstrated with various examples, ranging from simple geometrical entities to complex engineering applications. The present algorithms allow realistic modelling of a large number of complex engineering geometries using overlapping components defined discretely, Le. via surface triangulations. This capability is very useful for grid generation starting from data originated in measurements or images","['feature-preserving volumetric technique', 'merge surface triangulations', 'surface merging procedures', 'iso-surfaces extraction', 'multi-component objects', 'simple geometrical entities', 'complex engineering applications', 'overlapping components', 'images', 'mesh generation', 'unstructured grids', 'discrete data', 'surface intersection', 'geometric modelling', 'adaptive background grid', 'sharp edges', 'sharp comers', 'Delaunay triangulation method', 'local voxelizations', 'point enrichment technique background grid', 'ridges', 'Boolean operations', 'surface triangulations', 'arterial surfaces', 'haemoglobin molecule', 'mesh generation', 'physiological models']","['surface', 'surface triangulation', 'feature-preserving volumetric technique', 'Delaunay triangulation method', 'point enrichment technique', 'initial background grids', 'adaptive background grid', 'final merged surface', 'surface features', 'background grid']",1187,192,27,1186,191,10,3,1,9
"bisimulation minimization and symbolic model checking state space minimization techniques are crucial for combating state explosion. a variety of explicit-state verification tools use bisimulation minimization to check equivalence between systems, to minimize components before composition, or to reduce a state space prior to model checking. experimental results on bisimulation minimization in symbolic model checking contexts, however, are mixed. we explore bisimulation minimization as an optimization in symbolic model checking of invariance properties. we consider three bisimulation minimization algorithms. from each, we produce a bdd-based model checker for invariant properties and compare this model checker to a conventional one based on backwards reachability. our comparisons, both theoretical and experimental, suggest that bisimulation minimization is not viable in the context of invariance verification, because performing the minimization requires as many, if not more, computational resources as model checking the unminimized system through backwards reachability ","Bisimulation minimization and symbolic model checking State space minimization techniques are crucial for combating state explosion A variety of explicit-state verification tools use bisimulation minimization to check equivalence between systems, to minimize ‘components before composition, or to reduce a state space prior to model checking. Experimental results on bisimulation minimization in symbolic model checking contexts, however, are mixed. We explore bisimulation minimization as an optimization in symbolic model checking of invariance properties. We consider three bisimulation minimization algorithms. From each, we produce a BDD-based model checker for invariant properties and compare this model checker to a conventional ‘one based on backwards reachability. Our comparisons, both theoretical and experimental, suggest that bisimutation minimization is not viable in the context of invariance verification, because performing the minimization requires as many, if not more, computational resources as model checking the unminimized system through backwards reachability","['bisimulation minimization', 'symbolic model checking', 'state space minimization techniques', 'state explosion', 'explicit-state verification tools', 'experimental results', 'optimization', 'invariance properties', 'BDD', 'binary decision diagram', 'backwards reachability', 'invariance verification', 'binary decision diagrams', 'bisimulation equivalence', 'formal verification', 'minimisation', 'reachability analysis']","['symbolic model checking', 'minimization', 'model', 'simulation minimization algorithms', 'bisimutation minimization', 'BDD-based model checker', 'model checker', 'simulation minimization', 'model checking', 'symbolic model']",943,143,17,944,142,10,14,4,8
"supporting global user profiles through trusted authorities personalization generally refers to making a web site more responsive to the unique and individual needs of each user. we argue that for personalization to work effectively, detailed and interoperable user profiles should be globally available for authorized sites, and these profiles should dynamically reflect changes in user interests. creating user profiles from user click-stream data seems to be an effective way of generating detailed and dynamic user profiles. however, a user profile generated in this way is available only on the computer where the user accesses his browser, and is inaccessible when the same user works on a different computer. on the other hand, integration of the internet with telecommunication networks has made it possible for the users to connect to the web with a variety of mobile devices as well as desktops. this requires that user profiles should be available to any desktop or mobile device on the internet that users choose to work with. in this paper, we address these problems through the concept of ""trusted authority"". a user agent at the client side that captures the user click stream, dynamically generates a navigational history 'log' file in extensible markup language (xml). this log file is then used to produce 'user profiles' in a resource description framework (rdf). a user's right to privacy is provided through the platform for privacy preferences (p3p) standard. user profiles are uploaded to the trusted authority and served next time the user connects to the web ","Supporting global user profiles through trusted authorities Personalization generally refers to making a Web site more responsive to the unique and individual needs of each user. We argue that for personalization to work effectively, detailed and interoperable user profiles should be globally available for authorized sites, and these profiles should dynamically reflect changes in user interests. Creating User profiles from user click-stream data seems to be an effective way of generating detailed and dynamic user profiles. However, a user profile generated in this way is available only on the computer where the user accesses his browser, and is inaccessible when the same user works on a different computer. On the other hand, integration of the Internet with telecommunication networks has made it possible for the, users to connect to the Web with a variety of mobile devices as well as desktops. This requires that user profiles should be available to any desktop or mobile device on the Internet that users choose to work with. In this paper, we address these problems through the concept of “trusted authority"". A user agent at the client side that captures the user click stream, dynamically generates a navigational history ‘log’ file in Extensible Markup Language (XML). This log file is then used to produce ‘user profiles' in a resource description framework (RDF). A User's right to privacy is provided through the Platform for Privacy Preferences (P3P) standard. User profiles are uploaded to the trusted authority and served next time the user connects to the Web","['global user profiles', 'trusted authorities', 'personalization', 'Web site', 'Internet', 'telecommunication networks', 'mobile device', 'user agent', 'user click stream', 'navigational history log file', 'XML', 'resource description framework', 'privacy', 'Platform for Privacy Preferences standard', 'namespace qualifier', 'globally unique user ID/password identification', 'client-server systems', 'data privacy', 'file servers', 'hypermedia markup languages', 'information needs', 'information resources', 'Internet', 'online front-ends']","['user profiles', 'User profiles', 'users', 'interoperable user profiles', 'standard User profiles', 'dynamic user profiles', 'global user profiles', 'user click stream', 'user interests', 'same user']",1333,252,24,1334,251,10,4,4,10
"node-capacitated ring routing we consider the node-capacitated routing problem in an undirected ring network along with its fractional relaxation, the node-capacitated multicommodity flow problem. for the feasibility problem, farkas' lemma provides a characterization for general undirected graphs, asserting roughly that there exists such a flow if and only if the so-called distance inequality holds for every choice of distance functions arising from nonnegative node weights. for rings, this (straightforward) result will be improved in two ways. we prove that, independent of the integrality of node capacities, it suffices to require the distance inequality only for distances arising from (0-1-2)-valued node weights, a requirement that will be called the double-cut condition. moreover, for integer-valued node capacities, the double-cut condition implies the existence of a half-integral multicommodity flow. in this case there is even an integer-valued multicommodity flow that violates each node capacity by at most one. our approach gives rise to a combinatorial, strongly polynomial algorithm to compute either a violating double-cut or a node-capacitated multicommodity flow. a relation of the problem to its edge-capacitated counterpart will also be explained ","Node-capacitated ring routing We consider the node-capacitated routing problem in an undirected ring network along with its fractional relaxation, the node-capacitated multicommodity flow problem. For the feasibility problem, Farkas’ lemma provides a characterization for general undirected graphs, asserting roughly that there exists such a flow if and only if the so-called distance inequality holds for every choice of distance functions arising from nonnegative node weights. For rings, this, (straightforward) result will be improved in two ways. We prove that, independent of the integrality of node capacities, it suffices to require the distance inequality only for distances arising from (0-1-2)-valued node weights, a requirement that will be called the double-cut condition. Moreover, for integer-valued node capacities, the double-cut condition implies the existence of a half-integral multicommodity flow. In this case there is even an integer-valued multicommodity flow that violates each node capacity by at most one. Our approach gives rise to a combinatorial, strongly polynomial algorithm to compute either a violating double-cut or a node-capacitated multicommodity flow. A relation of the problem to its edge-capacitated counterpart will also be explained","['node-capacitated routing problem', 'node-capacitated ring routing', 'undirected ring network', 'fractional relaxation', 'node-capacitated multicommodity flow problem', 'feasibility problem', 'Farkas lemma', 'undirected graphs', 'distance inequality', 'distance functions', 'nonnegative node weights', 'node capacity integrality', 'double-cut condition', 'integer-valued node capacities', 'half-integral multicommodity flow', 'integer-valued multicommodity flow', 'combinatorial strongly polynomial algorithm', 'violating double-cut', 'edge-cut criterion', 'graph theory', 'operations research', 'telecommunication network routing']","['integer-valued multicommodity flow', 'half-integral multicommodity flow', 'integer-valued node capacities', 'so-called distance inequality', 'non-negative node weights', 'undirected ring network', 'Node-capacitated ring', 'feasibility problem', 'node capacity', 'problem']",1096,180,22,1097,179,10,1,2,3
"a conference's impact on undergraduate female students in september of 2000, the 3rd grace hopper celebration of women in computing was held in cape cod, massachusetts. along with a colleague from a nearby university, we accompanied seven of our female undergraduate students to this conference. this paper reports on how the conference experience immediately affected these students - what impressed them, what scared them, what it clarified for them. it also reports on how the context in which these students currently evaluate their ability, potential and opportunity in computer science is different now from what it was before the conference. hopefully, by understanding their experience, we can gain some insight into things we can do for all of our undergraduate female students to better support their computer science and engineering education ","Acconference's impact on undergraduate female students In September of 2000, the 3rd Grace Hopper Celebration of Women in Computing was held in Cape Cod, Massachusetts. Along with a colleague from a nearby university, we accompanied seven of our female undergraduate students to this conference. This paper reports on how the conference experience immediately affected these students - what impressed them, what scared them, what it clarified for them. It also reports on how the context in which these students currently evaluate their ability, Potential and opportunity in computer science is different now from What it was before the conference. Hopefully, by understanding their experience, we can gain some insight into things we can do for all of our undergraduate female students to better support their computer science and engineering education","['undergraduate female students', 'computer science education', 'engineering education', 'gender issues', 'conference', 'computer science education', 'gender issues', 'social aspects of automation']","['undergraduate female students', 'computer science', 'female undergraduate students', 'conference experience', 'nearby university', 'Acconference', 'conference', 'students', 'undergraduate', 'female']",723,132,8,724,130,10,424,130,1
"infrared-image classification using hidden markov trees an image of a three-dimensional target is generally characterized by the visible target subcomponents, with these dictated by the target-sensor orientation (target pose). an image often changes quickly with variable pose. we define a class as a set of contiguous target-sensor orientations over which the associated target image is relatively stationary with aspect. each target is in general characterized by multiple classes. a distinct set of wiener filters are employed for each class of images, to identify the presence of target subcomponents. a karhunen-loeve representation is used to minimize the number of filters (templates) associated with a given subcomponent. the statistical relationships between the different target subcomponents are modeled via a hidden markov tree (hmt). the hmt classifier is discussed and example results are presented for forward-looking-infrared (flir) imagery of several vehicles ","Infrared-image classification using hidden Markov trees An image of a three-dimensional target is generally characterized by the visible target subcomponents, with these dictated by the target-sensor orientation (target pose). An image often changes quickly with variable pose. We define a class as a set of contiguous target-sensor orientations over which the associated target image is relatively stationary with aspect. Each target is in general characterized by multiple classes. A distinct set of Wiener filters are employed for each class of images, to identify the presence of target subcomponents. A Karhunen-Loeve representation is used to minimize the number of filters (templates) associated with a given subcomponent. The statistical relationships between the different target subcomponents are modeled via a hidden Markov tree (HMT). The HMT classifier is, discussed and example results are presented for forward-looking-infrared (FLIR) imagery of several vehicles","['IR image classification', 'infrared-image classification', 'hidden Markov trees', '3D target image', 'target-sensor orientation', 'target pose', 'contiguous target-sensor orientations', 'Wiener filters', 'Karhunen-Loeve representation', 'minimization', 'HMT', 'forward-looking-infrared imagery', 'FLIR imagery', 'vehicles', 'image classification', 'infrared imaging', 'Karhunen-Loeve transforms', 'Markov processes', 'minimisation', 'trees (mathematics)', 'Wiener filters']","['target', 'hidden Markov tree', 'contiguous target-sensor orientations', 'target-sensor orientation target', 'different target subcomponents', 'Infrared-image classification', 'visible target subcomponents', 'three-dimensional target', 'target image', 'target subcomponents']",839,139,21,840,138,10,0,1,5
"impact of user satisfaction and trust on virtual team members pressured by the growing need for fast response times, mass customization, and globalization, many organizations are turning to flexible organizational forms, such as virtual teams. virtual teams consist of cooperative relationships supported by information technology to overcome limitations of time and/or location. virtual teams require their members to rely heavily on the use of information technology and trust in coworkers. this study investigates the impacts that the reliance on information technology (operationalized in our study via the user satisfaction construct) and trust have on the job satisfaction of virtual team members. the study findings reveal that both user satisfaction and trust are positively related to job satisfaction in virtual teams, while system use was not found to play a significant role. these findings emphasize that organizations seeking the benefits of flexible, it-enabled virtual teams must consider both the level of trust among colleagues, and the users' satisfaction with the information technology on which virtual teams rely ","Impact of user satisfaction and trust on virtual team members Pressured by the growing need for fast response times, mass customization, and globalization, many organizations are turning to flexible organizational forms, such as virtual teams. Virtual teams consist of cooperative relationships supported by information technology to ‘overcome limitations of time and/or location. Virtual teams require their members to rely heavily on the use of information technology and trust in coworkers. This study investigates the impacts that the reliance on information technology (operationalized in our study via the user satisfaction construct) and trust have on the job satisfaction of virtual team members. The study findings reveal that both user satisfaction and trust are positively related to job satisfaction in virtual teams, while system use was not found to play a significant role. These findings emphasize that organizations seeking the benefits of flexible, IT-enabled virtual teams must consider both the level of trust among colleagues, and the users’ satisfaction with the information technology on which virtual teams rely","['information technology', 'trust', 'IT', 'user satisfaction', 'job satisfaction', 'virtual team members', 'business data processing', 'groupware', 'human factors', 'information technology']","['information technology', 'trust', 'satisfaction', 'virtual team members', 'job satisfaction', 'user satisfaction construct', 'users satisfaction', 'teams', 'virtual teams', 'Virtual teams']",969,167,10,970,166,10,9,2,1
"the decision procedure for profitability of investment projects using the internal rate of return of single-period projects the internal rate of return (irr) criterion is often used to evaluate profitability of investment projects. in this paper, we focus on a single-period project which consists of two types of cash flows; an investment at one period and a return at a succeeding period, and a financing at one period and a repayment at a succeeding period. we decompose the given investment project into a series of the single-period projects. from the viewpoint of the single-period project, we point out the applicability issue of the irr criterion, namely the irr criterion cannot be applied in which a project is composed of both investment type and financing type. investigating the properties of a series of the single-period projects, we resolve the applicability issue of the irr criterion and propose the decision procedure for profitability judgment toward any type of investment project based on the comparison between the irr and the capital cost. we develop a new algorithm to obtain the value of the project investment rate (pir) for the given project, which is a function of the capital cost, only using the standard irr computing routine. this outcome is a theoretical breakthrough to widen the utilization of irr in practical applications ","The decision procedure for profitability of investment projects using the internal rate of return of single-period projects The internal rate of return (IRR) criterion is often used to evaluate profitability of investment projects. In this paper, we focus on a single-period project which consists of two types of cash flows; an investment at one period and a return at a succeeding period, and a financing at one period and a repayment at a succeeding period. We decompose the given investment project into a series of the single-period projects. From the viewpoint of the single-period project, we point out the applicability issue of the IRR criterion, namely the IRR criterion cannot be applied in which a project is composed of both investment type and financing type. Investigating the properties of a series of the single-period projects, we resolve the applicability issue of the IRR criterion and propose the decision procedure for profitability judgment toward any type of investment project based on the comparison between the IRR and the capital cost. We develop a new algorithm to obtain the value of the project investment rate (PIR) for the given project, which is a function of the capital cost, only using the standard IRR computing routine. This ‘outcome is a theoretical breakthrough to widen the utilization of IRR in practical applications","['decision procedure', 'investment project profitability', 'internal rate of return', 'single-period projects', 'profitability', 'cash flows', 'investment project decomposition', 'IRR criterion', 'project investment rate', 'PIR', 'decision theory', 'investment']","['single-person projects', 'project', 'investment projects', 'decision procedure', 'investment', 'standard IRR computing', 'return irr criterion', 'IRR criterion cannon', 'investment type', 'IRR criterion']",1142,219,12,1143,218,10,7,1,3
bayesian nonstationary autoregressive models for biomedical signal analysis we describe a variational bayesian algorithm for the estimation of a multivariate autoregressive model with time-varying coefficients that adapt according to a linear dynamical system. the algorithm allows for time and frequency domain characterization of nonstationary multivariate signals and is especially suited to the analysis of event-related data. results are presented on synthetic data and real electroencephalogram data recorded in event-related desynchronization and photic synchronization scenarios ,Bayesian nonstationary autoregressive models for biomedical signal analysis We describe a variational Bayesian algorithm for the estimation of a multivariate autoregressive model with time-varying coefficients that adapt according to a linear dynamical system. The algorithm allows for time and frequency domain characterization of nonstationary multivariate signals and is especially suited to the analysis of event-related data. Results are presented on synthetic data and real electroencephalogram data recorded in event-related desynchronization and photic synchronization scenarios,"['photic synchronization scenarios', 'event-related desynchronization', 'frequency domain characterization', 'time domain characterization', 'biomedical signal analysis', 'Kalman smoother', 'EEG analysis', 'Bayesian nonstationary autoregressive models', 'linear dynamical system', 'variational Bayesian algorithm', 'time-varying coefficients', 'autoregressive processes', 'Bayes methods', 'electroencephalography', 'frequency-domain analysis', 'medical signal processing', 'physiological models', 'time series', 'time-domain analysis']","['nonstationary multivariate signals', 'multivariate autoregressive model', 'frequency domain characterization', 'real electroencephalogram data', 'event-related data Results', 'biomedical signal analysis', 'linear dynamical system', 'Bayesian algorithm', 'models', 'Bayesian']",513,75,19,513,74,10,0,0,2
"use of bayesian belief networks when combining disparate sources of information in the safety assessment of software-based systems the paper discusses how disparate sources of information can be combined in the safety assessment of software-based systems. the emphasis is put on an emerging methodology, relevant for intelligent product-support systems, to combine information about disparate evidences systematically based on bayesian belief networks. the objective is to show the link between basic information and the confidence one can have in a system. how one combines the bayesian belief net (bbn) method with a software safety standard (rtca/do-178b,) for safety assessment of software-based systems is also discussed. finally, the applicability of the bbn methodology and experiences from cooperative research work together with kongsberg defence & aerospace and det norske veritas, and ongoing research with vtt automation are presented ","Use of Bayesian Belief Networks when combining disparate sources of information in the safety assessment of software-based systems, The paper discusses how disparate sources of information can be combined in the safety assessment of software-based systems. The emphasis is put on an emerging methodology, relevant for intelligent product-support systems, to combine information about disparate evidences systematically based ‘on Bayesian Belief Networks. The objective is to show the link between basic information and the confidence one can have in a system. How one combines the Bayesian Belief Net (BBN) method with a software safety standard (RTCA/DO-178B,) for safety assessment of software-based systems is also discussed. Finally, the applicability of the BBN methodology and experiences from cooperative research work together with Kongsberg Defence & Aerospace and Det Norske Veritas, and ongoing research with VTT Automation are presented","['Bayesian belief networks', 'intelligent product-support systems', 'software safety standard', 'safety assessment', 'software-based systems', 'belief networks', 'safety-critical software', 'software reliability']","['software-based systems', 'safety assessment', 'disparate sources', 'intelligent product-support systems', 'software safety standard', 'disparate evidences', 'basic information', 'Bayesian Belief', 'Bayesian Belief networks', 'Bayesian Belief Networks']",812,136,8,814,135,10,2,2,0
"online auctions: dynamic pricing and the lodging industry the traditional channels of distribution for overnight accommodation are rapidly being displaced by web site scripting, online intermediaries, and specialty brokers. businesses that pioneered internet usage relied on it as a sales and marketing alternative to predecessor product distribution channels. as such, web sites replace the traditional trading model to the internet. web-enabled companies are popular because the medium renders the process faster, less costly, highly reliable, and secure. auction-based models impact business models by converting the price setting mechanism from supplier-centric to market-centric and transforming the trading model from ""one to many"" to ""many to many."" historically, pricing was based on the cost of production plus a margin of profit. traditionally, as products and services move through the supply chain, from the producer to the consumer, various intermediaries added their share of profit to the price. as internet based mediums of distribution become more prevalent, traditional pricing models are being supplanted with dynamic pricing. a dynamic pricing model represents a flexible system that changes prices not only from product to product, but also from customer to customer and transaction to transaction. many industry leaders are skeptical of the long run impact of online auctions on lodging industry profit margins, despite the fact pricing theory suggests that an increase in the flow of information results in efficient market pricing. the future of such endeavors remains promising, but controversial ","Online auctions: dynamic pricing and the lodging industry The traditional channels of distribution for overnight accommodation are rapidly being displaced by Web site scripting, online intermediaries, and specialty brokers. Businesses that pioneered Internet usage relied on itas a sales and marketing alternative to predecessor product distribution channels. As such, Web sites replace the traditional trading model to the Internet. Web-enabled companies are popular because the medium renders the process faster, less costly, highly reliable, and secure. Auction-based models impact business models by converting the price setting mechanism from supplier-centric to market-centric and transforming the trading model from ""one to many"" to ""many to many."" Historically, pricing was based on the cost of production plus a margin of profit. Traditionally, as products and services move through the supply chain, from the producer to the consumer, various intermediaries added their share of profit to the price. As Internet based mediums of distribution become more prevalent, traditional pricing models are being supplanted with dynamic pricing. A ‘dynamic pricing model represents a flexible system that changes prices not only from product to product, but also from customer to customer and transaction to transaction. Many industry leaders are skeptical of the long run impact of online auctions on lodging industry profit margins, despite the fact pricing theory suggests that an increase in the flow of information results in efficient market pricing. The future of such endeavors remains promising, but controversial","['online auctions', 'dynamic pricing', 'lodging industry', 'overnight accommodations', 'Web site scripting', 'online intermediaries', 'specialty brokers', 'Internet usage', 'sales', 'marketing', 'trading model', 'business models', 'price setting mechanism', 'supply chain', 'costing', 'electronic commerce', 'information resources', 'Internet', 'travel industry']","['pricing', 'Online auctions', 'traditional trading model', 'efficient market pricing', 'price setting mechanism', 'industry profit margins', 'dynamic pricing model', 'fact pricing theory', 'trading model', 'dynamic pricing']",1386,237,19,1387,235,10,681,199,8
"flow measurement - future directions interest in the flow of liquids and its measurement can be traced back to early studies by the egyptians, the chinese and the romans. since these early times the science of flow measurement has undergone a massive change but during the last 25 years or so (1977-2002) it has matured enormously. one of the principal reasons for this is that higher accuracies and reliabilities have been demanded by industry in the measurement of fiscal transfers and today there is vigorous interest in the subject from both the flowmeter manufacturer and user viewpoints. this interest is coupled with the development of advanced computer techniques in fluid mechanics together with the application of increasingly sophisticated electronics ","Flow measurement - future directions Interest in the flow of liquids and its measurement can be traced back to early studies by the Egyptians, the Chinese and the Romans. Since these early times the science of flow measurement has undergone a massive change but during the last 25 years or so (1977-2002) it has matured enormously. One of the principal reasons for this is that higher accuracies and reliabilities have been demanded by industry in the measurement of fiscal transfers and today there is vigorous interest in the subject from both the flowmeter manufacturer and user viewpoints. This interest is coupled with the development of advanced computer techniques in fluid mechanics together with the application of increasingly sophisticated electronics","['flow measurement', 'flow metering', 'signal processing', 'liquid flow', 'Egyptians', 'Chinese', 'Romans', 'fiscal transfers', 'flowmeter manufacturer', 'advanced computer techniques', 'fluid mechanics', 'electronics application', 'computational fluid dynamics', 'flow measurement', 'flowmeters', 'technological forecasting']","['future directions Interest', 'early studies', 'early times', 'measurement', 'flow', 'early', 'Flow', 'directions', 'flow measurement', 'Flow measurement']",644,120,16,644,119,10,0,0,7
"developing a cd-rom as a teaching and learning tool in food and beverage management: a case study in hospitality education food and beverage management is the traditional core of hospitality education but, in its laboratory manifestation, has come under increasing pressure in recent years. it is an area that, arguably, presents the greatest challenges in adaptation to contemporary learning technologies but, at the same time, stands to benefit most from the potential of the web. this paper addresses the design and development of a cd-rom learning resource for food and beverage. it is a learning resource which is designed to integrate with rather than to replace existing conventional classroom and laboratory learning methods and, thus, compensate for the decline in the resource base faced in food and beverage education in recent years. the paper includes illustrative material drawn from the cd-rom which demonstrates its use in teaching and learning ","Developing a CD-ROM as a teaching and learning tool in food and beverage management: a case study in hospitality education Food and beverage management is the traditional core of hospitality education but, in its laboratory manifestation, has come under increasing pressure in recent years. It is an area that, arguably, presents the greatest challenges in adaptation to contemporary learning technologies but, at the same time, stands to benefit most from the potential of the Web. This paper addresses the design and development of a CD-ROM learning resource for food and beverage. It is a learning resource which is designed to integrate with rather than to replace existing conventional classroom and laboratory learning methods and, thus, ‘compensate for the decline in the resource base faced in food and beverage education in recent years. The paper includes illustrative material drawn from the CD-ROM which demonstrates its use in teaching and learning","['food and beverage management', 'hospitality education', 'CD-ROM', 'learning tool', 'teaching tool', 'catering industry', 'courseware', 'hotel industry', 'management education', 'teaching']","['beverage management', 'CD-ROM', 'contemporary learning technologies', 'laboratory learning methods', 'hospitality education Food', 'beverage education', 'resource base', 'beverage', 'food', 'hospitality education']",812,150,10,813,149,10,10,1,1
"support vector machines model for classification of thermal error in machine tools this paper addresses a change in the concept of machine tool thermal error prediction which has been hitherto carried out by directly mapping them with the temperature of critical elements on the machine. the model developed herein using support vector machines, a powerful data-training algorithm, seeks to account for the impact of specific operating conditions, in addition to temperature variation, on the effective prediction of thermal errors. several experiments were conducted to study the error pattern, which was found to change significantly with variation in operating conditions. this model attempts to classify the error based on operating conditions. once classified, the error is then predicted based on the temperature states. this paper also briefly describes the concept of the implementation of such a comprehensive model along with an on-line error assessment and calibration system in a pc-based open-architecture controller environment, so that it could be employed in regular production for the purpose of periodic calibration of machine tools ","Support vector machines model for classification of thermal error in machine tools This paper addresses a change in the concept of machine tool thermal error prediction which has been hitherto carried out by directly mapping them with the temperature of critical elements on the machine. The model developed herein using support vector machines, a powerful data-training algorithm, seeks to account for the impact of specific ‘operating conditions, in addition to temperature variation, on the effective prediction of thermal errors. Several experiments were conducted to study the error pattern, which was found to change significantly with variation in operating conditions. This model attempts to classify the error based on operating conditions. Once classified, the error is then predicted based on the temperature states. This paper also briefly describes the concept of the implementation of such a comprehensive model along with an on-line error assessment and calibration system in a PC-based open-architecture controller environment, so that it could be employed in regular production for the purpose of periodic calibration of machine tools","['SVM', 'support vector machines model', 'thermal error classification', 'machine tool thermal error prediction', 'critical element temperature', 'data-training algorithm', 'error pattern', 'online error assessment', 'online calibration system', 'PC-based open-architecture controller environment', 'calibration', 'errors', 'factory automation', 'learning automata', 'machine tools', 'mechanical engineering computing', 'microcomputer applications', 'pattern classification']","['support vector machines', 'error', 'machine tools', 'specific operating conditions', 'thermal error prediction', 'on-line error assessment', 'comprehensive model', 'error pattern', 'machine', 'thermal error']",981,171,18,982,170,10,9,1,4
"deterministic calculations of photon spectra for clinical accelerator targets a method is proposed to compute photon energy spectra produced in clinical electron accelerator targets, based on the deterministic solution of the boltzmann equation for coupled electron-photon transport in one-dimensional (1-d) slab geometry. it is shown that the deterministic method gives similar results as monte carlo calculations over the angular range of interest for therapy applications. relative energy spectra computed by deterministic and 3-d monte carlo methods, respectively, are compared for several realistic target materials and different electron beams, and are found to give similar photon energy distributions and mean energies. the deterministic calculations typically require 1-2 mins of execution time on a sun workstation, compared to 2-36 h for the monte carlo runs ","Deterministic calculations of photon spectra for clinical accelerator targets A method is proposed to compute photon energy spectra produced in clinical electron accelerator targets, based on the deterministic solution of the Boltzmann equation for coupled electron-photon transport in ‘one-dimensional (1-D) slab geometry. It is shown that the deterministic method gives similar results as Monte Carlo calculations over the angular range of interest for therapy applications. Relative energy spectra computed by deterministic and 3-D Monte Carlo methods, respectively, are compared for several realistic target materials and different electron beams, and are found to give similar photon energy distributions and mean energies. The deterministic calculations typically require 1-2 mins of execution time on a Sun Workstation, compared to 2-36 h for the Monte Carlo runs","['photon energy spectra', 'deterministic calculations', 'clinical electron accelerator targets', 'Boltzmann equation', 'coupled electron-photon transport', 'one-dimensional slab geometry', 'angular range of interest', 'therapy applications', 'relative energy spectra', '3-D Monte Carlo methods', 'linear accelerator', 'therapy planning', 'integrodifferential equation', 'pencil beam source representations', 'Boltzmann equation', 'determinants', 'dosimetry', 'medical computing', 'photon transport theory', 'radiation therapy']","['clinical electron accelerator', 'clinical accelerator targets', 'Monte Carlo calculations', 'Relative energy spectra', 'deterministic solution', 'photon energy spectra', 'deterministic method', 'photon spectra', 'deterministic calculations', 'Deterministic calculations']",747,124,20,748,123,10,15,1,5
"a portable auto attendant system with sophisticated dialog structure an attendant system connects the caller to the party he/she wants to talk to. traditional systems require the caller to know the full name of the party. if the caller forgets the name, the system fails to provide service for the caller. in this paper we propose a portable auto attendant system (aas) with sophisticated dialog structure that gives a caller more flexibility while calling. the caller may interact with the system to request a phone number by providing just a work area, specialty, surname, or title, etc. if the party is absent, the system may provide extra information such as where he went, when he will be back, and what he is doing. the system is built modularly, with components such as speech recognizer, language model, dialog manager and text-to-speech that can be replaced if necessary. by simply changing the personnel record database, the system can easily be ported to other companies. the sophisticated dialog manager applies many strategies to allow natural interaction between user and system. functions such as fuzzy request, user repairing, and extra information query, which are not provided by other systems, are integrated into our system. experimental results and comparisons to other systems show that our approach provides a more user friendly and natural interaction for auto attendant system ","A portable Auto Attendant System with sophisticated dialog structure An attendant system connects the caller to the party he/she wants to talk to. Traditional systems require the caller to know the full name of the party. If the caller forgets the name, the system fails to provide service for the caller. In this paper we propose a portable Auto Attendant System (AAS) with sophisticated dialog structure that gives a caller more flexibility while calling. The caller may interact with the system to request a phone number by providing just a work area, specialty, surname, or title, etc. If the party is absent, the system may provide extra information such as where he went, when he will be back, and what he is doing. The system is built modularly, with ‘components such as speech recognizer, language model, dialog manager and text-to-speech that can be replaced if necessary. By simply changing the personnel record database, the system can easily be ported to other companies. The sophisticated dialog manager applies many strategies to allow natural interaction between user and system Functions such as fuzzy request, user repairing, and extra information query, which are not provided by other systems, are integrated into our system. Experimental results and comparisons to other systems show that ‘our approach provides a more user friendly and natural interaction for auto attendant system","['attendant system', 'Auto Attendant System', 'fuzzy request', 'clear request', 'semantic frame', 'dialog manager', 'spoken dialog systems', 'telephone', 'speech recognizer', 'telephone-based system', 'information systems', 'speech recognition', 'telephony']","['system', 'caller', 'auto attendant system', 'sophisticated dialog structure', 'other systems', 'portable', 'sophisticated dialog manager', 'Traditional systems', 'system Functions', 'attendant system']",1179,224,13,1180,223,10,13,3,2
"quantum sensitive dependence wave functions of bounded quantum systems with time-independent potentials, being almost periodic functions, cannot have time asymptotics as in classical chaos. however, bounded quantum systems with time-dependent interactions, as used in quantum control, may have continuous spectrum and the rate of growth of observables is an issue of both theoretical and practical concern. rates of growth in quantum mechanics are discussed by constructing quantities with the same physical meaning as those involved in the classical lyapunov exponent. a generalized notion of quantum sensitive dependence is introduced and the mathematical structure of the operator matrix elements that correspond to different types of growth is characterized ","Quantum sensitive dependence Wave functions of bounded quantum systems with time-independent potentials, being almost periodic functions, cannot have time asymptotics as in classical chaos. However, bounded quantum systems with time-dependent interactions, as used in quantum control, may have continuous spectrum and the rate of growth of observables is an issue of both theoretical and practical concern. Rates of growth in quantum mechanics are discussed by constructing quantities with the same physical meaning as those involved in the classical Lyapunov exponent. A generalized notion of quantum sensitive dependence is introduced and the mathematical structure of the operator matrix elements that correspond to different types of growth is characterized","['quantum sensitive dependence', 'wave functions', 'bounded quantum systems', 'time-independent potentials', 'periodic functions', 'time asymptotics', 'classical chaos', 'time-dependent interactions', 'quantum control', 'classical Lyapunov exponent', 'operator matrix elements', 'quantum complexity', 'bound states', 'chaos', 'Lyapunov methods', 'matrix algebra', 'nonlinear control systems', 'quantum theory', 'wave functions']","['bounded quantum systems', 'sensitive dependence', 'quantum', 'classical Lyapunov exponent', 'periodic functions cannon', 'quantum mechanics', 'quantum control', 'sensitive', 'dependence', 'quantum systems']",655,108,19,655,107,10,0,0,5
"convergence of finite element approximations and multilevel linearization for ginzburg-landau model of d-wave superconductors in this paper, we consider the finite element approximations of a recently proposed ginzburg-landau-type model for d-wave superconductors. in contrast to the conventional ginzburg-landau model the scalar complex valued order-parameter is replaced by a multicomponent complex order-parameter and the free energy is modified according to the d-wave paring symmetry. convergence and optimal error estimates and some super-convergent estimates for the derivatives are derived. furthermore, we propose a multilevel linearization procedure to solve the nonlinear systems. it is proved that the optimal error estimates and super-convergence for the derivatives are preserved by the multi-level linearization algorithm ","Convergence of finite element approximations and multilevel linearization for Ginzburg-Landau model of d-wave superconductors In this paper, we consider the finite element approximations of a recently proposed Ginzburg-Landau-type model for d-wave superconductors. In contrast to the conventional Ginzburg-Landau model the scalar complex valued order-parameter is replaced by a multicomponent complex order-parameter and the free energy is modified according to the d-wave paring symmetry. Convergence and optimal error estimates and some super-convergent estimates for the derivatives are derived Furthermore, we propose a multilevel linearization procedure to solve the nonlinear systems. It is proved that the optimal error estimates and super-convergence for the derivatives are preserved by the multi-level linearization algorithm,","['Ginzburg-Landau model', 'd-wave', 'superconductivity', 'finite element method', 'nonlinear systems', 'error estimation', 'two-grid method', 'free energy', 'multilevel linearization', 'approximation theory', 'digital simulation', 'error analysis', 'finite element analysis', 'free energy', 'physics computing', 'superconductivity']","['finite element approximations', 'optimal error estimates', 'wave superconductors', 'multicomponent complex order-parameter', 'multilevel linearization procedure', 'conventional Ginzburg-Landau model', 'Ginzburg-Landau-type model', 'symmetry Convergence', 'Ginzburg-Landau model', 'multilevel linearization']",728,110,16,728,109,10,0,2,4
"baseball, optimization, and the world wide web the competition for baseball play-off spots-the fabled pennant race-is one of the most closely watched american sports traditions. while play-off race statistics, such as games back and magic number, are informative, they are overly conservative and do not account for the remaining schedule of games. using optimization techniques, one can model schedule effects explicitly and determine precisely when a team has secured a play-off spot or has been eliminated from contention. the riot baseball play-off races web site developed at the university of california, berkeley, provides automatic updates of new, optimization-based play-off race statistics each day of the major league baseball season. in developing the site, we found that we could determine the first-place elimination status of all teams in a division using a single linear-programming formulation, since a minimum win threshold for teams finishing in first place applies to all teams in a division. we identified a similar (but weaker) result for the problem of play-off elimination with wildcard teams ","Baseball, optimization, and the World Wide Web The competition for baseball play-off spots-the fabled pennant race-is one of the most closely watched American sports traditions. While play-off race statistics, such as games back and magic number, are informative, they are overly conservative and do not account for the remaining schedule of games. Using optimization techniques, one can model schedule effects explicitly and determine precisely when a team has secured a play-off spot or has been eliminated from contention. The RIOT Baseball Play-off Races Web site developed at the University of California, Berkeley, provides automatic updates of new, optimization-based play-off race statistics each day of the major league baseball season. In developing the site, we found that we could determine the first-place elimination status of all teams in a division using a single linear-programming formulation, since a minimum win threshold for teams finishing in first place applies to all teams in a division. We identified a similar (but weaker) result for the problem of play-off elimination with wildcard teams","['baseball play-off spot competition', 'optimization', 'World Wide Web', 'pennant race', 'play-off race statistics', 'games back', 'magic number', 'game schedule', 'RIOT Baseball Play-off Races Web site', 'linear programming', 'LP', 'minimum win threshold', 'administrative data processing', 'information resources', 'Internet', 'linear programming', 'sport']","['play-off race statistics', 'play-off', 'baseball play-off', 'play-off spot', 'optimization techniques', 'baseball optimization', 'play-off elimination', 'World Wide Web', 'optimization', 'baseball']",949,169,17,949,168,10,0,0,4
"comments on ""frequency decomposition and computing of ultrasound medical images with wavelet packets"" in this paper, errors and discrepancies in the subject paper [cincotti et al. (2002)] are highlighted. a comment, concerning the axial resolution associated to the adopted processing procedure is also reported ","Comments on ""Frequency decomposition and computing of ultrasound medical images with wavelet packets” In this paper, errors and discrepancies in the subject paper [Cincotti et al. (2002)] are highlighted. A comment, concerning the axial resolution associated to the adopted processing procedure is also reported","['ultrasound medical images', 'wavelet packets', 'frequency decomposition', 'axial resolution', 'medical diagnostic imaging', 'biomedical ultrasonics', 'image resolution', 'medical image processing', 'wavelet transforms']","['frequency decomposition', 'wavelets packets', 'medical images', 'subject paper', 'paper errors', 'ultrasound', 'computing', 'Comments', 'frequency', 'decomposition']",268,45,9,268,44,10,1,1,1
"digital rights (and wrongs) attempting to grasp the many conflicts and proposed safeguards for intellectual property is extremely difficult. legal, political, economic, and cultural issues-both domestic and international-loom large, almost dwarfing the daunting technological challenges. solutions devised by courts and legislatures and regulatory agencies are always late out of the blocks and fall ever farther behind. recently proposed legislation only illustrates the depth and complexity of the problem ","Digital rights (and wrongs) Attempting to grasp the many conflicts and proposed safeguards for intellectual property is extremely difficult. Legal, political, economic, and cultural issues-both domestic and international-loom large, almost ‘dwarfing the daunting technological challenges. Solutions devised by courts and legislatures and regulatory agencies are always late out of the blocks and fall ever farther behind. Recently proposed legislation only illustrates the depth and complexity of the problem","['intellectual property', 'cultural issues', 'economic issues', 'political issues', 'legal issues', 'industrial property', 'legislation']","['intellectual property', 'many conflicts', 'Digital rights', 'safeguards', 'wrongs', 'many', 'rights', 'Digital', 'conflicts', 'intellectual']",440,69,7,441,68,10,8,1,2
"arbortext: enabler of multichannel publishing a company has a document-say, dosage instructions for a prescription drug or a troubleshooting sheet for a dvd drive. that document starts its life in a predictable format, probably microsoft word or wordperfect, but then-to meet the needs of readers who nowadays demand access via multiple devices-the material has to be translated into many more formats: html, pagemaker, or quark, possibly rtf, almost certainly pdf, and nowadays, next-generation devices (cell phones, handheld computers) also impose their own requirements. and what if, suddenly, the dosage levels change or new workarounds emerge to handle dvd problems? that's when a company should put in a call to arbortext, a 20-year-old ann arbor, michigan-based company that exists to solve a single problem: helping clients automate multichannel publishing ","Arbortext: enabler of multichannel publishing Accompany has a document-say, dosage instructions for a prescription drug or a troubleshooting sheet for a DVD drive. That document starts its life in a predictable format, probably Microsoft Word or WordPerfect, but then-to meet the needs of readers who nowadays demand access via multiple devices-the material has to be translated into many more formats: HTML, PageMaker, or Quark, possibly RTF, almost certainly PDF, and nowadays, next-generation devices (cell phones, handheld computers) also impose their own requirements. And what if, suddenly, the dosage levels change or new workarounds emerge to handle DVD problems? That's when a company should put in a call to Arbortext, a 20-year-old Ann Arbor, Michigan-based company that exists to solve a single problem: helping clients automate multichannel publishing","['document format', 'next-generation devices', 'Arbortext', 'multichannel publishing', 'content assets', 'document handling', 'electronic publishing']","['Arbortext', 'multichannel publishing Accompany', 'Michigan-based company', '20-year-old Ann arbor', 'dosage instructions', 'single problem', 'problems thats', 'dosage levels', 'multichannel', 'multichannel publishing']",737,129,7,738,127,10,417,122,1
"the efficacy of electronic telecommunications in fostering interpersonal relationships the effectiveness of electronic telecommunications as a supplementary aid to instruction and as a communication link between students, and between students and instructors in fostering interpersonal relationships was explored in this study. more specifically, the impacts of e-mail, one of the most accessible, convenient, and easy to use computer-mediated communications, on student attitudes toward the instructor, group-mates, and other classmates were investigated. a posttest-only experimental design was adopted. in total, 68 prospective teachers enrolling in a ""computers in education"" course participated in the study for a whole semester. results from the study provided substantial evidence supporting e-mail's beneficial effects on student attitudes toward the instructor and other classmates ","The efficacy of electronic telecommunications in fostering interpersonal relationships The effectiveness of electronic telecommunications as a supplementary aid to instruction and as a communication link between students, and between students and instructors in fostering interpersonal relationships was explored in this study. More specifically, the impacts of e-mail, one of the most accessible, convenient, and easy to use computer-mediated ‘communications, on student attitudes toward the instructor, group-mates, and other classmates were investigated. A posttest-only experimental design was adopted. In total, 68 prospective teachers enrolling in a ""Computers in Education"" course participated in the study for a whole semester. Results from the study provided substantial evidence supporting e-mail's beneficial effects on student attitudes toward the instructor and other classmates","['interpersonal relationships', 'telecommunications', 'student communication link', 'e-mail', 'computer-mediated communications', 'student attitudes', 'Computers in Education course', 'educational technology', 'educational technology', 'electronic mail', 'human factors', 'social aspects of automation']","['electronic telecommunications', 'interpersonal relationships', 'student attitudes', 'students', 'e-mail beneficial effects', 'instructor group-mates', 'supplementary aid', 'electronic', 'interpersonal', 'telecommunications']",774,118,12,775,117,10,14,1,1
"measuring return: revealing roi the most critical part of the return-on-investment odyssey is to develop metrics that matter to the business and to measure systems in terms of their ability to help achieve those business goals. everything must flow from those key metrics. and don't forget to revisit those every now and then, too. since all systems wind down over time, it's important to keep tabs on how well your automation investment is meeting the metrics established by your company. manufacturers are clamoring for a tool to help quantify returns and analyze the results ","Measuring return: revealing ROI The most critical part of the return-on-investment odyssey is to develop metrics that matter to the business and to measure systems in terms of their ability to help achieve those business goals. Everything must flow from those key metrics. And don't forget to revisit those every now and then, too. Since all systems wind down over time, it's important to keep tabs on how well your automation investment is meeting the metrics established by your company. Manufacturers are ‘clamoring for a tool to help quantify returns and analyze the results","['technology purchases', 'return-on-investment', 'ROI', 'key metrics', 'automation investment', 'management', 'manufacturing industries']","['return-on-investment odyssey', 'company Manufacturers', 'business goals', 'critical part', 'key metrics', 'returns', 'part', 'metrics', 'critical', 'business']",484,95,7,485,94,10,9,1,2
"applied ethics in business information units the primary thesis of this paper is that business information professionals commonly overlook ethical dilemmas in the workplace. although the thesis remains unproven, the author highlights, by way of real and hypothetical case studies, a number of situations in which ethical tensions can be identified, and suggests that information professionals need to be more aware of the moral context of their actions. resolving ethical dilemmas should be one of the aims of competent information professionals and their managers, although it is recognized that dilemmas often cannot easily be resolved. a background to the main theories of applied ethics forms the framework for later discussion ","Applied ethics in business information units The primary thesis of this paper is that business information professionals ‘commonly overlook ethical dilemmas in the workplace. Although the thesis remains unproven, the author highlights, by way of real and hypothetical case studies, a number of situations in which ethical tensions can be identified, and suggests that information professionals need to be more aware of the moral context of their actions. Resolving ethical dilemmas should be one of the aims of competent information professionals and their managers, although it is recognized that dilemmas often cannot easily be resolved. A background to the main theories of applied ethics forms the framework for later discussion","['business information professionals', 'ethical dilemmas', 'moral context', 'applied ethics', 'business information units', 'information industry', 'information science', 'professional aspects']","['ethical dilemmas', 'Applied ethics', 'information', 'competent information professionals', 'business information professionals', 'business information units', 'ethical tensions', 'primary thesis', 'information professionals', 'ethics']",622,111,8,623,110,10,7,1,1
"hamiltonian modelling and nonlinear disturbance attenuation control of tcsc for improving power system stability to tackle the obstacle of applying passivity-based control (pbc) to power systems, an affine non-linear system widely existing in power systems is formulated as a standard hamiltonian system using a pre-feedback method. the port controlled hamiltonian with dissipation (pchd) model of a thyristor controlled serial compensator (tcsc) is then established corresponding with a revised hamiltonian function. furthermore, employing the modified hamiltonian function directly as the storage function, a non-linear adaptive l/sub 2/ gain control method is proposed to solve the problem of l/sub 2/ gain disturbance attenuation for this hamiltonian system with parametric perturbations. finally, simulation results are presented to verify the validity of the proposed controller ","Hamiltonian modelling and nonlinear disturbance attenuation control of TCSC for improving power system stability To tackle the obstacle of applying passivity-based control (PBC) to power systems, an affine non-linear system widely existing in power systems is formulated as a standard Hamiltonian system using a pre-feedback method. The port controlled Hamiltonian with dissipation (PCHD) model of a thyristor controlled serial compensator (TCSC) is then established corresponding with a revised Hamiltonian function. Furthermore, ‘employing the modified Hamiltonian function directly as the storage function, a non-linear adaptive L/sub 2/ gain control method is, proposed to solve the problem of L/sub 2/ gain disturbance attenuation for this Hamiltonian system with parametric perturbations. Finally, simulation results are presented to verify the validity of the proposed controller","['Hamiltonian modelling', 'thyristor controlled serial compensator', 'nonlinear disturbance attenuation control', 'power system stability', 'passivity-based control', 'affine nonlinear system', 'pre-feedback method', 'port controlled Hamiltonian with dissipation model', 'Hamiltonian function', 'storage function', 'nonlinear adaptive L/sub 2/ gain control method', 'parametric perturbations', 'adaptive control', 'closed loop systems', 'compensation', 'control system synthesis', 'feedback', 'nonlinear control systems', 'power system stability', 'thyristor applications']","['Hamiltonian', 'Hamiltonian function', 'power systems', 'gain disturbance attenuation', 'standard Hamiltonian system', 'power system stability', 'Hamiltonian modelling', 'gain control method', 'non-linear system', 'Hamiltonian system']",764,122,20,766,121,10,9,2,3
"quadratic programming algorithms for large-scale model predictive control quadratic programming (qp) methods are an important element in the application of model predictive control (mpc). as larger and more challenging mpc applications are considered, more attention needs to be focused on the construction and tailoring of efficient qp algorithms. in this study, we tailor and apply a new qp method, called qpschur, to large mpc applications, such as cross directional control problems in paper machines. written in c++, qpschur is an object oriented implementation of a novel dual space, schur complement algorithm. we compare this approach to three widely applied qp algorithms and show that qpschur is significantly more efficient (up to two orders of magnitude) than the other algorithms. in addition, detailed simulations are considered that demonstrate the importance of the flexible, object oriented construction of qpschur, along with additional features for constraint handling, warm starts and partial solution ","Quadratic programming algorithms for large-scale model predictive control Quadratic programming (QP) methods are an important element in the application of model predictive control (MPC). As larger and more challenging MPC applications are considered, more attention needs to be focused on the construction and tailoring of efficient QP algorithms. In this study, we tailor and apply a new QP method, called QPSchur, to large MPC applications, such as cross directional control problems in paper machines. Written in C++, QPSchur is an object oriented implementation of a novel dual space, Schur complement algorithm. We compare this approach to three widely applied QP algorithms and show that QPSchur is significantly more efficient (up to two orders of magnitude) than the other algorithms. In addition, detailed simulations are considered that demonstrate the importance of the flexible, object oriented construction of QPSchur, along with additional features for constraint handling, warm starts and partial solution","['large-scale model predictive control', 'quadratic programming algorithms', 'QPSchur', 'cross directional control problems', 'paper machines', 'object oriented implementation', 'dual space Schur complement algorithm', 'simulations', 'flexible object oriented construction', 'constraint handling', 'warm starts', 'partial solution', 'control system analysis computing', 'large-scale systems', 'object-oriented programming', 'paper industry', 'predictive control', 'quadratic programming']","['predictive control', 'Quadratic programming algorithms', 'challenging MPC applications', 'efficient QP algorithms', 'large MPC applications', 'complement algorithm', 'large-scale model', 'other algorithms', 'new QP method', 'QP algorithms']",873,150,18,873,149,10,0,0,7
"a new voltage-vector selection algorithm in direct torque control of induction motor drives ac drives based on direct torque control of induction machines allow high dynamic performance to be obtained with very simple control schemes. the drive behavior, in terms of current, flux and torque ripple, is dependent on the utilised voltage vector selection strategy and the operating conditions. in this paper a new voltage vector selection algorithm, which allows a sensible reduction of the rms value of the stator current ripple without increasing the average value of the inverter switching frequency and without the need of a pwm pulse generator block is presented numerical simulations have been carried out to validate the proposed method ","Anew voltage-vector selection algorithm in direct torque control of induction motor drives AC drives based on direct torque control of induction machines allow high dynamic performance to be obtained with very simple control schemes. The drive behavior, in terms of current, flux and torque ripple, is. dependent on the utilised voltage vector selection strategy and the ‘operating conditions. In this paper a new voltage vector selection algorithm, which allows a sensible reduction of the RMS value of the stator current ripple without increasing the average value of the inverter switching frequency and without the need of a PWM pulse generator block is presented Numerical simulations have been carried out to validate the proposed method","['voltage-vector selection algorithm', 'direct torque control', 'induction motor drives', 'AC drives', 'high dynamic performance', 'torque ripple', 'voltage vector selection strategy', 'operating conditions', 'RMS value', 'stator current ripple', 'inverter switching frequency', 'torque variations', 'flux variations', '4-poles induction motor', 'steady-state operation', 'dynamic behavior', 'torque step response', '220 V', '50 Hz', '4 kW', 'induction motor drives', 'invertors', 'machine control', 'stators', 'torque control']","['direct torque control', 'voltage-vector selection algorithm', 'simple control schemes', 'induction machines', 'drive behavior', 'torque ripple', 'torque', 'control', 'drive', 'selection']",628,116,25,630,114,10,419,114,5
"how does attitude impact it implementation: a study of small business owners according to previous studies, attitude towards information technology (it) among small business owners appears to be a key factor in achieving high quality it implementations. in an effort to extend this stream of research, we conducted case studies with small business owners and learned that high quality it implementations resulted with owners who had positive or negative attitudes toward it, but not with owners who had uncertain attitudes. owners with apolar attitude, either positive or negative, all took action to temper the uncertainty and risk surrounding the use of new it in their organization. in contrast, owners with uncertain attitudes did not make mitigating attempts to reduce uncertainty and risk. a consistent finding among those with high quality it implementations was an entrepreneurial, or shared, management style. it is proposed, based on case study data, that small business owners with an uncertain attitude towards it might experience higher quality it results in their organizations through practicing a more entrepreneurial, or shared, management style. the study provides insights for both computer specialists and small business owners planning it implementations ","How does attitude impact IT implementation: a study of small business owners According to previous studies, attitude towards information technology (IT) among small business owners appears to be a key factor in achieving high quality IT implementations. In an effort to extend this stream of research, we conducted case studies with small business owners and learned that high quality IT implementations resulted with owners who had positive or negative attitudes toward IT, but not with owners who had uncertain attitudes. Owners with apolar attitude, either positive ‘or negative, all took action to temper the uncertainty and risk surrounding the use of new IT in their organization. In contrast, ‘owners with uncertain attitudes did not make mitigating attempts to reduce uncertainty and risk. A consistent finding among those with high quality IT implementations was an entrepreneurial, or shared, management style. It is proposed, based on case study data, that small business owners with an uncertain attitude towards IT might experience higher quality IT results in their organizations through practicing a more entrepreneurial, or shared, management style. The study provides insights for both computer specialists and small business owners, planning IT implementations","['small business owners', 'information technology implementation', 'negative attitudes', 'positive attitudes', 'uncertain attitude', 'risk', 'organization', 'management style', 'computer specialists', 'planning', 'business data processing', 'human factors', 'information technology', 'personal computing', 'risk management']","['small business owners', 'uncertain attitudes Owners', 'previous studies attitude', 'negative attitudes', 'contrast owners', 'case study data', 'attitude impact', 'polar attitude', 'attitudes', 'uncertain attitude']",1086,191,15,1089,190,10,8,3,6
"sia shelves t+1 decision till 2004 the securities industry association has decided that a move to t+1 is more than the industry can handle right now. stp, however, will remain a focus ","SIA shelves T+1 decision till 2004 The Securities Industry Association has decided that a move to T+1 is more than the industry can handle right now. STP, however, will remain a focus","['Securities Industry Association', 'straight-through-processing', 'T+1', 'securities trading']","['Securities Industry Association', 'shelves t1 decision', 'move', 'more', 't1', 'shelves', 'decision', 'Industry', 'Securities', 'Association']",152,33,4,152,32,10,0,0,1
"control performance with three translational degrees of freedom for multiple degree-of-freedom (dof) systems, it is important to determine how accurately operators can control each dof and what influence perceptual, information processing, and psychomotor components have on performance. sixteen right-handed male students participated in 2 experiments: 1 involving positioning and 1 involving tracking with 3 translational dofs. to separate perceptual and psychomotor effects, we used 2 control-display mappings that differed in the coupling of vertical and depth dimensions to the up-down and fore-aft control axes. we observed information processing effects in the positioning task: initial error correction on the vertical dimension lagged in time behind the horizontal dimension. the depth dimension error correction lagged behind both, which was ascribed to the poorer perceptual information. we observed this perceptual effect also in the tracking experiment. motor effects were also present, with tracking errors along the up-down axis of the hand controller being 1.1 times larger than along the fore-aft axis. these results indicate that all 3 components contribute to control performance. actual applications of this research include interface design ","Control performance with three translational degrees of freedom For multiple degree-of-freedom (DOF) systems, it is important to determine how accurately operators can control each DOF and what influence perceptual, information processing, and psychomotor components have on performance. Sixteen right-handed male students participated in 2 experiment: 1 involving positioning and 1 involving tracking with 3 translational DOF. To separate perceptual and psychomotor effects, we used 2 contro-display mappings that differed in the coupling of vertical and depth dimensions to the up-down and fore-aft control axes. We observed information processing effects in the positioning task Initial error correction on the vertical dimension lagged in time behind the horizontal dimension. The depth dimension error correction lagged behind both, which was ascribed to the poorer perceptual information. We observed this perceptual effect also in the tracking experiment. Motor effects were also present, with tracking errors along the up-down axis of the hand controller being 1.1 times larger than along the fore-aft axis. These results indicate that all 3 components contribute to control performance. Actual applications of this research include interface design","['control performance', 'multi-DOF systems', 'positioning', 'tracking', 'perceptual effects', 'psychomotor effects', 'initial error correction', 'depth dimension error correction', 'interface design', 'remote control', 'virtual reality', 'graphical user interfaces', 'human factors']","['Control performance', 'perceptual information processing', 'information processing effects', 'poorer perceptual information', 'translational degrees', 'psychomotor effects', 'translational d.f.', 'perceptual effect', 'translational', 'performance']",1085,178,13,1081,177,10,10,4,5
"real-time transmission of pediatric echocardiograms using a single isdn line we tested the adequacy of a videoconferencing system using a single integrated systems digital network (isdn) line (128 kilobits per second) for the remote diagnosis of children with suspected congenital heart disease (chd). real-time echocardiogram interpretation was compared to subsequent videotape review in 401 studies with concordance in 383 (95.5%) studies. a new diagnosis of chd was made in 98 studies. immediate patient transfer was arranged based upon a real-time diagnosis in five studies. in 300 studies, a normal diagnosis obviated further evaluation. a single isdn line is adequate for transmission of pediatric echocardiograms and it allows for remote management of patients with chd ","Real-time transmission of pediatric echocardiograms using a single ISDN line We tested the adequacy of a videoconferencing system using a single integrated systems digital network (ISDN) line (128 kilobits per second) for the remote diagnosis of children with suspected congenital heart disease (CHD). Real-time echocardiogram interpretation was compared to subsequent videotape review in 401 studies with concordance in 383 (95.5%) studies. A new diagnosis of CHD was made in 98 studies. Immediate patient transfer was arranged based upon a real-time diagnosis in five studies. In 300 studies, a normal diagnosis obviated further evaluation. A single ISDN line is adequate for transmission of pediatric echocardiograms and it allows for remote management of Patients with CHD","['real-time pediatric echocardiogram transmission', 'single ISDN line', 'videoconferencing system', 'remote diagnosis', 'children', 'suspected congenital heart disease', 'real-time echocardiogram interpretation', 'videotape review', 'immediate patient transfer', 'remote patient management', 'diseases', 'echocardiography', 'ISDN', 'medical signal processing', 'paediatrics', 'patient diagnosis', 'real-time systems', 'teleconferencing', 'telemedicine']","['pediatric echocardiograms', 'single ISDN line', 'Real-time echocardiogram interpretation', 'videoconferencing system', 'Real-time transmission', 'remote diagnosis', 'normal diagnosis', 'Real-time', 'echocardiogram', 'single']",663,115,19,663,114,10,0,0,4
"semantic b2b integration: issues in ontology-based approaches solving queries to support e-commerce transactions can involve retrieving and integrating information from multiple information resources. often, users don't care which resources are used to answer their query. in such situations, the ideal solution would be to hide from the user the details of the resources involved in solving a particular query. an example would be providing seamless access to a set of heterogeneous electronic product catalogues. there are many problems that must be addressed before such a solution can be provided. in this paper, we discuss a number of these problems, indicate how we have addressed these and go on to describe the proof-of-concept demonstration system we have developed ","Semantic B2B integration: issues in ontology-based approaches Solving queries to support e-commerce transactions can involve retrieving and integrating information from multiple information resources. Often, users don't care which resources are used to answer their query. In such situations, the ideal solution would be to hide from the user the details of the resources involved in solving a particular query. An ‘example would be providing seamless access to a set of heterogeneous, electronic product catalogues. There are many problems that must be addressed before such a solution can be provided. In this paper, we discuss a number of these problems, indicate how we have addressed these and go on to describe the proof-of-concept demonstration system we have developed","['e-commerce transactions', 'queries', 'information integration', 'information retrieval', 'multiple information resources', 'heterogeneous electronic product catalogues', 'ontology-based approaches', 'semantic B2B integration', 'client-server systems', 'distributed databases', 'electronic commerce', 'knowledge engineering', 'nomenclature', 'query processing']","['multiple information resources', 'ontology-based approaches', 'commerce transactions', 'particular query', 'integration', 'Semantic', 'issues', 'bob', 'query', 'ontology-based']",658,118,14,660,117,10,7,2,2
"incremental motion control of linear synchronous motor in this study a particular incremental motion control problem, which is specified by the trapezoidal velocity profile using multisegment sliding mode control (mssmc), is proposed to control a permanent magnet linear synchronous motor (pmlsm) servo drive system. first, the structure and operating principle of the pmlsm are described in detail. second, a field-oriented control pmlsm servo drive is introduced. then, each segment of the multisegment switching surfaces is designed to match the corresponding part of the trapezoidal velocity profile, thus the motor dynamics on the specified-segment switching surface have the desired velocity or acceleration corresponding part of the trapezoidal velocity profile. in addition, the proposed control system is implemented in a pc-based computer control system. finally, the effectiveness of the proposed pmlsm servo drive system is demonstrated by some simulated and experimental results ","Incremental motion control of linear synchronous motor In this study a particular incremental motion control problem, which is specified by the trapezoidal velocity profile using multisegment sliding mode control (MSSMC), is proposed to control a permanent magnet linear synchronous motor (PMLSM) servo drive system. First, the structure and operating principle of the PMLSM are described in detail Second, a field-oriented control PMLSM servo drive is introduced. Then, each segment of the multisegment switching surfaces is designed to match the corresponding part of the trapezoidal velocity profile, thus the motor dynamics on the specified-segment switching surface have the desired velocity or acceleration corresponding part of the trapezoidal velocity profile. In addition, the proposed control system is implemented in a PC-based computer control system. Finally, the effectiveness of the proposed PMLSM servo drive system is demonstrated by some simulated and experimental results","['incremental motion control', 'linear synchronous motor', 'trapezoidal velocity profile', 'multisegment sliding mode control', 'permanent magnet motor', 'servo drive system', 'field-oriented control', 'multisegment switching surfaces', 'motor dynamics', 'linear synchronous motors', 'motion control', 'permanent magnet motors', 'servomotors', 'variable structure systems']","['trapezoidal velocity profile', 'Incremental motion control', 'linear synchronous motor', 'control', 'servo drive system', 'control system', 'multisegment switching surfaces', 'motor dynamics', 'mode control', 'motion']",852,141,14,851,140,10,0,1,0
what's in a name? [mobile telephony branding] mobile operators are frantically consolidating businesses into single international brands ,What's in a name? [mobile telephony branding] Mobile operators are frantically consolidating businesses into single international brands,"['mobile telephony', 'branding', 'consolidating businesses', 'cellular radio']","['single international brands', 'mobile telephony', 'Mobile operators', 'businesses', 'whats', 'name', 'mobile', 'Mobile', 'telephony', 'operators']",120,18,4,120,17,10,0,0,1
"data assimilation of local model error forecasts in a deterministic model one of the most popular data assimilation techniques in use today are of the kalman filter type, which provide an improved estimate of the state of a system up to the current time level, based on actual measurements. from a forecasting viewpoint, this corresponds to an updating of the initial conditions. the standard forecasting procedure is to then run the model uncorrected into the future, driven by predicted boundary and forcing conditions. the problem with this methodology is that the updated initial conditions quickly 'wash-out', thus, after a certain forecast horizon the model predictions are no better than from an initially uncorrected model. this study demonstrates that through the assimilation of error forecasts (in the present case made using so-called local models) entire model domains can be corrected for extended forecast horizons (i.e. long after updated initial conditions have become washed-out), thus demonstrating significant improvements over the conventional methodology. some alternate uses of local models are also explored for the re-distribution of error forecasts over the entire model domain, which are then compared with more conventional kalman filter type schemes ","Data assimilation of local model error forecasts in a deterministic model One of the most popular data assimilation techniques in use today are of the Kalman filter type, which provide an improved estimate of the state of a system up to the current time level, based on actual measurements. From a forecasting viewpoint, this corresponds to an updating of the initial conditions. The standard forecasting procedure is to then run the model uncorrected into the future, driven by predicted boundary and forcing conditions. The problem with this methodology is that the updated initial conditions quickly ‘wash-out thus, after a certain forecast horizon the model predictions are no better than from an initially uncorrected model. This study demonstrates that through the assimilation of error forecasts (in the present case made using so-called local models) entire model domains can be corrected for extended forecast horizons (i. long after updated initial conditions have become washed-out), thus demonstrating significant improvements cover the conventional methodology. Some alternate uses of local models are also explored for the re-distribution of error forecasts over the entire model domain, which are then compared with more conventional Kalman filter type schemes","['data assimilation', 'local model error forecasts', 'deterministic model', 'Kalman filter', 'forcing conditions', 'forecast horizon', 'error prediction', 'hydrodynamic modelling', 'computational fluid dynamics', 'deterministic algorithms', 'error statistics', 'hydrodynamics', 'Kalman filters']","['model', 'error forecasts', 'entire model domain', 'extended forecast horizons', 'certain forecast horizon', 'so-called local models', 'deterministic model', 'uncorrected model', 'model predictions', 'local models']",1088,192,13,1085,191,10,5,3,3
"vpp fortran and the design of hpf/ja extensions vpp fortran is a data parallel language that has been designed for the vpp series of supercomputers. in addition to pure data parallelism, it contains certain low-level features that were designed to extract high performance from user programs. a comparison of vpp fortran and high-performance fortran (hpf) 2.0 shows that these low-level features are not available in hpf 2.0. the features include asynchronous interprocessor communication, explicit shadow, and the local directive. they were shown in vpp fortran to be very useful in handling real-world applications, and they have been included in the hpf/ja extensions. they are described in the paper. the hpf/ja language specification version 1.0 is an extension of hpf 2.0 to achieve practical performance for real-world applications and is a result of collaboration in the japan association for hpf (jahpf). some practical programming and tuning procedures with the hpf/ja language specification are described, using the nas parallel benchmark bt as an example ","VpP Fortran and the design of HPF/JA extensions VPP Fortran is a data parallel language that has been designed for the VPP series of supercomputers. In addition to pure data parallelism, it contains certain low-level features that were designed to extract high performance from user programs. A comparison of VPP Fortran and High-Performance Fortran (HPF) 2.0 shows that these low-level features are not available in HPF 2.0. The features include asynchronous interprocessor communication, explicit shadow, and the LOCAL directive They were shown in VPP Fortran to be very useful in handling real-world applications, and they have been included in the HPF/JA extensions. They are described in the paper. The HPF/JA Language Specification Version 1.0 is an extension of HPF 2.0 to achieve practical performance for real-world applications and is a result of collaboration in the Japan Association for HPF (JAHPF). Some practical programming and tuning procedures with the HPF/JA Language Specification are described, using the NAS Parallel Benchmark BT as an example","['VPP Fortran', 'data parallel language', 'data parallelism', 'high performance', 'asynchronous interprocessor communication', 'explicit shadow', 'benchmark', 'asynchronous communication', 'data locality', 'FORTRAN', 'parallel languages']","['pp Fortran', 'HPF/JA Language Specification', 'HPF/JA extensions', 'High-Performance Fortran hpf', 'certain low-level features', 'data parallel language', 'pure data parallelism', 'pp series', 'pp', 'Fortran']",905,163,11,904,162,10,0,1,4
"robot trajectory control using neural networks the use of a new type of neural network (nn) for controlling the trajectory of a robot is discussed. a control system is described which comprises an nn-based controller and a fixed-gain feedback controller. the nn-based controller employs a modified recurrent nn, the weights of which are obtained by training another nn to identify online the inverse dynamics of the robot. the work has confirmed the superiority of the proposed nn-based control system in rejecting large disturbances ","Robot trajectory control using neural networks The use of a new type of neural network (NN) for controlling the trajectory of a robot is discussed. A control system is described which comprises an NN-based controller and a fixed-gain feedback controller. The NN-based controller employs a modified recurrent NN, the weights of which are obtained by training another NN to identify online the inverse dynamics of the robot. The work has confirmed the superiority of the proposed NN-based control system in rejecting large disturbances","['robot trajectory control', 'neural networks', 'neural network-based controller', 'control system', 'fixed-gain feedback controller', 'modified recurrent neural network', 'neural network training', 'robot inverse dynamics', 'large disturbance rejection', 'robot manipulators', 'time-varying nonlinear multivariable plant', 'fourth-order Runge-Kutta algorithm', 'feedback', 'learning (artificial intelligence)', 'manipulator dynamics', 'mobile robots', 'multivariable control systems', 'neurocontrollers', 'nonlinear control systems', 'position control', 'recurrent neural nets', 'Runge-Kutta methods', 'time-varying systems']","['control', 'nt-based controller', 'fixed-gain feedback controller', 'Robot trajectory control', 'nt-based control system', 'neural network anne', 'neural networks', 'neural', 'trajectory', 'control system']",451,84,23,451,83,10,0,0,0
"numerical approximation of nonlinear bvps by means of bvms boundary value methods (bvms) would seem to be suitable candidates for the solution of nonlinear boundary value problems (bvps). they have been successfully used for solving linear bvps together with a mesh selection strategy based on the conditioning of the linear systems. our aim is to extend this approach so as to use them for the numerical approximation of nonlinear problems. for this reason, we consider the quasi-linearization technique that is an application of the newton method to the nonlinear differential equation. consequently, each iteration requires the solution of a linear bvp. in order to guarantee the convergence to the solution of the continuous nonlinear problem, it is necessary to determine how accurately the linear bvps must be solved. for this goal, suitable stopping criteria on the residual and on the error for each linear bvp are given. numerical experiments on stiff problems give rather satisfactory results, showing that the experimental code, called tom, that uses a class of bvms and the quasi-linearization technique, may be competitive with well known solvers for bvps ","Numerical approximation of nonlinear BVPs by means of BVMs Boundary Value Methods (BVMs) would seem to be suitable candidates for the solution of nonlinear Boundary Value Problems (BVPs). They have been successfully used for solving linear BVPs together with a mesh selection strategy based on the conditioning of the linear systems. Our aim is to extend this approach so as to use them for the numerical approximation of nonlinear problems. For this reason, we consider the quasi-linearization technique that is an application of the Newton method to the nonlinear differential equation. Consequently, each iteration requires the solution of a linear BVP. In order to guarantee the convergence to the solution of the continuous nonlinear problem, it is necessary to determine how accurately the linear BVPs must be solved. For this goal, suitable stopping criteria on the residual and ‘on the error for each linear BVP are given. Numerical experiments on stiff problems give rather satisfactory results, showing that the experimental code, called TOM, that uses a class of BVMs and the quasi-linearization technique, may be competitive with well known solvers for BVPS","['numerical approximation', 'nonlinear boundary value problems', 'boundary value methods', 'mesh selection strategy', 'quasi-linearization technique', 'Newton method', 'nonlinear differential equation', 'stopping criteria', 'stiff problems', 'BVMs', 'boundary-value problems', 'Newton method', 'nonlinear differential equations']","['Numerical approximation', 'nonlinear', 'nonlinear differential equation', 'continuous nonlinear problem', 'nonlinear problems', 'linear systems', 'nonlinear bps', 'linear b.p', 'linear bp', 'linear bps']",987,183,13,988,182,10,2,1,7
"nurturing clients' trust to encourage engagement success during the customization of erp systems customization is a crucial, lengthy, and costly aspect in the successful implementation of erp systems, and has, accordingly, become a major specialty of many vendors and consulting companies. the study examines how such companies can increase their clients' perception of engagement success through increased client trust that is brought about through responsive and dependable customization. survey data from erp customization clients show that, as hypothesized, clients' trust influenced their perception of engagement success with the company. the data also show that clients' trust in the customization company was increased when the company behaved in accordance with client expectations by being responsive, and decreased when the company behaved in a manner that contradicted these expectations by not being dependable. responses to an open-ended question addendum attached to the survey corroborated the importance of responsiveness and dependability. implications for customization companies and research on trust are discussed ","Nurturing clients’ trust to encourage engagement success during the customization of ERP systems Customization is a crucial, lengthy, and costly aspect in the successful implementation of ERP systems, and has, accordingly, become a major specialty of many vendors and consulting companies. The study examines how such companies can increase their clients' perception of engagement success through increased client trust that is brought about through responsive and dependable customization. Survey data from ERP customization clients show that, as hypothesized, clients’ trust influenced their perception of engagement success with the company. The data also show that clients’ trust in the customization company was increased when the company behaved in accordance with client expectations by being responsive, and decreased when the company behaved in a manner that contradicted these expectations by not being dependable. Responses to an open-ended question addendum attached to the survey corroborated the importance of responsiveness and dependability. Implications for customization companies and research on trust are discussed","['client trust', 'engagement success', 'customization', 'ERP systems', 'enterprise resource planning systems', 'vendors', 'consulting companies', 'perceived responsiveness', 'MRP II implementation', 'integrity', 'benevolence', 'dependability', 'behavioural sciences', 'business data processing', 'human factors', 'management science']","['engagement success', 'ERP systems Customization', 'ERP customization clients', 'customization company', 'client expectations', 'clients', 'clients perception', 'such companies', 'client trust', 'companies']",977,159,16,977,158,10,3,3,2
"statistical analysis of nonlinearly reconstructed near-infrared tomographic images. i. theory and simulations near-infrared (nir) diffuse tomography is an emerging method for imaging the interior of tissues to quantify concentrations of hemoglobin and exogenous chromophores noninvasively in vivo. it often exploits an optical diffusion model-based image reconstruction algorithm to estimate spatial property values from measurements of the light flux at the surface of the tissue. in this study, mean-squared error (mse) over the image is used to evaluate methods for regularizing the ill-posed inverse image reconstruction problem in nir tomography. estimates of image bias and image standard deviation were calculated based upon 100 repeated reconstructions of a test image with randomly distributed noise added to the light flux measurements. it was observed that the bias error dominates at high regularization parameter values while variance dominates as the algorithm is allowed to approach the optimal solution. this optimum does not necessarily correspond to the minimum projection error solution, but typically requires further iteration with a decreasing regularization parameter to reach the lowest image error. increasing measurement noise causes a need to constrain the minimum regularization parameter to higher values in order to achieve a minimum in the overall image mse ","Statistical analysis of nonlinearly reconstructed near-infrared tomographic images. |. Theory and simulations Near-infrared (NIR) diffuse tomography is an emerging method for imaging the interior of tissues to quantify concentrations of hemoglobin and ‘exogenous chromophores noninvasively in vivo. It often exploits an optical diffusion model-based image reconstruction algorithm to estimate spatial property values from measurements of the light flux at the surface of the tissue. In this study, mean-squared error (MSE) over the image is used to evaluate methods for regularizing the ill-posed inverse image reconstruction problem in NIR tomography. Estimates of image bias and image standard deviation were calculated based upon 100 repeated reconstructions of a test image with randomly distributed noise added to the light flux measurements. It was observed that the bias error dominates at high regularization parameter values while variance dominates as the algorithm is allowed to approach the optimal solution. This optimum does not necessarily correspond to the minimum projection error solution, but typically requires further iteration with a decreasing regularization parameter to reach the lowest image error. Increasing measurement noise causes a need to constrain the minimum regularization parameter to higher values in order to achieve a minimum in the overall image MSE","['medical diagnostic imaging', 'hemoglobin', 'oxygen saturation', 'photon migration', 'optical diffusion model-based image reconstruction algorithm', 'decreasing regularization parameter', 'lowest image error', 'minimum regularization parameter constraint', 'bias error', 'optimal solution', 'light flux', 'mean-squared error', 'ill-posed inverse image reconstruction problem regularization', 'spatial property values estimation', 'test image', 'randomly distributed noise', 'O/sub 2/', 'image reconstruction', 'infrared imaging', 'inverse problems', 'iterative methods', 'medical image processing', 'optical tomography', 'statistical analysis']","['reconstruction', 'minimum regularization parameter', 'light flux measurements', 'Statistical analysis', 'lowest image error', 'overall image', 'test image', 'image bias', 'imaging', 'Statistical']",1191,199,24,1192,198,10,10,2,3
"new tuning method for pid controller in this paper, a tuning method for proportional-integral-derivative (pid) controller and the performance assessment formulas for this method are proposed. this tuning method is based on a genetic algorithm based pid controller design method. for deriving the tuning formula, the genetic algorithm based design method is applied to design pid controllers for a variety of processes. the relationship between the controller parameters and the parameters that characterize the process dynamics are determined and the tuning formula is then derived. using simulation studies, the rules for assessing the performance of a pid controller tuned by the proposed method are also given. this makes it possible to incorporate the capability to determine if the pid controller is well tuned or not into an autotuner. an autotuner based on this new tuning method and the corresponding performance assessment rules is also established. simulations and real-time experimental results are given to demonstrate the effectiveness and usefulness of these formulas ","New tuning method for PID controller In this paper, a tuning method for proportional-integral-derivative (PID) controller and the performance assessment formulas for this method are proposed. This tuning method is based on a genetic algorithm based PID controller design method. For deriving the tuning formula, the genetic algorithm based design method is applied to design PID controllers for a variety of processes. The relationship between the controller parameters and the parameters that characterize the process dynamics are determined and the tuning formula is then derived. Using simulation studies, the rules for assessing the performance of a PID controller tuned by the proposed method are also given. This makes it possible to incorporate the capability to determine if the PID controller is well tuned or not into an autotuner. An autotuner based on this new tuning method and the corresponding performance assessment rules is also established. Simulations and real-time experimental results are given to demonstrate the effectiveness and usefulness of these formulas","['tuning method', 'PID controller', 'proportional-integral-derivative controller', 'genetic algorithm', 'controller design method', 'process dynamics', 'autotuner', 'control system analysis', 'control system synthesis', 'genetic algorithms', 'three-term control', 'tuning']","['method', 'tuning formula', 'performance assessment formulas', 'controller design method', 'controller parameters', 'controllers', 'tuning method', 'design method', 'new tuning method', 'New tuning method']",921,162,12,921,161,10,0,0,3
"geotensity: combining motion and lighting for 3d surface reconstruction this paper is about automatically reconstructing the full 3d surface of an object observed in motion by a single static camera. based on the two paradigms, structure from motion and linear intensity subspaces, we introduce the geotensity constraint that governs the relationship between four or more images of a moving object. we show that it is possible in theory to solve for 3d lambertian surface structure for the case of a single point light source and propose that a solution exists for an arbitrary number point light sources. the surface may or may not be textured. we then give an example of automatic surface reconstruction of a face under a point light source using arbitrary unknown object motion and a single fixed camera ","Geotensity: combining motion and lighting for 3D surface reconstruction This paper is about automatically reconstructing the full 3D surface of an object observed in motion by a single static camera. Based on the two paradigms, structure from motion and linear intensity subspaces, we introduce the geotensity constraint that governs the relationship between four or more images of a moving object. We show that it is possible in theory to solve for 3D Lambertian surface structure for the case of a single point light source and propose that a solution exists for an arbitrary number point light sources. The surface may or may not be textured. We then give an example of automatic surface reconstruction of a face under a point light source using arbitrary unknown object motion and a single fixed camera","['full 3D surface', 'single static camera', 'linear intensity subspaces', 'geotensity constraint', '3D Lambertian surface structure', 'single point light source', 'arbitrary number point light sources', 'automatic surface reconstruction', 'point light source', 'linear image subspaces', 'structure-from-motion', 'computer vision', 'image reconstruction']","['point light source', 'motion', 'surface', 'automatic surface reconstruction', 'Lambertian surface structure', 'd surface reconstruction', 'arbitrary number point', 'single static camera', 'full d surface', 'light sources']",675,133,13,675,132,10,0,0,2
"sbc gets more serious on regulatory compliance with one eye on the past and the other on its future, sbc communications last week created a unit it hopes will bring a cohesiveness and efficiency to its regulatory compliance efforts that previously had been lacking. the carrier also hopes the new regulatory compliance unit will help it accomplish its short-term goal of landing fcc approval. to provide long-distance service throughout its region, and its longer-term, goal of reducing the regulatory burdens under which it and currently operate ","SBC gets more serious on regulatory compliance With one eye on the past and the other on its future, SBC Communications last week created a unit it hopes will bring a cohesiveness and efficiency to its regulatory compliance efforts that previously had been lacking. The carrier also hopes the new regulatory compliance unit will help it accomplish its short-term goal of landing FCC approval. to provide long-distance service throughout its region, and its longer-term, goal of reducing the regulatory burdens under which it and currently operate","['SBC Communications', 'regulatory compliance', 'telecom carrier', 'telecommunication']","['regulatory compliance efforts', 'future SBC Communications', 'regulatory burdens', 'longer-term goal', 'short-term goal', 'regulatory', 'last week', 'SBC', 'compliance', 'regulatory compliance']",461,87,4,461,86,10,0,0,0
"watermarking techniques for electronic delivery of remote sensing images earth observation missions have recently attracted a growing interest, mainly due to the large number of possible applications capable of exploiting remotely sensed data and images. along with the increase of market potential, the need arises for the protection of the image products. such a need is a very crucial one, because the internet and other public/private networks have become preferred means of data exchange. a critical issue arising when dealing with digital image distribution is copyright protection. such a problem has been largely addressed by resorting to watermarking technology. a question that obviously arises is whether the requirements imposed by remote sensing imagery are compatible with existing watermarking techniques. on the basis of these motivations, the contribution of this work is twofold: assessment of the requirements imposed by remote sensing applications on watermark-based copyright protection, and modification of two well-established digital watermarking techniques to meet such constraints. more specifically, the concept of near-lossless watermarking is introduced and two possible algorithms matching such a requirement are presented. experimental results are shown to measure the impact of watermark introduction on a typical remote sensing application, i.e., unsupervised image classification ","Watermarking techniques for electronic delivery of remote sensing images Earth observation missions have recently attracted a growing interest, mainly due to the large number of possible applications capable of exploiting remotely sensed data and images. Along with the increase of market potential, the need arises for the protection of the image products. Such a need is a very crucial one, because the Internet and other public/private networks have become preferred means of data exchange. A critical issue arising when dealing with digital image distribution is copyright protection. Such a problem has been largely addressed by resorting to watermarking technology. A question that obviously arises is whether the requirements imposed by remote sensing imagery are compatible with existing watermarking techniques. On the basis of these motivations, the contribution of this work is twofold: assessment of the requirements imposed by remote sensing applications on watermark-based copyright protection, and modification of two well-established digital watermarking techniques to meet such constraints. More specifically, the concept of near-lossless watermarking is introduced and two possible algorithms matching such a requirement are presented. Experimental results are shown to measure the impact of watermark introduction on a typical remote sensing application, ie., unsupervised image classification","['remote sensing images', 'electronic delivery', 'watermarking techniques', 'Earth observation missions', 'copyright protection', 'digital watermarking', 'near-lossless watermarking', 'digital image distribution', 'unsupervised image classification', 'copy protection', 'image classification', 'image coding', 'remote sensing']","['watermarking techniques', 'remote', 'watermark-based copyright protection', 'images', 'unsupervised image classification', 'digital image distribution', 'possible applications', 'electronic delivery', 'image products', 'techniques']",1217,198,13,1216,197,10,3,1,1
"virtual development center the virtual development center of the institute for women and technology seeks to significantly enhance the impact of women on technology. it addresses this goal by increasing the number of women who have input on created technology, enhancing the ways people teach and develop technology, and developing need-based technology that serves the community. through activities of the virtual development center, a pattern is emerging regarding how computing technologies do or do not satisfy the needs of community groups, particularly those communities serving women. this paper describes the virtual development center program and offers observations on the impact of computing technology on non-technical communities ","Virtual Development Center The Virtual Development Center of the Institute for Women and Technology seeks to significantly enhance the impact of women on technology. It addresses this goal by increasing the number of women who have input on created technology, enhancing the ways people teach and develop technology, and developing need-based technology that serves the ‘community. Through activities of the Virtual Development Center, a pattern is emerging regarding how computing technologies do or do not satisfy the needs of community groups, particularly those communities serving women. This paper describes the Virtual Development Center program and offers observations on the impact of computing technology ‘on non-technical communities","['Virtual Development Center', 'women', 'information technology', 'teaching', 'community groups', 'gender issues', 'computer science education', 'computer science education', 'gender issues', 'information technology', 'social aspects of automation']","['non-technical communities', 'leeds-based technology', 'community groups', 'technology', 'communities', 'Center', 'Virtual', 'Development', 'Virtual Development center', 'Virtual Development Center']",637,107,11,639,106,10,11,2,1
"teaching modeling in management science this essay discusses how we can most effectively teach management science to students in mba or similar programs who will be, at best, part-time practitioners of these arts. i take as a working hypothesis the radical proposition that the heart of management science itself is not the impressive array of tools that have been built up over the years (optimization, simulation, decision analysis, queuing, and so on) but rather the art of reasoning logically with formal models. i believe it is necessary with this group of students to teach basic modeling skills, and in fact it is only when such students have these basic skills as a foundation that they are prepared to acquire the more sophisticated skills needed to employ management science. in this paper i present a hierarchy of modeling skills, from numeracy skills through sophisticated management science skills, as a framework within which to plan courses for the occasional practitioner ","Teaching modeling in management science This essay discusses how we can most effectively teach Management Science to students in MBA or similar programs who will be, at best, part-time practitioners of these arts. | take as a working hypothesis the radical proposition that the heart of Management Science itselt is not the impressive array of tools that have been built up over the years (optimization, simulation, decision analysis, queuing, and so on) but rather the art of reasoning logically with formal models. | believe it is necessary with this group of students to teach basic modeling skills, and in fact it is only when such students have these basic skills as a foundation that they are prepared to acquire the more sophisticated skills needed to employ Management Science. In this paper | present a hierarchy of modeling skills, from numeracy skills through sophisticated Management Science skills, as a framework within which to plan courses for the occasional practitioner","['management science', 'modeling', 'numeracy skills', 'formal models', 'decision analysis', 'educational courses', 'management science', 'modelling']","['basic modelling skills', 'sophisticated skills', 'Teaching modelling', 'numeracy skills', 'formal models', 'basic skills', 'modelling skills', 'management science', 'Management science', 'Management Science']",830,159,8,830,158,10,4,4,2
"a multimodal data collection tool using realbasic and mac os x this project uses realbasic 3.5 in the mac os x environment for development of a configuration tool that builds a data collection procedure for investigating the effectiveness of sonified graphs. the advantage of using realbasic with the mac os x system is that it provides rapid development of stimulus presentation, direct recording of data to files, and control over other procedural issues. the program can be made to run natively on the new mac os x system, older mac os systems, and windows (98se, me, 2000 pro). with modification, similar programs could be used to present any number of visual/auditory stimulus combinations, complete with questions for each stimulus ","‘A multimodal data collection tool using REALbasic and Mac OS X This project uses REALbasic 3.5 in the Mac OS X environment for development of a configuration too! that builds a data collection procedure for investigating the effectiveness of sonified graphs. The advantage of using REALbasic with the Mac OS X system is that it provides rapid development of stimulus presentation, direct recording of data to files, and control over other procedural issues. The program can be made to run natively on the new Mac OS X system, older Mac OS systems, and Windows (98SE, ME, 2000 PRO). With modification, similar programs, could be used to present any number of visual/auditory stimulus combinations, complete with questions for each stimulus","['multimodal data collection tool', 'REALbasic', 'Mac OS X environment', 'configuration tool', 'data collection', 'sonified graphs', 'visual data comprehension', 'psychology', 'visual stimulus', 'auditory stimulus', 'stimulus presentation', 'direct data recording', 'Windows', 'Apple computers', 'BASIC', 'graphs', 'operating systems (computers)', 'psychology', 'user interfaces']","['Mac OS X', 'REALbasic', 'data collection procedure', 'other procedural issues', 'rapid development', 'X', 'data', 'collection', 'OS', 'Mac']",619,120,19,621,119,10,2,3,7
"an exactly solvable random satisfiability problem we introduce a new model for the generation of random satisfiability problems. it is an extension of the hyper-sat model of ricci-tersenghi, weigt and zecchina (2001), which is a variant of the famous k-sat model: it is extended to q-state variables and relates to a different choice of the statistical ensemble. the model has an exactly solvable statistic: the critical exponents and scaling functions of the sat/unsat transition are calculable at zero temperature, with no need of replicas, also with exact finite-size corrections. we also introduce an exact duality of the model, and show an analogy of thermodynamic properties with the random energy model of disordered spin system theory. relations with error correcting codes are also discussed ","An exactly solvable random satisfiability problem We introduce a new model for the generation of random satisfiability problems. Itis an extension of the hyper-SAT model of Ricci-Tersenghi, Weigt and Zecchina (2001), which is a variant of the famous K-SAT model: it is extended to q-state variables and relates to a different choice of the statistical ensemble. The model has an exactly solvable statistic: the critical exponents and scaling functions of the SAT/UNSAT transition are calculable at zero temperature, with no need of replicas, also with exact finite-size corrections. We also introduce an exact duality of the model, and show an analogy of thermodynamic properties with the random energy model of disordered spin system theory. Relations with error correcting codes are also discussed","['exactly solvable random satisfiability problem', 'hyper-SAT model', 'q-state variables', 'statistical ensemble', 'exact finite-size corrections', 'exact duality', 'thermodynamic properties', 'random energy model', 'disordered spin system theory', 'error correcting codes', 'combinatorial mathematics', 'computability', 'computational complexity', 'optimisation']","['model', 'random satisfiability problems', 'statistical ensemble', 'solvable statistics', 'random energy model', 'hyper-SAT model', 'new model', 'random', 'solvable', 'problems']",678,124,14,678,122,10,341,104,6
"industrial/sup it/ for performance buildings abb has taken a close look at how buildings are used and has come up with a radical solution for the technical infrastructure that places the end-user's processes at the center and integrates all the building's systems around their needs. the new solution is based on the realization that tasks like setting up an office meeting, registering a hotel guest or moving a patient in a hospital, can all benefit from the same industrial it concepts employed by abb to optimize manufacturing, for example in the automotive industry ","Industrial/sup IT/ for performance buildings ABB has taken a close look at how buildings are used and has come up with a radical solution for the technical infrastructure that places the end-user's processes at the center and integrates all the building's systems around their needs. The new solution is based on the realization that tasks like setting up an office meeting, registering a hotel guest or moving a patient in a hospital, can all benefit from the same Industrial IT concepts employed by ABB to optimize manufacturing, for example in the automotive industry","['Industrial/sup IT/', 'ABB', 'building management system', 'technical infrastructure', 'building systems integration', 'industrial IT concepts', 'access control', 'building management systems', 'computer networks']","['performance buildings ABB', 'technical infrastructure', 'end-users processes', 'buildings systems', 'radical solution', 'new solution', 'close look', 'buildings', 'solution', 'ABB']",478,94,9,478,93,10,0,0,2
"formalising optimal feature weight setting in case based diagnosis as linear programming problems many approaches to case based reasoning (cbr) exploit feature weight setting algorithms to reduce the sensitivity to distance functions. we demonstrate that optimal feature weight setting in a special kind of cbr problems can be formalised as linear programming problems. therefore, the optimal weight settings can be calculated in polynomial time instead of searching in exponential weight space using heuristics to get sub-optimal settings. we also demonstrate that our approach can be used to solve classification problems ","Formalising optimal feature weight setting in case based diagnosis as linear programming problems Many approaches to case based reasoning (CBR) exploit feature weight setting algorithms to reduce the sensitivity to distance functions. We demonstrate that optimal feature weight setting in a special kind of CBR problems can be formalised as linear programming problems. Therefore, the optimal weight settings can be calculated in polynomial time instead of searching in exponential weight space using heuristics to get sub-optimal settings. We also demonstrate that our approach can be used to solve classification problems","['optimal feature weight setting', 'case based diagnosis', 'linear programming', 'case based reasoning', 'distance functions', 'polynomial time', 'searching', 'exponential weight space', 'classification', 'heuristics', 'case-based reasoning', 'classification', 'computational complexity', 'diagnostic reasoning', 'linear programming']","['feature weight', 'linear programming problems', 'exponential weight space', 'optimal weight settings', 'classification problems', 'sub-optimal settings', 'weight', 'CBR problems', 'optimal', 'settings']",534,91,15,534,90,10,0,0,6
"dense coding in entangled states we consider the dense coding of entangled qubits shared between two parties, alice and bob. the efficiency of classical information gain through quantum entangled qubits is also considered for the case of pairwise entangled qubits and maximally entangled qubits. we conclude that using the pairwise entangled qubits can be more efficient when two parties communicate whereas using the maximally entangled qubits can be more efficient when the n parties communicate ","Dense coding in entangled states We consider the dense coding of entangled qubits shared between two parties, Alice and Bob. The efficiency of classical information gain through quantum entangled qubits is also considered for the case of pairwise entangled qubits and maximally entangled qubits. We conclude that using the pairwise entangled qubits can be more efficient when two parties communicate whereas using the maximally entangled qubits can be more efficient when the N parties communicate","['dense coding', 'entangled states', 'pairwise entangled qubits', 'Alice', 'Bob', 'classical information gain efficiency', 'maximally entangled qubits', 'quantum information processing', 'quantum communication', 'quantum communication', 'quantum cryptography']","['classical information gain', 'entangled states', 'parties Alice', 'efficiency', 'N parties', 'parties', 'coding', 'entangled', 'dense coding', 'Dense coding']",423,76,11,423,75,10,0,0,1
"hysteretic threshold logic and quasi-delay insensitive asynchronous design we introduce the class of hysteretic linear-threshold (hlt) logic functions as a novel extension of linear threshold logic, and prove their general applicability for constructing state-holding boolean functions. we then demonstrate a fusion of hlt logic with the quasi-delay insensitive style of asynchronous circuit design, complete with logical design examples. future research directions are also identified ","Hysteretic threshold logic and quasi-delay insensitive asynchronous design We introduce the class of hysteretic linear-threshold (HLT) logic functions as a novel extension of linear threshold logic, and prove their general applicability for constructing state-holding Boolean functions. We then demonstrate a fusion of HLT logic with the quasi-delay insensitive style of asynchronous circuit design, complete with logical design examples. Future research directions are also identified","['hysteretic linear-threshold logic functions', 'state-holding Boolean functions', 'HLT logic', 'quasi-delay insensitive style', 'asynchronous circuit design', 'logic design', 'digital logic', 'CMOS implementation', 'asynchronous circuits', 'Boolean functions', 'CMOS logic circuits', 'delays', 'hysteresis', 'logic design', 'threshold logic', 'VLSI']","['logic', 'quasi-delay insensitive style', 'hysteresis linear-threshold', 'asynchronous circuit design', 'hysteresis threshold logic', 'logical design examples', 'linear threshold logic', 'hit logic', 'design', 'hysteresis']",422,65,16,422,64,10,0,0,2
"an automated irradiation device for use in cyclotrons two cyclotrons are being operated at ipen-cnen/sp: one model cv-28, capable of accelerating protons with energies up to 24 mev and beam currents up to 30 mu a, and three other particles; the other one, model cyclone 30, accelerates protons with energy of 30 mev and currents up to 350 mu a. both have the objective of irradiating targets both for radioisotope production for use in nuclear medicine and general research. the development of irradiating systems completely automatized was the objective of this work, always aiming to reduce the radiation exposition dose to the workers and to increase the reliability of use of these systems ","An automated irradiation device for use in cyclotrons Two cyclotrons are being operated at IPEN-CNEN/SP: one model CV-28, capable of accelerating protons with energies up to 24 MeV and beam currents up to 30 mu A, and three other particles; the other one, model Cyclone 30, accelerates protons with energy of 30 MeV and currents up to 350 mu A Both have the objective of irradiating targets both for radioisotope production for use in nuclear medicine and general research. The development of irradiating systems completely automatized was the objective of this work, always aiming to reduce the radiation exposition dose to the workers and to increase the reliability of use of these systems","['automated irradiation device', 'cyclotrons', 'CV-28', 'protons', 'Cyclone 30', 'radioisotope production', 'nuclear medicine', 'general research', 'radiation exposition dose', 'accelerator control systems', 'cyclotrons', 'nuclear bombardment targets', 'proton accelerators', 'radioactive tracers']","['cyclotron', 'use', 'irradiation device', 'model Cyclone m0', 'other particles', 'beam currents', 'model CV-28', 'model', 'device', 'irradiation']",581,114,14,580,113,10,0,1,7
"robust control of nonlinear systems with parametric uncertainty probabilistic robustness analysis and synthesis for nonlinear systems with uncertain parameters are presented. monte carlo simulation is used to estimate the likelihood of system instability and violation of performance requirements subject to variations of the probabilistic system parameters. stochastic robust control synthesis searches the controller design parameter space to minimize a cost that is a function of the probabilities that design criteria will not be satisfied. the robust control design approach is illustrated by a simple nonlinear example. a modified feedback linearization control is chosen as controller structure, and the design parameters are searched by a genetic algorithm to achieve the tradeoff between stability and performance robustness ","Robust control of nonlinear systems with parametric uncertainty Probabilistic robustness analysis and synthesis for nonlinear systems with uncertain parameters are presented. Monte Carlo simulation is used to estimate the likelinood of system instability and violation of performance requirements subject to variations of the probabilistic system parameters. Stochastic robust control synthesis searches the controller design parameter space to minimize a cost that is a function of the probabilities that design criteria will not be satisfied. The robust control design approach is illustrated by a simple nonlinear ‘example. A modified feedback linearization control is chosen as controller structure, and the design parameters are searched by a genetic algorithm to achieve the tradeoff between stability and performance robustness","['robust control', 'nonlinear systems', 'parametric uncertainty', 'probabilistic robustness analysis', 'probabilistic robustness synthesis', 'uncertain parameters', 'Monte Carlo simulation', 'system instability', 'performance requirements violation', 'stochastic control synthesis', 'modified feedback linearization control', 'genetic algorithm', 'input-to-state stability', 'control system analysis', 'control system synthesis', 'feedback', 'genetic algorithms', 'linearisation techniques', 'Monte Carlo methods', 'nonlinear control systems', 'probability', 'robust control', 'search problems', 'stochastic systems', 'uncertain systems']","['control', 'nonlinear systems', 'feedback linearization control', 'simple nonlinear example', 'robust control synthesis', 'controller structure', 'system instability', 'design parameters', 'system', 'Robust control']",719,116,25,720,115,10,9,2,4
"access matters discusses accessibility needs of people with disabilities, both from the perspective of getting the information from i&r programs (including accessible web sites, tty access, braille, and other mechanisms) and from the perspective of being aware of accessibility needs when referring clients to resources. includes information on ada legislation requiring accessibility to public places and recommends several organizations and web sites for additional information ","Access matters Discusses accessibility needs of people with disabilities, both from the perspective of getting the information from 1&R programs (including accessible Web sites, TTY access, Braille, and other mechanisms) and from the perspective of being aware of accessibility needs when referring clients to resources. Includes information on ADA legislation requiring accessibility to public places and recommends several organizations and Web sites for additional information","['accessibility needs', 'disabled people', 'information and referral programs', 'accessible Web sites', 'TTY access', 'Braille', 'ADA legislation', 'public places', 'handicapped aids', 'information services', 'legislation']","['accessibility needs', 'Access', 'perspective', 'additional information', 'accessible Web sites', 'access braille', 'Access matters', 'Web sites', 'information', 'accessibility']",415,66,11,415,65,10,1,1,3
"manufacturing data analysis of machine tool errors within a contemporary small manufacturing enterprise the main focus of the paper is directed at the determination of manufacturing errors within the contemporary smaller manufacturing enterprise sector. the manufacturing error diagnosis is achieved through the manufacturing data analysis of the results obtained from the inspection of the component on a co-ordinate measuring machine. this manufacturing data analysis activity adopts a feature-based approach and is conducted through the application of a forward chaining expert system, called the product data analysis distributed diagnostic expert system, which forms part of a larger prototype feedback system entitled the production data analysis framework. the paper introduces the manufacturing error categorisations that are associated with milling type operations, knowledge acquisition and representation, conceptual structure and operating procedure of the prototype manufacturing data analysis facility. the paper concludes with a brief evaluation of the logic employed through the simulation of manufacturing error scenarios. this prototype manufacturing data analysis expert system provides a valuable aid for the rapid diagnosis and elimination of manufacturing errors on a 3-axis vertical machining centre in an environment where operator expertise is limited ","Manufacturing data analysis of machine tool errors within a contemporary small manufacturing enterprise The main focus of the paper is directed at the determination of manufacturing errors within the contemporary smaller manufacturing enterprise sector. ‘The manufacturing error diagnosis is achieved through the manufacturing data analysis of the results obtained from the inspection of the ‘component on a co-ordinate measuring machine. This manufacturing data analysis activity adopts a feature-based approach and is conducted through the application of a forward chaining expert system, called the product data analysis distributed diagnostic expert system, which forms. part of a larger prototype feedback system entitled the production data analysis framework. The paper introduces the manufacturing error categorisations that are associated with milling type operations, knowledge acquisition and representation, conceptual structure and operating procedure of the prototype manufacturing data analysis facility. The paper concludes with a brief evaluation of the logic employed through the simulation of manufacturing error scenarios. This prototype manufacturing data analysis expert system provides a valuable aid for the rapid diagnosis and elimination of manufacturing errors on a 3-axis vertical machining centre in an environment where operator expertise is limited","['manufacturing data analysis', 'machine tool errors', 'contemporary small manufacturing enterprise', 'fixturing errors', 'programming errors', '2 1/2D components', '3-axis vertical machining centre', 'inspection', 'milling type operations', 'knowledge acquisition', 'knowledge representation', 'conceptual structure', 'operating procedure', 'co-ordinate measuring machine', 'feature-based approach', 'forward chaining expert system', 'product data analysis distributed diagnostic expert system', 'condition monitoring', 'data analysis', 'diagnostic expert systems', 'inspection', 'knowledge acquisition', 'knowledge representation', 'machine tools', 'machining', 'tolerance analysis']","['manufacturing', 'manufacturing error categorisations', 'manufacturing error diagnosis', 'product data analysis', 'machine tool errors', 'error scenarios', 'analysis', 'data', 'manufacturing data analysis', 'Manufacturing data analysis']",1190,188,26,1193,187,10,12,3,11
"online coverage of the olympic games in 1956 a new medium was evolving which helped shape not only the presentation of the games to a worldwide audience, but created entirely new avenues for marketing and sponsorship which changed the entire economic relevance of the games. the medium in 1956 was television, and the medium now, of course, is the internet. not since 1956 has olympic coverage been so impacted by the onset of new technology as the current olympiad has been. but now the ioc finds itself in another set of circumstances not altogether different from 1956 ","Online coverage of the Olympic Games In 1956 a new medium was evolving which helped shape not only the presentation of the Games to a worldwide audience, but created entirely new avenues for marketing and sponsorship which changed the entire economic relevance of the Games. The medium in 1956 was television, and the medium now, of course, is the Internet. Not since 1956 has Olympic coverage been so impacted by the onset of new technology as the current Olympiad has been. But now the IOC finds itself in another set of circumstances not altogether different from 1956","['Olympic Games', 'online coverage', 'marketing', 'sponsorship', 'economic relevance', 'Olympiad', 'IOC', 'online rights', 'e-broadcast', 'information resources', 'multimedia communication', 'sport']","['Olympic coverage', 'Online coverage', 'new technology', 'Olympic Games', 'new avenues', 'new medium', 'new', 'coverage', 'Games', 'medium']",475,98,12,475,97,10,0,0,5
"new jersey african american women writers and their publications: a study of identification from written and oral sources this study examines the use of written sources, and personal interviews and informal conversations with individuals from new jersey's religious, political, and educational community to identify african american women writers in new jersey and their intellectual output. the focus on recognizing the community as an oral repository of history and then tapping these oral sources for collection development and acquisition purposes is supported by empirical and qualitative evidence. findings indicate that written sources are so limited that information professionals must rely on oral sources to uncover local writers and their publications ","New Jersey African American women writers and their publications: a study of identification from written and oral sources This study examines the use of written sources, and personal interviews and informal conversations with individuals from New Jersey's religious, Political, and educational community to identify African American women writers in New Jersey and their intellectual output. The focus on recognizing the community as an oral repository of history and then tapping these oral sources for collection development and acquisition purposes is supported by empirical and qualitative evidence. Findings indicate that written sources are so limited that information professionals must rely on oral sources to uncover local writers and their publications","['New Jersey African American women writers', 'written sources', 'personal interviews', 'informal conversations', 'intellectual output', 'oral repository', 'history', 'collection development', 'local writers', 'special collections', 'bibliographies', 'gender issues', 'libraries', 'literature']","['oral sources', 'New Jersey', 'written sources', 'educational community', 'oral repository', 'local writers', 'sources', 'oral', 'writers', 'New']",654,110,14,654,109,10,0,0,5
"where tech is cheap [servers] talk, consultancy, support, not tech is the expensive part of network installations. it's a good job that small-scale servers can either be remotely managed, or require little actual management ","Where tech is cheap [servers] Talk, consultancy, support, not tech is the expensive part of network installations. It's a good job that small-scale servers can either be remotely managed, or require little actual management","['small-scale servers', 'network installations', 'management', 'computer network management', 'network servers']","['little actual management', 'network installations', 'small-scale servers', 'consultancy support', 'tech', 'expensive part', 'cheap servers', 'good job', 'servers', 'consultancy']",190,35,5,190,34,10,0,0,0
"developing web-enhanced learning for information fluency-a liberal arts college's perspective learning is likely to take a new form in the twenty-first century, and a transformation is already in process. under the framework of information fluency, efforts are being made at rollins college to develop a web-enhanced course that encompasses information literacy, basic computer literacy, and critical thinking skills. computer-based education can be successful when librarians use technology effectively to enhance their integrated library teaching. in an online learning environment, students choose a time for learning that best suits their needs and motivational levels. they can learn at their own pace, take a nonlinear approach to the subject, and maintain constant communication with instructors and other students. the quality of a technology-facilitated course can be upheld if the educational objectives and methods for achieving those objectives are carefully planned and explored ","Developing Web-enhanced learning for information fluency-a liberal arts college's perspective Learning is likely to take a new form in the twenty-first century, and a transformation is already in process. Under the framework of information fluency, efforts are being made at Rollins College to develop a Web-enhanced course that encompasses information literacy, basic computer literacy, and critical thinking skills. Computer-based education can be successful when librarians use technology effectively to enhance their integrated library teaching. In an online learning environment, students choose a time for learning that best suits their needs and motivational levels. They can learn at their own pace, take a nonlinear approach to the subject, and maintain constant communication with instructors and other students. The quality of a technology-facilitated course can be upheld if the educational objectives and methods for achieving those objectives are carefully planned and explored","['Web-enhanced learning', 'information fluency', 'liberal arts college', 'information literacy', 'computer literacy', 'critical thinking skills', 'computer-based education', 'librarians', 'integrated library teaching', 'online learning', 'computer literacy', 'educational computing', 'information resources', 'information science', 'Internet']","['information fluency efforts', 'basic computer literacy', 'Web-enhanced learning', 'information literacy', 'Web-enhanced course', 'Rollins College', 'information', 'Web-enhanced', 'fluency', 'information fluency']",852,141,15,852,140,10,0,0,4
going electronic [auditing] a study group examines the issues auditors face in gathering electronic information as evidence and its impact on the audit ,"Going electronic [auditing] A study group examines the issues auditors face in gathering electronic information as evidence and its impact on the audit,","['auditing', 'electronic information', 'assurance standards', 'audit evidence', 'auditing']","['electronic information', 'electronic auditing', 'issues auditors', 'study group', 'electronic', 'group', 'issues', 'auditors', 'auditing', 'information']",129,24,5,130,23,10,0,1,0
a fuzzy logic adaptation circuit for control systems of deformable space vehicles: its design a fuzzy-logic adaptation algorithm is designed for adjusting the discreteness period of a control system for ensuring the stability and quality of control process with regard to the elastic structural vibrations of a deformable space vehicle. its performance is verified by digital modeling of a discrete control system with two objects ,Atuzzy logic adaptation circuit for control systems of deformable space vehicles: its design A uzzy-logic adaptation algorithm is designed for adjusting the discreteness period of a control system for ensuring the stability and quality of control process with regard to the elastic structural vibrations of a deformable space vehicle. Its performance is verified by digital modeling of a discrete control system with two objects,"['fuzzy logic adaptation circuit', 'control systems', 'deformable space vehicles', 'discreteness period', 'stability', 'elastic structural vibrations', 'digital modeling', 'adaptive control', 'aerospace control', 'closed loop systems', 'discrete systems', 'fuzzy control', 'graph theory', 'space vehicles']","['uzzy-logic adaptation algorithm', 'elastic structural vibrations', 'control', 'deformable space vehicles', 'discrete control system', 'discreteness period', 'control systems', 'control process', 'systems', 'adaptation']",366,66,14,365,64,10,214,64,2
"optimization-based design of fixed-order controllers for command following for discrete-time scalar systems, we propose an approach for designing feedback controllers of fixed order to minimize an upper bound on the peak magnitude of the tracking error to a given command input. the work makes use of linear programming to design over a class of closed-loop systems proposed for the rejection of non-zero initial conditions and bounded disturbances. we incorporate performance robustness in the form of a guaranteed upper bound on the peak magnitude of the tracking error under plant coprime factor uncertainty ","Optimization-based design of fixed-order controllers for command following For discrete-time scalar systems, we propose an approach for designing feedback controllers of fixed order to minimize an upper bound on the peak magnitude of the tracking error to a given command input. The work makes use of linear programming to design over a class of closed-loop systems proposed for the rejection of non-zero initial conditions and bounded disturbances. We incorporate performance robustness in the form of a guaranteed upper bound on the peak magnitude of the tracking error under plant coprime factor uncertainty","['optimization-based design', 'fixed-order controllers', 'command following', 'discrete-time scalar systems', 'feedback controllers', 'tracking error', 'linear programming', 'closed-loop systems', 'performance robustness', 'guaranteed upper bound', 'coprime factor uncertainty', 'closed loop systems', 'control system synthesis', 'discrete time systems', 'feedback', 'linear programming', 'minimisation', 'robust control']","['peak magnitude', 'discrete-time scalar systems', 'Optimization-based design', 'fixed-order controllers', 'feedback controllers', 'closed-loop systems', 'command following', 'command input', 'controllers', 'design']",519,93,18,519,92,10,0,0,6
the maximum possible evpi in this paper we calculate the maximum expected value of perfect information (evpi) for any probability distribution for the states of the world. this maximum evpi is an upper bound for the evpi with given probabilities and thus an upper bound for any partial information about the states of the world ,The maximum possible EVPI In this paper we calculate the maximum expected value of perfect information (EVP!) for any probability distribution for the states of the world. This maximum EVPI is an upper bound for the EVP! with given probabilities and thus an upper bound for any partial information about the states of the world,"['decision analysis', 'expected value of perfect information', 'operations research', 'management science', 'probability distribution', 'optimisation', 'decision theory', 'management science', 'operations research', 'optimisation', 'probability']","['maximum', 'states', 'world', 'upper', 'probability distribution', 'perfect information', 'partial information', 'probabilities', 'paper', 'information']",273,56,11,273,55,10,2,2,0
"dedekind zeta-functions and dedekind sums in this paper we use dedekind zeta functions of two real quadratic number fields at -1 to denote dedekind sums of high rank. our formula is different from that of siegel's (1969). as an application, we get a polynomial representation of zeta /sub k/(-1) = zeta /sub k/(-1) = 1/45(26n/sup 3/ - 41n +or- 9), n identical to +or-2(mod 5), where k = q( square root (5q)), prime q = 4n/sup 2/ + 1, and the class number of quadratic number field k/sub 2/ = q( square root q) is 1 ","Dedekind zeta-functions and Dedekind sums, In this paper we use Dedekind zeta functions of two real quadratic number fields at -1 to denote Dedekind sums of high rank. Our formula is different from that of Siegel's (1969). As an application, we get a polynomial representation of zeta /Sub K/(-1) = zeta /sub K/(-1) = 1/45(26n/sup 3/ - 41n +or- 9), n identical to +or-2(mod 5), where K = Q( square root (5q)), prime q = 4n/sup 2/ + 1, and the class number of quadratic number field K/sub 2/ = Q( square root q) is 1","['Dedekind zeta-functions', 'Dedekind sums', 'real quadratic number fields', 'polynomial representation', 'number theory', 'polynomials']","['Dedekind sums', 'zeta sub K', 'Dedekind zeta-functions', 'Dedekind zeta functions', 'Dedekind', 'class number', '= 4n/sup r', 'zeta', '=', 'sums']",419,97,6,420,96,10,0,1,1
"the development and evaluation of a fuzzy logic expert system for renal transplantation assignment: is this a useful tool? allocating donor kidneys to patients is a complex, multicriteria decision-making problem which involves not only medical, but also ethical and political issues. in this paper, a fuzzy logic expert system approach was proposed as an innovative way to deal with the vagueness and complexity faced by medical doctors in kidney allocation decision making. a pilot fuzzy logic expert system for kidney allocation was developed and evaluated in comparison with two existing allocation algorithms: a priority sorting system used by multiple organ retrieval and exchange (more) in canada and a point scoring systems used by united network for organ sharing (unos) in us. our simulated experiment based on real data indicated that the fuzzy logic system can represent the expert's thinking well in handling complex tradeoffs, and overall, the fuzzy logic derived recommendations were more acceptable to the expert than those from the more and unos algorithms ","The development and evaluation of a fuzzy logic expert system for renal transplantation assignment: Is this a useful tool? Allocating donor kidneys to patients is a complex, multicriteria decision-making problem which involves not only medical, but also ethical and political issues. In this paper, a fuzzy logic expert system approach was proposed as an innovative way to deal with the vagueness and complexity faced by medical doctors in kidney allocation decision making. A pilot fuzzy logic expert system for kidney allocation was developed and evaluated in comparison with two existing allocation algorithms: a priority sorting system used by multiple organ retrieval and exchange (MORE) in Canada and a point scoring systems used by united network for organ sharing (UNOS) in US. Our simulated experiment based on real data indicated that the fuzzy logic system can represent the expert's thinking well in handling complex tradeoffs, and overall, the fuzzy logic derived recommendations were more acceptable to the expert than those from the MORE and UNOS algorithms","['renal transplantation assignment', 'fuzzy logic expert system', 'donor kidneys', 'multicriteria decision-making problem', 'kidney allocation decision making', 'priority sorting system', 'multiple organ retrieval exchange', 'point scoring systems', 'united network for organ sharing', 'simulated experiment', 'complex tradeoff handling', 'decision support systems', 'fuzzy logic', 'health care', 'kidney', 'medical expert systems', 'professional aspects']","['renal transplantation assignment', 'kidney allocation decision', 'fuzzy logic system', 'donor kidneys', 'experts', 'logic', 'fuzzy', 'system', 'fuzzy logic', 'kidney allocation']",908,166,17,908,165,10,0,0,4
"'virtual family': an approach to introducing java programming this paper introduces and discusses virtual family (vf): a gender-neutral game-based software that introduces java programming. vf provides a completely functioning game that students extend and enhance via programming. we discuss the background and context within which virtual family was developed and other available multimedia resources for teaching programming. the paper then goes on to describe virtual family's concept and design. finally, feedback received from virtual family teaching workshops is related, as well as preliminary results from using vf in high-school teaching units. virtual family is under development in a research lab at the university of british columbia and is an initiative of supporting women in information technology (swift). swift is a five-year research action and implementation project to increase the participation of women in information technology ","‘Virtual Family’: an approach to introducing Java programming This paper introduces and discusses Virtual Family (VF): a gender-neutral game-based software that introduces Java programming. VF provides a completely functioning game that students extend and enhance via programming. We discuss the background and context within which Virtual Family was developed and other available multimedia resources for teaching programming. The paper then goes on to describe Virtual Family's concept and design. Finally, feedback received from Virtual Family teaching workshops is related, as well as preliminary results from using VF in high-school teaching units. Virtual Family is under development in a research lab at the University of British Columbia and is an initiative of Supporting Women in Information Technology (SWIFT). SWIFT is a five-year research action and implementation project to increase the participation of women in information technology","['Virtual Family', 'gender-neutral game-based software', 'Java programming teaching', 'multimedia resources', 'teaching workshops', 'high-school teaching units', 'Supporting Women in Information Technology', 'computer games', 'computer science education', 'courseware', 'gender issues', 'Java', 'multimedia computing', 'object-oriented programming', 'teaching']","['gender-neutral home-based software', 'Virtual family concept', 'Java programming VF', 'programming', 'family', 'Virtual', 'Java', 'virtual family', 'Virtual Family', 'Java programming']",817,136,15,817,135,10,2,2,5
comparison of non-stationary time series in the frequency domain in this paper we compare two nonstationary time series using nonparametric procedures. evolutionary spectra are estimated for the two series. randomization tests are performed on groups of spectral estimates for both related and independent time series. simulation studies show that in certain cases the tests perform reasonably well. the tests are applied to observed geological and financial time series ,Comparison of non-stationary time series in the frequency domain In this paper we compare two nonstationary time series using nonparametric procedures. Evolutionary spectra are estimated for the two series. Randomization tests are performed on groups of spectral estimates for both related and independent time series. Simulation studies show that in certain cases the tests perform reasonably well. The tests are applied to observed geological and financial time series,"['nonstationary time series', 'nonparametric procedures', 'evolutionary spectra estimation', 'randomization tests', 'spectral estimates', 'related time series', 'lag window', 'time window', 'independent time series', 'simulation', 'geological time series', 'financial time series', 'nonparametric statistics', 'spectral analysis', 'time series']","['series', 'series Randomization tests', 'non-stationary time series', 'nonstationary time series', 'non-parametric procedures', 'financial time series', 'frequency domain', 'time', 'Comparison', 'tests']",403,69,15,403,68,10,0,0,2
"war games: the truth [network security] with al qaeda on the tip of tongues around the world, find out how terror groups could target your network. what are the dangers and how do you fight them? ","War games: The truth [network security] With al Qaeda on the tip of tongues around the world, find out how terror groups could target your network. What are the dangers and how do you fight them?","['networks', 'malicious attacks', 'security', 'employees', 'computer crime', 'computer network management', 'security of data']","['truth network security', 'terror groups', 'War games', 'al maeda', 'al', 'War', 'truth', 'games', 'network', 'security']",160,37,7,160,36,10,0,0,0
"voip makeover transforms ugly duckling network surrey county council's swan project is europe's biggest implementation of voice over ip. six wans and countless lans are are being consolidated into a single network covering 6,000 users at 200 sites. the contract was signed in october 2001 for pounds 13m over five years and rollout will be completed in may 2003 ","VoIP makeover transforms ugly duckling network Surrey County Council's Swan project is Europe's biggest implementation of voice over IP. Six Wans and countless Lans are are being consolidated into a single network covering 6,000 users at 200 sites. The contract was signed in October 2001 for Pounds 13m over five years and rollout will be completed in May 2003,","['Surrey County Council', 'Swan', 'voice over IP', 'WAN', 'LAN', 'Internet telephony', 'local area networks', 'public administration', 'wide area networks']","['europes biggest implementation', 'single network', 'countless Lans', 'void makeover', 'duckling', 'Surrey', 'ugly', 'void', 'network', 'makeover']",303,60,9,304,59,10,0,1,4
"the development of virtual reality therapy (vrt) system for the treatment of acrophobia and therapeutic case virtual reality therapy (vrt), based on this sophisticated technology, has been used in the treatment of subjects diagnosed with acrophobia, a disorder that is characterized by marked anxiety upon exposure to heights and avoidance of heights. conventional vr systems for the treatment of acrophobia have limitations, over-costly devices or somewhat unrealistic graphic scenes. the goal of this study was to develop an inexpensive and more realistic virtual environment (ve) in which to perform exposure therapy for acrophobia. it is based on a personal computer, and a virtual scene of a bunge-jump tower in the middle of a large city. the virtual scenario includes an open lift surrounded by props beside a tower, which allows the patient to feel a sense of heights. the effectiveness of the ve was evaluated through the clinical treatment of a subject who was suffering from the fear of heights. as a result, it was proved that this vr environment was effective and realistic at overcoming acrophobia according not only to the comparison results of a variety of questionnaires before and after treatment but also to the subject's comments that the ve seemed to evoke more fearful feelings than the real situation ","The development of virtual reality therapy (VRT) system for the treatment of acrophobia and therapeutic case Virtual reality therapy (VRT), based on this sophisticated technology, has been used in the treatment of subjects diagnosed with acrophobia, a disorder that is characterized by marked anxiety upon exposure to heights and avoidance of heights. Conventional VR systems for the treatment of acrophobia have limitations, over-costly devices or somewhat unrealistic graphic scenes. The goal of this study was to develop an inexpensive and more realistic virtual environment (VE) in which to perform exposure therapy for acrophobia. It is based on a personal ‘computer, and a virtual scene of a bunge-jump tower in the middle of a large city. The virtual scenario includes an open lift surrounded by props beside a tower, which allows the patient to feel a sense of heights. The effectiveness of the VE was evaluated through the clinical treatment of a subject who was suffering from the fear of heights. As a result, it was proved that this VR environment was effective and realistic at overcoming acrophobia according not only to the comparison results of a variety of questionnaires before and after treatment but also to the subject's comments that the VE seemed to evoke more fearful feelings than the real situation","['virtual reality therapy system', 'acrophobia treatment', 'therapeutic case', 'patient anxiety', 'patient treatment', 'realistic virtual environment', 'exposure therapy', 'personal computer', 'virtual scene', 'clinical treatment', 'psychotherapy', 'heights phobia', 'medical computing', 'patient treatment', 'psychology', 'user interfaces', 'virtual reality']","['acrophobia', 'Virtual reality therapy', 'virtual', 'realistic virtual environment', 'Conventional VR systems', 'clinical treatment', 'virtual scenario', 'exposure therapy', 'virtual scene', 'therapy']",1112,213,17,1113,212,10,9,1,2
"integration is lims inspiration for software manufacturers, blessings come in the form of fast-moving application areas. in the case of lims, biotechnology is still in the driving seat, inspiring developers to maintain consistently rapid and creative levels of innovation. current advancements are no exception. integration and linking initiatives are still popular and much of the activity appears to be coming from a very productive minority ","Integration is LIMS inspiration For software manufacturers, blessings come in the form of fast-moving application areas. In the case of LIMS, biotechnology is stil in the driving seat, inspiring developers to maintain consistently rapid and creative levels of innovation. Current advancements are no exception. Integration and linking initiatives are still popular and much of the activity appears to be coming from a very productive minority","['software manufacturers', 'LIMS', 'biotechnology', 'biology computing', 'biotechnology', 'chemistry computing', 'laboratory techniques', 'software packages']","['software manufacturers blessings', 'fast-moving application areas', 'exception Integration', 'inspiring developers', 'Current advancements', 'lims biotechnology', 'LIMS inspiration', 'driving seat', 'inspiring', 'Integration']",379,66,8,378,65,10,0,1,0
"place/transition petri net evolutions: recording ways, analysis and synthesis four semantic domains for place/transition petri nets and their relationships are considered. they are monoids of respectively: firing sequences, processes, traces and dependence graphs. for each of them the analysis and synthesis problem is stated and solved. the monoid of processes is defined in a non-standard way, nets under consideration involve weights of arrows and capacities (finite or infinite) of places. however, the analysis and synthesis tasks require nets to be pure, i.e. each of their transition must have the pre-set and post-set disjoint ","Place/Transition Petri net evolutions: recording ways, analysis and synthesis Four semantic domains for Place/Transition Petri nets and their relationships are considered. They are monoids of respectively: firing sequences, processes, traces and dependence graphs. For each of them the analysis and synthesis problem is stated and solved. The monoid of processes is defined in a non-standard way, Nets under consideration involve weights of arrows and capacities (finite or infinite) of places. However, the analysis and synthesis tasks require nets to be pure, i.e. each of their transition must have the pre-set and post-set disjoint","['place/transition Petri net evolutions', 'semantic domains', 'monoids', 'firing sequences', 'dependence graphs', 'post-set disjoint', 'pre-set disjoint', 'formal specification', 'group theory', 'Petri nets']","['Place/Transition Petri nets', 'non-standard way Nets', 'synthesis problem', 'synthesis tasks', 'net evolutions', 'ways analysis', 'synthesis', 'nets', 'Place/Transition', 'Place/Transition Petri']",543,94,10,543,93,10,0,0,5
"exploiting structure in adaptive dynamic programming algorithms for a stochastic batch service problem the purpose of this paper is to illustrate the importance of using structural results in dynamic programming algorithms. we consider the problem of approximating optimal strategies for the batch service of customers at a service station. customers stochastically arrive at the station and wait to be served, incurring a waiting cost and a service cost. service of customers is performed in groups of a fixed service capacity. we investigate the structure of cost functions and establish some theoretical results including monotonicity of the value functions. then, we use our adaptive dynamic programming monotone algorithm that uses structure to preserve monotonicity of the estimates at each iterations to approximate the value functions. since the problem with homogeneous customers can be solved optimally, we have a means of comparison to evaluate our heuristic. finally, we compare our algorithm to classical forward dynamic programming methods ","Exploiting structure in adaptive dynamic programming algorithms for a stochastic batch service problem The purpose of this paper is to illustrate the importance of using structural results in dynamic programming algorithms. We consider the problem of approximating optimal strategies for the batch service of customers at a service station. Customers stochastically arrive at the station and wait to be served, incurring a waiting cost and a service cost. Service of customers is performed in groups of a fixed service capacity. We investigate the structure of cost functions and establish some theoretical results including monotonicity of the value functions. Then, we use our adaptive dynamic programming monotone algorithm that uses structure to preserve monotonicity of the estimates at each iterations to approximate the value functions. Since the problem with homogeneous customers can be solved optimally, we have a means of comparison to evaluate our heuristic. Finally, we compare our algorithm to classical forward dynamic programming methods","['stochastic batch service problem', 'adaptive dynamic programming algorithms', 'structural results', 'optimal strategy approximation', 'service station', 'waiting cost', 'service cost', 'fixed service capacity', 'cost function structure', 'value function monotonicity', 'inventory theory', 'dynamic programming', 'queueing theory', 'stochastic processes', 'stock control']","['dynamic programming algorithms', 'structure', 'service', 'batch service', 'dynamic programming methods', 'service station Customers', 'fixed service capacity', 'service cost Service', 'structural results', 'dynamic']",899,156,15,899,155,10,0,0,0
"nonlinear systems arising from nonisothermal, non-newtonian hele-shaw flows in the presence of body forces and sources in this paper, we first give a formal derivation of several systems of equations for injection moulding. this is done starting from the basic equations for nonisothermal, non-newtonian flows in a three-dimensional domain. we derive systems for both (t/sup 0/, p/sup 0/) and (t/sup 1/, p/sup 1/) in the presence of body forces and sources. we find that body forces and sources have a nonlinear effect on the systems. we also derive a nonlinear ""darcy law"". our formulation includes not only the pressure gradient, but also body forces and sources, which play the role of a nonlinearity. later, we prove the existence of weak solutions to certain boundary value problems and initial-boundary value problems associated with the resulting equations for (t/sup 0/, p/sup 0/) but in a more general mathematical setting ","Nonlinear systems arising from nonisothermal, non-Newtonian Hele-Shaw flows in the presence of body forces and sources In this paper, we first give a formal derivation of several systems of equations for injection moulding. This is done starting from the basic equations for nonisothermal, non-Newtonian flows in a three-dimensional domain. We derive systems for both (T/sup O/, p/sup 0/) and (T/sup 1/, psup 1/) in the presence of body forces and sources. We find that body forces and sources have a nonlinear effect on the systems. We also derive a nonlinear ""Darcy law"". Our formulation includes not only the pressure gradient, but also body forces and sources, which play the role of a nonlinearity. Later, we prove the existence of weak solutions to certain boundary value problems and initial-boundary value problems associated with the resulting equations for (T/sup O/, p/sup O/) but in a more general mathematical setting","['injection moulding', 'body forces', 'sources', 'Darcy law', 'nonlinear systems', 'boundary value problems', 'Hele-Shaw flows', 'boundary-value problems', 'moulding', 'nonlinear systems']","['body forces', 'pup c/o', 'non-Newtonian hee-haw', 'nonlinear darcy law', 'non-Newtonian flows', 'Nonlinear systems', 'several systems', 'basic equations', 'systems', 'body']",785,148,10,784,147,10,6,4,4
pitch post-processing technique based on robust statistics a novel pitch post-processing technique based on robust statistics is proposed. performances in terms of pitch error rates and pitch contours show the superiority of the proposed method compared with the median filtering technique. further improvement is achieved through incorporating an uncertainty term in the robust statistics model ,Pitch post-processing technique based on robust statistics Anovel pitch post-processing technique based on robust statistics is proposed Performances in terms of pitch error rates and pitch contours show the superiority of the proposed method compared with the median filtering technique. Further improvement is achieved through incorporating an uncertainty term in the robust statistics model,"['pitch post-processing technique', 'robust statistics', 'pitch error rates', 'pitch contours', 'median filtering', 'uncertainty term', 'speech quality', 'speech communications', 'speech processing', 'statistical analysis']","['post-processing technique', 'robust statistics model', 'pitch error rates', 'pitch', 'pitch contours', 'novel pitch', 'robust', 'statistics', 'robust statistics', 'technique']",341,56,10,340,54,10,179,47,2
"statistical inference with partial prior information based on a gauss-type inequality potter and anderson (1983) have developed a bayesian decision procedure requiring the specification of a class of prior distributions restricted to have a minimal probability content for a given subset of the parameter space. they do not, however, provide a method for the selection of that subset. we show how a generalization of gauss' inequality can be used to determine the relevant parameter subset ","Statistical inference with partial prior information based on a Gauss-type inequality Potter and Anderson (1983) have developed a Bayesian decision procedure requiring the specification of a cass of prior distributions restricted to have a minimal probability content for a given subset of the parameter space. They do not, however, provide a method for the selection of that subset. We show how a generalization of Gauss’ inequality can be used to determine the relevant parameter subset","['Bayesian decision procedure', 'prior distributions', 'minimal probability content', 'parameter space', 'Gauss inequality', 'prior-to-posterior sensitivity', 'partial prior information', 'decision theory', 'inference mechanisms', 'probability']","['Gauss-type inequality Potter', 'minimal probability content', 'Bayesian decision procedure', 'relevant parameter subset', 'partial prior information', 'Statistical inference', 'prior distributions', 'gauss inequality', 'prior', 'Statistical']",415,76,10,414,75,10,3,2,0
"reconfigurable context-sensitive middleware for pervasive computing context-sensitive applications need data from sensors, devices, and user actions, and might need ad hoc communication support to dynamically discover new devices and engage in spontaneous information exchange. reconfigurable context-sensitive middleware facilitates the development and runtime operations of context-sensitive pervasive computing software ","Reconfigurable context-sensitive middleware for pervasive computing Context-sensitive applications need data from sensors, devices, and user actions, and might need ad hoc communication support to dynamically discover new devices and engage in spontaneous information exchange Reconfigurable Context-Sensitive Middleware facilitates the development and runtime operations of context-sensitive pervasive computing software","['pervasive computing', 'Reconfigurable Context-Sensitive Middleware', 'context-sensitive pervasive computing', 'middleware', 'context-sensitive applications', 'distributed object management', 'mobile computing', 'portable computers']","['pervasive computing', 'spontaneous information exchange', 'Context-sensitive applications', 'context-sensitive', 'sensors devices', 'new devices', 'configurable', 'pervasive', 'configurable context-sensitive middleware', 'configurable Context-Sensitive Middleware']",375,49,8,374,48,10,0,1,0
"cutting through the confusion [workflow & content management] information management vendors are rushing to re-position themselves and put a portal spin on their products, says itnet's graham urquhart. the result is confusion, with a range of different definitions and claims clouding the true picture ","Cutting through the contusion [workflow & content management] Information management vendors are rushing to re-position themselves and put a Portal spin on their products, says ITNET's Graham Urquhart. The result is confusion, with a range of different definitions and claims clouding the true picture","['ITNET', 'portals', 'collaboratively', 'workflow', 'workflow management software']","['management', 'contusion workflow', 'Graham urquhart', 'Portal spin', 'Information', 'vendors', 'content', 'Portal', 'workflow', 'contusion']",258,45,5,258,44,10,1,1,1
a sufficient condition for optimality in nondifferentiable invex programming a sufficient optimality condition is established for a nonlinear programming problem without differentiability assumption on the data wherein clarke's (1975) generalized gradient is used to define invexity ,Assufficient condition for optimality in nondifferentiable invex programming A sufficient optimality condition is established for a nonlinear programming problem without differentiability assumption on the data wherein Clarke's (1975) generalized gradient is used to define invexity,"['nondifferentiable invex programming', 'sufficient optimality condition', 'nonlinear programming problem', 'generalized gradient', 'invexity', 'locally Lipschitz function', 'semiconvex function', 'functions', 'nonlinear programming', 'set theory']","['sufficient optimality condition', 'nonlinear programming problem', 'differentiability assumption', 'sufficient condition', 'nondifferentiable', 'index', 'sufficient', 'condition', 'optimality', 'programming']",247,37,10,248,35,10,138,35,2
"a simple etalon-stabilized visible laser diode visible laser diodes (lds) are inexpensively available with single-transverse-mode, single-longitudinal-mode operation with a coherence length in the metre range. with constant current bias and constant operating temperature, the optical output power and operating wavelength are stable. a simple and inexpensive way is developed to maintain a constant ld temperature as the temperature of the local environment varies, by monitoring the initially changing wavelength with an external etalon and using this information to apply a heating correction to the monitor photodiode commonly integral to the ld package. the fractional wavelength stability achieved is limited by the solid etalon to 7*10/sup -6/ degrees c/sup -1/ ","A simple etalon-stabilized visible laser diode Visible laser diodes (LDs) are inexpensively available with single-transverse-mode, single-longitudinal-mode operation with a coherence length in the metre range. With constant current bias and constant operating temperature, the optical output power and operating wavelength are stable. A simple and inexpensive way is developed to maintain a constant LD temperature as the temperature of the local environment varies, by monitoring the initially changing wavelength with an external etalon and using this information to apply a heating correction to the monitor photodiode commonly integral to the LD package. The fractional wavelength stability achieved is limited by the solid etalon to 7*10/sup -6/ degrees C/sup -1/","['etalon-stabilized laser diode', 'visible laser diode', 'constant current bias', 'constant operating temperature', 'heating correction', 'monitor photodiode', 'fractional wavelength stability', 'single-transverse-mode', 'single-longitudinal-mode', 'index-guided multi-quantum-well', 'closed-loop operation', 'feedback loop', 'closed loop systems', 'laser cavity resonators', 'laser feedback', 'laser stability', 'light interferometers', 'optical control', 'quantum well lasers', 'temperature control']","['diode', 'simple', 'single-longitudinal-mode operation', 'fractional wavelength stability', 'constant operating temperature', 'laser', 'constant LD temperature', 'constant current bias', 'operating wavelength', 'operation']",660,110,20,660,109,10,0,0,3
"the cataloger's workstation revisited: utilizing cataloger's desktop a few years into the development of cataloger's desktop, an electronic cataloging tool aggregator available through the library of congress, is an opportune time to assess its impact on cataloging operations. a search for online cataloging tools on the internet indicates a proliferation of cataloging tool aggregators; which provide access to online documentation related to cataloging practices and procedures. cataloger's desktop stands out as a leader among these aggregators. results of a survey to assess 159 academic arl and large public libraries' reasons for use or non-use of cataloger's desktop highlight the necessity of developing strategies for its successful implementation including training staff, providing documentation, and managing technical issues ","The cataloger's workstation revisited: utilizing Cataloger's Desktop A few years into the development of Cataloger’s Desktop, an electronic cataloging tool aggregator available through the Library of Congress, is an opportune time to assess its impact on cataloging operations. A search for online cataloging tools on the Internet indicates a proliferation of cataloging tool aggregators; which provide access to ‘online documentation related to cataloging practices and procedures. Cataloger's Desktop stands out as a leader among these aggregators. Results of a survey to assess 159 academic ARL and large public libraries' reasons for use or non-use of Cataloger's Desktop highlight the necessity of developing strategies for its successful implementation including training staff, providing documentation, and managing technical issues","[""Cataloger's Desktop"", 'electronic cataloging tool', 'online cataloging tools', 'Internet', 'cataloging tool aggregators', 'online documentation', 'large public libraries', 'academic ARL', 'staff training', 'documentation', 'managing technical issues', ""cataloger's workstation"", 'academic libraries', 'cataloguing', 'library automation', 'public libraries', 'training']","['cataloguers Desktop A', 'cataloguers', 'procedures cataloguers Desktop', 'cataloguing tool aggregators', 'online cataloguing tools', 'cataloguers workstation', 'cataloguing operations', 'cataloguers desktop', 'cataloguers Desktop', 'Desktop']",723,117,17,724,116,10,7,2,7
"devs simulation of distributed intrusion detection systems an intrusion detection system (ids) attempts to identify unauthorized use, misuse, and abuse of computer and network systems. as intrusions become more sophisticated, dealing with them moves beyond the scope of one ids. the need arises for systems to cooperate with one another, to manage diverse attacks across networks. the feature of recent attacks is that the packet delivery is moderately slow, and the attack sources and attack targets are distributed. these attacks are called ""stealthy attacks."" to detect these attacks, the deployment of distributed idss is needed. in such an environment, the ability of an ids to share advanced information about these attacks is especially important. in this research, the ids model exploits blacklist facts to detect the attacks that are based on either slow or highly distributed packets. to maintain the valid blacklist facts in the knowledge base of each ids, the model should communicate with the other idss. when attack level goes beyond the interaction threshold, id agents send interaction messages to id agents in other hosts. each agent model is developed as an interruptible atomic-expert model in which the expert system is embedded as a model component ","DEVS simulation of distributed intrusion detection systems An intrusion detection system (IDS) attempts to identify unauthorized use, misuse, and abuse of computer and network systems. As intrusions become more sophisticated, dealing with them moves beyond the scope of one IDS. The need arises for systems to cooperate with one another, to manage diverse attacks across networks. The feature of recent attacks is that the packet delivery is moderately slow, and the attack sources and attack targets are distributed. These attacks are called ""stealthy attacks."" To detect these attacks, the deployment of distributed IDSs is needed. In such an environment, the ability of an IDS to share advanced information about these attacks is especially important. In this research, the IDS model exploits blacklist facts to detect the attacks that are based on either slow or highly distributed packets. To maintain the valid blacklist facts in the knowledge base of each IDS, the model should communicate with the other IDSs. When attack level goes beyond the interaction threshold, ID agents send interaction messages to ID agents in other hosts. Each agent model is developed as, an interruptible atomic-expert model in which the expert system is embedded as a model component","['intrusion detection system', 'IDS', 'intrusions', 'expert system', 'distributed intrusion detection system', 'cooperative intrusion detection', 'warning threshold', 'authorisation', 'computer network management', 'cooperative systems', 'discrete event simulation', 'expert systems']","['system', 'intrusion detection systems', 'stealthy attacks', 'network systems', 'diverse attacks', 'recent attacks', 'attack targets', 'attack sources', 'expert system', 'intrusions']",1072,199,12,1073,198,10,0,1,1
"on fractal dimension in information systems. toward exact sets in infinite information systems the notions of an exact as well as a rough set are well-grounded as basic notions in rough set theory. they are however defined in the setting of a finite information system i.e. an information system having finite numbers of objects as well as attributes. in theoretical studies e.g. of topological properties of rough sets, one has to trespass this limitation and to consider information systems with potentially unbound number of attributes. in such setting, the notions of rough and exact sets may be defined in terms of topological operators of interior and closure with respect to an appropriate topology following the ideas from the finite case, where it is noticed that in the finite case rough-set-theoretic operators of lower and upper approximation are identical with the interior, respectively, closure operators in topology induced by equivalence classes of the indiscernibility relation. extensions of finite information systems are also desirable from application point of view in the area of knowledge discovery and data mining, when demands of e.g. mass collaboration and/or huge experimental data call for need of working with large data tables so the sound theoretical generalization of these cases is an information system with the number of attributes not bound in advance by a fixed integer i.e. an information system with countably but infinitely many attributes, in large information systems, a need arises for qualitative measures of complexity of concepts involved free of parameters, cf. e.g. applications for the vapnik-czervonenkis dimension. we study here in the theoretical setting of infinite information system a proposal to apply fractal dimensions suitably modified as measures of concept complexity ","On fractal dimension in information systems. Toward exact sets in infinite information systems The notions of an exact as well as a rough set are well-grounded as basic notions in rough set theory. They are however defined in the setting of a finite information system i.e. an information system having finite numbers of objects as well as attributes. In theoretical studies e.g. of topological properties of rough sets, one has to trespass this, limitation and to consider information systems with potentially unbound number of attributes. In such setting, the notions of rough and exact sets may be defined in terms of topological operators of interior and closure with respect to an appropriate topology following the ideas from the finite case, where it is noticed that in the finite case rough-set-theoretic operators of lower and upper approximation are identical with the interior, respectively, closure operators in topology induced by equivalence classes of the indiscernibility relation. Extensions of finite information systems are also desirable from application point of view in the area of knowledge discovery and data mining, when demands of e.g. mass collaboration and/or huge experimental data call for need of working with large data tables so the sound theoretical generalization of these cases is an information system with the number of attributes not bound in advance by a fixed integer i.e. an information system with countably but infinitely many attributes, In large information systems, a need arises for qualitative measures of complexity of concepts involved free of parameters, cf. .g. applications for the Vapnik-Czervonenkis dimension. We study here in the theoretical setting of infinite information system a proposal to apply fractal dimensions suitably modified as measures of concept complexity","['fractal dimension', 'information systems', 'exact sets', 'infinite information systems', 'rough set', 'topological properties', 'closure operators', 'equivalence classes', 'knowledge discovery', 'data mining', 'qualitative measures', 'complexity', 'computational complexity', 'data mining', 'equivalence classes']","['information system', 'information', 'exact sets', 'infinite information systems', 'large information systems', 'theoretical setting', 'systems', 'rough set theory', 'rough set', 'finite information system']",1551,280,15,1551,279,10,3,2,10
"noise effect on memory recall in dynamical neural network model of hippocampus we investigate some noise effect on a neural network model proposed by araki and aihara (1998) for the memory recall of dynamical patterns in the hippocampus and the entorhinal cortex; the noise effect is important since the release of transmitters at synaptic clefts, the operation of gate of ion channels and so on are known as stochastic phenomena. we consider two kinds of noise effect due to a deterministic noise and a stochastic noise. by numerical simulations, we find that reasonable values of noise give better performance on the memory recall of dynamical patterns. furthermore we investigate the effect of the strength of external inputs on the memory recall ","Noise effect on memory recall in dynamical neural network model of hippocampus We investigate some noise effect on a neural network model proposed by Araki and Aihara (1998) for the memory recall of dynamical patterns in the hippocampus and the entorhinal cortex; the noise effect is important since the release of transmitters at synaptic clefts, the operation of gate of ion channels and so on are known as stochastic phenomena. We consider two kinds of noise effect due to a deterministic noise and a stochastic noise. By numerical simulations, we find that reasonable values of noise give better performance on the memory recall of ‘dynamical patterns. Furthermore we investigate the effect of the strength of external inputs on the memory recall","['hippocampus', 'dynamical neural network model', 'noise effect', 'memory recall', 'dynamical patterns', 'entorhinal cortex', 'synaptic clefts', 'gate of ion channels', 'stochastic phenomena', 'deterministic noise', 'stochastic noise', 'brain functions', 'numerical simulations', 'synaptic strength', 'inhibitory connection', 'brain models', 'neural nets', 'neurophysiology', 'nonlinear dynamical systems', 'random noise', 'stochastic processes']","['memory recall', 'neural network model', 'dynamical patterns', 'noise', 'deterministic noise', 'stochastic noise', 'effect', 'memory', 'noise effect', 'Noise effect']",629,122,21,630,121,10,9,1,5
"the use of visual search for knowledge gathering in image decision support this paper presents a new method of knowledge gathering for decision support in image understanding based on information extracted from the dynamics of saccadic eye movements. the framework involves the construction of a generic image feature extraction library, from which the feature extractors that are most relevant to the visual assessment by domain experts are determined automatically through factor analysis. the dynamics of the visual search are analyzed by using the markov model for providing training information to novices on how and where to look for image features. the validity of the framework has been evaluated in a clinical scenario whereby the pulmonary vascular distribution on computed tomography images was assessed by experienced radiologists as a potential indicator of heart failure. the performance of the system has been demonstrated by training four novices to follow the visual assessment behavior of two experienced observers. in all cases, the accuracy of the students improved from near random decision making (33%) to accuracies ranging from 50% to 68% ","The use of visual search for knowledge gathering in image decision support This paper presents a new method of knowledge gathering for decision support in image understanding based on information extracted from the dynamics of saccadic eye movements. The framework involves the construction of a generic image feature extraction library, from which the feature extractors that are most relevant to the visual assessment by domain experts are determined automatically through factor analysis. The ‘dynamics of the visual search are analyzed by using the Markov model {or providing training information to novices on how and where to look for image features. The validity of the framework has been evaluated in a clinical scenario whereby the pulmonary vascular distribution on Computed Tomography images was assessed by experienced radiologists as a potential indicator of heart failure. The performance of the system has been demonstrated by training four novices to follow the visual assessment behavior of two experienced observers. In all cases, the accuracy of the students improved from near random decision making (33%) to accuracies ranging from 50% to 68%","['pulmonary vascular distribution', 'experienced radiologists', 'heart failure indicator', 'visual assessment behavior', 'experienced observers', 'student accuracy', 'Markov model', 'training information', 'image features', 'domain experts', 'saccadic eye movements dynamics', 'near random decision making', 'medical diagnostic imaging', 'biomechanics', 'cardiology', 'computerised tomography', 'decision support systems', 'eye', 'feature extraction', 'lung', 'Markov processes', 'medical image processing', 'visual perception']","['knowledge gathering', 'visual search', 'visual assessment behavior', 'Computed Tomography images', 'image decision support', 'image understanding', 'image features', 'visual', 'decision support', 'visual assessment']",986,178,23,987,177,10,9,2,8
"reaching strong consensus in a general network the strong consensus (sc) problem is a variant of the conventional distributed consensus problem (also known as the byzantine agreement problem). the sc problem requires that the agreed value among fault-free processors be one of the fault-free processor's initial values. originally, the problem was studied in a fully connected network with malicious faulty processors. in this paper, the sc problem is re-examined in a general network, in which the components (processors and communication links) may be subjected to different faulty types simultaneously (also called the hybrid fault model or mixed faulty types) and the network topology does not have to be fully connected. the proposed protocol can tolerate the maximum number of tolerable faulty components such that each fault-free processor obtains a common value for the sc problem in a general network ","Reaching strong consensus in a general network The strong consensus (SC) problem is a variant of the conventional distributed consensus problem (also known as the Byzantine agreement problem). The SC problem requires that the agreed value among fault-free processors be one of the fault-free processor's initial values. Originally, the problem was studied in a fully connected network with malicious faulty processors. In this paper, the SC problem is re-examined in a general network, in which the components (processors and communication links) may be subjected to different faulty types simultaneously (also called the hybrid fault model or mixed faulty types) and the network topology does not have to be fully connected. The proposed protocol can tolerate the maximum number of tolerable faulty components such that each fault-free processor obtains a common value for the SC problem in a general network","['strong consensus problem', 'distributed consensus problem', 'Byzantine agreement', 'fault-free processors', 'fully connected network', 'hybrid fault model', 'fault-tolerant distributed system', 'strong consensus', 'concurrency control', 'distributed processing', 'fault tolerant computing', 'protocols']","['fat-free processor', 'general network', 'SC problem', 'problem', 'strong consensus', 'malicious faulty processors', 'Byzantine agreement problem', 'consensus problem', 'connected network', 'network topology']",771,140,12,771,139,10,0,0,1
"it's time to buy there is an upside to a down economy: over-zealous suppliers are willing to make deals that were unthinkable a few years ago. that's because vendors are experiencing the same money squeeze as manufacturers, which makes the year 2002 the perfect time to invest in new technology. the author states that when negotiating the deal, provisions for unexpected costs, an exit strategy, and even shared risk with the vendor should be on the table ","It's time to buy There is an upside to a down economy: over-zealous suppliers are willing to make deals that were unthinkable a few years ago. That's because vendors are experiencing the same money squeeze as manufacturers, which makes the year 2002 the perfect time to invest in new technology. The author states that when negotiating the deal, provisions for unexpected costs, an exit strategy, and even shared risk with the vendor should be on the table","['negotiation', 'unexpected costs', 'exit strategy', 'shared risk', 'vendor', 'suppliers', 'money squeeze', 'buyers market', 'bargaining power', 'costing', 'management', 'purchasing']","['over-zealous suppliers', 'same money squeeze', 'deal provisions', 'new technology', 'perfect time', 'few years', 'deals', 'time', 'years', 'over-zealous']",380,78,12,380,77,10,0,0,4
separate accounts go mainstream [investment] new entrants are shaking up the separate-account industry by supplying web-based platforms that give advisers the tools to pick independent money managers ,Separate accounts go mainstream [investment] New entrants are shaking up the separate-account industry by supplying Web-based platforms that give advisers the tools to pick independent money managers,"['separate-account industry', 'Web-based platforms', 'independent money managers', 'investment', 'financial advisors', 'investment']","['independent money managers', 'separate-account industry', 'pen-based platforms', 'Separate accounts', 'mainstream', 'investment', 'entrants', 'New', 'Separate', 'accounts']",173,28,6,173,27,10,0,0,0
"exploiting structure in quantified formulas we study the computational problem ""find the value of the quantified formula obtained by quantifying the variables in a sum of terms."" the ""sum"" can be based on any commutative monoid, the ""quantifiers"" need only satisfy two simple conditions, and the variables can have any finite domain. this problem is a generalization of the problem ""given a sum-of-products of terms, find the value of the sum"" studied by r.e. stearns and h.b. hunt iii (1996). a data structure called a ""structure tree"" is defined which displays information about ""subproblems"" that can be solved independently during the process of evaluating the formula. some formulas have ""good"" structure trees which enable certain generic algorithms to evaluate the formulas in significantly less time than by brute force evaluation. by ""generic algorithm,"" we mean an algorithm constructed from uninterpreted function symbols, quantifier symbols, and monoid operations. the algebraic nature of the model facilitates a formal treatment of ""local reductions"" based on the ""local replacement"" of terms. such local reductions ""preserve formula structure"" in the sense that structure trees with nice properties transform into structure trees with similar properties. these local reductions can also be used to transform hierarchical specified problems with useful structure into hierarchically specified problems having similar structure ","Exploiting structure in quantified formulas We study the computational problem ""find the value of the quantified formula obtained by quantifying the variables in a sum of terms."" The ""sum"" can be based on any commutative monoid, the ""quantifiers"" need only satisty two simple conditions, and the variables can have any finite domain. This problem is a generalization of the problem ""given a sum-of-products of terms, find the value of the sum"" studied by R.E. Stearns and H.B. Hunt Ill (1996). A data structure called a ""structure tree"" is defined which displays information about ""subproblems” that can be solved independently during the process of evaluating the formula. Some formulas have ""good"" structure trees which enable certain generic algorithms to evaluate the formulas in significantly less time than by brute force evaluation. By ""generic algorithm,"" we mean an algorithm constructed from uninterpreted function symbols, quantifier symbols, and monoid operations. The algebraic nature of the model facilitates a formal treatment of ""local reductions” based on the “local replacement"" of terms. Such local reductions ""preserve formula structure” in the sense that structure trees with nice properties transform into structure trees with similar properties. These local reductions can also be used to transform hierarchical specified problems with useful structure into hierarchically specified problems having similar structure","['quantified formulas', 'structure exploitation', 'commutative monoid', 'data structure', 'structure tree', 'satisfiability problems', 'constraint satisfaction problems', 'dynamic programming', 'computational complexity', 'generic algorithms', 'function symbols', 'quantifier symbols', 'monoid operations', 'hierarchically specified problems', 'computational complexity', 'constraint theory', 'data structures', 'dynamic programming', 'genetic algorithms']","['structure', 'computational problem', 'Such local reductions', 'good structure trees', 'similar structure', 'formula structure', 'useful structure', 'data structure', 'formula', 'structure trees']",1229,212,19,1229,211,10,7,6,7
"existence theorems for nonconvex problems of variational calculus a solution to a variational calculus problem is studied under the conditions of integrant convexity. the existence theorem is proved. as an example, a function is given, which satisfies all the conditions of the theorem but is not convex ","Existence theorems for nonconvex problems of variational calculus Assolution to a variational calculus problem is studied under the conditions of integrant convexity. The existence theorem is proved. As an example, a function is given, which satisfies all the conditions of the theorem but is not convex","['existence theorems', 'nonconvex problems', 'variational calculus', 'integrant convexity', 'convex programming', 'variational techniques']","['variation calculus absolution', 'variation calculus problem', 'integrand convexity', 'nonconvex problems', 'Existence theorems', 'variation', 'theorem', 'calculus', 'problems', 'existence theorem']",257,48,6,258,46,10,104,38,0
"williams nears end of chapter 11 [telecom] leucadia national corp. comes through with a $330 million boost for williams communications, which should keep the carrier afloat through the remainder of its bankruptcy ","Williams nears end of Chapter 11 [telecom] Leucadia National Corp. comes through with a $330 million boost for Wiliams Communications, which should keep the carrier afloat through the remainder of its bankruptcy","['Leucadia National Corp', 'Williams Communications', 'bankruptcy', 'telecommunication']","['williams communications', 'National', 'Leucadia', 'telecom', 'Chapter', 'boost', 'Corp.', 'end', 'williams', 'Williams']",181,33,4,180,32,10,4,1,1
"cultural differences in developers' perceptions of information systems success factors: japan vs. the united states the study examined the perceptions of information systems (is) developers from japan and the united states regarding the strategies that are considered most important for successful implementation of an is. the results of principal component analysis revealed that the is strategies could be reduced to five components: (1) characteristics of the team members, (2) characteristics of the project leader, (3) management/user input, (4) proper technology, and (5) communication. the results indicated that there was a significant difference in the perceptions of japanese and us developers with respect to the importance of the five components. japanese developers perceived the project leader as the most crucial component for determining the success of an is project. team member characteristics was viewed as the least important by japanese developers. on the other hand, developers from the us viewed communications as the most critical component. project leader characteristics were perceived to be the least important by us developers. the results were discussed in terms of cultural differences ","Cultural differences in developers' perceptions of information systems success factors: Japan vs. the United States The study examined the perceptions of information systems (IS) developers from Japan and the United States regarding the strategies that are considered most important for successful implementation of an IS. The results of principal component analysis revealed that the IS strategies could be reduced to five components: (1) characteristics of the team members, (2) characteristics of the project leader, (3) managementluser input, (4) proper technology, and (5) communication. The results indicated that there was a significant difference in the perceptions of Japanese and US developers with respect to the importance of the five components. Japanese developers perceived the project leader as the most crucial component for determining the success of an IS project. Team member characteristics was viewed as the least important by Japanese developers. On the other hand, developers from the US viewed ‘communications as the most critical component. Project leader characteristics were perceived to be the least important by US developers. The results were discussed in terms of cultural differences","['cultural differences', 'information systems success factors', 'information systems developer perceptions', 'Japan', 'United States', 'principal component analysis', 'team member characteristics', 'project leader characteristics', 'management/user input', 'proper technology', 'communication', 'IS project', 'information systems', 'principal component analysis', 'project management', 'social aspects of automation', 'software development management', 'systems analysis']","['developers', 'Cultural differences', 'information systems', 'Japanese developers', 'US developers', 'components a1 characteristics', 'principal component analysis', 'significant difference', 'developers perceptions', 'other hand developers']",1039,178,18,1040,177,10,14,2,6
"laptops zip to 2 ghz-plus intel's pentium 4-m processor has reached the coveted 2-ghz mark, and speed-hungry mobile users will be tempted to buy a laptop with the chip. however, while our exclusive tests found 2-ghz p4-m notebooks among the fastest units we've tested, the new models failed to make dramatic gains compared with those based on intel's 1.8-ghz mobile chip. since 2-ghz notebooks carry a hefty price premium, buyers seeking both good performance and a good price might prefer a 1.8-ghz unit instead ","Laptops zip to 2 GHz-plus Intel's Pentium 4-M processor has reached the coveted 2-GHz mark, and speed-hungry mobile users will be tempted to buy a laptop with the chip. However, while our exclusive tests found 2-GHz P4-M notebooks among the fastest units we've tested, the new models failed to make dramatic gains compared with those based on Intel's 1.8-GHz mobile chip. Since 2-GHz notebooks carry a hefty price premium, buyers seeking both good performance and a good price might prefer a 1.8-GHz unit instead","['Intel Pentium 4-M processor', 'mobile', 'laptop', 'notebooks', '2 GHz', 'computer evaluation', 'microprocessor chips', 'notebook computers']","['speed-hungry mobile users', '2-GHz poem notebooks', 'coveted 2-GHz mark', '2-GHz notebooks', 'intel 1.8-GHz', 'mobile chips', 'Pentium', 'Laptops', '2-GHz', 'intel']",429,85,8,429,84,10,0,0,2
"the design and performance evaluation of alternative xml storage strategies this paper studies five strategies for storing xml documents including one that leaves documents in the file system, three that use a relational database system, and one that uses an object manager. we implement and evaluate each approach using a number of xquery queries. a number of interesting insights are gained from these experiments and a summary of the advantages and disadvantages of the approaches is presented ","The design and performance evaluation of alternative XML storage strategies This paper studies five strategies for storing XML documents including one that leaves documents in the file system, three that use a relational database system, and one that uses an object manager. We implement and evaluate each approach using a number of XQuery queries. A number of interesting insights are gained from these experiments and a summary of the advantages and disadvantages of the approaches is presented","['XML document storage', 'file system', 'relational database system', 'object manager', 'performance evaluation', 'XQuery queries', 'hypermedia markup languages', 'object-oriented databases', 'query processing', 'relational databases', 'storage management']","['strategies', 'relational database system', 'performance evaluation', 'paper studies', 'ml documents', 'alternative', 'design', 'ml', 'evaluation', 'performance']",420,78,11,420,77,10,0,0,2
"linear models of circuits based on the multivalued components linearization and planarization of the circuit models is pivotal to the submicron technologies. on the other hand, the characteristics of the vlsi circuits can be sometimes improved by using the multivalued components. it was shown that any l-level circuit based on the multivalued components is representable as an algebraic model based on l linear arithmetic polynomials mapped correspondingly into l decision diagrams that are linear and planar by nature. complexity of representing a circuit as the linear decision diagram was estimated as o(g) with g for the number of multivalued components in the circuit. the results of testing the lineardesignmv algorithm on circuits of more than 8000 lgsynth 93 multivalued components were presented ","Linear models of circuits based on the multivalued components Linearization and pianarization of the circuit models is pivotal to the submicron technologies. On the other hand, the characteristics of the VLSI circuits can be sometimes improved by using the multivalued ‘components. It was shown that any -level circuit based on the multivalued components is representable as an algebraic model based on linear arithmetic polynomials mapped correspondingly into | decision diagrams that are linear and planar by nature. Complexity of representing a circuit as the linear decision diagram was estimated as 0(G) with G for the number of multivalued components in the circuit. The results of testing the LinearDesignMV algorithm on circuits of more than 8000 LGSynth 93 multivalued components were presented","['linear circuit model', 'linearization', 'planarization', 'submicron technologies', 'VLSI circuits', 'linear arithmetic polynomials', 'linear planar decision diagrams', 'circuit representation complexity', 'LinearDesignMV algorithm', 'LGSynth 93 multivalued components', 'circuit analysis computing', 'circuit complexity', 'linearisation techniques', 'multivalued logic circuits', 'polynomials', 'VLSI']","['circuit', 'multivalued components Linearization', 'linear decision diagram', 'algebraic model', 'circuit models', 'level circuit', 'VLSI circuits', 'Linear models', 'multivalued', 'multivalued components']",684,123,16,683,121,10,216,63,4
"estimating populations for collective dose calculations the collective dose provides an estimate of the effects of facility operations on the public based on an estimate of the population in the area. geographic information system software, electronic population data resources, and a personal computer were used to develop estimates of population within 80 km radii of two sites ","Estimating populations for collective dose calculations The collective dose provides an estimate of the effects of facility operations ‘on the public based on an estimate of the population in the area. Geographic information system software, electronic population data resources, and a personal computer were used to develop estimates of Population within 80 km radii of two sites","['collective dose calculations', 'facility operations', 'public', 'geographic information system software', 'electronic population data resources', 'personal computer', 'dosimetry', 'geographic information systems', 'medical computing']","['estimate', 'collective dose calculations', 'facility operations', 'personal computer', 'populations', 'effects', 'dose', 'collective', 'calculations', 'collective dose']",323,58,9,324,57,10,2,1,3
"the real story behind calpoint [telecom] a former qwest executive sheds light on the carrier's controversial deal with calpoint. discusses why calpoint gets a monthly check from quest, regardless of whether it provides services ","The real story behind Calpoint [telecom] A former Qwest executive sheds light on the carrier's controversial deal with Calpoint. Discusses why Calpoint gets a monthly check from Quest, regardless of whether it provides services","['Qwest', 'Calpoint', 'telecom carrier', 'telecommunication']","['former west executive', 'controversial deal', 'monthly check', 'real story', 'telecom', 'west', 'real', 'story', 'former', 'executive']",194,35,4,194,34,10,0,0,2
"becoming a computer scientist the focus of this report is pipeline shrinkage for women in computer science. we describe the situation for women at all stages of training in computer science, from the precollege level through graduate school. because many of the problems discussed are related to the lack of role models for women who are in the process of becoming computer scientists, we also concern ourselves with the status of women faculty members. we not only describe the problems, but also make specific recommendations for change and encourage further study of those problems whose solutions are not yet well understood ","Becoming a computer scientist The focus of this report is pipeline shrinkage for women in computer science. We describe the situation for women at all stages of training in ‘computer science, from the precollege level through graduate school Because many of the problems discussed are related to the lack of role models for women who are in the process of becoming computer scientists, we also concern ourselves with the status of women faculty members. We not only describe the problems, but also make specific recommendations for change and encourage further study of those problems whose solutions are not yet well understood","['pipeline shrinkage', 'women', 'computer science', 'role models', 'women faculty members', 'computer science education', 'gender issues']","['computer scientist', 'computer science', 'women faculty members', 'pipeline shrinkage', 'precollege level', 'role models', 'computer', 'women', 'science', 'scientist']",528,102,7,528,101,10,8,2,0
"molecular descriptor selection combining genetic algorithms and fuzzy logic: application to database mining procedures a new algorithm, devoted to molecular descriptor selection in the context of data mining problems, has been developed. this algorithm is based on the concepts of genetic algorithms (ga) for descriptor hyperspace exploration and combined with a stepwise approach to get local convergence. its selection power was evaluated by a fitness function derived from a fuzzy clustering method. different training and test sets were randomly generated at each ga generation. the fitness score was derived by combining the scores of the training and test sets. the ability of the proposed algorithm to select relevant subsets of descriptors was tested on two data sets. the first one, an academic example, corresponded to the artificial problem of bullseye, the second was a real data set including 114 olfactory compounds divided into three odor categories. in both cases, the proposed method allowed to improve the separation between the different data set classes ","Molecular descriptor selection combining genetic algorithms and fuzzy logic: application to database mining procedures Anew algorithm, devoted to molecular descriptor selection in the context of data mining problems, has been developed. This algorithm is based on the concepts of genetic algorithms (GA) for descriptor hyperspace exploration and combined with a stepwise approach to get local convergence. Its selection power was evaluated by a fitness function derived from a fuzzy clustering method. Different training and test sets were randomly generated at each GA generation. The fitness score was derived by combining the scores of the training and test sets. The ability of the proposed algorithm to select relevant subsets of descriptors was tested on two data sets. The first one, an academic ‘example, corresponded to the artificial problem of Bullseye, the second was a real data set including 114 olfactory compounds divided into three odor categories. In both cases, the proposed method allowed to improve the separation between the different data set classes","['molecular descriptor selection', 'database mining', 'data mining', 'genetic algorithms', 'fuzzy clustering method', 'fuzzy logic', 'descriptor hyperspace exploration', 'local convergence', 'stepwise approach', 'fitness function', 'test sets', 'training sets', 'fitness score', 'Bullseye', 'olfactory compounds', 'odor categories', 'chemistry computing', 'convergence of numerical methods', 'data mining', 'fuzzy logic', 'genetic algorithms', 'pattern clustering', 'scientific information systems']","['descriptor hyperspace exploration', 'fuzzy logic application', 'genetic algorithms gas', 'data mining problems', 'selection power', 'descriptor', 'algorithm', 'genetic algorithms', 'molecular descriptor selection', 'Molecular descriptor selection']",911,164,23,912,162,10,491,148,8
"cool and green [air conditioning] in these days of global warming, air conditioning engineers need to specify not just for the needs of the occupants, but also to maximise energy efficiency. julian brunnock outlines the key areas to consider for energy efficient air conditioning systems ","Cool and green [air conditioning] In these days of global warming, air conditioning engineers need to specify not just for the needs of the occupants, but also to maximise energy efficiency. Julian Brunnock outlines the key areas to consider for energy efficient air conditioning systems","['air conditioning', 'energy efficiency', 'air conditioning']","['energy', 'green air conditioning', 'efficiency', 'key areas', 'global', 'days', 'Cool', 'air', 'green', 'conditioning']",243,46,3,243,45,10,0,0,0
"the crossing number of p(n, 3) it is proved that the crossing number of the generalized petersen graph p(3k + h, 3) is k + h if h in {0, 2} and k + 3 if h = 1, for each k >or= 3, with the single exception of p(9,3), whose crossing number is 2 ","The crossing number of P(N, 3) It is proved that the crossing number of the generalized Petersen graph P(3k + h, 3) isk +hif h in (0, 2} and k +3 if h = 1, for each k >or=3, with the single exception of P(9,3), whose crossing number is 2","['crossing number', 'generalized Petersen graph', 'graph theory']","['number', 'Petersen graph park', 'single exception', 'pink', 'h', '=', 'k', 'park', 'graph', 'Petersen']",188,56,3,188,50,10,55,28,0
"knowledge management the article defines knowledge management, discusses its role, and describes its functions. it also explains the principles of knowledge management, enumerates the strategies involved in knowledge management, and traces its history in brief. the focus is on its interdisciplinary nature. the steps involved in knowledge management i.e. identifying, collecting and capturing, selecting, organizing and storing, sharing, applying, and creating, are explained. the pattern of knowledge management initiatives is also considered ","Knowledge management The article defines knowledge management, discusses its role, and describes its functions. It also explains the principles of knowledge management, enumerates the strategies involved in knowledge management, and traces its history in brief. The focus is on its interdisciplinary nature. The steps involved in knowledge management i.e. identifying, collecting and capturing, selecting, organizing and storing, sharing, applying, and creating, are explained. The pattern of knowledge management initiatives is also considered","['knowledge management', 'business data processing', 'data warehouses', 'management information systems', 'marketing data processing']","['knowledge management initiatives', 'interdisciplinary nature', 'functions', 'article', 'role', 'knowledge', 'Knowledge', 'management', 'knowledge management', 'Knowledge management']",473,73,5,473,72,10,0,0,0
"it at the heart of joined-up policing police it is to shift from application-focused to component-based technology. the change of strategy, part of the valiant programme, will make information held by individual forces available on a national basis ","IT at the heart of joined-up policing Police IT is to shift from application-focused to component-based technology. The change of strategy, part of the Valiant Programme, will make information held by individual forces available on a national basis","['Valiant Programme', 'police IT', 'UK', 'police']","['component-based technology', 'application-focused', 'individual forces', 'Valiant programme', 'strategy part', 'joined-up', 'Police', 'heart', 'technology', 'component-based']",211,39,4,211,38,10,0,0,0
"press shop. industrial it solutions for the press shop globalization of the world's markets is challenging the traditional limits of manufacturing efficiency. the competitive advantage belongs to those who understand the new requirements and opportunities, and who commit to integrated solutions that span the value chain all the way from demand to production. abb's automation and it expertise and the process know-how gained from its long involvement with the automotive industry, have been brought together in new, state-of-the-art software solutions for press shops. integrated into industrial it architecture, they allow the full potential of the shops to be realized, with advantages at every step in the supply chain ","Press shop. Industrial IT solutions for the press shop Globalization of the world's markets is challenging the traditional limits of manufacturing efficiency. The competitive advantage belongs to those who understand the new requirements and opportunities, and who commit to integrated solutions that span the value chain all the way from demand to production. ABB's automation and IT expertise and the process know-how gained from its long involvement with the automotive industry, have been brought together in new, state-of-the-art software solutions for press shops. Integrated into Industrial IT architecture, they allow the full potential of the shops to be realized, with advantages at every step in the supply chain","['press shops', 'industrial IT solutions', 'market globalisation', 'manufacturing efficiency', 'automation', 'state-of-the-art', 'software solutions', 'car manufacturing business', 'supply chain', 'automobile industry', 'industrial control', 'industrial robots']","['solutions', 'IT', 'Industrial IT architecture', 'press shop Globalization', 'press shops Integrated', 'competitive advantage', 'new requirements', 'press', 'shops', 'Industrial']",616,109,12,616,108,10,0,0,4
"eliminating counterevidence with applications to accountable certificate management this paper presents a method to increase the accountability of certificate management by making it intractable for the certification authority (ca) to create contradictory statements about the validity of a certificate. the core of the method is a new primitive, undeniable attester, that allows someone to commit to some set s of bitstrings by publishing a short digest of s and to give attestations for any x that it is or is not a member of s. such an attestation can be verified by obtaining in an authenticated way the published digest and applying a verification algorithm to the triple of the bitstring, the attestation and the digest. the most important feature of this primitive is the intractability of creating two contradictory proofs for the same candidate element x and digest. we give an efficient construction for undeniable attesters based on authenticated search trees. we show that the construction also applies to sets of more structured elements. we also show that undeniable attesters exist iff collision-resistant hash functions exist ","Eliminating counterevidence with applications to accountable certificate management This paper presents a method to increase the accountability of certificate management by making it intractable for the certification authority (CA) to create contradictory statements about the validity of a certificate. The core of the method is a new primitive, undeniable attester, that allows someone to commit to some set S of bitstrings by publishing a short digest of S and to give attestations for any x that itis or is not a member of S. Such an attestation can be verified by obtaining in an authenticated way the published digest and applying a verification algorithm to the triple of the bitstring, the attestation and the digest. The most important feature of this primitive is the Intractability of creating two contradictory proofs for the same candidate element x and digest. We give an efficient construction for undeniable attesters based on authenticated search trees. We show that the construction also applies to sets of more structured elements. We also show that undeniable attesters exist iff collision-resistant hash functions exist","['counterevidence elimination', 'accountable certificate management', 'accountability', 'certification authority', 'undeniable attester primitive', 'attestations', 'published digest', 'verification algorithm', 'bitstring', 'contradictory proofs', 'authenticated search trees', 'structured elements', 'collision-resistant hash functions', 'long-term authenticity', 'non repudiation', 'public-key infrastructure', 'time-stamping', 'certification', 'message authentication', 'public key cryptography', 'tree searching']","['method', 'accountable certificate management', 'contradictory statements', 'certification authority', 'counterevidence', 'accountability', 'applications', 'certificate', 'management', 'certificate management']",965,178,21,965,176,10,289,100,7
"srp rolls out reliability and asset management initiative reliability planning analysis at the salt river project (srp, tempe, arizona, us) prioritizes geographic areas for preventive inspections based on a cost benefit model. however, srp wanted a new application system to prioritize inspections and to predict when direct buried cable would fail using the same cost benefit model. in the business cases, the represented type of kilowatt load-residential, commercial or critical circuit-determines the cost benefit per circuit. the preferred solution was to develop a geographical information system (gis) application allowing for a circuit query for the specific geographic areas it crosses and the density of load points of a given type within those areas. the query returns results based on the type of equipment analysis execution: wood pole, preventive maintenance for a line or cable replacement. this differentiation insures that all the facilities relevant to a specific analysis type influence prioritization of the geographic areas ","SRP rolls out reliability and asset management initiative Reliability planning analysis at the Salt River Project (SRP, Tempe, Arizona, US) prioritizes geographic areas for preventive inspections based on a cost benefit model. However, SRP wanted a new application system to prioritize inspections and to predict when direct buried cable would fail using the same cost benefit model. In the business cases, the represented type of kilowatt load-residential, commercial or critical circuit-determines the cost benefit per circuit. The preferred solution was to develop a geographical information system (GIS) application allowing for a circuit query for the specific geographic areas it crosses and the density of load points of a given type within those areas. The query returns results based on the type of equipment analysis execution: wood pole, preventive maintenance for a line or cable replacement. This differentiation insures that all the facilities, relevant to a specific analysis type influence prioritization of the geographic areas","['Salt River Project', 'Tempe', 'Arizona', 'USA', 'geographic areas', 'preventive inspections', 'reliability planning analysis', 'cost benefit model', 'direct buried cable', 'geographical information system', 'GIS', 'equipment analysis execution', 'wood pole', 'cable replacement', 'condition monitoring', 'condition monitoring', 'cost-benefit analysis', 'geographic information systems', 'inspection', 'maintenance engineering', 'planning', 'poles and towers', 'power cables', 'power engineering computing', 'power system reliability', 'underground cables']","['cost benefit model', 'reliability', 'geographical information system', 'specific geographic areas', 'management', 'sri rolls', 'asset', 'sri', 'rolls', 'geographic areas']",890,155,26,891,154,10,0,1,11
regional flux target with minimum energy an extension of a gradient controllability problem to the case where the target subregion is a part of the boundary of a parabolic system domain is discussed. a definition and some properties adapted to this case are presented. the focus is on the characterisation of the control achieving a regional boundary gradient target with minimum energy. an approach is developed that leads to a numerical algorithm for the computation of optimal control. numerical illustrations show the efficiency of the approach and lead to conjectures ,Regional flux target with minimum energy ‘An extension of a gradient controllability problem to the case where the target subregion is a part of the boundary of a parabolic system domain is discussed. A definition and some properties adapted to this case are presented. The focus is on the characterisation of the control achieving a regional boundary gradient target with minimum energy. An approach is developed that leads to a numerical algorithm for the computation of optimal control. Numerical illustrations show the efficiency of the approach and lead to conjectures,"['regional flux target', 'minimum energy', 'gradient controllability problem', 'target subregion', 'parabolic system domain boundary', 'regional boundary gradient target', 'numerical algorithm', 'optimal control', 'controllability', 'distributed parameter systems', 'gradient methods', 'minimisation', 'numerical analysis', 'optimal control']","['minimum energy', 'gradient controllability problem', 'parabolic system domain', 'Regional flux target', 'target subregions', 'optimal control', 'target', 'controllability', 'energy', 'minimum']",483,91,14,484,90,10,2,1,1
"electromagnetics computations using the mpi parallel implementation of the steepest descent fast multipole method (sdfmm) the computational solution of large-scale linear systems of equations necessitates the use of fast algorithms but is also greatly enhanced by employing parallelization techniques. the objective of this work is to demonstrate the speedup achieved by the mpi (message passing interface) parallel implementation of the steepest descent fast multipole method (sdfmm). although this algorithm has already been optimized to take advantage of the structure of the physics of scattering problems, there is still the opportunity to speed up the calculation by dividing tasks into components using multiple processors and solve them in parallel. the sdfmm has three bottlenecks ordered as (1) filling the sparse impedance matrix associated with the near-field method of moments interactions (mom), (2) the matrix vector multiplications associated with this sparse matrix and (3) the far field interactions associated with the fast multipole method. the parallel implementation task is accomplished using a thirty-one node intel pentium beowulf cluster and is also validated on a 4-processor alpha workstation. the beowulf cluster consists of thirty-one nodes of 350 mhz intel pentium iis with 256 mb of ram and one node of a 4*450 mhz intel pentium ii xeon shared memory processor with 2 gb of ram with all nodes connected to a 100 basetx ethernet network. the alpha workstation has a maximum of four 667 mhz processors. our numerical results show significant linear speedup in filling the sparse impedance matrix. using the 32-processors on the beowulf cluster lead to a 7.2 overall speedup while a 2.5 overall speedup is gained using the 4-processors on the alpha workstation ","Electromagnetics computations using the MPI parallel implementation of the steepest descent fast multipole method (SDFMM) The computational solution of large-scale linear systems of equations necessitates the use of fast algorithms but is also greatly enhanced by employing parallelization techniques. The objective of this work is to demonstrate the speedup achieved by the MPI (message passing interface) parallel implementation of the steepest descent fast multipole method (SDFMM). Although this algorithm has already been optimized to take advantage of the structure of the physics of scattering problems, there is stil the opportunity to speed up the calculation by dividing tasks into components using multiple processors and solve them in parallel The SDFMM has three bottlenecks ordered as (1) filing the sparse impedance matrix associated with the near-field method of moments interactions (MoM), (2) the matrix vector multiplications associated with this sparse matrix and (3) the far field interactions associated with the fast multipole method. The parallel implementation task is accomplished using a thirty-one node Intel Pentium Beowulf cluster and is also validated on a 4-processor Alpha workstation. The Beowulf cluster consists of thirty-one nodes of 350 MHz Intel Pentium Ils with 256 MB of RAM and one node of a 4*450 MHz Intel Pentium II Xeon shared memory processor with 2 GB of RAM with all nodes connected to a 100 BaseTX Ethernet network. The Alpha workstation has a maximum of four 667 MHz processors. Our numerical results show significant linear speedup in filing the sparse impedance matrix. Using the 32-processors ‘on the Beowulf cluster lead to a 7.2 overall speedup while a 2.5 overall speedup is gained using the 4-processors on the Alpha workstation","['electromagnetics computations', 'MPI parallel implementation', 'steepest descent fast multipole method', 'large-scale linear systems', 'fast algorithms', 'message passing interface', 'physics', 'multiple processors', 'sparse impedance matrix', 'near-field MoM', 'method of moments', 'scattering problems', 'matrix vector multiplications', 'Intel Pentium Beowulf cluster', '4-processor Alpha workstation', 'Intel Pentium II', 'RAM', 'Xeon shared memory processor', '100 BaseTX Ethernet network', 'scattered electric field', 'scattered magnetic field', '350 MHz', '256 MByte', '450 MHz', '2 GByte', '667 MHz', 'application program interfaces', 'electric fields', 'electromagnetic wave scattering', 'electromagnetism', 'impedance matrix', 'magnetic fields', 'matrix multiplication', 'message passing', 'method of moments', 'parallel algorithms', 'parallel architectures', 'physics computing', 'sparse matrices', 'workstations']","['fast multiple method', 'parallel', 'interface parallel implementation', 'matrix vector multiplications', 'parallel implementation task', 'electromagnetic computations', 'mp parallel implementation', 'computational solution', 'multiple processors', 'multiple method']",1517,274,40,1514,273,10,9,6,13
"the open archives initiative: realizing simple and effective digital library interoperability the open archives initiative (oai) is dedicated to solving problems of digital library interoperability. its focus has been on defining simple protocols, most recently for the exchange of metadata from archives. the oai evolved out of a need to increase access to scholarly publications by supporting the creation of interoperable digital libraries. as a first step towards such interoperability, a metadata harvesting protocol was developed to support the streaming of metadata from one repository to another, ultimately to a provider of user services such as browsing, searching, or annotation. this article provides an overview of the mission, philosophy, and technical framework of the oai ","The Open Archives Initiative: realizing simple and effective digital library interoperability The Open Archives Initiative (OAl) is dedicated to solving problems of digital library interoperability. Its focus has been on defining simple protocols, most recently for the exchange of metadata from archives. The OAI evolved out of a need to increase access to scholarly publications by supporting the creation of interoperable digital libraries. As a first step towards such interoperability, a metadata harvesting protocol was developed to support the streaming of metadata from one repository to another, ultimately to a provider of user services such as browsing, searching, or annotation. This article provides an overview of the mission, philosophy, and technical framework of the OAl","['Open Archives Initiative', 'digital library interoperability', 'protocols', 'exchange metadata', 'scholarly publications', 'metadata harvesting protocol', 'streaming metadata', 'annotation', 'searching', 'browsing', 'user services', 'digital libraries', 'electronic publishing', 'meta data', 'open systems', 'protocols']","['digital library interoperability', 'Open Archives initiative', 'interoperable digital libraries', 'teradata harvesting protocol', 'such interoperability', 'simple protocols', 'digital', 'interoperable', 'Open', 'library']",673,116,16,673,115,10,2,2,5
"effect of multileaf collimator leaf width on physical dose distributions in the treatment of cns and head and neck neoplasms with intensity modulated radiation therapy the purpose of this work is to examine physical radiation dose differences between two multileaf collimator (mlc) leaf widths (5 and 10 mm) in the treatment of cns and head and neck neoplasms with intensity modulated radiation therapy (imrt). three clinical patients with cns tumors were planned with two different mlc leaf sizes, 5 and 10 mm, representing varian-120 and varian-80 millennium multileaf collimators, respectively. two sets of imrt treatment plans were developed. the goal of the first set was radiation dose conformality in three dimensions. the goal for the second set was organ avoidance of a nearby critical structure while maintaining adequate coverage of the target volume. treatment planning utilized the cadplan/helios system (varian medical systems, milpitas ca) for dynamic mlc treatment delivery. all beam parameters and optimization (cost function) parameters were identical for the 5 and 10 mm plans. for all cases the number of beams, gantry positions, and table positions were taken from clinically treated three-dimensional conformal radiotherapy plans. conformality was measured by the ratio of the planning isodose volume to the target volume. organ avoidance was measured by the volume of the critical structure receiving greater than 90% of the prescription dose (v/sub 90/). for three patients with squamous cell carcinoma of the head and neck (t2-t4 n0-n2c m0) 5 and 10 mm leaf widths were compared for parotid preservation utilizing nine coplanar equally spaced beams delivering a simultaneous integrated boost. because modest differences in physical dose to the parotid were detected, a ntcp model based upon the clinical parameters of eisbruch et al. was then used for comparisons. the conformality improved in all three cns cases for the 5 mm plans compared to the 10 mm plans. for the organ avoidance plans, v/sub 90/ also improved in two of the three cases when the 5 mm leaf width was utilized for imrt treatment delivery. in the third case, both the 5 and 10 mm plans were able to spare the critical structure with none of the structure receiving more than 90% of the prescription dose, but in the moderate dose range, less dose was delivered to the critical structure with the 5 mm plan. for the head and neck cases both the 5 and 10*2.5 mm beamlets dmlc sliding window techniques spared the contralateral parotid gland while maintaining target volume coverage. the mean parotid dose was modestly lower with the smaller beamlet size (21.04 gy vs 22.36 gy). the resulting average ntcp values were 13.72% for 10 mm dmlc and 8.24% for 5 mm dmlc. in conclusion, five mm leaf width results in an improvement in physical dose distribution over 10 mm leaf width that may be clinically relevant in some cases. these differences may be most pronounced for single fraction radiosurgery or in cases where the tolerance of the sensitive organ is less than or close to the target volume prescription ","Effect of muttileaf collimator leaf width on physical dose distributions in the treatment of CNS and head and neck neoplasms with intensity modulated radiation therapy The purpose of this work is to examine physical radiation dose differences between two multileaf collimator (MLC) leaf widths (5 and 10 mm) in the treatment of CNS and head and neck neoplasms with intensity modulated radiation therapy (IMRT). Three clinical patients with CNS tumors were planned with two different MLC leaf sizes, 5 and 10 mm, representing Varian-120 and Varian-80 Millennium multileaf collimators, respectively. Two sets of IMRT treatment plans were developed. The goal of the first set was radiation dose conformaity in three dimensions. The goal for the second set was organ avoidance of a nearby critical structure while maintaining adequate coverage of the target volume. Treatment planning utilized the CadPlan/Helios system (Varian Medical Systems, Milpitas CA) for dynamic MLC treatment delivery. All beam parameters and optimization (cost function) parameters were identical for the 5 and 10 mm plans. For all cases the number of beams, gantry positions, and table positions were taken from clinically treated three-dimensional conformal radiotherapy plans. Conformality was measured by the ratio of the planning isodose volume to the target volume. Organ avoidance was measured by the volume of the critical structure receiving greater than 90% of the prescription dose (V/sub 90/). For three patients with squamous cell carcinoma of the head and neck (T2-T4 NO-N2c MO) 5 and 10 mm leaf widths were compared for parotid preservation utilizing nine coplanar equally spaced beams delivering a simultaneous integrated boost. Because modest differences in physical dose to the parotid were detected, a NTCP model based upon the clinical parameters of Eisbruch et al. was then used for ‘comparisons. The conformality improved in all three CNS cases for the 5 mm plans compared to the 10 mm plans. For the organ avoidance plans, Visub 90/ also improved in two of the three cases when the 5 mm leaf width was utlized for IMRT treatment delivery. In the third case, both the 5 and 10 mm plans were able to spare the critical structure with none of the structure receiving more than 90% of the prescription dose, but in the moderate dose range, less dose was delivered to the critical structure with the 5 mm plan. For the head and neck cases both the 5 and 10°2.5 mm beamlets dMLC sliding window techniques spared the contralateral parotid gland while maintaining target volume coverage. The mean parotid dose was modestly lower with the smaller beamlet size (21.04 Gy vs 22.36 Gy). The resulting average NTCP values were 13.72% for 10 mm MLC and 8.24% for 5 mm dMLC. In conclusion, five mm leaf width results in an improvement in physical dose distribution over 10 mm leaf width that may be clinically relevant in some cases. These differences may be most pronounced for single fraction radiosurgery or in cases where the tolerance of the sensitive organ is less than or close to the target volume prescription","['multileaf collimator leaf width', 'physical dose distributions', 'head and neck neoplasms', 'intensity modulated radiation therapy', 'CNS neoplasms', 'CNS tumors', 'treatment planning', 'parotid preservation', 'optimization parameters', 'conformal radiotherapy', 'single fraction radiosurgery', 'acceptable tumor coverage', 'minimal toxicity', 'collimator rotation', 'beamlet size', '5 mm', '10 mm', '21.04 Gy', '22.36 Gy', 'dosimetry', 'intensity modulation', 'medical computing', 'neurophysiology', 'radiation therapy', 'tumours']","['mm plans', 'physical dose distributions', 'radiation dose conformity', 'imre treatment plans', 'mean carotid dose', 'mm leaflets dml', 'mm leaf widths', 'mm dec', 'mm MLC', 'physical dose']",2599,503,25,2597,502,10,28,9,8
"development of an internet-based intelligent design support system for rolling element bearings this paper presents a novel approach to developing an intelligent agile design system for rolling bearings based on artificial intelligence (ai), internet and web technologies and expertise. the underlying philosophy of the approach is to use ai technology and web-based design support systems as smart tools from which design customers can rapidly and responsively access the systems' built-in design expertise. the approach is described in detail with a novel ai model and system implementation issues. the major issues in implementing the approach are discussed with particular reference to using ai technologies, network programming, client-server technology and open computing of bearing design and manufacturing requirements ","Development of an Internet-based intelligent design support system for rolling element bearings This paper presents a novel approach to developing an intelligent agile design system for rolling bearings based on artificial intelligence (Al), Internet and Web technologies and expertise. The underlying philosophy of the approach is to use Al technology and Web-based design support systems as smart tools from which design customers can rapidly and responsively access the systems’ built-in design expertise. The approach is described in detail with a novel Al model and system implementation issues. The major issues in implementing the approach are discussed with particular reference to using Al technologies, network programming, client-server technology and open computing of bearing design and manufacturing requirements,","['Internet-based intelligent design support system', 'rolling element bearings', 'intelligent agile design system', 'artificial intelligence', 'Web technologies', 'Internet technologies', 'smart tools', 'network programming', 'client-server technology', 'manufacturing requirements', 'bearing design', 'artificial intelligence', 'CAD/CAM', 'knowledge based systems', 'rolling', 'software agents']","['intelligent', 'systems', 'design', 'system implementation issues', 'built-in design expertise', 'artificial intelligence', 'design customers', 'novel approach', 'novel Al model', 'Al technology']",711,117,16,712,116,10,5,6,6
"design methodology for diagnostic strategies for industrial systems this paper presents a method for the construction of diagnostic systems for complex industrial applications. the approach has been explicitely developed to shorten the design cycle and meet some specific requirements, such as modularity, flexibility, and the possibility of merging many different sources of information. the method allows one to consider multiple simultaneous failures and is specifically designed to make easier the coordination and simplification of local diagnostic algorithms developed by different teams ","Design methodology for diagnostic strategies for industrial systems This paper presents a method for the construction of diagnostic systems for ‘complex industrial applications. The approach has been explicitely developed to shorten the design cycle and meet some specific requirements, such as modularity, flexibility, and the possibility of merging many different sources of information. The method allows one to consider multiple simultaneous failures and is specifically designed to make easier the coordination and simplification of local diagnostic algorithms developed by different teams","['design methodology', 'modularity', 'local diagnostic algorithms', 'diagnostic strategies', 'industrial systems', 'industrial control', 'software maintenance', 'systems analysis']","['complex industrial applications', 'local diagnostic algorithms', 'many different sources', 'diagnostic strategies', 'industrial systems', 'diagnostic systems', 'Design methodology', 'diagnostic', 'systems', 'industrial']",513,82,8,514,81,10,7,1,1
"global comparison of stages of growth based on critical success factors with increasing globalization of business, the management of it in international organizations is faced with the complex task of dealing with the difference between local and international it needs. this study evaluates, and compares, the level of it maturity and the critical success factors (csfs) in selected geographic regions, namely, norway, australia/new zealand, north america, europe, asia/pacific, and india. the results show that significant differences in the it management needs in these geographic regions exist, and that the it management operating in these regions must balance the multiple critical success factors for achieving an optimal local-global mix for business success ","Global comparison of stages of growth based on critical success factors With increasing globalization of business, the management of IT in international organizations is faced with the complex task of dealing with the difference between local and international IT needs. This study evaluates, and compares, the level of IT maturity and the critical success factors (CSF) in selected geographic regions, namely, Norway, Australia/New Zealand, North America, Europe, Asia/Pacific, and India. The results show that significant differences in the IT management needs in these geographic regions exist, and that the IT management operating in these regions must balance the multiple critical success factors for achieving an optimal local-global mix for business success","['business globalization', 'IT management', 'international IT needs', 'local IT needs', 'IT maturity', 'critical success factors', 'Norway', 'Australia', 'New Zealand', 'North America', 'Europe', 'Asia/Pacific', 'India', 'optimal local-global mix', 'business success', 'DP management', 'international trade', 'management of change']","['critical success factors', 'geographic regions', 'IT management operating', 'Global comparison', 'business success', 'success', 'Global', 'IT management', 'factors', 'critical']",656,112,18,655,111,10,1,1,8
"exploring developments in web based relationship marketing within the hotel industry this paper provides a content analysis study of the application of world wide web marketing by the hotel industry. there is a lack of historical perspective on industry related web marketing applications and this paper attempts to resolve this with a two-year follow-up case study of the changing use of the web to develop different types of relationships. specifically, the aims are: (1) to identify key changes in the way hotels are using the web; (2) to look for evidence of the adoption of a relationship marketing (rm) model as a strategy for the development of hotel web sites and the use of new technologies; and, (3) to investigate the use of multimedia in hotel web sites. the development and strategic exploitation of the internet has transformed the basis of marketing. using the evidence from a web content survey this study reveals the way relationships are being created and managed within the hotel industry by its use of the web as a marketing tool. the authors have collected evidence by means of a descriptive study on the way hotels build and create relationships with their web presence delivering multimedia information as well as channel and interactive means of communication. in addition a strategic framework is offered as the means to describe the mechanism and orientation of web based marketing by hotels. the study utilizes a model by gilbert (1996) as a means of developing a measurement instrument to allow a content analysis of the current approach by hotels to the development of web sites. the results indicate hotels are aware of the new uses of web technology and are promoting hotel products in the global electronic market in new and sophisticated ways ","Exploring developments in Web based relationship marketing within the hotel industry This paper provides a content analysis study of the application of World Wide Web marketing by the hotel industry. There is a lack of historical perspective on industry related Web marketing applications and this, paper attempts to resolve this with a two-year follow-up case study of the changing use of the Web to develop different types of relationships. Specifically, the aims are: (1) to identify key changes in the way hotels are using the Web; (2) to look for evidence of the adoption of a relationship marketing (RM) model as a strategy for the development of hotel Web sites and the use of new technologies; and, (3) To investigate the use of multimedia in hotel Web sites. The development and strategic exploitation of the Internet has transformed the basis of marketing. Using the evidence from a Web content survey this study reveals the way relationships are being created and managed within the hotel industry by its use of the Web as a marketing tool The authors have collected evidence by means of a descriptive study on the way hotels build and create relationships with their Web presence delivering multimedia information as well as channel and interactive means of communication. In addition a strategic framework is offered as the means to describe the mechanism and orientation of Web based marketing by hotels. The study utilizes a model by Gilbert (1996) as a means of developing a measurement instrument to allow a content analysis of the current approach by hotels to the development of Web sites. The results indicate hotels are aware of the new uses of Web technology and are promoting hotel products in the global electronic market in new and sophisticated ways","['Web based relationship marketing', 'hotel industry', 'World Wide Web marketing', 'hotel Web sites', 'multimedia', 'Web content survey', 'global electronic market', 'hotel industry', 'information resources', 'Internet', 'marketing data processing', 'multimedia computing']","['Web', 'hotel industry', 'relationship marketing', 'hotel Web sites', 'way hotels', 'Web content survey', 'hotel products', 'Web technology', 'Web presence', 'Web sites']",1483,294,12,1483,293,10,0,2,3
"solution of the reconstruction problem of a source function in the coagulation-fragmentation equation we study the problem of reconstructing a source function in the kinetic coagulation-fragmentation equation. the study is based on optimal control methods, the solvability theory of operator equations, and the use of iteration algorithms ","Solution of the reconstruction problem of a source function in the coagulation-fragmentation equation We study the problem of reconstructing a source function in the kinetic coagulation-fragmentation equation. The study is based on optimal control methods, the solvability theory of operator equations, and the use of iteration algorithms","['source function reconstruction', 'kinetic coagulation-fragmentation equation', 'optimal control methods', 'solvability', 'operator equations', 'iteration algorithms', 'coagulation', 'iterative methods', 'physics computing']","['source function', 'kinetic coagulation-fragmentation equation', 'reconstruction problem', 'operator equations', 'coagulation-fragmentation', 'problem', 'equations', 'coagulation-fragmentation equation', 'source', 'function']",292,48,9,292,47,10,0,0,4
"homogenization in l/sup infinity / homogenization of deterministic control problems with l/sup infinity / running cost is studied by viscosity solutions techniques. it is proved that the value function of an l/sup infinity / problem in a medium with a periodic micro-structure converges uniformly on the compact sets to the value function of the homogenized problem as the period shrinks to 0. our main convergence result extends that of ishii (stochastic analysis, control, optimization and applications, pp. 305-324, birkhauser boston, boston, ma, 1999.) to the case of a discontinuous hamiltonian. the cell problem is solved, but, as nonuniqueness occurs, the effective hamiltonian must be selected in a careful way. the paper also provides a representation formula for the effective hamiltonian and gives illustrations to calculus of variations, averaging and one-dimensional problems ","Homogenization in L/sup infinity / Homogenization of deterministic control problems with L/sup infinity / running cost is studied by viscosity solutions techniques. It is proved that the value function of an L/sup infinity / problem in a medium with a periodic micro-structure converges uniformly on the compact sets to the value function of the homogenized problem as the period shrinks to 0. Our main convergence result extends that of Ishii (Stochastic Analysis, control, optimization and applications, pp. 305-324, Birkhauser Boston, Boston, MA, 1999.) to the case of a discontinuous Hamiltonian. The cell problem is solved, but, as nonuniqueness occurs, the effective Hamiltonian must be selected in a careful way. The paper also provides a representation formula for the effective Hamiltonian and gives illustrations to calculus of variations, averaging and one-dimensional problems","['deterministic control', 'L/sup infinity / running cost', 'homogenization', 'value function', 'averaging', 'calculus of variations', 'convergence', 'optimal control', 'cell problem', 'convergence', 'optimal control', 'variational techniques']","['sup infinity', 'value function', 'Homogenization', 'deterministic control problems', 'one-dimensional problems', 'homogenised problem', 'cell problem', 'problems', 'sup', 'infinity']",758,132,12,758,131,10,0,0,4
"p-bezier curves, spirals, and sectrix curves we elucidate the connection between bezier curves in polar coordinates, also called p-bezier or focal bezier curves, and certain families of spirals and sectrix curves. p-bezier curves are the analogue in polar coordinates of nonparametric bezier curves in cartesian coordinates. such curves form a subset of rational bezier curves characterized by control points on radial directions regularly spaced with respect to the polar angle, and weights equal to the inverse of the polar radius. we show that this subset encompasses several classical sectrix curves, which solve geometrically the problem of dividing an angle into equal spans, and also spirals defining the trajectories of particles in central fields. first, we identify as p-bezier curves a family of sinusoidal spirals that includes tschirnhausen's cubic. second, the trisectrix of maclaurin and their generalizations, called arachnidas. finally, a special class of epi spirals that encompasses the trisectrix of delanges ","p-Bezier curves, spirals, and sectrix curves We elucidate the connection between Bezier curves in polar coordinates, also called p-Bezier or focal Bezier curves, and certain families of spirals and sectrix curves. p-Bezier curves are the analogue in polar coordinates of nonparametric Bezier curves in Cartesian coordinates. Such curves form a subset of rational Bezier curves characterized by control points on radial directions regularly spaced with respect to the polar angle, and weights equal to the inverse of the polar radius. ‘We show that this subset encompasses several classical sectrix curves, which solve geometrically the problem of dividing an angle into equal spans, and also spirals defining the trajectories of particles in central fields. First, we identify as p-Bezier curves a family of sinusoidal spirals that includes Tschirnhausen's cubic. Second, the trisectrix of Maciaurin and their generalizations, called arachnidas. Finally, a special class of epi spirals that encompasses the trisectrix of Delanges","['p-Bezier curves', 'spirals', 'sectrix curves', 'polar coordinates', 'focal Bezier curves', 'rational Bezier curves', 'control points', 'radial directions', 'polar angle', 'geometry', 'angle division', 'equal spans', 'particle trajectories', 'central fields', 'sinusoidal spirals', 'cubic', 'arachnidas', 'epi spirals', 'trisectrix', 'computational geometry']","['curves', 'bezier curves', 'Bezier curves', 'beatrix curves', 'bezier', 'non-parametric Bezier curves', 'rational Bezier curves', 'bezier curves spirals', 'focal Bezier curves', 'Such curves']",878,152,20,879,151,10,3,2,9
"simulation and transient testing of numerical relays a hybrid and practical solution for relay evaluation is presented. two main issues are taken into account: power system simulation and relay simulation, both of which consist of different stages. system simulation is carried out by means of emtp and is complemented by additional features, such as filtering for location and determination of fault parameters that allow comparing simulated and actual fault records to improve and guarantee a correct system simulation. relay simulation includes filtering algorithms, all the relaying units, and the decision logic. playing simulated or real faults over the actual relay and comparing simulated and real responses can check for correct relay simulation ","Simulation and transient testing of numerical relays A hybrid and practical solution for relay evaluation is presented. Two main issues are taken into account: power system simulation and relay simulation, both of which consist of different stages. System simulation is carried out by means of EMTP and is complemented by additional features, such as fitering for location and determination of fault parameters that allow comparing simulated and actual fault records to improve and guarantee a correct system simulation. Relay simulation includes filtering algorithms, all the relaying units, and the decision logic. Playing simulated or real faults over the actual relay and comparing simulated and real responses can check for correct relay simulation","['relay evaluation', 'power system simulation', 'relay simulation', 'EMTP', 'numerical relays', 'filtering', 'fault location', 'filtering algorithms', 'decision logic', 'real faults', 'simulated faults', 'relay testing', 'digital simulation', 'EMTP', 'fault location', 'power system protection', 'power system simulation', 'relay protection', 'testing']","['relay', 'correct relay simulation', 'actual fault records', 'transient testing', 'relay evaluation', 'numerical relays', 'actual relay', 'simulation', 'Simulation', 'relay simulation']",643,113,19,642,112,10,6,1,6
single-phase shunt active power filter with harmonic detection an advanced active power filter for the compensation of instantaneous harmonic current components in nonlinear current loads is presented. a signal processing technique using an adaptive neural network algorithm is applied for the detection of harmonic components generated by nonlinear current loads and it can efficiently determine the instantaneous harmonic components in real time. the validity of this active filtering processing system to compensate current harmonics is substantiated by simulation results ,Single-phase shunt active power fiter with harmonic detection An advanced active power filter for the compensation of instantaneous harmonic current components in nonlinear current loads is presented. A signal processing technique using an adaptive neural network algorithm is applied for the detection of harmonic components generated by nonlinear current loads and it can efficiently determine the instantaneous harmonic components in real time. The validity of this active filtering processing system to compensate current harmonics is substantiated by simulation results,"['single-phase shunt active power filter', 'harmonic detection', 'instantaneous harmonic current components compensation', 'nonlinear current loads', 'signal processing technique', 'adaptive neural network algorithm', 'instantaneous harmonic components', 'simulation', 'active filters', 'compensation', 'control system analysis', 'control system synthesis', 'harmonic distortion', 'load (electric)', 'neurocontrollers', 'power harmonic filters', 'power system control', 'power system harmonics']","['nonlinear current loads', 'active power filter', 'instantaneous harmonic components', 'harmonic detection', 'Single-phase shunt', 'current harmonics', 'harmonics', 'harmonic components', 'power', 'active']",497,80,18,496,79,10,3,1,4
"coarse-grained reduction and analysis of a network model of cortical response: i. drifting grating stimuli we present a reduction of a large-scale network model of visual cortex developed by mclaughlin, shapley, shelley, and wielaard. the reduction is from many integrate-and-fire neurons to a spatially coarse-grained system for firing rates of neuronal subpopulations. it accounts explicitly for spatially varying architecture, ordered cortical maps (such as orientation preference) that vary regularly across the cortical layer, and disordered cortical maps (such as spatial phase preference or stochastic input conductances) that may vary widely from cortical neuron to cortical neuron. the result of the reduction is a set of nonlinear spatiotemporal integral equations for ""phase-averaged"" firing rates of neuronal subpopulations across the model cortex, derived asymptotically from the full model without the addition of any extra phenomological constants. this reduced system is used to study the response of the model to drifting grating stimuli - where it is shown to be useful for numerical investigations that reproduce, at far less computational cost, the salient features of the point-neuron network and for analytical investigations that unveil cortical mechanisms behind the responses observed in the simulations of the large-scale computational model. for example, the reduced equations clearly show (1) phase averaging as the source of the time-invariance of cortico-cortical conductances, (2) the mechanisms in the model for higher firing rates and better orientation selectivity of simple cells which are near pinwheel centers, (3) the effects of the length-scales of cortico-cortical coupling, and (4) the role of noise in improving the contrast invariance of orientation selectivity ","Coarse-grained reduction and analysis of a network model of cortical response |. Drifting grating stimuli We present a reduction of a large-scale network model of visual cortex developed by McLaughlin, Shapley, Shelley, and Wielaard. The reduction is from many integrate-and-fire neurons to a spatially coarse-grained system for firing rates of neuronal subpopulations. It accounts explicitly for spatially varying architecture, ordered cortical maps (such as orientation preference) that vary regularly across the cortical layer, and disordered cortical maps (Such as spatial phase preference or stochastic input conductances) that may vary widely from cortical neuron to cortical neuron. The result of the reduction is a set of nonlinear spatiotemporal integral equations for ""phase-averaged” firing rates of neuronal subpopulations across the model cortex, derived asymptotically from the full model without the addition of any extra phenomological constants. This reduced system is used to study the response of the model to drifting grating stimuli - where it is, shown to be useful for numerical investigations that reproduce, at far less computational cost, the salient features of the point-neuron network and for analytical investigations that unveil cortical mechanisms behind the responses observed in the simulations of the large-scale computational model. For example, the reduced equations clearly show (1) phase averaging as the source of the time-invariance of cortico-cortical conductances, (2) the mechanisms in the model for higher firing rates and better orientation selectivity of simple cells which are near pinwheel centers, (3) the effects of the length-scales of cortico-cortical coupling, and (4) the role of noise in improving the contrast invariance of orientation selectivity","['large-scale network model', 'visual cortex', 'neuronal networks', 'coarse-graining', 'point-neuron network', 'phase-averaged firing rates', 'nonlinear spatiotemporal integral equations', 'dynamics', 'orientation selectivity', 'brain models', 'neural nets']","['cortical maps', 'large-scale computational model', 'large-scale network model', 'Coarse-grained reduction', 'cortical response p.', 'cortical mechanisms', 'cortical neurons', 'model cortex', 'full model', 'network model']",1546,260,11,1546,259,10,2,4,2
"p systems with symport/antiport rules: the traces of objects we continue the study of those p systems where the computation is performed by the communication of objects, that is, systems with symport and antiport rules. instead of the (number of) objects collected in a specified membrane, as the result of a computation we consider the itineraries of a certain object through membranes, during a halting computation, written as a coding of the string of labels of the visited membranes. the family of languages generated in this way is investigated with respect to its place in the chomsky hierarchy. when the (symport and antiport) rules are applied in a conditional manner, promoted or inhibited by certain objects which should be present in the membrane where a rule is applied, then a characterization of recursively enumerable languages is obtained; the power of systems with the rules applied freely is only partially described ","P systems with symportantiport rules: the traces of objects ‘We continue the study of those P systems where the computation is performed by the communication of objects, that is, systems with symport and antiport rules. Instead of the (number of) objects collected in a specified membrane, as the result of a computation we consider the itineraries of a certain object through membranes, during a halting computation, written as a coding of the string of labels of the visited membranes. The family of languages generated in this way is, investigated with respect to its place in the Chomsky hierarchy. When the (symport and antiport) rules are applied in a conditional manner, promoted or inhibited by certain objects which should be present in the membrane where a rule is applied, then a characterization of recursively enumerable languages is obtained; the power of systems with the rules applied freely is only partially described","['P systems', 'object communication', 'object traces', 'antiport rules', 'symport rules', 'itineraries', 'halting computation', 'label string coding', 'languages', 'Chomsky hierarchy', 'recursively enumerable languages', 'computational linguistics', 'grammars']","['membranes', 'P systems', 'symportantiport rules', 'specified membrane', 'certain objects', 'P', 'rules', 'objects', 'antiport rules', 'systems']",785,151,13,786,150,10,10,3,5
"compatibility of systems of linear constraints over the set of natural numbers criteria of compatibility of a system of linear diophantine equations, strict inequations, and nonstrict inequations are considered. upper bounds for components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. these criteria and the corresponding algorithms for constructing a minimal supporting set of solutions can be used in solving all the considered types of systems and systems of mixed types ","Compatibility of systems of linear constraints over the set of natural numbers Criteria of compatibility of a system of linear Diophantine equations, strict inequations, and nonstrict inequations are considered. Upper bounds for ‘components of a minimal set of solutions and algorithms of construction of minimal generating sets of solutions for all types of systems are given. These criteria and the corresponding algorithms for constructing ‘a minimal supporting set of solutions can be used in solving all the, considered types of systems and systems of mixed types","['linear constraints', 'set of natural numbers', 'linear Diophantine equations', 'strict inequations', 'nonstrict inequations', 'upper bounds', 'minimal generating sets', 'linear programming']","['systems', 'solutions', 'linear Diophantine equations', 'natural numbers Criteria', 'minimal supporting set', 'linear constraints', 'mixed types', 'minimal set', 'set', 'linear']",480,87,8,483,86,10,11,3,3
"a dynamic method for weighted linear least squares problems a new method for solving the weighted linear least squares problems with full rank is proposed. based on the theory of liapunov's stability, the method associates a dynamic system with a weighted linear least squares problem, whose solution we are interested in and integrates the former numerically by an a-stable numerical method. the numerical tests suggest that the new method is more than comparative with current conventional techniques based on the normal equations ","‘A dynamic method for weighted linear least squares problems ‘Anew method for solving the weighted linear least squares problems with full rank is proposed. Based on the theory of Liapunov's stability, the method associates a dynamic system with a weighted linear least squares problem, whose solution we are interested in and integrates the former numerically by an A-stable numerical method. The numerical tests suggest that the new method is more than comparative with current conventional techniques based on the normal equations","['dynamic method', 'weighted linear least squares problems', 'Lyapunov stability', 'A-stable numerical method', 'eigenvalues and eigenfunctions', 'least squares approximations', 'Lyapunov methods']","['weighted', 'squares', 'linear', 'least', 'new method', 'problems', 'method', 'astable numerical method', 'dynamic system', 'dynamic method']",451,83,7,453,81,10,247,73,0
"recruiting and retaining women in undergraduate computing majors this paper recommends methods for increasing female participation in undergraduate computer science. the recommendations are based on recent and on-going research into the gender gap in computer science and related disciplines. they are intended to work in tandem with the computing research association's recommendations for graduate programs to promote a general increase in women's participation in computing professions. most of the suggestions offered could improve the educational environment for both male and female students. however, general improvements are likely to be of particular benefit to women because women in our society do not generally receive the same level of support that men receive for entering and persisting in this field ","Recruiting and retaining women in undergraduate computing majors This paper recommends methods for increasing female participation in undergraduate computer science. The recommendations are based on recent and on-going research into the gender gap in computer science and related disciplines. They are intended to work in tandem with the Computing Research Association's recommendations for graduate programs to promote a general increase in women's participation in computing professions. Most of the suggestions offered could improve the educational environment for both male and female students. However, general improvements are likely to be of particular benefit to women because women in our society do not generally receive the same level of support that men receive for entering and persisting in this field","['undergraduate computing majors', 'women retention', 'women recruitment', 'female participation', 'gender gap', 'computer science', 'computer science education', 'gender issues']","['undergraduate computing majors', 'undergraduate computer science', 'womens participation', 'female participation', 'general increase', 'female students', 'undergraduate', 'computer', 'women', 'computer science']",698,119,8,698,118,10,0,0,1
"verifying concurrent systems with symbolic execution current techniques for interactively proving temporal properties of concurrent systems translate transition systems into temporal formulas by introducing program counter variables. proofs are not intuitive, because control flow is not explicitly considered. for sequential programs symbolic execution is a very intuitive, interactive proof strategy. in this paper we adopt this technique for parallel programs. properties are formulated in interval temporal logic. an implementation in the interactive theorem prover kiv has shown that this technique offers a high degree of automation and allows simple, local invariants ","Verifying concurrent systems with symbolic execution Current techniques for interactively proving temporal properties of concurrent systems translate transition systems into temporal formulas by introducing program counter variables. Proofs are not intuitive, because control flow is not explicitly considered. For sequential programs symbolic execution is a very intuitive, interactive proof strategy. In this paper we adopt this technique for parallel programs. Properties are formulated in interval temporal logic. An implementation in the interactive theorem prover KIV has shown that this technique offers a high degree of automation and allows simple, local invariants","['concurrent systems verification', 'symbolic execution', 'temporal properties', 'concurrent systems', 'transition systems', 'temporal formulas', 'program counter variables', 'sequential programs', 'parallel programs', 'interactive theorem prover KIV', 'local invariants', 'parallel programming', 'program verification', 'temporal logic', 'theorem proving']","['symbolic execution', 'concurrent systems', 'parallel programs Properties', 'interval temporal logic', 'temporal properties', 'sequential programs', 'transition systems', 'Current techniques', 'temporal formulas', 'systems']",584,92,15,584,91,10,0,0,3
"bad pixel identification by means of principal components analysis bad pixels are defined as those pixels showing a temporal evolution of the signal different from the rest of the pixels of a given array. principal component analysis helps us to understand the definition of a statistical distance associated with each pixels, and using this distance it is possible to identify those pixels labeled as bad pixels. the spatiality of a pixel is also calculated. an assumption about the normality of the distribution of the distances of the pixels is revised. although the influence on the robustness of the identification algorithm is negligible, the definition of a parameter related with this nonnormality helps to identify those principal components and eigenimages responsible for the departure from a multinormal distribution. the method for identifying the bad pixels is successfully applied to a set of frames obtained from a ccd visible and a focal plane array (fpa) ir camera ","Bad pixel identification by means of principal components analysis Bad pixels are defined as those pixels showing a temporal evolution of the, signal different from the rest of the pixels of a given array. Principal component analysis helps us to understand the definition of a statistical distance associated with each pixels, and using this, distance it is possible to identify those pixels labeled as bad pixels. The spatiality of a pixel is also calculated. An assumption about the normality of the distribution of the distances of the pixels is revised. Although the influence on the robustness of the identification algorithm is negligible, the definition of a parameter related with this nonnormality helps to identify those principal components and eigenimages responsible for the departure from a multinormal distribution. The method for identifying the bad pixels is successfully applied to a set of frames obtained from a CCD visible and a focal plane array (FPA) IR camera","['bad pixel identification', 'principal components analysis', 'temporal evolution', 'statistical distance', 'robustness', 'identification algorithm', 'eigenimages', 'multinormal distribution', 'CCD visible camera', 'focal plane array', 'IR camera', 'CCD image sensors', 'focal planes', 'image recognition', 'principal component analysis']","['principal components analysis', 'identification algorithm', 'Bad pixel identification', 'statistical distance', 'pixels', 'Bad', 'component', 'principal components', 'bad pixels', 'Bad pixels']",828,156,15,830,155,10,0,2,6
"an interactive self-replicator implemented in hardware self-replicating loops presented to date are essentially worlds unto themselves, inaccessible to the observer once the replication process is launched. we present the design of an interactive self-replicating loop of arbitrary size, wherein the user can physically control the loop's replication and induce its destruction. after introducing the biowall, a reconfigurable electronic wall for bio-inspired applications, we describe the design of our novel loop and delineate its hardware implementation in the wall ","An interactive selt-replicator implemented in hardware Self-replicating loops presented to date are essentially worlds unto themselves, inaccessible to the observer once the replication process is launched. We present the design of an interactive self-replicating loop of arbitrary size, wherein the user can physically control the loop's replication and induce its destruction. After introducing the BioWall, a reconfigurable electronic wall for bio-inspired applications, we describe the design of our novel loop and delineate its hardware implementation in the wall","['interactive self-replicator', 'interactive self-replicating loop', 'BioWall', 'reconfigurable electronic wall', 'bio-inspired applications', 'hardware implementation', 'self-replication', 'field programmable gate array', 'cellular automata', 'reconfigurable computing', 'artificial life', 'artificial life', 'cellular automata', 'field programmable gate arrays', 'reconfigurable architectures', 'self-reproducing automata']","['interactive self-replicating loop', 'configurable electronic wall', 'interactive selt-replicator', 'hardware implementation', 'Self-replicating loops', 'replication process', 'loops replication', 'novel loop', 'loops', 'interactive']",491,79,16,491,78,10,1,1,2
controls help harmonic spray do ok removing residues looks at how innovative wafer-cleaning equipment hit the market in a timely fashion thanks in part to controls maker rockwell automation ,Controls help harmonic spray do OK removing residues Looks at how innovative wafer-cleaning equipment hit the market in a timely fashion thanks in part to controls maker Rockwell Automation,"['harmonic spray', 'residues removal', 'wafer-cleaning equipment', 'Rockwell Automation', 'PSI machine', 'Allen-Bradley ControlLogix automation control platform', 'motion control', 'Allen-Bradley 1336 Plus II variable frequency ac drives', 'control systems', 'motion control', 'process control', 'semiconductor device manufacture', 'surface cleaning']","['innovative wafer-cleaning equipment', 'timely fashion thanks', 'harmonic spray', 'Controls help', 'residues', 'OK', 'help', 'spray', 'harmonic', 'Controls']",161,30,13,161,29,10,0,0,1
"a conflict between language and atomistic information fred dretske and jerry fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind: the thesis of information-based semantics (ibs), the thesis of content atomism (atomism) and the thesis of the language of thought (lot). lot concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires. it maintains that all such representations must have syntactic structures mirroring the structure of their contents. ibs is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents (semantic relations). it holds that these relations supervene solely on relations of the kind that support information content, perhaps with some help from logical principles of combination. atomism is a thesis about the nature of the content of simple symbols. it holds that each substantive simple symbol possesses its content independently of all other symbols in the representational system. i argue that dretske's and fodor's theories are false and that their falsehood results from a conflict ibs and atomism, on the one hand, and lot, on the other ","A conflict between language and atomistic information Fred Dretske and Jerry Fodor are responsible for popularizing three well-known theses in contemporary philosophy of mind: the thesis of Information-Based Semantics (IBS), the thesis of Content Atomism (Atomism) and the thesis of the Language of Thought (LOT). LOT concerns the semantically relevant structure of representations involved in cognitive states such as beliefs and desires. It maintains that all such representations must have syntactic structures mirroring the structure of their contents. IBS is a thesis about the nature of the relations that connect cognitive representations and their parts to their contents (semantic relations). It holds that these relations supervene solely on relations of the kind that support information content, perhaps with some help from logical principles of combination. Atomism is a thesis about the nature of the content of simple symbols. Itholds that each substantive simple symbol possesses its content independently of all other symbols in the representational system. | argue that Dretske's and Fodor's theories are false and that their falsehood results from a conflict IBS and Atomism, on the one hand, and LOT, on the other","['philosophy of mind', 'Information-Based Semantics', 'Content Atomism', 'IBS', 'Language of Thought', 'LOT', 'cognitive states', 'beliefs', 'desires', 'artificial intelligence', 'cognitive systems', 'philosophical aspects']","['thesis', 'atomistic information Fred', 'cognitive representations', 'Content Atomism atomism', 'information content', 'contents IBS', 'conflict IBS', 'Jerry Fodor', 'information', 'conflict']",1049,187,12,1049,185,10,144,47,6
necessary conditions of optimality for impulsive systems on banach spaces we present necessary conditions of optimality for optimal control problems arising in systems governed by impulsive evolution equations on banach spaces. basic notations and terminologies are first presented and necessary conditions of optimality are presented. special cases are discussed and we present an application to the classical linear quadratic regulator problem ,Necessary conditions of optimality for impulsive systems on Banach spaces We present necessary conditions of optimality for optimal control problems arising in systems governed by impulsive evolution equations on Banach spaces. Basic notations and terminologies are first presented and necessary conditions of optimality are presented. Special cases are discussed and we present an application to the classical linear quadratic regulator problem,"['linear quadratic regulator', 'optimality', 'impulsive systems', 'optimal control', 'impulsive evolution equations', 'Banach spaces', 'necessary conditions', 'Banach spaces', 'differential equations', 'optimal control']","['optimality', 'branch spaces', 'impulsive evolution equations', 'optimal control problems', 'impulsive systems', 'impulsive', 'systems', 'conditions', 'necessary conditions', 'Necessary conditions']",385,62,10,385,61,10,0,0,1
"optimal control using the transport equation: the liouville machine transport theory describes the scattering behavior of physical particles such as photons. here we show how to connect this theory to optimal control theory and to adaptive behavior of agents embedded in an environment. environments and tasks are defined by physical boundary conditions. given some task, we compute a set of probability densities on continuous state and action and time. from these densities we derive an optimal policy such that for all states the most likely action maximizes the probability of reaching a predefined goal state. liouville's conservation theorem tells us that the conditional density at time t, state s, and action a must equal the density at t + dt, s + ds, a + da. discretization yields a linear system that can be solved directly and whose solution corresponds to an optimal policy. discounted reward schemes are incorporated naturally by taking the laplace transform of the equations. the liouville machine quickly solves rather complex maze problems ","Optimal control using the transport equation: the Liouville machine Transport theory describes the scattering behavior of physical particles such as photons. Here we show how to connect this theory to optimal control theory and to adaptive behavior of agents embedded in an environment. Environments and tasks are defined by physical boundary conditions. Given some task, we compute a set of probability densities on continuous state and action and time. From these densities we derive an optimal policy such that for all states the most likely action maximizes the probability of reaching a predefined goal state. Liouville's conservation theorem tells us that the conditional density at time t, state s, and action a must equal the density at t + dt, s + ds, a + da. Discretization yields a linear system that can be solved directly and whose solution corresponds to an optimal policy. Discounted reward schemes are incorporated naturally by taking the Laplace transform of the equations. The Liouville machine quickly solves rather complex maze problems","['optimal control', 'transport equation', 'Liouville machine', 'scattering behavior', 'physical particles', 'adaptive behavior', 'embedded agents', 'adaptive systems', 'difference equations', 'Laplace transforms', 'learning (artificial intelligence)', 'optimal control', 'probability']","['couville machine', 'optimal policy', 'physical boundary conditions', 'optimal control theory', 'scattering behavior', 'transport equation', 'equations', 'control', 'theory', 'Optimal control']",890,168,13,890,167,10,0,0,1
"a new approach to the problem of structural identification. ii the subject under discussion is a new approach to the problem of structural identification, which relies on the recognition of a decisive role of the human factor in the process of structural identification. potential possibilities of the suggested approach are illustrated by the statement of a new mathematical problem of structural identification ","‘Anew approach to the problem of structural identification. 11 The subject under discussion is a new approach to the problem of structural identification, which relies on the recognition of a decisive role of the human factor in the process of structural identification. Potential possibilities of the suggested approach are illustrated by the statement of a new mathematical problem of structural identification","['structural identification', 'human factor', 'mathematical equations', 'decision-maker', 'functional equations', 'identification', 'set theory']","['structural identification', 'new approach', 'new mathematical problem', 'Potential possibilities', 'decisive role', 'new', 'problem', 'approach', 'structural', 'identification']",351,63,7,352,61,10,196,61,1
"information architecture in jasist: just where did we come from? the emergence of information architecture within the information systems world has been simultaneously drawn out yet rapid. those with an eye on history are quick to point to wurman's 1976 use of the term ""architecture of information,"" but it has only been in the last 2 years that ia has become the source of sufficient interest for people to label themselves professionally as information architects. the impetus for this recent emergence of ia can be traced to a historical summit, supported by asis&t in may 2000 at boston. it was here that several hundred of us gathered to thrash out the questions of just what ia was and what this new field might become. at the time of the summit, invited to present a short talk on my return journey from the annual acm sigchi conference, i entered the summit expecting little and convinced that ia was nothing new. i left 2 days later refreshed, not just by the enthusiasm of the attendees for this term but by ia's potential to unify the disparate perspectives and orientations of professionals from a range of disciplines. it was at this summit that the idea for the special issue took root. i proposed the idea to don kraft, hoping he would find someone else to run with it. as luck would have it, i ended up taking charge of it myself, with initial support from david blair. from the suggestion to the finished product-has been the best part of 2 years, and in that time more than 50 volunteers reviewed over 20 submissions ","Information architecture in JASIST: just where did we come from? The emergence of Information Architecture within the information systems world has been simultaneously drawn out yet rapid. Those with an eye on history are quick to point to Wurman's 1976 use of the term “architecture of information,"" but it has only been in the last 2 years that IA has become the source of sufficient interest for people to label themselves professionally as Information Architects. The impetus for this recent emergence of IA can be traced to a historical summit, supported by ASIS&T in May 2000 at Boston. It was here that several hundred of us gathered to thrash out the questions of just what IA was and what this new field might become. At the time of the summit, invited to present a short talk on my return journey from the annual ACM SIGCHI conference, | entered the summit expecting little and convinced that IA was nothing new. | left 2 days later refreshed, not just by the enthusiasm of the attendees for this term but by IA'S Potential to unify the disparate perspectives and orientations of professionals from a range of disciplines. It was at this summit that the idea for the special issue took root. | proposed the idea to Don Kraft, hoping he would find someone else to run with it. AS luck would have it, | ended up taking charge of it myself, with initial support from David Blair. From the suggestion to the finished product-has been the best part of 2 years, and in that time more than 50 volunteers reviewed over 20 submissions","['information architecture', 'information systems', 'metadata fields', 'controlled vocabularies', 'Web sites', 'CD-ROM', 'qualified information architect', 'certification', 'hypermedia', 'information resources']","['IA', 'Information', 'information systems world', 'Information architects', 'term architecture', 'recent emergence', 'architecture', 'emergence', 'Information architecture', 'Information Architecture']",1266,271,10,1266,270,10,5,5,0
"voltage control methods with grid connected wind turbines: a tutorial review within electricity grid networks it is conventional for large-scale central generators to both provide power and control grid node voltage. therefore when wind turbines replace conventional power stations on a substantial scale, they must not only generate power, but also control grid node voltages. this paper reviews the basic principles of voltage control for tutorial benefit and then considers application of grid-connected wind turbines for voltage control. the most widely used contemporary wind turbine types are considered and further detail is given for determining the range of variables that allow control ","Voltage control methods with grid connected wind turbines: a tutorial review Within electricity grid networks it is conventional for large-scale central generators to both provide power and control grid node voltage. Therefore when wind turbines replace conventional power stations on a substantial scale, they must not only generate power, but also control grid node voltages. This paper reviews the basic principles of voltage control for tutorial benefit and then considers application of grid-connected wind turbines for voltage control. The most widely used contemporary wind turbine types are considered and further detail is given for determining the range of variables that allow control","['electricity grid networks', 'large-scale central generators', 'grid connected wind turbines', 'grid node voltages control', 'voltage control', 'reactive power', 'direct drive', 'variable speed', 'offshore wind park', 'squirrel cage induction generator', 'doubly fed induction generator', 'direct drive synchronous generator', 'weak grid', 'converter rating', 'asynchronous generators', 'power generation control', 'power system control', 'synchronous generators', 'voltage control', 'wind turbines']","['grid node', 'grid-connected wind turbines', 'conventional power stations', 'electricity grid networks', 'Voltage control methods', 'grid', 'control', 'wind turbines', 'voltage control', 'connected wind turbines']",594,103,20,594,102,10,0,0,1
"embedded linux and the law the rising popularity of linux, combined with perceived cost savings, has spurred many embedded developers to consider a real-time linux variant as an alternative to a traditional rtos. the paper presents the legal implications for the proprietary parts of firmware ","Embedded Linux and the law The rising popularity of Linux, combined with perceived cost savings, has spurred many embedded developers to consider a real-time Linux variant as an alternative to a traditional RTOS. The paper presents the legal implications for the proprietary parts of firmware","['embedded Linux', 'real-time Linux', 'legal implications', 'proprietary firmware', 'embedded systems', 'firmware', 'industrial property', 'operating systems (computers)', 'public domain software', 'real-time systems']","['many embedded developers', 'real-time linx variant', 'legal implications', 'cost savings', 'popularity', 'law', 'many', 'linx', 'cost', 'savings']",248,46,10,248,45,10,0,0,1
"quality image metrics for synthetic images based on perceptual color differences due to the improvement of image rendering processes, and the increasing importance of quantitative comparisons among synthetic color images, it is essential to define perceptually based metrics which enable to objectively assess the visual quality of digital simulations. in response to this need, this paper proposes a new methodology for the determination of an objective image quality metric, and gives an answer to this problem through three metrics. this methodology is based on the llab color space for perception of color in complex images, a modification of the cielab1976 color space. the first metric proposed is a pixel by pixel metric which introduces a local distance map between two images. the second metric associates, to a pair of images, a global value. finally, the third metric uses a recursive subdivision of the images to obtain an adaptative distance map, rougher but less expensive to compute than the first method ","Quality image metrics for synthetic images based on perceptual color differences Due to the improvement of image rendering processes, and the increasing importance of quantitative comparisons among synthetic color images, it is essential to define perceptually based metrics which enable to objectively assess the visual quality of digital simulations. in response to this need, this paper proposes a new methodology for the determination of an objective image quality metric, and gives an answer to this problem through three metrics. This methodology is based on the LLAB color space for perception of color in complex images, a modification of the CIELab1976 color space. The first metric proposed is a pixel by pixel metric which introduces a local distance map between two images. The second metric associates, to a pair of images, a global value. Finally, the third metric uses a recursive subdivision of the images to obtain an adaptative distance map, rougher but less expensive to compute than the first method","['quality image metrics', 'synthetic images', 'perceptual color differences', 'image rendering', 'color images', 'perceptually based metrics', 'visual quality', 'digital simulations', 'LLAB color space', 'CIELab1976 color space', 'pixel by pixel metric', 'local distance map', 'global value', 'recursive subdivision', 'adaptative distance map', 'image colour analysis', 'rendering (computer graphics)']","['metric', 'images', 'perceptual color differences', 'second metric associates', 'objective image quality', 'synthetic color images', 'Quality image metrics', 'third metric uses', 'synthetic images', 'complex images']",860,161,17,860,160,10,0,0,5
mammogram synthesis using a 3d simulation. i. breast tissue model and image acquisition simulation a method is proposed for generating synthetic mammograms based upon simulations of breast tissue and the mammographic imaging process. a computer breast model has been designed with a realistic distribution of large and medium scale tissue structures. parameters controlling the size and placement of simulated structures (adipose compartments and ducts) provide a method for consistently modeling images of the same simulated breast with modified position or acquisition parameters. the mammographic imaging process is simulated using a compression model and a model of the x-ray image acquisition process. the compression model estimates breast deformation using tissue elasticity parameters found in the literature and clinical force values. the synthetic mammograms were generated by a mammogram acquisition model using a monoenergetic parallel beam approximation applied to the synthetically compressed breast phantom ,Mammogram synthesis using a 3D simulation. |. Breast tissue model and image acquisition simulation ‘A method is proposed for generating synthetic mammograms based upon simulations of breast tissue and the mammographic imaging process. A computer breast model has been designed with a realistic distribution of large and medium scale tissue structures. Parameters controlling the size and placement of simulated structures (adipose compartments and ducts) provide a method for consistently modeling images of the same simulated breast with modified position or acquisition parameters. The mammographic imaging process is simulated using a compression model and a model of the X-ray image acquisition process. The compression model estimates breast deformation using tissue elasticity parameters found in the literature and clinical force values. The synthetic mammograms were generated by a mammogram acquisition model using a monoenergetic parallel beam approximation applied to the synthetically compressed breast phantom,"['mammogram synthesis', '3D simulation', 'breast tissue model', 'image acquisition simulation', 'mammographic compression', 'computer breast model', 'adipose compartments', 'ducts', 'X-ray image acquisition', 'tissue elasticity parameters', 'force values', 'monoenergetic parallel beam approximation', 'breast lesions', 'rectangular slice approximation', 'composite beam model', ""linear Young's moduli"", 'biological tissues', 'computer graphics', 'diagnostic radiography', 'image registration', 'mammography', 'medical image processing', 'physiological models', ""Young's modulus""]","['mammography imaging process', 'compression model', 'breast tissue', 'image acquisition simulation', 'mammogram acquisition model', 'same simulated breast', 'computer breast model', 'simulated structures', 'Mammogram synthesis', 'simulated']",880,143,24,881,142,10,2,2,4
"virus hunting we all appreciate the need for, and hopefully we have all deployed, anti-virus software. the good news is that av software has come a long way fast. four or so years ago it was true to write that av software could not detect trojan horses and similar intrusion attempts. now it can and does. mcafee's virusscan, for example, goes one further; it detects viruses, worms and trojan horses and deploys itself as a firewall to filter data packets, control access to internet resources, activate rule sets for specific applications, in general to protect against hackers. but like so much software, we use it with little thought as to how it came to do its job. behind the scenes there is an army of top notch programmers trying to stay ahead of the baddies who, at the last count, had produced some 60,000 viruses ","Virus hunting We all appreciate the need for, and hopefully we have all deployed, anti-virus software. The good news is that AV software has come a long way fast. Four or so years ago it was true to write that AV software could not detect Trojan Horses and similar intrusion attempts. Now it can and does. McAfee's VirusScan, for example, goes one further; it detects viruses, worms and Trojan Horses and deploys itself as a firewall to filter data packets, control access to Internet resources, activate Tule sets for specific applications, in general to protect against hackers. But like So much software, we use it with litte thought as to how it came to do its job. Behind the scenes there is an army of top notch programmers trying to stay ahead of the baddies who, at the last count, had produced some 60,000 viruses","['anti-virus software', 'programmers', 'worms', 'Trojan Horses', 'computer viruses']","['AV software', 'Virus', 'anti-virus software', 'software', 'much software', 'Virus hunting', 'good news', 'need', 'hunting', 'anti-virus']",679,146,5,678,145,10,2,2,3
"steinmetz system design under unbalanced conditions this paper studies and develops general analytical expressions to obtain three-phase current symmetrization under unbalanced voltage conditions. it proposes two procedures for this symmetrization: the application of the traditional expressions assuming symmetry conditions and the use of optimization methods based on the general analytical equations. specifically, the paper applies and evaluates these methods to analyze the steinmetz system design. several graphics evaluating the error introduced by assumption of balanced voltage in the design are plotted and an example is studied to compare both procedures. in the example the necessity to apply the optimization techniques in highly unbalanced conditions is demonstrated ","Steinmetz system design under unbalanced conditions This paper studies and develops general analytical expressions to obtain three-phase current symmetrization under unbalanced voltage conditions. It proposes two procedures for this symmetrization: the application of the traditional expressions assuming symmetry conditions and the use of optimization methods based on the general analytical equations. Specifically, the paper applies and evaluates these methods to analyze the Steinmetz system design. Several graphics evaluating the error introduced by assumption of balanced voltage in the design are plotted and an example is studied to compare both procedures. In the example the necessity to apply the optimization techniques in highly unbalanced conditions is demonstrated","['three-phase current symmetrization', 'unbalanced voltage conditions', 'Steinmetz system design', 'power system control design', 'optimization methods', 'general analytical equations', 'balanced voltage assumption', 'control system analysis', 'control system synthesis', 'electric current control', 'harmonic distortion', 'harmonics suppression', 'optimal control', 'power system control', 'power system harmonics']","['steinitz system design', 'unbalanced conditions', 'three-phase current symmetrization', 'general analytical expressions', 'unbalanced voltage conditions', 'general analytical equations', 'conditions', 'optimization methods', 'symmetry conditions', 'design']",675,107,15,675,106,10,0,0,0
"adaptive image denoising using scale and space consistency this paper proposes a new method for image denoising with edge preservation, based on image multiresolution decomposition by a redundant wavelet transform. in our approach, edges are implicitly located and preserved in the wavelet domain, whilst image noise is filtered out. at each resolution level, the image edges are estimated by gradient magnitudes (obtained from the wavelet coefficients), which are modeled probabilistically, and a shrinkage function is assembled based on the model obtained. joint use of space and scale consistency is applied for better preservation of edges. the shrinkage functions are combined to preserve edges that appear simultaneously at several resolutions, and geometric constraints are applied to preserve edges that are not isolated. the proposed technique produces a filtered version of the original image, where homogeneous regions appear separated by well-defined edges. possible applications include image presegmentation, and image denoising ","Adaptive image denoising using scale and space consistency This paper proposes a new method for image denoising with edge preservation, based on image multiresolution decomposition by a redundant wavelet transform. In our approach, edges are implicitly located and preserved in the wavelet domain, whilst image noise is filtered out. At each resolution level, the image edges are estimated by gradient magnitudes (obtained from the wavelet coefficients), which are modeled probabilistically, and a shrinkage function is assembled based on the model obtained. Joint use of space and scale consistency is applied for better preservation of edges. The shrinkage functions are combined to preserve edges that appear simultaneously at several resolutions, and geometric constraints are applied to preserve edges that are not isolated. The proposed technique produces a filtered version of the original image, where homogeneous regions appear separated by well-defined edges. Possible applications include image presegmentation, and image denoising","['adaptive image denoising', 'scale consistency', 'space consistency', 'edge preservation', 'image multiresolution decomposition', 'redundant wavelet transform', 'image edges', 'gradient magnitudes', 'shrinkage function', 'geometric constraints', 'edge enhancement', 'adaptive signal processing', 'edge detection', 'image enhancement', 'image resolution', 'image restoration', 'interference suppression', 'wavelet transforms']","['image', 'image presegmentation', 'well-defined edges', 'space consistency', 'edge preservation', 'original image', 'approach edges', 'Adaptive image', 'image noise', 'image edges']",895,149,18,895,148,10,0,0,4
"neural and neuro-fuzzy integration in a knowledge-based system for air quality prediction we propose a unified approach for integrating implicit and explicit knowledge in neurosymbolic systems as a combination of neural and neuro-fuzzy modules. in the developed hybrid system, a training data set is used for building neuro-fuzzy modules, and represents implicit domain knowledge. the explicit domain knowledge on the other hand is represented by fuzzy rules, which are directly mapped into equivalent neural structures. the aim of this approach is to improve the abilities of modular neural structures, which are based on incomplete learning data sets, since the knowledge acquired from human experts is taken into account for adapting the general neural architecture. three methods to combine the explicit and implicit knowledge modules are proposed. the techniques used to extract fuzzy rules from neural implicit knowledge modules are described. these techniques improve the structure and the behavior of the entire system. the proposed methodology has been applied in the field of air quality prediction with very encouraging results. these experiments show that the method is worth further investigation ","Neural and neuro-fuzzy integration in a knowledge-based system for air quality prediction We propose a unified approach for integrating implicit and explicit knowledge in neurosymbolic systems as a combination of neural and neuro-fuzzy modules. In the developed hybrid system, a training data set is used for building neuro-fuzzy modules, and represents implicit domain knowledge. The explicit domain knowledge on the other hand is represented by fuzzy rules, which are directly mapped into equivalent neural structures. The aim of this approach is to improve the abilities of modular neural structures, which are based on incomplete learning data sets, since the knowledge acquired from human experts is taken into account for adapting the general neural architecture. Three methods to combine the explicit and implicit knowledge modules are proposed. The techniques used to extract fuzzy rules from neural implicit knowledge modules are described. These techniques improve the structure and the behavior of the entire system. The proposed methodology has been applied in the field of air quality prediction with very encouraging results. These experiments show that the method is worth further investigation","['neuro-fuzzy integration', 'knowledge-based system', 'air quality prediction', 'neurosymbolic systems', 'hybrid system', 'training data set', 'implicit domain knowledge representation', 'fuzzy rules', 'incomplete learning', 'neural architecture', 'experiments', 'air pollution', 'air pollution', 'environmental science computing', 'fuzzy logic', 'fuzzy neural nets', 'geophysics computing', 'knowledge based systems', 'knowledge representation', 'learning (artificial intelligence)']","['implicit knowledge modules', 'air quality prediction', 'neuro-fuzzy modules', 'equivalent neural structures', 'modular neural structures', 'implicit domain knowledge', 'explicit domain knowledge', 'neuro-fuzzy integration', 'knowledge-based system', 'explicit knowledge']",1031,180,20,1031,179,10,0,0,5
"virtual-reality-based multidimensional therapy for the treatment of body image disturbances in binge eating disorders: a preliminary controlled study the main goal of this paper is to preliminarily evaluate the efficacy of a virtual-reality (vr)-based multidimensional approach in the treatment of body image attitudes and related constructs. the female binge eating disorder (bed) patients (n=20), involved in a residential weight control treatment including low-calorie diet (1200 cal/day) and physical training, were randomly assigned either to the multidimensional vr treatment or to psychonutritional groups based on the cognitive-behavior approach. patients were administered a battery of outcome measures assessing eating disorders symptomathology, attitudes toward food, body dissatisfaction, level of anxiety, motivation for change, level of assertiveness, and general psychiatric symptoms. in the short term, the vr treatment was more effective than the traditional cognitive-behavioral psychonutritional groups in improving the overall psychological state of the patients. in particular, the therapy was more effective in improving body satisfaction, self-efficacy, and motivation for change. no significant differences were found in the reduction of the binge eating behavior. the possibility of inducing a significant change in body image and its associated behaviors using a vr-based short-term therapy can be useful to improve the body satisfaction in traditional weight reduction programs. however, given the nature of this research that does not include a followup study, the obtained results are preliminary only ","Virtual-reality-based multidimensional therapy for the treatment of body image disturbances in binge eating disorders: a preliminary controlled study The main goal of this paper is to preliminarily evaluate the efficacy of a virtual-reality (VR)-based multidimensional approach in the treatment of body image attitudes and related constructs. The female binge eating disorder (BED) patients (n=20), involved in a residential weight control treatment including low-calorie diet (1200 cal/day) and physical training, were randomly assigned either to the multidimensional VR treatment or to psychonutritional groups based on the cognitive-behavior approach. Patients were administered a battery of outcome measures assessing eating disorders symptomathology, attitudes toward food, body dissatisfaction, level of anxiety, motivation for change, level of assertiveness, and general psychiatric symptoms. In the short term, the VR treatment was more effective than the traditional cognitive-behavioral psychonutritional groups in improving the overall psychological state of the patients. In particular, the therapy was more effective in improving body satisfaction, self-efficacy, and motivation for change. No significant differences were found in the reduction of the binge eating behavior. The possibilty of inducing a significant change in body image and its associated behaviors using a VR-based short-term therapy can be useful to improve the body satisfaction in traditional weight reduction programs. However, given the nature of this research that does not include a followup study, the obtained results are preliminary only","['virtual reality', 'multidimensional therapy', 'body image disturbances', 'binge eating disorders', 'obesity', 'patient therapy', 'residential weight control treatment', 'psychonutritional groups', 'cognitive-behavior approach', 'anxiety', 'psychiatric symptoms', 'medical computing', 'patient treatment', 'psychology', 'user interfaces', 'virtual reality']","['Virtual-reality-based multidimensional therapy', 'body satisfaction self-efficacy', 'multidimensional VR treatment', 'body image disturbances', 'binge eating disorders', 'binge eating behavior', 'body image attitudes', 'body image', 'VR treatment', 'body satisfaction']",1408,224,16,1407,223,10,2,1,4
"an efficient and stable ray tracing algorithm for parametric surfaces in this paper, we propose an efficient and stable algorithm for finding ray-surface intersections. newton's method and bezier clipping are adapted to form the core of our algorithm. ray coherence is used to find starting points for newton iteration. we introduce an obstruction detection technique to verify whether an intersection point found using newton's method is the closest. when newton's method fails to achieve convergence, we use bezier clipping substitution to find the intersection points. this combination achieves a significant improvement in tracing primary rays. a similar approach successfully improves the performance of tracing secondary rays ","An efficient and stable ray tracing algorithm for parametric surfaces In this paper, we propose an efficient and stable algorithm for finding ray-surface intersections. Newton's method and Bezier clipping are adapted to form the core of our algorithm. Ray coherence is used to find starting points for Newton iteration. We introduce an obstruction detection technique to verify whether an intersection point found using Newton's method is the closest. When Newton's method fails to achieve convergence, we use Bezier clipping substitution to find the intersection points. This combination achieves a significant improvement in tracing primary rays. A similar approach successfully improves the performance of tracing secondary rays","['efficient stable ray tracing algorithm', 'parametric surfaces', 'ray-surface intersections', 'Newton method', 'Bezier clipping', 'ray coherence', 'Newton iteration', 'obstruction detection technique', 'convergence', 'primary ray tracing', 'secondary ray tracing', 'computational geometry', 'Newton method', 'numerical stability', 'ray tracing', 'realistic images']","['newtons method', 'efficient', 'algorithm Ray coherence', 'parametric surfaces', 'intersection points', 'stable algorithm', 'secondary rays', 'primary rays', 'stable ray', 'algorithm']",626,107,16,626,106,10,0,0,5
"on quasi-linear pdaes with convection: applications, indices, numerical solution for a class of partial differential algebraic equations (pdaes) of quasi-linear type which include nonlinear terms of convection type, a possibility to determine a time and spatial index is considered. as a typical example we investigate an application from plasma physics. especially we discuss the numerical solution of initial boundary value problems by means of a corresponding finite difference splitting procedure which is a modification of a well-known fractional step method coupled with a matrix factorization. the convergence of the numerical solution towards the exact solution of the corresponding initial boundary value problem is investigated. some results of a numerical solution of the plasma pdae are given ","On quasi-linear PDAEs with convection: applications, indices, numerical solution For a class of partial differential algebraic equations (PDAES) of quasi-linear type which include nonlinear terms of convection type, a possibility to determine a time and spatial index is considered. As a typical example we investigate an application trom plasma physics. Especially we discuss the numerical solution of initial boundary value problems by means of a corresponding finite difference splitting procedure which is a modification of a well-known fractional step method coupled with a matrix factorization. The convergence of the numerical solution towards the exact solution of the corresponding initial boundary value problem is investigated. Some results of a numerical solution of the plasma PDAE are given","['quasi-linear partial differential algebraic equations', 'spatial index', 'plasma physics', 'initial boundary value problems', 'finite difference splitting procedure', 'fractional step method', 'matrix factorization', 'convection', 'indices', 'numerical solution', 'initial value problems', 'matrix decomposition', 'partial differential equations']","['numerical solution', 'convection applications indices', 'quasi-linear type', 'nonlinear terms', 'convection type', 'exact solution', 'solution', 'convection', 'quasi-linear', 'numerical']",689,117,13,689,116,10,1,1,6
"the internet, knowledge and the academy as knowledge is released from the bounds of libraries, as research becomes no longer confined to the academy, and education/certification is available, any time/any place, the university and the faculty must redefine themselves. liberal studies, once the core, and currently eschewed in favor of science and technology, will be reborn in those institutions that can rise above the mundane and embrace an emerging ""third culture"" ","The Internet, knowledge and the academy As knowledge is released from the bounds of libraries, as research becomes no longer confined to the academy, and education/certification is available, any time/any place, the university and the faculty must redefine themselves. Liberal studies, once the core, and currently eschewed in favor of science and technology, will be reborn in those institutions that can rise above the mundane and embrace an emerging “third culture”","['Internet', 'knowledge', 'academy', 'education', 'certification', 'university', 'faculty', 'liberal studies', 'certification', 'humanities', 'Internet', 'teaching']","['academy', 'education/certification', 'internet knowledge', 'Liberal studies', 'time/any place', 'libraries', 'research', 'bounds', 'internet', 'knowledge']",398,72,12,398,71,10,2,2,5
"a round of cash, a pound of flesh [telecom] despite the upheaval across telecom, venture capital firms are still investing in start-ups. but while a promising idea and a catchy name were enough to guarantee millions in funding at the peak of the dotcom frenzy, now start-ups must prove-their long-term viability, and be willing to concede control of their business to their vc suitors ","Around of cash, a pound of flesh [telecom] Despite the upheaval across telecom, venture capital firms are still investing in start-ups. But while a promising idea and a catchy name were enough to guarantee millions in funding at the peak of the dotcom frenzy, now start-ups must prove-their long-term viability, and be willing to concede control of their business to their VC suitors","['telecom', 'venture capital firms', 'viability', 'telecommunication']","['start-ups', 'promising idea', 'flesh telecom', 'catchy name', 'upheaval', 'venture', 'pound', 'cash', 'flesh', 'telecom']",321,65,4,321,63,10,193,63,1
"work sequencing in a manufacturing cell with limited labour constraints this study focuses on the analysis of group scheduling heuristics in a dual-constrained, automated manufacturing cell, where labour utilization is limited to setups, tear-downs and loads/unloads. this scenario is realistic in today's automated manufacturing cells. the results indicate that policies for allocating labour to tasks have very little impact in such an environment. furthermore, the performance of efficiency oriented, exhaustive, group scheduling heuristics deteriorated while the performance of the more complex, non-exhaustive heuristics improved. thus, it is recommended that production managers use the simplest labour scheduling policy, and instead focus their efforts to activities such as job scheduling and production planning in such environments ","Work sequencing in a manufacturing cell with limited labour constraints This study focuses on the analysis of group scheduling heuristics ina dual-constrained, automated manufacturing cell, where labour utilization is limited to setups, tear-downs and loads/unioads. This scenario is realistic in today's automated manufacturing cells. The results indicate that policies for allocating labour to tasks have very little impact in such an environment. Furthermore, the performance of efficiency oriented, exhaustive, group scheduling heuristics deteriorated while the performance of the more complex, non-exhaustive heuristics improved. Thus, it is recommended that production managers use the simplest labour scheduling policy, and instead focus their efforts to activities such as job scheduling and production planning in such environments","['work sequencing', 'manufacturing cell', 'limited labour constraints', 'group scheduling heuristics', 'dual-constrained automated manufacturing cell', 'automated manufacturing cells', 'labour allocation policies', 'efficiency oriented exhaustive group scheduling heuristics', 'nonexhaustive heuristics', 'production planning', 'job scheduling', 'assembly planning', 'heuristic programming', 'production control', 'production engineering computing', 'resource allocation']","['automated manufacturing cells', 'group scheduling heuristics', 'complex non-exhaustive heuristics', 'labour utilization', 'labour constraints', 'Work sequencing', 'labour', 'manufacturing', 'cells', 'manufacturing cell']",728,115,16,728,113,10,369,93,1
"speaker adaptive modeling by vocal tract normalization this paper presents methods for speaker adaptive modeling using vocal tract normalization (vtn) along with experimental tests on three databases. we propose a new training method for vtn: by using single-density acoustic models per hmm state for selecting the scale factor of the frequency axis, we avoid the problem that a mixture-density tends to learn the scale factors of the training speakers and thus cannot be used for selecting the scale factor. we show that using single gaussian densities for selecting the scale factor in training results in lower error rates than using mixture densities. for the recognition phase, we propose an improvement of the well-known two-pass strategy: by using a non-normalized acoustic model for the first recognition pass instead of a normalized model, lower error rates are obtained. in recognition tests, this method is compared with a fast variant of vtn. the two-pass strategy is an efficient method, but it is suboptimal because the scale factor and the word sequence are determined sequentially. we found that for telephone digit string recognition this suboptimality reduces the vtn gain in recognition performance by 30% relative. in summary, on the german spontaneous speech task verbmobil, the wsj task and the german telephone digit string corpus sietill, the proposed methods for vtn reduce the error rates significantly ","Speaker adaptive modeling by vocal tract normalization This paper presents methods for speaker adaptive modeling using vocal tract normalization (VTN) along with experimental tests on three databases. We propose a new training method for VTN: By using single-density acoustic models per HMM state for selecting the scale factor of the frequency axis, we avoid the problem that a mixture-density tends to learn the scale factors of the training speakers and thus cannot be used for selecting the scale factor. We show that using single Gaussian densities for selecting the scale factor in training results in lower error rates than using mixture densities. For the recognition phase, we propose an improvement of the well-known two-pass strategy: by using a non-normalized acoustic model for the first recognition pass instead of a normalized model, lower error rates are obtained. In recognition tests, this method is compared with a fast variant of VTN. The two-pass strategy is an efficient method, but it is suboptimal because the scale factor and the word sequence are determined sequentially. We found that for telephone digit string recognition this suboptimality reduces the \VTN gain in recognition performance by 30% relative. In summary, on the German spontaneous speech task Verbmobil, the WSJ task and the German telephone digit string corpus SieTill, the proposed methods for VTN reduce the error rates significantly","['speaker adaptive modeling', 'vocal tract normalization', 'databases', 'training method', 'single-density acoustic models', 'HMM state', 'frequency scale factor', 'training speakers', 'single Gaussian densities', 'training results', 'error rate reduction', 'two-pass strategy', 'nonnormalized acoustic model', 'word sequence', 'telephone digit string recognition', 'German spontaneous speech task', 'Verlimobil', 'WSJ task', 'German telephone digit string corpus', 'SieTill', 'acoustic signal processing', 'adaptive signal processing', 'Gaussian processes', 'hidden Markov models', 'speech recognition']","['vocal tract normalization', 'adaptive modelling', 'lower error rates', 'scale factors', 'single-density acoustic models', 'non-normalized acoustic model', 'model', 'new training method', 'recognition tests', 'efficient method']",1208,222,25,1209,221,10,3,1,9
"orthogonal decompositions of complete digraphs a family g of isomorphic copies of a given digraph g is said to be an orthogonal decomposition of the complete digraph d/sub n/ by g, if every arc of d/sub n/ belongs to exactly one member of g and the union of any two different elements from g contains precisely one pair of reverse arcs. given a digraph h, an h family mh is the vertex-disjoint union of m copies of h . in this paper, we consider orthogonal decompositions by h-families. our objective is to prove the existence of such an orthogonal decomposition whenever certain necessary conditions hold and m is sufficiently large ","Orthogonal decompositions of complete digraphs A family G of isomorphic copies of a given digraph G is said to be an orthogonal decomposition of the complete digraph Disub n/ by G, if every arc of D/sub n/ belongs to exactly one member of G and the union of any two different elements from G contains precisely one pair of reverse arcs. Given a digraph h, an h family mh is the vertex-disjoint union of m copies of h . In this paper, we consider orthogonal decompositions by h-families. Our objective is to prove the existence of such an orthogonal decomposition whenever certain necessary conditions hold and m is sufficiently large","['orthogonal decompositions', 'complete digraphs', 'isomorphic copies', 'vertex-disjoint union', 'necessary conditions', 'computational geometry', 'directed graphs']","['Orthogonal decompositions', 'digraph', 'complete digraph disk', 'complete digraphs', 'h family mh', 'digraph G', 'family G', 'G', 'decomposition', 'orthogonal decomposition']",524,111,7,524,110,10,1,1,3
"analysis of exclusively kinetic two-link underactuated mechanical systems analysis of exclusively kinetic two-link underactuated mechanical systems is undertaken. it is first shown that such systems are not full-state feedback linearizable around any equilibrium point. also, the equilibrium points for which the system is small-time locally controllable (stlc) is at most a one-dimensional submanifold. a concept less restrictive than stlc, termed the small-time local output controllability (stloc) is introduced, the satisfaction of which guarantees that a chosen configuration output can be controlled at its desired value. it is shown that the class of systems considered is stloc, if the inertial coupling between the input and output is nonzero. also, in such a case, the system is nonminimum phase. an example section illustrates all the results presented ","Analysis of exclusively kinetic two-link underactuated mechanical systems Analysis of exclusively kinetic two-link underactuated mechanical systems is undertaken. It is first shown that such systems are not full-state feedback linearizable around any equilibrium point. Also, the equilibrium points for which the system is small-time locally controllable (STLC) is at most a one-dimensional submanifold. A concept less restrictive than STLC, termed the small-time local output controllability (STLOC) is introduced, the satisfaction of which guarantees that a chosen configuration output can be controlled at its, desired value. It is shown that the class of systems considered is STLOG, if the inertial coupling between the input and output is, nonzero. Also, in such a case, the system is nonminimum phase. An ‘example section illustrates all the results presented","['exclusively kinetic two-link underactuated mechanical systems', 'equilibrium points', 'small-time locally controllable system', 'one-dimensional submanifold', 'small-time local output controllability', 'nonminimum phase', 'asymptotic stability', 'controllability', 'feedback', 'linearisation techniques', 'manipulators']","['underactuated', 'mechanical', 'two-line', 'Analysis', 'kinetic', 'full-scale feedback linearizable', 'configuration output', 'equilibrium points', 'such systems', 'system']",739,126,11,742,125,10,8,4,3
"optimal strategies for a semi-markovian inventory system control for a semi-markovian inventory system is considered. under general assumptions on system functioning, conditions for existence of an optimal nonrandomized markovian strategy are found. it is shown that under some additional assumptions on storing conditions for the inventory, the optimal strategy has a threshold (s, s)-frame ","Optimal strategies for a semi-Markovian inventory system Control for a semi-Markovian inventory system is considered. Under general assumptions on system functioning, conditions for existence of an ‘optimal nonrandomized Markovian strategy are found. It is shown that under some additional assumptions on storing conditions for the inventory, the optimal strategy has a threshold (s, S)-frame","['optimal strategies', 'semi-Markovian inventory system', 'system functioning', 'optimal nonrandomized Markovian strategy', 'optimal strategy', 'Markov processes', 'probability', 'stock control']","['semi-Markovian inventory system', 'additional assumptions', 'general assumptions', 'Optimal strategies', 'optimal strategy', 'assumptions', 'strategy', 'system', 'inventory', 'semi-Markovian']",338,55,8,339,54,10,7,1,0
"caring for your new lawyers in any given year, a striking number of lawyers are in a state of flux, from newly minted law school graduates looking for their first job, to senior litigators migrating to new challenges with new firms. the one certainty is that lawyers new to any firm need care and feeding in myriad ways. all of them need to know and understand three things: (1) the firm's culture; (2) the resources available to help them develop their practices; and (3) where to get help and guidance for research and practice purposes. obtaining a thorough understanding of a new firm's workings may be the greatest research project lawyers face. how can a firm help its new lawyers learn what they need to know? to offer an example, here are programs in place at my firm ","Caring for your new lawyers In any given year, a striking number of lawyers are in a state of flux, from newly minted law school graduates looking for their first job, to senior litigators migrating to new challenges with new firms. The one certainty is that lawyers new to any firm need care and feeding in myriad ways. All of them need to know and understand three things: (1) the firm's culture; (2) the resources available to help them develop their practices; and (3) where to get help and guidance for research and practice purposes. Obtaining a thorough understanding of a new firm's workings may be the greatest research project lawyers face. How can a firm help its new lawyers learn what they need to know? To offer an example, here are programs in place at my firm","['new lawyers', ""firm's culture"", 'resources', 'law administration']","['firm', 'new lawyers', 'new', 'new firms workings', 'firms culture u2', 'new challenges', 'firm need care', 'firm help', 'new firms', 'lawyers']",638,139,4,638,138,10,0,0,1
"evaluation of videoconferenced grand rounds we evaluated various aspects of grand rounds videoconferenced from a tertiary care hospital to a regional hospital in nova scotia. during a five-month study period, 29 rounds were broadcast (19 in medicine and 10 in cardiology). the total recorded attendance at the remote site was 103, comprising 70 specialists, nine family physicians and 24 other health-care professionals. we received 55 evaluations, a response rate of 53%. on a five-point likert scale (on which higher scores indicated better quality), mean ratings by remote-site participants of the technical quality of the videoconference were 3.0-3.5, with the lowest ratings being for ability to hear the discussion (3.0) and to see visual aids (3.1). mean ratings for content, presentation, discussion and educational value were 3.8 or higher. of the 49 physicians who presented the rounds, we received evaluations from 41, a response rate of 84%. the presenters rated all aspects of the videoconference and interaction with remote sites at 3.8 or lower. the lowest ratings were for ability to see the remote sites (3.0) and the usefulness of the discussion (3.4). we received 278 evaluations from participants at the presenting site, an estimated response rate of about 55%. the results indicated no adverse opinions of the effect of videoconferencing (mean scores 3.1-3.3). the estimated costs of videoconferencing one grand round to one site and four sites were c$723 and c$1515, respectively. the study confirmed that videoconferenced rounds can provide satisfactory continuing medical education to community specialists, which is an especially important consideration as maintenance of certification becomes mandatory ","Evaluation of videoconferenced grand rounds. We evaluated various aspects of grand rounds videoconferenced from a tertiary care hospital to a regional hospital in Nova Scotia. During a five-month study period, 29 rounds were broadcast (19 in medicine and 10 in cardiology). The total recorded attendance at the remote site was 103, comprising 70 specialists, nine family physicians and 24 other health-care professionals. We received 55 evaluations, a response rate of 53%. On a five-point Likert scale (on which higher scores indicated better quality), mean ratings by remote-site participants of the technical quality of the videoconference were 3.0-3.5, with the lowest ratings being for ability to hear the discussion (3.0) and to see visual aids (3.1). Mean ratings for content, presentation, discussion and educational value were 3.8 or higher. Of the 49 physicians who presented the rounds, we received evaluations from 41, a response rate of 84%. The presenters rated all aspects of the videoconference and interaction with remote sites at 3.8 or lower. The lowest ratings were for ability to see the remote sites (3.0) and the usefulness of the discussion (3.4). We received 278 evaluations from participants at the presenting site, an estimated response rate of about 55%. The results indicated no adverse opinions of the effect of videoconferencing (mean scores 3.1-3.3). The estimated costs of videoconferencing one grand round to one site and four sites were C$723 and C$1515, respectively. The study confirmed that videoconferenced rounds can provide satisfactory continuing medical education to community specialists, which is an especially important consideration as maintenance of certification becomes mandatory","['videoconferenced grand rounds', 'tertiary care hospital', 'regional hospital', 'telemedicine', 'cardiology', 'health-care professionals', 'five-point Likert scale', 'remote sites', 'continuing medical education', 'certification', 'biomedical education', 'certification', 'continuing education', 'educational computing', 'medical computing', 'teleconferencing', 'telemedicine']","['remote site', 'round', 'videoconferencing mean scores', 'videoconferenced grand rounds', 'videoconferenced rounds', 'estimated response rate', 'tertiary care hospital', 'grand round', 'response rate', 'videoconferencing']",1470,260,17,1471,259,10,0,1,6
tracking with sensor failures studies the reliability with sensor failures of the asymptotic tracking problem for linear time invariant systems using the factorization approach. the plant is two-output and the compensator is two-degree-of-freedom. necessary and sufficient conditions are presented for the general problem and a simple solution is given for problems with stable plants ,Tracking with sensor failures Studies the reliability with sensor failures of the asymptotic tracking problem for linear time invariant systems using the factorization approach. The plant is two-output and the compensator is two-degree-of-freedom. Necessary and sufficient conditions are presented for the general problem and a simple solution is given for problems with stable plants,"['sensor failures', 'reliability', 'asymptotic tracking problem', 'linear time invariant systems', 'factorization approach', 'two-output plant', 'two-degree-of-freedom compensator', 'necessary and sufficient conditions', 'closed loop systems', 'compensation', 'continuous time systems', 'control system synthesis', 'fault tolerance', 'linear systems', 'sensors', 'stability', 'tracking']","['asymptotic tracking problem', 'sensor failures Studies', 'general problem', 'reliability', 'linear time', 'problem', 'sensor', 'failures', 'Studies', 'sensor failures']",331,55,17,331,54,10,0,0,2
"adaptive optimizing compilers for the 21st century historically, compilers have operated by applying a fixed set of optimizations in a predetermined order. we call such an ordered list of optimizations a compilation sequence. this paper describes a prototype system that uses biased random search to discover a program-specific compilation sequence that minimizes an explicit, external objective function. the result is a compiler framework that adapts its behavior to the application being compiled, to the pool of available transformations, to the objective function, and to the target machine. this paper describes experiments that attempt to characterize the space that the adaptive compiler must search. the preliminary results suggest that optimal solutions are rare and that local minima are frequent. if this holds true, biased random searches, such as a,genetic algorithm, should find good solutions more quickly than simpler strategies, such as hill climbing ","Adaptive optimizing compilers for the 21st century Historically, compilers have operated by applying a fixed set of optimizations in a predetermined order. We call such an ordered list of optimizations a compilation sequence. This paper describes a prototype system that uses biased random search to discover a program-specific compilation sequence that minimizes an explicit, external objective function. The result is a compiler framework that adapts its behavior to the application being compiled, to the pool of available transformations, to the objective function, and to the target machine. This paper describes experiments that attempt to characterize the space that the adaptive compiler must search. The preliminary results suggest that optimal solutions are rare and that local minima are frequent. If this holds true, biased random searches, such as a,genetic algorithm, should find good solutions more quickly than simpler strategies, such as hill climbing","['compilers', 'optimizations', 'compilation sequence', 'adaptive compiler', 'optimizing compilers', 'biased random search', 'configurable compilers', 'optimising compilers']","['objective function', 'optimization', 'compilers', 'program-specific compilation sequence', 'compiler framework', 'optimal solutions', 'adaptive compiler', 'random searches', 'just century', 'compilation sequence']",827,143,8,827,142,10,0,0,1
verizon leapfrogs sprint pcs with q2 subscriber numbers the wireless carrier industry's second-quarter results showed a surprising shift in market share as sprint pcs fell from grace after a nearly four-year lead in subscriber additions and verizon wireless added considerably more customers than analysts expected ,Verizon leapfrogs Sprint PCS with Q2 subscriber numbers The wireless carrier industry's second-quarter results showed a surprising shift in market share as Sprint PCS fell from grace after a nearly four-year lead in subscriber additions and Verizon Wireless added considerably more customers than analysts expected,"['wireless carrier industry', 'Sprint PCS', 'Verizon Wireless', 'radio access networks', 'telecommunication']","['Sprint PCS', 'wireless carrier industry', 'Q2 subscriber numbers', 'subscriber additions', 'version Wireless', 'four-year lead', 'subscriber', 'version', 'PCS', 'Sprint']",270,46,5,270,45,10,0,0,0
"techniques for compiling and implementing all nas parallel benchmarks in hpf the nas parallel benchmarks (npb) are a well-known benchmark set for high-performance machines. much effort has been made to implement them in high-performance fortran (hpf). in previous attempts, however, the hpf versions did not include the complete set of benchmarks, and the performance was not always good. in this study, we implement all eight benchmarks of the npb in hpf, and parallelize them using an hpf compiler that we have developed. this report describes the implementation techniques and compiler features necessary to achieve good performance. we evaluate the hpf version on the hitachi sr2201, a distributed-memory parallel machine. with 16 processors, the execution time of the hpf version is within a factor of 1.5 of the hand-parallelized version of the npb 2.3 beta ","Techniques for compiling and implementing all NAS parallel benchmarks in HPF. The NAS parallel benchmarks (NPB) are a well-known benchmark set for high-performance machines. Much effort has been made to implement them in High-Performance Fortran (HPF). In previous attempts, however, the HPF versions did not include the complete set of benchmarks, and the performance was not always good. In this study, we implement all eight benchmarks of the NPB in HPF, and parallelize them using an HPF compiler that we have developed. This report describes the implementation techniques and compiler features necessary to achieve good performance. We evaluate the HPF version on the Hitachi SR2201, a distributed-memory parallel machine. With 16 processors, the execution time of the HPF version is within a factor of 1.5 of the hand-paralelized version of the NPB 2.3 beta","['NAS parallel benchmarks', 'high-performance machines', 'compiler', 'distributed-memory parallel supercomputers', 'HPF compiler', 'FORTRAN', 'parallel programming', 'parallelising compilers', 'performance evaluation']","['HPF version', 'NAS parallel benchmarks', 'distributed-memory parallel machine', 'hand-paralelized version', 'well-known benchmark', 'HPF compiler', 'HPF', 'parallel', 'NAS', 'benchmarks']",730,135,9,730,134,10,6,2,2
"a framework for evaluating the data-hiding capacity of image sources an information-theoretic model for image watermarking and data hiding is presented in this paper. previous theoretical results are used to characterize the fundamental capacity limits of image watermarking and data-hiding systems. capacity is determined by the statistical model used for the host image, by the distortion constraints on the data hider and the attacker, and by the information available to the data hider, to the attacker, and to the decoder. we consider autoregressive, block-dct, and wavelet statistical models for images and compute data-hiding capacity for compressed and uncompressed host-image sources. closed-form expressions are obtained under sparse-model approximations. models for geometric attacks and distortion measures that are invariant to such attacks are considered ","‘A framework for evaluating the data-hiding capacity of image sources An information-theoretic model for image watermarking and data hiding is presented in this paper. Previous theoretical results are used to characterize the fundamental capacity limits of image watermarking and data-hiding systems. Capacity is determined by the statistical model used for the host image, by the distortion constraints on the data hider and the attacker, and by the information available to the data hider, to the attacker, and to the decoder. We consider autoregressive, block-DCT, and wavelet statistical models for images and compute data-hiding capacity for compressed and uncompressed host-image sources. Closed-form expressions are obtained under sparse-model approximations. Models for geometric attacks and distortion measures that are invariant to such attacks are considered","['data-hiding capacity', 'image sources', 'information-theoretic model', 'watermarking', 'capacity limits', 'statistical model', 'distortion constraints', 'autoregressive statistical models', 'block-DCT statistical models', 'wavelet statistical models', 'compressed host-image sources', 'uncompressed host-image sources', 'closed-form expressions', 'sparse-model approximations', 'geometric attacks', 'distortion measures', 'autoregressive processes', 'copy protection', 'data compression', 'data encapsulation', 'discrete cosine transforms', 'image coding', 'statistical analysis', 'wavelet transforms']","['data-hiding capacity', 'image watermarking', 'uncompressed host-image sources', 'data-hiding systems Capacity', 'information-theoretic model', 'fundamental capacity limits', 'statistical models', 'host image', 'images', 'image sources']",747,123,24,748,122,10,1,1,5
"local activity criteria for discrete-map cnn discrete-time cnn systems are studied in this paper by the application of chua's local activity principle. these systems are locally active everywhere except for one isolated parameter value. as a result, nonhomogeneous spatiotemporal patterns may be induced by any initial setting of the cnn system when the strength of the system diffusion coupling exceeds a critical threshold. the critical coupling coefficient can be derived from the loaded cell impedance of the cnn system. three well-known 1d map cnn's (namely, the logistic map cnn, the magnetic vortex pinning map cnn, and the spiral wave reproducing map cnn) are introduced to illustrate the applications of the local activity principle. in addition, we use the cell impedance to demonstrate the period-doubling scenario in the logistic and the magnetic vortex pinning maps ","Local activity criteria for discrete-map CNN Discrete-time CNN systems are studied in this paper by the application of Chua's local activity principle. These systems are locally active everywhere except for one isolated parameter value. AS a result, nonhomogeneous spatiotemporal patterns may be induced by any initial setting of the CNN system when the strength of the system diffusion coupling exceeds a critical threshold. The critical coupling coefficient can be derived from the loaded cell impedance of the CNN system. Three well-known 1D map CNN's (namely, the logistic map CNN, the magnetic vortex pinning map CNN, and the spiral wave reproducing map CNN) are introduced to illustrate the applications of the local activity principle. In addition, we use the cell impedance to ‘demonstrate the period-doubling scenario in the logistic and the magnetic vortex pinning maps","['discrete-time CNN systems', 'local activity criteria', 'difference equation', 'discrete-map CNN', 'loaded cell impedance', ""Chua's local activity principle"", 'nonhomogeneous spatiotemporal patterns', 'critical coupling coefficient', 'logistic map CNN', 'magnetic vortex pinning map CNN', 'spiral wave reproducing map CNN', 'period-doubling', 'cellular neural nets', 'chaos', 'difference equations']","['local activity principle', 'CNN', 'system diffusion coupling', 'Discrete-time CNN systems', 'Local activity criteria', 'logistic map cnn', 'discrete-map CNN', 'activity', 'systems', 'CNN system']",745,135,15,746,134,10,11,1,3
"an approach to developing computational supports for reciprocal tutoring this study presents a novel approach to developing computational supports for reciprocal tutoring. reciprocal tutoring is a collaborative learning activity, where two participants take turns to play the role of a tutor and a tutee. the computational supports include scaffolding tools for the tutor and a computer-simulated virtual participant. the approach, including system architecture, implementations of scaffolding tools for the tutor and of a virtual participant is presented herein. furthermore, a system for reciprocal tutoring is implemented as an example of the approach ","An approach to developing computational supports for reciprocal tutoring This study presents a novel approach to developing computational supports for reciprocal tutoring. Reciprocal tutoring is a collaborative learning activity, where two participants take turns to play the role of a tutor and a tutee. The computational supports include scaffolding tools for the tutor and a computer-simulated virtual participant. The approach, including system architecture, implementations of scaffolding tools for the tutor and of a virtual participant is presented herein. Furthermore, a system for reciprocal tutoring is implemented as an example of the approach","['reciprocal tutoring computational support', 'collaborative learning', 'scaffolding tools', 'computer-simulated virtual participant', 'system architecture', 'intelligent tutoring system', 'groupware', 'intelligent tutoring systems', 'teaching', 'user interfaces']","['computational supports', 'computer-simulated virtual participants', 'system architecture implementations', 'novel approach', 'approach', 'virtual participant', 'tutoring', 'computational', 'reciprocal tutoring', 'Reciprocal tutoring']",563,93,10,563,92,10,0,0,2
"parcel boundary identification with computer-assisted boundary overlay process for taiwan the study investigates the design of a process for parcel boundary identification with cadastral map overlay using the principle of least squares. the objective of this research is to provide an objective tool for boundary identification survey. the proposed process includes an adjustment model, a weighting scheme, and other related operations. a numerical example is included ","Parcel boundary identification with computer-assisted boundary overlay process for Taiwan The study investigates the design of a process for parcel boundary identification with cadastral map overlay using the principle of least squares. The objective of this research is to provide an objective tool for boundary identification survey. The proposed process includes an adjustment model, a weighting scheme, and other related operations. A numerical example is included","['parcel boundary identification', 'computer assisted boundary overlay process', 'Taiwan', 'cadastral map overlay', 'objective tool', 'boundary identification survey', 'adjustment model', 'weighting scheme', 'Gauss-Marker model', 'geographic information system', 'weighted least squares adjustment', 'cartography', 'geographic information systems', 'least squares approximations', 'town and country planning']","['process', 'boundary identification survey', 'cadastral map overlay', 'objective tool', 'boundary', 'identification', 'Parcel', 'overlay', 'parcel boundary identification', 'Parcel boundary identification']",403,67,15,403,66,10,0,0,3
"direct self control with minimum torque ripple and high dynamics for a double three-level gto inverter drive a highly dynamic control scheme with very low torque ripple-direct self control (dsc) with torque hysteresis control-for very high-power medium-voltage induction motor drives fed by a double three-level inverter (d3li) is presented. in this arrangement, two three-level inverters that are connected in parallel at their dc sides are feeding the open motor windings. the dsc, well known from two- and three-level inverters, is adapted to the d3li and optimized for a minimum torque ripple. an 18-corner trajectory is chosen for the stator flux of the induction machine since it is approaching the ideal circle much better than the hexagon known from dsc for two-level inverters, without any detriment to the torque ripple. the machine and inverter control are explained and the proposed torque quality and dynamics are verified by measurements on a 180-kw laboratory drive ","Direct self control with minimum torque ripple and high dynamics for a double three-level GTO inverter drive A highly dynamic control scheme with very low torque ripple-direct self control (DSC) with torque hysteresis control-for very high-power medium-voltage induction motor drives fed by a double three-level inverter (D3LI) is presented. In this arrangement, two three-level inverters that are connected in parallel at their DC sides are feeding the open motor windings. The DSC, well known from two- and three-level inverters, is adapted to the D3LI and optimized for a minimum torque ripple. An 18-corner trajectory is chosen for the stator flux of the induction machine since it is approaching the ideal circle much better than the hexagon known from DSC for two-level inverters, without any detriment to the torque ripple. The machine and inverter control are explained and the proposed torque quality and dynamics are verified by measurements on a 180-kW laboratory drive","['highly dynamic control scheme', 'very low torque ripple', 'direct self control', 'torque hysteresis control', 'medium-voltage induction motor drives', 'double three-level inverter', 'parallel connected inverters', 'open motor windings', 'stator flux', 'torque quality', 'variable-speed drives', 'multilevel converters', 'machine observer', '180 kW', 'induction motor drives', 'machine control', 'stators', 'thyristor convertors', 'torque control']","['three-level inverters', 'minimum torque ripple', 'Direct self control', 'torque hysteresis control-for', 'double three-level inverter', 'dynamic control scheme', 'inverter drive A', 'inverter control', 'low torque', 'torque ripple']",829,153,19,829,152,10,0,0,4
"developing a high-performance web server in concurrent haskell server applications, and in particular network-based server applications, place a unique combination of demands on a programming language: lightweight concurrency, high i/o throughput, and fault tolerance are all important. this paper describes a prototype web server written in concurrent haskell (with extensions), and presents two useful results: firstly, a conforming server could be written with minimal effort, leading to an implementation in less than 1500 lines of code, and secondly the naive implementation produced reasonable performance. furthermore, making minor modifications to a few time-critical components improved performance to a level acceptable for anything but the most heavily loaded web servers ","Developing a high-performance web server in Concurrent Haskell Server applications, and in particular network-based server applications, place @ unique combination of demands on a programming language: lightweight concurrency, high /O throughput, and fault tolerance are all important. This paper describes a prototype Web server written in Concurrent Haskell (with extensions), and presents two useful results: firstly, a conforming server could be written with minimal effort, leading to an implementation in less than 1500 lines of code, and secondly the naive implementation produced reasonable performance. Furthermore, making minor modifications to a few time-critical ‘components improved performance to a level acceptable for anything but the most heavily loaded Web servers","['high-performance Web server', 'Concurrent Haskell', 'network-based server applications', 'lightweight concurrency', 'high I/O throughput', 'fault tolerance', 'conforming server', 'time-critical components', 'concurrency control', 'fault tolerant computing', 'file servers', 'functional programming', 'Internet']","['Concurrent Haskell', 'server', 'applications', 'high-performance web server', 'prototype Web server', 'loaded Web servers', 'web', 'high-performance', 'Haskell', 'Concurrent']",675,109,13,675,108,10,13,3,4
"elastically adaptive deformable models we present a technique for the automatic adaptation of a deformable model's elastic parameters within a kalman filter framework for shape estimation applications. the novelty of the technique is that the model's elastic parameters are not constant, but spatio-temporally varying. the variation of the elastic parameters depends on the distance of the model from the data and the rate of change of this distance. each pass of the algorithm uses physics-based modeling techniques to iteratively adjust both the geometric and the elastic degrees of freedom of the model in response to forces that are computed from the discrepancy between the model and the data. by augmenting the state equations of an extended kalman filter to incorporate these additional variables, we are able to significantly improve the quality of the shape estimation. therefore, the model's elastic parameters are always initialized to the same value and they are subsequently modified depending on the data and the noise distribution. we present results demonstrating the effectiveness of our method for both two-dimensional and three-dimensional data ","Elastically adaptive deformable models We present a technique for the automatic adaptation of a deformable model's, elastic parameters within a Kalman filter framework for shape estimation applications. The novelty of the technique is that the model's elastic parameters are not constant, but spatio-temporally varying. The variation of the elastic parameters depends on the distance of the model from the data and the rate of change of this, distance. Each pass of the algorithm uses physics-based modeling techniques to iteratively adjust both the geometric and the elastic degrees of freedom of the model in response to forces that are computed from the discrepancy between the model and the data. By augmenting the state equations of an extended Kalman filter to incorporate these additional variables, we are able to significantly improve the quality of the shape estimation. Therefore, the model's elastic parameters are always initialized to the same value and they are subsequently modified depending on the data and the noise distribution. We present results demonstrating the effectiveness of our method for both two-dimensional and three-dimensional data","['elastically adaptive deformable models', 'automatic adaptation', 'elastic parameters', 'Kalman filter framework', 'shape estimation', 'physics-based modeling techniques', 'geometric degrees of freedom', 'elastic degrees of freedom', 'state equations', 'extended Kalman filter', 'computer vision', 'filtering theory', 'geometry', 'Kalman filters', 'nonlinear filters', 'parameter estimation']","['elastic parameters', 'model', 'physics-based modelling techniques', 'shape estimation applications', 'adaptive deformable models', 'automatic adaptation', 'elastic degrees', 'elastic', 'adaptation', 'deformable models']",989,176,16,991,175,10,0,2,3
quantum computing with spin qubits in semiconductor structures we survey recent work on designing and evaluating quantum computing implementations based on nuclear or bound-electron spins in semiconductor heterostructures at low temperatures and in high magnetic fields. general overview is followed by a summary of results of our theoretical calculations of decoherence time scales and spin-spin interactions. the latter were carried out for systems for which the two-dimensional electron gas provides the dominant carrier for spin dynamics via exchange of spin-excitons in the integer quantum hall regime ,Quantum computing with spin qubits in semiconductor structures We survey recent work on designing and evaluating quantum computing implementations based on nuclear or bound-electron spins in semiconductor heterostructures at low temperatures and in high magnetic fields. General overview is followed by a summary of results of our theoretical calculations of decoherence time scales and spin-spin interactions. The latter were carried out for systems for which the two-dimensional electron gas provides the dominant carrier for spin ‘dynamics via exchange of spin-excitons in the integer quantum Hall regime,"['quantum computing', 'semiconductor heterostructures', 'low temperatures', 'high magnetic fields', 'spin-spin interactions', '2D electron gas', '2DEG', 'spin dynamics', 'dominant carrier', 'spin-excitons exchange', 'integer quantum Hall regime', 'integer QHE', 'spin qubits', 'semiconductor structures', 'excitons', 'interface states', 'quantum computing', 'quantum Hall effect', 'semiconductor heterojunctions', 'spin dynamics', 'two-dimensional electron gas']","['quantum computing implementations', 'semiconductor heterostructures', 'semiconductor structures', 'spin dynamics', 'recent work', 'semiconductor', 'Quantum', 'computing', 'spin', 'Quantum computing']",521,87,21,522,86,10,8,1,6
"rapid microwell polymerase chain reaction with subsequent ultrathin-layer gel electrophoresis of dna large-scale genotyping, mapping and expression profiling require affordable, fully automated high-throughput devices enabling rapid, high-performance analysis using minute quantities of reagents. in this paper, we describe a new combination of microwell polymerase chain reaction (pcr) based dna amplification technique with automated ultrathin-layer gel electrophoresis analysis of the resulting products. this technique decreases the reagent consumption (total reaction volume 0.75-1 mu l), the time requirement of the pcr (15-20 min) and subsequent ultrathin-layer gel electrophoresis based fragment analysis (5 min) by automating the current manual procedure and reducing the human intervention using sample loading robots and computerized real time data analysis. small aliquots (0.2 mu l) of the submicroliter size pcr reaction were transferred onto loading membranes and analyzed by ultrathin-layer gel electrophoresis which is a novel, high-performance and automated microseparation technique. this system employs integrated scanning laser-induced fluorescence-avalanche photodiode detection and combines the advantages of conventional slab and capillary gel electrophoresis. visualization of the dna fragments was accomplished by ""in migratio"" complexation with ethidium bromide during the electrophoresis process also enabling real time imaging and data analysis ","Rapid microwell polymerase chain reaction with subsequent ultrathin-layer gel electrophoresis of DNA Large-scale genotyping, mapping and expression profiling require affordable, fully automated high-throughput devices enabling rapid, high-performance analysis using minute quantities of reagents. In this, paper, we describe a new combination of microwell polymerase chain reaction (PCR) based DNA amplification technique with automated ultrathin-layer gel electrophoresis analysis of the resulting products. This technique decreases the reagent consumption (total reaction volume 0.75-1 mu L), the time requirement of the PCR (15-20 min) and subsequent ultrathin-layer gel electrophoresis based fragment analysis (5 min) by automating the current manual procedure and reducing the human intervention using sample loading robots and computerized real time data analysis. Small aliquots (0.2 mu L) of the submicroliter size PCR reaction were transferred onto loading membranes and analyzed by ultrathin-layer gel electrophoresis which is a novel, high-performance and automated microseparation technique. This system employs integrated scanning laser-induced fluorescence-avalanche photodiode detection and ‘combines the advantages of conventional slab and capillary gel electrophoresis. Visualization of the DNA fragments was accomplished by “in migratio"" complexation with ethidium bromide during the electrophoresis process also enabling real time imaging and data analysis","['rapid microwell polymerase chain reaction', 'ultrathin-layer gel electrophoresis', 'DNA amplification', 'large-scale genotyping', 'expression profiling', 'rapid high-performance analysis', 'automated electrophoresis analysis', 'reagent consumption', 'sample loading robots', 'computerized real time data analysis', 'automated microseparation', 'integrated scanning LIF APD detection', 'complexation with ethidium bromide', 'real time imaging', 'biochemistry', 'biocontrol', 'biological techniques', 'DNA', 'electrochemical analysis', 'electrophoresis', 'physical instrumentation control', 'separation']","['ultrathin-layer gel electrophoresis', 'polymerase', 'mcdowell', 'rapid high-performance analysis', 'DNA amplification technique', 'chain', 'electrophoresis process', 'total reaction volume', 'electrophoresis', 'reaction']",1285,190,22,1287,189,10,9,3,8
"education, training and development policies and practices in medium-sized companies in the uk: do they really influence firm performance? this paper sets out to examine the relationship between training and firm performance in middle-sized uk companies. it recognises that there is evidence that ""high performance work practices"" appear to be associated with better performance in large us companies, but argues that this relationship is less likely to be present in middle-sized companies. the paper's key contribution is to justify the wider concept of education, training and development (etd) as applicable to such companies. it then finds that clusters of some etd variables do appear to be associated with better middle-sized company performance ","Education, training and development policies and practices in medium-sized companies in the UK: do they really influence firm performance? This paper sets out to examine the relationship between training and firm performance in middle-sized UK companies. It recognises that there is evidence that ""high performance work practices"" appear to be associated with better performance in large US companies, but argues that this relationship is less likely to be present in middle-sized companies. The paper's key contribution is to justify the wider concept of education, training and development (ETD) as applicable to such companies. It then finds that clusters of some ETD variables do appear to be associated with better middle-sized company performance","['medium-sized UK companies', 'training', 'firm performance', 'education', 'development policies', 'high performance work practices', 'ETD variable clusters', 'human resources', 'education', 'human resource management', 'management science', 'training']","['education training', 'firm performance', 'middle-sized UK companies', 'middle-sized companies', 'medium-sized companies', 'development policies', 'large US companies', 'better performance', 'such companies', 'company']",641,113,12,641,112,10,0,0,0
"national learning systems: a new approach on technological change in late industrializing economies and evidences from the cases of brazil and south korea the paper has two intertwined parts. the first one is a proposal for a conceptual and theoretical framework to understand technical change in late industrializing economies. the second part develops a kind of empirical test of the usefulness of that new framework by means of a comparative study of the brazilian and south korean cases. all the four types of macroevidences of the technical change processes of brazil and korea corroborate, directly or indirectly, the hypothesis of the existence of actual cases of national learning systems (nlss) of passive and active nature, as it is shown to be the cases of brazil and south korea, respectively. the contrast between the two processes of technical change prove remarkable, despite both processes being essentially confined to learning. the concepts of passive and active nlss show how useful they are to apprehend the diversity of those realities, and, consequently, to avoid, for instance, interpretations that misleadingly suppose (based on conventional economic theory) that those countries have a similar lack of technological dynamism ","National learning systems: a new approach on technological change in late industrializing economies and evidences from the cases of Brazil and South Korea The paper has two intertwined parts. The first one is a proposal for a conceptual and theoretical framework to understand technical change in late industrializing economies. The second part develops a kind of empirical test of the usefulness of that new framework by means of a ‘comparative study of the Brazilian and South Korean cases. All the four types of macroevidences of the technical change processes of Brazil and Korea corroborate, directly or indirectly, the hypothesis of the existence of actual cases of national learning systems (NLSs) of passive and active nature, as it is shown to be the cases of Brazil and South Korea, respectively. The contrast between the two processes of technical change prove remarkable, despite both processes being essentially confined to learning. The concepts of passive and active NLSs show how useful they are to apprehend the diversity of those realities, and, consequently, to avoid, for instance, interpretations that misleadingly suppose (based on conventional economic theory) that those countries have a similar lack of technological dynamism","['national learning systems', 'technological change', 'late industrializing economies', 'Brazil', 'South Korea', 'national innovation system', 'economic cybernetics', 'technological forecasting']","['late industrializing economies', 'national learning systems', 'technical change processes', 'technological change', 'South Korean cases', 'new framework', 'new approach', 'actual cases', 'change', 'technical change']",1058,193,8,1059,192,10,11,1,1
"enhanced product support through intelligent product manuals the scope of this paper is the provision of intelligent product support within the distributed intranet/internet environment. from the point of view of user requirements, the limitations of conventional product manuals and methods of authoring them are first outlined. it is argued that enhanced product support requires new technology solutions both for product manuals and for their authoring and presentation. the concept and the architecture of intelligent product manuals are then discussed. a prototype system called proartweb is presented to demonstrate advanced features of intelligent product manuals. next, the problem of producing such manuals in a cost-effective way is addressed and a concurrent engineering approach to their authoring is proposed. an integrated environment for collaborative authoring called proauthor is described to illustrate the approach suggested and to show how consistent, up-to-date and user-oriented-product manuals can be designed. the solutions presented here enable product knowledge to be captured and delivered to users and developers of product manuals when, where and in the form they need it ","Enhanced product support through intelligent product manuals The scope of this paper is the provision of intelligent product support within the distributed Intranet/Internet environment. From the point of view of user requirements, the limitations of conventional product manuals and methods of authoring them are first outlined. It is argued that enhanced product support requires new technology solutions both for product manuals and for their authoring and presentation. The concept and the architecture of intelligent product manuals are then discussed. A prototype system called ProARTWeb is presented to demonstrate advanced features of intelligent product manuals. Next, the problem of producing such manuals in a cost-effective way is addressed and a concurrent engineering approach to their authoring is proposed. An integrated environment for collaborative authoring called ProAuthor is described to illustrate the approach suggested and to show how consistent, up-to-date and user-oriented-product manuals can be designed. The solutions presented here enable product knowledge to be captured and delivered to users and developers of product manuals when, where and in the form they need it","['intelligent product support', 'intelligent product manuals', 'product manuals', 'ProARTWeb', 'concurrent engineering', 'product knowledge', 'technical information', 'concurrent engineering', 'electronic publishing']","['product', 'intelligent product manuals', 'product manuals', 'manuals', 'user-oriented-product manuals', 'conventional product manuals', 'intelligent product support', 'product knowledge', 'such manuals', 'product support']",1029,173,9,1029,172,10,0,0,3
"reproducibility of mammary gland structure during repeat setups in a supine position purpose: in breast conserving therapy, complete excision of the tumor with an acceptable cosmetic outcome depends on accurate localization in terms of both the position of the lesion and its extent. we hypothesize that preoperative contrast-enhanced magnetic resonance (mr) imaging of the patient in a supine position may be used for accurate tumor localization and marking of its extent immediately prior to surgery. our aims in this study are to assess the reproducibility of mammary gland structure during repeat setups in a supine position, to evaluate the effect of a breast immobilization device, and to derive reproducibility margins that take internal tissue shifts into account occurring between repeat setups. materials methods: the reproducibility of mammary gland structure during repeat setups in a supine position is estimated by quantification of tissue shifts in the breasts of healthy volunteers between repeat mr setups. for each volunteer fiducials are identified and registered with their counter locations in corresponding mr volumes. the difference in position denotes the shift of breast tissue. the dependence on breast volume and the part of the breast, as well as the effect of a breast immobilization cast are studied. results: the tissue shifts are small with a mean standard deviation on the order of 1.5 mm, being slightly larger in large breasts (v>1000 cm/sup 3/), and in the posterior part (toward the pectoral muscle) of both small and large breasts. the application of a breast immobilization cast reduces the tissue shifts in large breasts. a reproducibility margin on the order of 5 mm will take the internal tissue shifts into account that occur between repeat setups. conclusion: the results demonstrate a high reproducibility of mammary gland structure during repeat setups in a supine position ","Reproducibility of mammary gland structure during repeat setups in a supine position Purpose: In breast conserving therapy, complete excision of the tumor with an acceptable cosmetic outcome depends on accurate localization in terms of both the position of the lesion and its extent. We hypothesize that preoperative contrast-enhanced magnetic resonance (MR) imaging of the Patient in a supine position may be used for accurate tumor localization and marking of its extent immediately prior to surgery. Our aims in this study are to assess the reproducibility of mammary gland structure during repeat setups in a supine position, to evaluate the effect of a breast immobilization device, and to derive reproducibility margins that take internal tissue shifts into account occurring between repeat setups. Materials Methods: The reproducibility ‘of mammary gland structure during repeat setups in a supine position is estimated by quantification of tissue shifts in the breasts of healthy volunteers between repeat MR setups. For each volunteer fiducials are identified and registered with their counter locations in corresponding MR volumes. The difference in position denotes the shift of breast tissue. The dependence on breast volume and the part of the breast, as well as the effect of a breast immobilization cast are studied Results: The tissue shifts are small with a mean standard deviation on the order of 1.5 mm, being slightly larger in large breasts (V>1000 cm/sup 3/), and in the posterior part (toward the pectoral muscle) of both small and large breasts. The application of a breast immobilization cast reduces the tissue shifts in large breasts. A reproducibility margin on the order of 5 mm will take the internal tissue shifts into account that occur between repeat setups. Conclusion: The results demonstrate a high reproducibility of mammary gland structure during repeat setups in a supine position","['mammary gland structure reproducibility', 'repeat setups', 'supine position', 'breast conserving therapy', 'contrast-enhanced magnetic resonance imaging', 'accurate tumor localization', 'breast immobilization device', 'reproducibility margins', 'internal tissue shifts', 'localization methods', 'biomedical MRI', 'image registration', 'image segmentation', 'mammography', 'medical image processing', 'tumours']","['mammary gland structure', 'breast immobilization cast', 'breast immobilization device', 'repeat setups conclusion', 'supine position purpose', 'repeat MR setups', 'breast tissue', 'breast immobilization', 'repeat setups', 'supine position']",1623,298,16,1623,297,10,2,2,2
"efficient parallel programming on scalable shared memory systems with high performance fortran openmp offers a high-level interface for parallel programming on scalable shared memory (smp) architectures. it provides the user with simple work-sharing directives while it relies on the compiler to generate parallel programs based on thread parallelism. however, the lack of language features for exploiting data locality often results in poor performance since the non-uniform memory access times on scalable smp machines cannot be neglected. high performance fortran (hpf), the de-facto standard for data parallel programming, offers a rich set of data distribution directives in order to exploit data locality, but it has been mainly targeted towards distributed memory machines. in this paper we describe an optimized execution model for hpf programs on smp machines that avails itself with mechanisms provided by openmp for work sharing and thread parallelism, while exploiting data locality based on user-specified distribution directives. data locality does not only ensure that most memory accesses are close to the executing threads and are therefore faster, but it also minimizes synchronization overheads, especially in the case of unstructured reductions. the proposed shared memory execution model for hpf relies on a small set of language extensions, which resemble the openmp work-sharing features. these extensions, together with an optimized shared memory parallelization and execution model, have been implemented in the adaptor hpf compilation system and experimental results verify the efficiency of the chosen approach ","Efficient parallel programming on scalable shared memory systems with High Performance Fortran OpenMP offers a high-level interface for parallel programming on scalable shared memory (SMP) architectures. It provides the user with simple work-sharing directives while it relies on the compiler to generate parallel programs based on thread parallelism. However, the lack of language features for exploiting data locality often results in poor performance since the non-uniform memory access times on scalable SMP machines cannot be neglected. High Performance Fortran (HPF), the de-facto standard for data parallel programming, offers a rich set of data distribution directives in order to exploit data locality, but it has been mainly targeted towards distributed memory machines. In this, paper we describe an optimized execution model for HPF programs on SMP machines that avails itself with mechanisms provided by OpenMP for work sharing and thread parallelism, while exploiting data locality based on user-specified distribution directives. Data locality does not only ensure that most memory accesses are close to the executing threads and are therefore faster, but it also minimizes synchronization overheads, especially in the case of unstructured reductions. The proposed shared memory execution model for HPF relies on a small set of language extensions, which resemble the OpenMP work-sharing features. These extensions, together with an optimized shared memory parallelization and execution model, have been implemented in the ADAPTOR HPF compilation system and experimental results verity the efficiency of the chosen approach","['parallel programming', 'scalable shared memory', 'High Performance Fortran', 'multiprocessor architectures', 'scalable hardware', 'shared memory multiprocessor', 'FORTRAN', 'parallel programming', 'shared memory systems']","['parallel', 'thread parallelism', 'Efficient parallel programming', 'shared memory parallelisation', 'data distribution directives', 'memory execution model', 'most memory accesses', 'memory machines', 'HPF programs', 'parallel programming']",1403,236,9,1404,235,10,1,2,2
"automatic multilevel thresholding for image segmentation by the growing time adaptive self-organizing map in this paper, a growing tasom (time adaptive self-organizing map) network called ""gtasom"" along with a peak finding process is proposed for automatic multilevel thresholding. the proposed gtasom is tested for image segmentation. experimental results demonstrate that the gtasom is a reliable and accurate tool for image segmentation and its results outperform other thresholding methods ","Automatic multilevel thresholding for image segmentation by the growing time adaptive self-organizing map In this paper, a Growing TASOM (Time Adaptive Self-Organizing Map) network called ""GTASOM"" along with a peak finding process is proposed for automatic multilevel thresholding. The proposed GTASOM is tested for image segmentation. Experimental results demonstrate that the GTASOM is a reliable and accurate tool for image segmentation and its results ‘outperform other thresholding methods","['automatic multilevel thresholding', 'image segmentation', 'growing time adaptive self-organizing map', 'Growing TASOM', 'GTASOM', 'peak finding process', 'image segmentation', 'self-organising feature maps']","['image segmentation', 'automatic multilevel thresholding', 'adaptive self-organizing map', 'other thresholding methods', 'peak finding process', 'task time', 'thresholding', 'image', 'multilevel thresholding', 'segmentation']",426,69,8,427,68,10,10,1,2
"on optimality in auditory information processing we study limits for the detection and estimation of weak sinusoidal signals in the primary part of the mammalian auditory system using a stochastic fitzhugh-nagumo model and an action-recovery model for synaptic depression. our overall model covers the chain from a hair cell to a point just after the synaptic connection with a cell in the cochlear nucleus. the information processing performance of the system is evaluated using so-called phi -divergences from statistics that quantify ""dissimilarity"" between probability measures and are intimately related to a number of fundamental limits in statistics and information theory (it). we show that there exists a set of parameters that can optimize several important phi -divergences simultaneously and that this set corresponds to a constant quiescent firing rate (qfr) of the spiral ganglion neuron. the optimal value of the qfr is frequency dependent but is essentially independent of the amplitude of the signal (for small amplitudes). consequently, optimal processing according to several standard it criteria can be accomplished for this model if and only if the parameters are ""tuned"" to values that correspond to one and the same qfr. this offers a new explanation for the qfr and can provide new insight into the role played by several other parameters of the peripheral auditory system ","On optimality in auditory information processing We study limits for the detection and estimation of weak sinusoidal signals in the primary part of the mammalian auditory system using a stochastic Fitzhugh-Nagumo model and an action-recovery model for synaptic depression. Our overall model covers the chain from a hair cell toa Point just after the synaptic connection with a cell in the cochlear nucleus. The information processing performance of the system is evaluated using so-called phi -divergences from statistics that quantify ""dissimilarity"" between probability measures and are intimately related to a number of fundamental limits in statistics and information theory (IT). We show that there exists a set of parameters that can optimize several important phi -divergences simultaneously and that this set corresponds to a constant quiescent firing rate (QFR) of the spiral ganglion neuron. The optimal value of the QFR is frequency dependent but is essentially independent of the amplitude of the signal (for small amplitudes). Consequently, optimal processing according to several standard IT criteria can be accomplished for this model if and only if the parameters are ""tuned"" to values that correspond to one and the same QFR. This offers a new explanation for the QFR and can provide new insight into the role played by several other parameters of the peripheral auditory system","['weak sinusoidal signals', 'mammalian auditory system', 'stochastic Fitzhugh-Nagumo model', 'action-recovery model', 'peripheral auditory system', 'quiescent firing rate', 'spiral ganglion neuron', 'brain', 'brain models', 'dendrites', 'hearing', 'neural nets']","['information processing performance', 'stochastic Fitzhugh-Nagumo model', 'auditory information processing', 'peripheral auditory system', 'mammalian auditory system', 'several other parameters', 'weak sinusoidal signals', 'optimal processing', 'optimal value', 'optimal']",1181,217,12,1181,215,10,560,166,2
"an attack-finding algorithm for security protocols this paper proposes an automatic attack construction algorithm in order to find potential attacks on security protocols. it is based on a dynamic strand space model, which enhances the original strand space model by introducing active nodes on strands so as to characterize the dynamic procedure of protocol execution. with exact causal dependency relations between messages considered in the model, this algorithm can avoid state space explosion caused by asynchronous composition. in order to get a finite state space, a new method called strand-added on demand is exploited, which extends a bundle in an incremental manner without requiring explicit configuration of protocol execution parameters. a finer granularity model of term structure is also introduced, in which subterms are divided into check subterms and data subterms. moreover, data subterms can be further classified based on the compatible data subterm relation to obtain automatically the finite set of valid acceptable terms for an honest principal. in this algorithm, terms core is designed to represent the intruder's knowledge compactly, and forward search technology is used to simulate attack patterns easily. using this algorithm, a new attack on the dolve-yao protocol can be found, which is even more harmful because the secret is revealed before the session terminates ","An attack-finding algorithm for security protocols This paper proposes an automatic attack construction algorithm in order to find Potential attacks on security protocols. It is based on a dynamic strand space model, which enhances the original strand space model by introducing active nodes on strands so as to characterize the dynamic procedure of protocol execution. With exact causal dependency relations between messages considered in the model, this algorithm can avoid state space explosion caused by asynchronous composition. In order to get a finite state space, a new method called strand-added on demand is, exploited, which extends a bundle in an incremental manner without requiring explicit configuration of protocol execution parameters. A finer granularity model of term structure is also introduced, in which subterms are divided into check subterms and data subterms. Moreover, data subterms can be further classified based on the compatible data subterm relation to obtain automatically the finite set of valid acceptable terms for an honest principal. In this algorithm, terms core is designed to represent the intruder's knowledge compactly, and forward search technology is used to simulate attack patterns easily. Using this algorithm, a new attack on the Dolve-Yao protocol can be found, which is even more harmful because the secret is revealed before the session terminates","['attack-finding algorithm', 'security protocols', 'dynamic strand space model', 'strand space model', 'state space explosion', 'asynchronous composition', 'strand-added on demand', 'check subterms', 'data subterms', 'Dolve-Yao protocol', 'data structures', 'protocols', 'security of data']","['security protocols', 'protocol', 'protocol execution parameters', 'algorithm', 'attack-finding algorithm', 'algorithm terms core', 'finite state space', 'Dolve-Yao protocol', 'new attack', 'protocol execution']",1189,211,13,1190,210,10,0,1,4
"adaptive state feedback control for a class of linear systems with unknown bounds of uncertainties the problem of adaptive robust stabilization for a class of linear time-varying systems with disturbance and nonlinear uncertainties is considered. the bounds of the disturbance and uncertainties are assumed to be unknown, being even arbitrary. for such uncertain dynamical systems, the adaptive robust state feedback controller is obtained. and the resulting closed-loop systems are asymptotically stable in theory. moreover, an adaptive robust state feedback control scheme is given. the scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering. finally, simulations show that the control scheme is effective ","Adaptive state feedback control for a class of linear systems with unknown bounds of uncertainties The problem of adaptive robust stabilization for a class of linear time-varying systems with disturbance and nonlinear uncertainties is considered. The bounds of the disturbance and uncertainties are assumed to be unknown, being even arbitrary. For such uncertain dynamical systems, the adaptive robust state feedback controller is obtained. And the resulting closed-loop systems are asymptotically stable in theory. Moreover, an adaptive robust state feedback control scheme is given. The scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering. Finally, simulations show that the control scheme is effective","['robust stabilization', 'adaptive stabilization', 'linear time-varying systems', 'nonlinear uncertainties', 'closed-loop systems', 'uncertain dynamical systems', 'state feedback', 'adaptive controller', 'robust control', 'uncertain systems', 'adaptive systems', 'linear systems', 'robust control', 'state feedback', 'time-varying systems', 'uncertain systems']","['feedback', 'closed-loop systems', 'control scheme', 'state', 'systems', 'adaptive robust stabilization', 'linear time-varying systems', 'unknown bounds', 'linear systems', 'controller']",658,109,16,658,108,10,0,0,1
"operations that do not disturb partially known quantum states consider a situation in which a quantum system is secretly prepared in a state chosen from the known set of states. we present a principle that gives a definite distinction between the operations that preserve the states of the system and those that disturb the states. the principle is derived by alternately applying a fundamental property of classical signals and a fundamental property of quantum ones. the principle can be cast into a simple form by using a decomposition of the relevant hilbert space, which is uniquely determined by the set of possible states. the decomposition implies the classification of the degrees of freedom of the system into three parts depending on how they store the information on the initially chosen state: one storing it classically, one storing it nonclassically, and the other one storing no information. then the principle states that the nonclassical part is inaccessible and the classical part is read-only if we are to preserve the state of the system. from this principle, many types of no-cloning, no-broadcasting, and no-imprinting conditions can easily be derived in general forms including mixed states. it also gives a unified view on how various schemes of quantum cryptography work. the principle helps one to derive optimum amount of resources (bits, qubits, and ebits) required in data compression or in quantum teleportation of mixed-state ensembles ","Operations that do not disturb partially known quantum states Consider a situation in which a quantum system is secretly prepared in a state chosen from the known set of states. We present a principle that gives a definite distinction between the operations that preserve the states of the system and those that disturb the states. The principle is derived by alternately applying a fundamental property of classical signals and a fundamental property of quantum ones. The principle can be cast into a simple form by using a decomposition of the relevant Hilbert space, which is uniquely determined by the set of possible states. The decomposition implies the classification of the degrees of freedom of the system into three parts depending on how they store the information on the initially chosen state: one storing it classically, ‘one storing it nonclassically, and the other one storing no information. Then the principle states that the nonclassical part is, inaccessible and the classical part is read-only if we are to preserve the state of the system. From this principle, many types of no-cloning, no-broadcasting, and no-imprinting conditions can easily be derived in general forms including mixed states. It also gives a unified view on how various schemes of quantum cryptography work. The principle helps ‘one to derive optimum amount of resources (bits, qubits, and ebits) required in data compression or in quantum teleportation of mixed-state ensembles","['partially known quantum states', 'quantum system', 'secretly prepared quantum state', 'classical signals', 'Hilbert space', 'degrees of freedom', 'nonclassical part', 'quantum cryptography', 'bits', 'qubits', 'ebits', 'quantum teleportation', 'mixed-state ensembles', 'Hilbert spaces', 'quantum communication', 'quantum cryptography', 'quantum theory']","['principle', 'state', 'quantum', 'quantum cryptography work', 'quantum teleportation', 'possible states', 'quantum system', 'quantum states', 'quantum ones', 'mixed states']",1236,233,17,1239,232,10,6,3,8
"an intelligent information gathering method for dynamic information mediators the internet is spreading into our society rapidly and is becoming one of the information infrastructures that are indispensable for our daily life. in particular, the www is widely used for various purposes such as sharing personal information, academic research, business work, and electronic commerce, and the amount of available information is increasing rapidly. we usually utilize information sources on the internet as individual stand-alone sources, but if we can integrate them, we can add more value to each of them. hence, information mediators, which integrate information distributed on the internet, are drawing attention. in this paper, under the assumption that the information sources to be integrated are updated frequently and asynchronously, we propose an information gathering method that constructs an answer to a query from a user, accessing information sources to be integrated properly within an allowable time period. the proposed method considers the reliability of data in the cache and the quality of answer in order to efficiently access information sources and to provide appropriate answers to the user. as evaluation, we show the effectiveness of the proposed method by using an artificial information integration problem, in which some parameters can be modified, and a real-world flight information service compared with a conventional fifo information gathering method ","An intelligent information gathering method for dynamic information mediators The Internet is spreading into our society rapidly and is becoming one of the, information infrastructures that are indispensable for our daily life. In particular, the WWW is widely used for various purposes such as sharing personal information, academic research, business work, and electronic commerce, and the amount of available information is increasing rapidly. We usually utilize information sources on the Internet as individual stand-alone sources, but if we can integrate them, we can add more value to each of them. Hence, information mediators, which integrate information distributed on the Internet, are drawing attention. In this paper, under the assumption that the information sources to be integrated are updated frequently and asynchronously, we propose an information gathering method that constructs an answer to a query from a user, accessing information sources to be integrated properly within an allowable time period. The proposed method considers the reliability of data in the cache and the quality of answer in order to efficiently access information sources and to provide appropriate answers to the user. As evaluation, we show the effectiveness of the proposed method by using an artificial information integration problem, in which some parameters can be modified, and a real-world flight information service compared with a conventional FIFO information gathering method","['intelligent information gathering method', 'dynamic information mediators', 'Internet', 'information infrastructures', 'WWW', 'academic research', 'business work', 'electronic commerce', 'artificial information integration problem', 'real-world flight information service', 'information resources', 'Internet', 'software agents']","['information', 'information sources', 'information gathering method', 'individual stand-alone sources', 'dynamic information mediators', 'information infrastructures', 'available information', 'personal information', 'information mediators', 'method']",1265,219,13,1266,218,10,0,1,6
"the n-tier hub technology during 2001, the enterprise engineering laboratory at george mason university was contracted by the boeing company to develop an ehub capability for aerospace suppliers in taiwan. in a laboratory environment, the core technology was designed, developed, and tested, and now a large first-tier aerospace supplier in taiwan is commercializing the technology. the project objective was to provide layered network and application services for transporting xml-based business transaction flows across multi-tier, heterogeneous data processing environments. this paper documents the business scenario, the ehub application, and the network transport mechanisms that were used to build the n-tier hub. in contrast to most ehubs, this solution takes the point of view of suppliers, pushing data in accordance with supplier requirements; hence, enhancing the probability of supplier adoption. the unique contribution of this project is the development of an ehub that meets the needs of small and medium enterprises (smes) and first-tier suppliers ","The n-tier hub technology During 2001, the Enterprise Engineering Laboratory at George Mason University was contracted by the Boeing Company to develop an eHub capability for aerospace suppliers in Taiwan. In a laboratory environment, the core technology was designed, developed, and tested, and now a large first-tier aerospace supplier in Taiwan is commercializing the technology. The project objective was to provide layered network and application services for transporting XML-based business transaction flows across multi-tier, heterogeneous data processing environments. This paper documents the business scenario, the eHub application, and the network transport mechanisms that were used to build the n-tier hub. In contrast to most eHubs, this solution takes the point of view of suppliers, pushing data in accordance with supplier requirements; hence, enhancing the probability of supplier adoption. The unique contribution of this project is the development of an eHub that meets the needs of small and medium enterprises (SMEs) and first-tier suppliers","['n-tier hub technology', 'aerospace suppliers', 'Boeing Company', 'Taiwan', 'XML-based business transaction flows', 'multi-tier heterogeneous data processing environments', 'business scenario', 'network transport mechanisms', 'supplier adoption', 'small and medium enterprises', 'first-tier suppliers', 'aerospace industry', 'electronic commerce', 'hypermedia markup languages', 'transaction processing']","['hub', 'Enterprise Engineering Laboratory', 'George Mason University', 'first-timer suppliers', 'tier hub technology', 'aerospace suppliers', 'supplier adoption', 'hub application', 'hub capability', 'tier hub']",912,154,15,912,153,10,0,0,6
"process specialization: defining specialization for state diagrams a precise definition of specialization and inheritance promises to be as useful in organizational process modeling as it is in object modeling. it would help us better understand, maintain, reuse, and generate process models. however, even though object-oriented analysis and design methodologies take full advantage of the object specialization hierarchy, the process specialization hierarchy is not supported in major process representations, such as the state diagram, data flow diagram, and uml representations. partly underlying this lack of support is an implicit assumption that we can always specialize a process by treating it as ""just another object."" we argue in this paper that this is not so straightforward as it might seem; we argue that a process-specific approach must be developed. we propose such an approach in the form of a set of transformations which, when applied to a process description, always result in specialization. we illustrate this approach by applying it to the state diagram representation and demonstrate that this approach to process specialization is not only theoretically possible, but shows promise as a method for categorizing and analyzing processes. we point out apparent inconsistencies between our notion of process specialization and existing work on object specialization but show that these inconsistencies are superficial and that the definition we provide is compatible with the traditional notion of specialization ","Process specialization: defining specialization for state diagrams A precise definition of specialization and inheritance promises to be as useful in organizational process modeling as it is in object modeling. It would help us better understand, maintain, reuse, and generate process models. However, even though object-oriented analysis and design methodologies take full advantage of the object specialization hierarchy, the process specialization hierarchy is not supported in major process representations, such as the state diagram, data flow diagram, and UML representations. Partly underlying this lack of support is an implicit assumption that we can always specialize a process by treating it as ""just another object."" We argue in this paper that this is not so straightforward as it might seem; we argue that a process-specific approach must be developed. We propose such an approach in the form of a set of transformations which, when applied to a process description, always result in specialization. We illustrate this approach by applying it to the state diagram representation and demonstrate that this approach to process specialization is not only theoretically possible, but shows promise as a method for categorizing and analyzing processes. We point out apparent inconsistencies between ‘ur notion of process specialization and existing work on object specialization but show that these inconsistencies are superficial and that the definition we provide is compatible with the traditional notion of specialization","['process specialization', 'state diagrams', 'inheritance', 'organizational process modeling', 'object-oriented analysis', 'object specialization hierarchy', 'process representation', 'object-oriented design', 'business data processing', 'corporate modelling', 'diagrams', 'inheritance', 'object-oriented methods']","['specialization', 'process specialization hierarchy', 'organizational process modelling', 'object specialization hierarchy', 'major process representations', 'state diagram representation', 'process description', 'process models', 'object specialization', 'process specialization']",1310,226,13,1310,225,10,1,1,4
"optimal online algorithm for scheduling on two identical machines with machine availability constraints this paper considers the online scheduling on two identical machines with machine availability constraints for minimizing makespan. we assume that machine m/sub j/ is unavailable during period from s/sub j/ to t/sub j/ (0 <or= s/sub j/ < t/sub j/), j = 1, 2, and the unavailable periods of two machines do not overlap. we show that the competitive ratio of list scheduling is 3. we further give an optimal algorithm with a competitive ratio 5/2 ","Optimal online algorithm for scheduling on two identical machines with machine availability constraints This paper considers the online scheduling on two identical machines with machine availability constraints for minimizing makespan. We assume that machine M/sub j is unavailable during period from s/sub j/ to tusub j/ (0 <or= s/sub j/ < Usub j/), j= 1, 2, and the unavailable periods of two machines do not overlap. We show that the competitive ratio of list scheduling is 3. We further give an optimal algorithm with a competitive ratio 5/2","['optimal online algorithm', 'makespan minimisation', 'list scheduling', 'identical machines scheduling', 'machine availability constraints', 'processor scheduling']","['sub j.', 'machine availability constraints', 'identical machines', 'Optimal online algorithm', 'optimal algorithm', 'online scheduling', 'list scheduling', 'machine sub j', 'scheduling', 'machines']",460,90,6,458,88,10,96,38,0
"stability of w-methods with applications to operator splitting and to geometric theory we analyze the stability properties of w-methods applied to the parabolic initial value problem u' + au = bu. we work in an abstract banach space setting, assuming that a is the generator of an analytic semigroup and that b is relatively bounded with respect to a. since w-methods treat the term with a implicitly, whereas the term involving b is discretized in an explicit way, they can be regarded as splitting methods. as an application of our stability results, convergence for nonsmooth initial data is shown. moreover, the layout of a geometric theory for discretizations of semilinear parabolic problems u' + au = f (u) is presented ","Stability of W-methods with applications to operator splitting and to geometric theory We analyze the stability properties of W-methods applied to the parabolic initial value problem u' + Au = Bu. We work in an abstract Banach space setting, assuming that A is the generator of an analytic semigroup and that B is relatively bounded with respect to A. Since W-methods treat the term with A implicitly, whereas the term involving B is discretized in an explicit way, they can be regarded as splitting methods. As an application of our stability results, convergence for nonsmooth initial data is shown. Moreover, the layout of a geometric theory for discretizations of semilinear parabolic problems u' + Au =f (u) is presented","['W-methods stability', 'operator splitting', 'geometric theory', 'parabolic initial value problem', 'abstract Banach space', 'analytic semigroup', 'nonsmooth initial data', 'linearly implicit Runge-Kutta methods', 'Banach spaces', 'initial value problems', 'Runge-Kutta methods', 'stability']","['geometric theory', 'application', 'semilinear parabolic problems', 'nonsmooth initial data', 'stability properties', 'operator splitting', 'splitting methods', 'splitting', 'methods', 'Stability']",607,121,12,607,119,10,5,4,2
"turning telecommunications call details to churn prediction: a data mining approach as deregulation, new technologies, and new competitors open up the mobile telecommunications industry, churn prediction and management has become of great concern to mobile service providers. a mobile service provider wishing to retain its subscribers needs to be able to predict which of them may be at-risk of changing services and will make those subscribers the focus of customer retention efforts. in response to the limitations of existing churn-prediction systems and the unavailability of customer demographics in the mobile telecommunications provider investigated, we propose, design, and experimentally evaluate a churn-prediction technique that predicts churning from subscriber contractual information and call pattern changes extracted from call details. this proposed technique is capable of identifying potential churners at the contract level for a specific prediction time-period. in addition, the proposed technique incorporates the multi-classifier class-combiner approach to address the challenge of a highly skewed class distribution between churners and non-churners. the empirical evaluation results suggest that the proposed call-behavior-based churn-prediction technique exhibits satisfactory predictive effectiveness when more recent call details are employed for the churn prediction model construction. furthermore, the proposed technique is able to demonstrate satisfactory or reasonable predictive power within the one-month interval between model construction and churn prediction. using a previous demographics-based churn-prediction system as a reference, the lift factors attained by our proposed technique appear largely satisfactory ","Turning telecommunications call details to churn prediction: a data mining approach As deregulation, new technologies, and new competitors open up the mobile telecommunications industry, churn prediction and management has become of great concern to mobile service providers. A mobile service provider wishing to retain its subscribers needs to be able to predict which of them may be at-risk of changing services and will make those subscribers the focus of customer retention efforts. In response to the limitations of existing churn-prediction systems and the unavailability of customer demographics in the mobile telecommunications provider investigated, we propose, design, and experimentally evaluate a churn-prediction technique that predicts churning from subscriber contractual information and call pattern changes extracted from call details. This proposed technique is capable of identifying potential churners at the contract level for a specific prediction time-period In addition, the proposed technique incorporates the multi-classifier class-combiner approach to address the challenge of a highly skewed class distribution between churners and non-churners. The empirical evaluation results suggest that the proposed call-behavior-based churn-prediction technique exhibits satisfactory predictive effectiveness when more recent call details are employed for the churn prediction model construction. Furthermore, the proposed technique is able to demonstrate satisfactory or reasonable predictive power within the one-month interval between model construction and churn prediction. Using a previous demographics-based churn-prediction system as a reference, the lift factors attained by our proposed technique appear largely satisfactory","['telecommunications call details', 'mobile telecommunications industry', 'mobile service providers', 'deregulation', 'customer retention efforts', 'customer demographics', 'subscriber contractual information', 'call pattern changes', 'multi-classifier class-combiner approach', 'skewed class distribution', 'lift factors', 'decision tree induction', 'data mining', 'decision trees', 'demography', 'mobile communication', 'telecommunication computing', 'very large databases']","['call-behavior-based churn-prediction technique', 'satisfactory predictive effectiveness', 'mobile telecommunications provider', 'mobile telecommunications industry', 'specific prediction time-period', 'prediction model construction', 'reasonable predictive power', 'mobile service providers', 'recent call details', 'churn-prediction technique']",1524,231,18,1523,230,10,0,1,8
"secrets of the glasgow haskell compiler inliner higher-order languages such as haskell encourage the programmer to build abstractions by composing functions. a good compiler must inline many of these calls to recover an efficiently executable program. in principle, inlining is dead simple: just replace the call of a function by an instance of its body. but any compiler-writer will tell you that inlining is a black art, full of delicate compromises that work together to give good performance without unnecessary code bloat. the purpose of this paper is, therefore, to articulate the key lessons we learned from a full-scale ""production"" inliner, the one used in the glasgow haskell compiler. we focus mainly on the algorithmic aspects, but we also provide some indicative measurements to substantiate the importance of various aspects of the inliner ","Secrets of the Glasgow Haskell compiler inliner Higher-order languages such as Haskell encourage the programmer to build abstractions by composing functions. A good compiler must inline many of these calls to recover an efficiently executable program. In principle, inlining is dead simple: just replace the call of a function by an instance of its body. But any compiler-writer will tell you that inlining is a black art, full of delicate compromises that work together to give good performance without unnecessary code bloat. The purpose of this paper is, therefore, to articulate the key lessons we leamed from a full-scale ""production"" inliner, the one used in the Glasgow Haskell compiler. We focus mainly on the algorithmic aspects, but we also provide some indicative measurements to substantiate the importance of various aspects of the inliner","['Glasgow Haskell compiler inliner', 'higher-order languages', 'functional programming', 'abstractions', 'executable program', 'performance', 'algorithmic aspects', 'functional language', 'optimising compiler', 'functional languages', 'functional programming', 'optimising compilers']","['Glasgow Haskell compiler', 'full-scale production inliner', 'inner Higher-order languages', 'algorithmic aspects', 'various aspects', 'good compiler', 'Secrets', 'compiler', 'Haskell', 'Glasgow']",721,134,12,720,133,10,3,1,3
"visual-word identification thresholds for the 260 fragmented words of the snodgrass and vanderwart pictures in spanish word difficulty varies from language to language; therefore, normative data of verbal stimuli cannot be imported directly from another language. we present mean identification thresholds for the 260 screen-fragmented words corresponding to the total set of snodgrass and vanderwart (1980) pictures. individual words were fragmented in eight levels using turbo pascal, and the resulting program was implemented on a pc microcomputer. the words were presented individually to a group of 40 spanish observers, using a controlled time procedure. an unspecific learning effect was found showing that performance improved due to practice with the task. finally, of the 11 psycholinguistic variables that previous researchers have shown to affect word identification, only imagery accounted for a significant amount of variance in the threshold values ","Visual-word identification thresholds for the 260 fragmented words of the Snodgrass and Vanderwart pictures in Spanish Word difficulty varies from language to language; therefore, normative data of verbal stimuli cannot be imported directly from another language. We present mean identification thresholds for the 260 screen-fragmented words corresponding to the total set of Snodgrass and Vanderwart (1980) pictures. Individual words were fragmented in eight levels using Turbo Pascal, and the resulting program was implemented on a PC microcomputer. The words were presented individually to a group of 40 Spanish observers, using a controlled time procedure. An unspecific learning effect was found showing that performance improved due to practice with the task. Finally, of the 11 psycholinguistic variables that previous researchers have shown to affect word identification, only imagery accounted for a significant amount of variance in the threshold values","['visual-word identification thresholds', 'fragmented words', 'Snodgrass and Vanderwart pictures', 'Spanish', 'word difficulty', 'verbal stimuli', 'mean identification thresholds', 'screen-fragmented words', 'Turbo Pascal', 'PC microcomputer', 'controlled time procedure', 'unspecific learning effect', 'psycholinguistic variables', 'word identification', 'linguistics', 'psychology']","['word', 'Visual-word identification thresholds', 'screen-fragmented words', 'Vanderwart pictures', 'threshold values', 'Individual words', 'identification', 'fragmented words', 'word identification', 'identification thresholds']",826,139,16,826,138,10,0,0,7
"implementation of dimsims for stiff differential systems some issues related to the implementation of diagonally implicit multistage integration methods for stiff differential systems are discussed. they include reliable estimation of the local discretization error, construction of continuous interpolants, solution of nonlinear systems of equations by simplified newton iterations, choice of initial stepsize and order, and step and order changing strategy. numerical results are presented which indicate that an experimental matlab code based on type 2 methods of order one, two and three outperforms ode15s code from matlab ode suite on problems whose jacobian has eigenvalues which are close to the imaginary axis ","Implementation of DIMSIMs for stiff differential systems ‘Some issues related to the implementation of diagonally implicit multistage integration methods for stiff differential systems are discussed. They include reliable estimation of the local discretization error, construction of continuous interpolants, solution of nonlinear systems of equations by simplified Newton iterations, choice of initial stepsize and order, and step and order changing strategy. Numerical results are presented which indicate that an experimental Matlab code based on type 2 methods of order one, two and three outperforms ode15s code from Matlab ODE suite on problems whose Jacobian has eigenvalues which are close to the imaginary axis","['DIMSIMs', 'stiff differential systems', 'reliable estimation', 'local discretization error', 'interpolants', 'nonlinear systems of equations', 'simplified Newton iterations', 'experimental Matlab code', 'diagonally implicit multistage integration methods', 'difference equations', 'eigenvalues and eigenfunctions', 'matrix algebra', 'stability']","['stiff differential systems', 'Implementation', 'methods', 'Newton iterations choice', 'reliable estimation', 'nonlinear systems', 'issues', 'systems', 'stiff', 'differential']",617,103,13,618,102,10,4,1,4
"robust l/sub 2/ disturbance attenuation for nonlinear systems with input dynamical uncertainty deals with the problem of robust l/sub 2/ disturbance attenuation for nonlinear systems with input dynamical uncertainty. the input dynamical uncertainty is restricted to be minimum-phase and relative degree zero. a sufficient condition. is given such that the nonlinear system satisfies the l/sub 2/ gain performance and input-to-state stable property. using this condition, a design approach is given for a smooth state feedback control law that solves the robust l/sub 2/ disturbance attenuation problem, and the approach is extended to a more general case where the nominal system has higher relative degree. finally, a numerical example is given to demonstrate the proposed approach ","Robust L/sub 2/ disturbance attenuation for nonlinear systems with input ‘dynamical uncertainty Deals with the problem of robust L/sub 2/ disturbance attenuation for nonlinear systems with input dynamical uncertainty. The input dynamical uncertainty is restricted to be minimum-phase and relative degree zero. Assufficient condition. is given such that the nonlinear system satisfies the L/sub 2/ gain performance and input-to-state stable property. Using this condition, a design approach is given for a smooth state feedback control law that solves the robust L/sub 2/ disturbance attenuation problem, and the approach is extended to a more general ‘case where the nominal system has higher relative degree. Finally, a numerical example is given to demonstrate the proposed approach","['robust L/sub 2/ disturbance attenuation', 'nonlinear systems', 'input dynamical uncertainty', 'sufficient condition', 'L/sub 2/ gain performance', 'input-to-state stable property', 'design approach', 'smooth state feedback control law', 'nominal system', 'robust control', 'control system synthesis', 'nonlinear control systems', 'robust control', 'state feedback', 'uncertain systems']","['nonlinear systems', 'r', 'disturbance attenuation problem', 'dynamical uncertainty Deals', 'sub', 'sub r', 'dynamical uncertainty', 'disturbance attenuation', 'robust sub r', 'Robust sub r']",668,116,15,671,114,10,252,73,7
"traffic engineering with traditional ip routing protocols traffic engineering involves adapting the routing of traffic to network conditions, with the joint goals of good user performance and efficient use of network resources. we describe an approach to intradomain traffic engineering that works within the existing deployed base of interior gateway protocols, such as open shortest path first and intermediate system-intermediate system. we explain how to adapt the configuration of link weights, based on a networkwide view of the traffic and topology within a domain. in addition, we summarize the results of several studies of techniques for optimizing ospf/is-is weights to the prevailing traffic. the article argues that traditional shortest path routing protocols are surprisingly effective for engineering the flow of traffic in large ip networks ","Traffic engineering with traditional IP routing protocols Traffic engineering involves adapting the routing of traffic to network conditions, with the joint goals of good user performance and efficient se of network resources. We describe an approach to intradomain traffic engineering that works within the existing deployed base of interior gateway protocols, such as Open Shortest Path First and Intermediate System-Intermediate System. We explain how to adapt the configuration of link weights, based on a networkwide view of the traffic and topology within a domain. In addition, we summarize the results of several studies of techniques for optimizing OSPF/IS-IS weights to the prevailing traffic. The article argues that traditional shortest path routing protocols are surprisingly effective for engineering the flow of traffic in large IP networks","['IP routing protocols', 'interior gateway protocols', 'link weights configuration', 'traffic routing', 'network conditions', 'user performance', 'network resources', 'intradomain traffic engineering', 'network topology', 'OSPF/IS-IS weights', 'shortest path routing protocols', 'IP networks', 'TCP', 'transmission control protocol', 'Open Shortest Path First protocol', 'Intermediate System-Intermediate System protocol', 'Internet', 'internetworking', 'network topology', 'optimisation', 'telecommunication network routing', 'telecommunication traffic', 'transport protocols']","['traffic', 'intradomain traffic engineering', 'protocols Traffic engineering', 'interior gateway protocols', 'traditional shortest path', 'network conditions', 'large IP networks', 'traditional IP', 'engineering', 'Traffic engineering']",732,126,23,731,125,10,2,1,2
"transmission of real-time video over ip differentiated services multimedia applications require high bandwidth and guaranteed quality of service (qos). the current internet, which provides 'best effort' services, cannot meet the stringent qos requirements for delivering mpeg videos. it is proposed that mpeg frames are transported through various service models of diffserv. performance analysis and simulation results show that the proposed approach can not only guarantee qos but can also achieve high bandwidth utilisation ","Transmission of real-time video over IP differentiated services Multimedia applications require high bandwidth and guaranteed quality of service (QoS). The current Internet, which provides ‘best effort’ services, cannot meet the stringent QoS requirements for delivering MPEG videos. It is proposed that MPEG frames are transported through various service models of DiffServ. Performance analysis and simulation results show that the proposed approach can not only guarantee QoS but can also achieve high bandwidth utilisation","['IP differentiated services', 'real-time video transmission', 'multimedia applications', 'quality of service', 'QoS guarantees', 'Internet', 'MPEG video', 'DiffServ', 'high bandwidth utilisation', 'Internet', 'multimedia communication', 'quality of service', 'visual communication']","['services Multimedia applications', 'stringent cos requirements', 'high bandwidth utilisation', 'various service models', 'real-time video', 'MPEG videos', 'MPEG frames', 'video', 'services', 'high bandwidth']",454,74,13,454,73,10,2,2,3
"resolving web user on the fly identity authentication systems and procedures are rapidly becoming central issues in the practice and study of information systems development and security. requirements for web transaction security (wts) include strong authentication of a user, non-repudiation and encryption of all traffic. in this paper, we present an effective mechanism involving two different channels, which addresses the prime concerns involved in the security of electronic commerce transactions (ect) viz. user authentication and non-repudiation. although the product is primarily targeted to provide a fillip to transactions carried out over the web, this product can also be effectively used for non-internet transactions that are carried out where user authentication is required ","Resolving Web user on the fly Identity authentication systems and procedures are rapidly becoming central issues in the practice and study of information systems development and security. Requirements for Web transaction security (WTS) include strong authentication of a user, non-repudiation and encryption of all traffic. In this paper, we present an effective mechanism involving two different channels, which addresses the prime concerns involved in the security of electronic commerce transactions (ECT) viz. user authentication and non-repudiation. Although the product is primarily targeted to provide a filip to transactions carried out over the Web, this product can also be effectively used for non-Internet transactions, that are carried out where user authentication is required","['identity authentication systems', 'information systems development', 'information systems security', 'Web transaction security', 'nonrepudiation', 'encryption', 'traffic', 'electronic commerce transactions', 'cryptography', 'electronic commerce', 'Internet', 'message authentication', 'transaction processing']","['user authentication', 'electronic commerce transactions', 'information systems development', 'non-interest transactions', 'Web transaction security', 'strong authentication', 'security Requirements', 'Resolving Web user', 'users', 'authentication']",679,113,13,679,112,10,2,2,3
"dual nature of mass multi-agent systems dual nature of mass multi-agent systems (mmas) emerging as an internal discord of two spheres - micro (virtual) consisting of agents and their internal phenomena, and macro arising at the interface to the real world $stems the necessity of a new approach to analysis, design and utilisation of such systems. based on the concept of vr decomposition, the problem of management of such systems is discussed. as a sub-type that makes mmas closer to the application sphere, an evolutionary multi-agent system (emas) is proposed. emas combines features of mmas with advantages of an evolutionary model of computation. as an illustration of this consideration two particular emas are presented, which allow us to obtain promising results in the fields of multiobjective optimisation and time-series prediction, and thus justify the approach ","Dual nature of mass multi-agent systems, Dual nature of mass multi-agent systems (mMAS) emerging as an internal discord of two spheres - micro (virtual) consisting of agents and their internal phenomena, and macro arising at the interface to the real world Sstems the necessity of a new approach to analysis, design and utilisation of such systems. Based on the concept of VR decomposition, the problem of management of such systems is discussed. As a sub-type that makes mMAS closer to the application sphere, an evolutionary multi-agent system (EMAS) is proposed. EMAS combines features of mMAS. with advantages of an evolutionary model of computation. As an illustration of this consideration two particular EMAS are presented, Which allow us to obtain promising results in the fields of multiobjective optimisation and time-series prediction, and thus justify the approach","['mass multiple agent systems', 'VR decomposition', 'virtual reality', 'multiobjective optimisation', 'time-series prediction', 'micro-macro link', 'formal model', 'evolutionary multiple agent system', 'forecasting theory', 'multi-agent systems', 'optimisation', 'time series', 'virtual reality']","['multi-agency systems', 'mass', 'such systems', 'Dual nature', 'systems', 'real world systems', 'internal discord', 'Dual', 'nature', 'multi-agency']",740,136,13,742,135,10,1,3,4
"well behaved women rarely make history! the author considers women in the history of computer science. prior to the eniac, women were extremely important to the computing business as ""computers"". just as women had taken over the tasks as secretaries in the late 1800s with the advent of the typewriter, and in the early 1900s staffing telephone exchanges, so computing relied on women as the ""workhorses"" of the business ","Well behaved women rarely make history! The author considers women in the history of computer science. Prior to the ENIAC, women were extremely important to the computing business as “computers”. Just as women had taken over the tasks as secretaries in the late 1800s with the advent of the typewriter, and in the early 1900s staffing telephone exchanges, So computing relied on women as the ""workhorses"" of the business","['women', 'history', 'computer science', 'ENIAC', 'business', 'gender issues', 'gender issues', 'history', 'social aspects of automation']","['women', 'history', 'computing business', 'computer science', 'late 1800s', 'important', 'author', 'science', 'computer', 'business']",352,70,9,352,69,10,2,1,1
digital-domain self-calibration technique for video-rate pipeline a/d converters using gaussian white noise a digital-domain self-calibration technique for video-rate pipeline a/d converters based on a gaussian white noise input signal is presented. the proposed algorithm is simple and efficient. a design example is shown to illustrate that the overall linearity of a pipeline adc can be highly improved using this technique ,Digital-domain self-calibration technique for video-rate pipeline AD converters using Gaussian white noise A digital-domain self-calibration technique for video-rate pipeline AD converters based on a Gaussian white noise input signal is presented. The proposed algorithm is simple and efficient. A design example is shown to illustrate that the overall linearity of a pipeline ADC can be highly improved using this technique,"['digital-domain self-calibration technique', 'video-rate pipeline A/D converters', 'Gaussian white noise input signal', 'pipeline ADC linearity', 'analogue-digital conversion', 'calibration', 'Gaussian noise', 'pipeline processing', 'white noise']","['Gaussian white noise', 'video-tape', 'converters', 'pipeline ADC', 'AD', 'technique', 'pipeline', 'self-calibration', 'digital-domain self-calibration technique', 'Digital-domain self-calibration technique']",367,61,9,365,60,10,2,2,1
automation of the recovery of efficiency of complex structure systems basic features are set forth of the method for automation of the serviceability recovery of systems of complex structures in real time without the interruption of operation. specific features of the method are revealed in an important example of the system of control of hardware components of ships ,"Automation of the recovery of efficiency of complex structure systems, Basic features are set forth of the method for automation of the serviceability recovery of systems of complex structures in real time without the interruption of operation. Specific features of the method are revealed in an important example of the system of control of hardware components of ships","['efficiency recovery', 'serviceability recovery', 'complex structure systems', 'ships', 'hardware components', 'boilers', 'fault tolerance', 'graph theory', 'large-scale systems', 'ships']","['complex structure systems', 'serviceability recovery', 'complex structures', 'Specific features', 'Basic features', 'real time', 'structure', 'complex', 'system', 'recovery']",312,59,10,313,58,10,0,1,3
"it as a key enabler to law firm competitiveness professional services firms have traditionally been able to thrive in virtually any market conditions. they have been consistently successful for several decades without ever needing to reexamine or change their basic operating model. however, gradual but inexorable change in client expectations and the business environment over recent years now means that more of the same is no longer enough. in future, law firms will increasingly need to exploit it more effectively in order to remain competitive. to do this, they will need to ensure that all their information systems function as an integrated whole and are available to their staff, clients and business partners. the authors set out the lessons to be learned for law firms in the light of the recent pa consulting survey ","IT as a key enabler to law firm competitiveness Professional services firms have traditionally been able to thrive in virtually any market conditions. They have been consistently successful for several decades without ever needing to reexamine or change their basic operating model. However, gradual but inexorable change in client expectations and the business environment over recent years now means that more of the same is no longer enough. In future, law firms will increasingly need to exploit IT more effectively in order to remain ‘competitive. To do this, they will need to ensure that all their information systems function as an integrated whole and are available to their staff, clients and business partners. The authors set out the lessons to be learned for law firms in the light of the recent PA Consulting survey","['professional services firms', 'client expectations', 'business environment', 'information systems', 'law firms', 'information technology', 'legislation', 'professional aspects']","['Professional services firms', 'law firm competitiveness', 'basic operating model', 'future law firms', 'key enabler', 'firms', 'law', 'key', 'law firms', 'competitiveness']",695,135,8,696,134,10,12,1,3
"business data management for business-to-business electronic commerce business-to-business electronic commerce (b2b ec) opens up new possibilities for trade. for example, new business partners from around the globe can be found, their offers can be compared, even complex negotiations can be conducted electronically, and a contract can be drawn up and fulfilled via an electronic marketplace. however, sophisticated data management is required to provide such facilities. in this paper, the results of a multi-national project on creating a business-to-business electronic marketplace for small and medium-sized enterprises are presented. tools for information discovery, protocol-based negotiations, and monitored contract enactment are provided and based on a business data repository. the repository integrates heterogeneous business data with business communication. specific problems such as multilingual nature, data ownership, and traceability of contracts and related negotiations are addressed and it is shown that the present approach provides efficient business data management for b2b ec ","Business data management for business-to-business electronic commerce Business-to-business electronic commerce (B2B EC) opens up new possibilities for trade. For example, new business partners from around the globe can be found, their offers can be compared, even complex negotiations can be conducted electronically, and a contract can be drawn up and tuffiled via an electronic marketplace. However, sophisticated data management is required to provide such facilities. In this paper, the results of a multi-national project on creating a business-to-business electronic marketplace for small and medium-sized enterprises are presented. Tools for information discovery, protocol-based Negotiations, and monitored contract enactment are provided and based ‘on a business data repository. The repository integrates heterogeneous business data with business communication. Specific problems such as. multilingual nature, data ownership, and traceability of contracts and related negotiations are addressed and it is shown that the present approach provides efficient business data management for B2B EC","['business-to-business electronic commerce', 'business data management', 'electronic marketplace', 'small and medium-sized enterprises', 'multi-national project', 'information discovery', 'protocol-based negotiations', 'monitored contract enactment', 'business data repository', 'heterogeneous business data', 'business communication', 'data ownership', 'multilingual system', 'traceability', 'business communication', 'contracts', 'electronic commerce']","['Business data management', 'electronic marketplace', 'electronic commerce Business-to-business', 'data', 'sophisticated data management', 'heterogeneous business data', 'electronic commerce bob', 'business communication', 'new business partners', 'business data']",953,149,17,954,148,10,6,3,7
"girls, boys, and computers today north american girls, boys, teachers, and parents frequently regard computer science and programming as something boys are better at. the author considers how many of the factors that contribute to the low participation of women in computing occur first, and perhaps most forcefully, in childhood. she presents four recommendations to address the situation ","Girls, boys, and computers Today North American girls, boys, teachers, and parents frequently regard ‘computer science and programming as something boys are better at. The author considers how many of the factors that contribute to the low Participation of women in computing occur first, and perhaps most forcefully, in childhood. She presents four recommendations to address the situation","['girls', 'teachers', 'computer science', 'programming', 'women', 'childhood', 'gender issues', 'boys', 'computer science education', 'gender issues', 'social aspects of automation']","['girls boys', 'low Participation', 'computer science', 'something boys', 'American', 'Today', 'North', 'boys', 'computer', 'girls']",332,59,11,333,58,10,8,1,4
"max and min limiters if a contained in omega , n>or=2, and the function max({x/sub 1/,...,x/sub n/} intersection a) is partial recursive, it is easily seen that a is recursive. in this paper, we weaken this hypothesis in various ways (and similarly for ""min"" in place of ""max"") and investigate what effect this has on the complexity of a. we discover a sharp contrast between retraceable and co-retraceable sets, and we characterize sets which are the union of a recursive set and a co-r.e., retraceable set. most of our proofs are noneffective. several open questions are raised ","Max and min limiters, ITA contained in omega , n>or=2, and the function max({x/sub 1/,....x/Sub 1} intersection A) is partial recursive, it is easily seen that A is recursive. In this paper, we weaken this hypothesis in various ways (and similarly for ""min"" in place of ""max"") and investigate what effect this has on the complexity of A. We discover a sharp contrast between retraceable and co-retraceable sets, and we characterize sets which are the union of a recursive set and a co-r.e., retraceable set. Most of ur proofs are noneffective. Several open questions are raised","['min limiters', 'max limiters', 'complexity', 'retraceable sets', 'recursive set', 'computational complexity', 'formal logic']","['n', 'min limiters ITA', 'retractable set', 'recursive set', 'function max', 'omega', 'Max', 'ITA', 'min', 'limiters']",483,98,7,482,96,10,256,93,2
"data mining efforts increase business productivity and efficiency the use and acquisition of information is a key part of the way any business makes money. data mining technologies provide greater insight into how this information can be better used and more effectively acquired. steven kudyba, an expert in the field of data mining technologies, shares his expertise in an interview ","Data mining efforts increase business productivity and efficiency The use and acquisition of information is a key part of the way any business makes money. Data mining technologies provide greater insight into how this information can be better used and more effectively acquired. Steven Kudyba, an expert in the field of data mining technologies, shares his expertise in an interview","['data mining', 'productivity', 'efficiency', 'data mining']","['technologies', 'information', 'business productivity', 'greater insight', 'Steven Kudyba', 'money Data', 'key part', 'Data', 'business', 'productivity']",325,61,4,325,60,10,0,0,1
"information architecture without internal theory: an inductive design process this article suggests that information architecture (ia) design is primarily an inductive process. although top-level goals, user attributes and available content are periodically considered, the process involves bottom-up design activities. ia is inductive partly because it lacks internal theory, and partly because it is an activity that supports emergent phenomena (user experiences) from basic design components. the nature of ia design is well described by constructive induction (ci), a design process that involves locating the best representational framework for the design problem, identifying a solution within that framework and translating it back to the design problem at hand. the future of ia, if it remains inductive or develops a body of theory (or both), is considered ","Information architecture without internal theory: an inductive design process This article suggests that Information Architecture (IA) design is primarily an inductive process. Although top-level goals, user attributes and available content are periodically considered, the process involves bottom-up design activities. 1A is inductive partly because it lacks internal theory, and partly because it is an activity that supports emergent phenomena (user experiences) from basic design components. The nature of IA design is well described by Constructive Induction (Cl), a design process that involves locating the best representational framework for the design problem, identifying a solution within that framework and translating it back to the design problem at hand. The future of IA, if it remains inductive or develops a body of theory (or both), is considered","['information architecture design', 'inductive design process', 'bottom-up design activities', 'internal theory', 'emergent phenomena', 'user experiences', 'constructive induction', 'electronic publishing', 'hypermedia', 'information resources']","['design', 'Information architecture', 'internal theory', 'design problem', 'bottom-up design activities', 'inductive design process', 'basic design components', 'inductive process', 'IA design', 'design process']",741,126,10,741,125,10,2,2,3
"greenberger-horne-zeilinger paradoxes for many qubits we construct greenberger-horne-zeilinger (ghz) contradictions for three or more parties sharing an entangled state, the dimension of each subsystem being an even integer d. the simplest example that goes beyond the standard ghz paradox (three qubits) involves five ququats (d = 4). we then examine the criteria that a ghz paradox must satisfy in order to be genuinely m partite and d dimensional ","Greenberger-Horne-Zellinger paradoxes for many qubits We construct Greenberger-Horne-Zeilinger (GHZ) contradictions for three or more parties sharing an entangled state, the dimension of each subsystem being an even integer d. The simplest example that goes beyond the standard GHZ paradox (three qubits) involves five ququats (d = 4). We then examine the criteria that a GHZ paradox must satisfy in order to be genuinely M partite and d dimensional","['Greenberger-Horne-Zeilinger paradoxes', 'many qubits', 'GHZ contradictions', 'entangled state', 'GHZ paradox', 'quantum computing', 'quantum theory']","['construct Greenberger-Horne-Zeilinger', 'Greenberger-Horne-Zellinger paradoxes', 'standard gh paradox', 'more parties', 'integer d', 'squats d', 'd', 'paradoxes', 'construct', 'gh paradox']",382,69,7,382,68,10,1,1,2
"lower bounds on the information rate of secret sharing schemes with homogeneous access structure we present some new lower bounds on the optimal information rate and on the optimal average information rate of secret sharing schemes with homogeneous access structure. these bounds are found by using some covering constructions and a new parameter, the k-degree of a participant, that is introduced in this paper. our bounds improve the previous ones in almost all cases ","Lower bounds on the information rate of secret sharing schemes with homogeneous access structure We present some new lower bounds on the optimal information rate and on the ‘optimal average information rate of secret sharing schemes with homogeneous access structure. These bounds are found by using some covering constructions and a new parameter, the k-degree of a Participant, that is introduced in this paper. Our bounds improve the previous ones in almost all cases","['lower bounds', 'optimal information rate', 'optimal average information rate', 'k-degree', 'cryptography', 'information rate', 'secret sharing schemes', 'homogeneous access structure', 'cryptography']","['homogeneous access structure', 'secret', 'optimal information rate', 'new lower bounds', 'new parameters', 'bounds', 'rate', 'information', 'Lower bounds', 'information rate']",396,75,9,397,74,10,7,1,1
"fuzzy system modeling in pharmacology: an improved algorithm in this paper, we propose an improved fuzzy system modeling algorithm to address some of the limitations of the existing approaches identified during our modeling with pharmacological data. this algorithm differs from the existing ones in its approach to the cluster validity problem (i.e., number of clusters), the projection schema (i.e., input membership assignment and rule determination), and significant input determination. the new algorithm is compared with the bazoon-turksen model, which is based on the well-known sugeno-yasukawa approach. the comparison was made in terms of predictive performance using two different data sets. the first comparison was with a two variable nonlinear function prediction problem and the second comparison was with a clinical pharmacokinetic modeling problem. it is shown that the proposed algorithm provides more precise predictions. determining the degree of significance for each input variable, allows the user to distinguish their relative importance ","Fuzzy system modeling in pharmacology: an improved algorithm In this paper, we propose an improved fuzzy system modeling algorithm to address some of the limitations of the existing approaches identified during our modeling with pharmacological data. This algorithm differs from the existing ones in its approach to the cluster validity problem (Le., number of clusters), the projection schema (i.e., input membership assignment and rule determination), and significant input determination. The new algorithm is compared with the Bazoon-Turksen model, which is based on the well-known Sugeno-Yasukawa approach. The ‘comparison was made in terms of predictive performance using two different data sets. The first comparison was with a two variable nonlinear function prediction problem and the second comparison was with a clinical pharmacokinetic modeling problem. It is shown that the proposed algorithm provides more precise predictions. Determining the degree of significance for each input variable, allows the user to distinguish their relative importance","['fuzzy system modeling', 'pharmacology', 'cluster validity problem', 'projection schema', 'significant input determination', 'predictive performance', 'fuzzy sets', 'fuzzy logic', 'pharmacokinetic modeling', 'fuzzy logic', 'medicine', 'modelling']","['Fuzzy system modelling', 'well-known Sugeno-Yasukawa approach', 'significant input determination', 'cluster validity problem', 'pharmacological data', 'Bazoon-Turksen model', 'new algorithm', 'model', 'algorithm', 'system']",910,152,12,910,151,10,14,2,4
"the impact of ead adoption on archival programs: a pilot survey of early implementers the article reports the results of a survey conducted to assess the impact that the implementation of encoded archival description (ead) has on archival programs. by gathering data related to the funding, staffing, and evaluation of ead programs and about institutional goals for ead implementation, the study explored how ead has affected the operations of the institutions which are utilizing it and the extent to which ead has become a part of regular repository functions ","The impact of EAD adoption on archival programs: a pilot survey of early implementers The article reports the results of a survey conducted to assess the impact that the implementation of Encoded Archival Description (EAD) has on archival programs. By gathering data related to the funding, staffing, and evaluation of EAD programs and about institutional goals for EAD implementation, the study explored how EAD has affected the operations of the institutions which are utilizing it and the extent to which EAD has become a part of regular repository functions,","['EAD adoption', 'archival programs', 'Encoded Archival Description', 'funding', 'staffing', 'EAD programs', 'institutional goals', 'EAD implementation', 'regular repository functions', 'archival descriptive standards', 'diffusion of innovation', 'information retrieval systems', 'page description languages', 'personnel', 'records management']","['archival programs', 'impact', 'EAD', 'early implementers', 'EAD implementation', 'pilot survey', 'EAD programs', 'EAD adoption', 'programs', 'implementers']",473,90,15,474,89,10,0,1,4
"what you get is what you see [web performance monitoring] to get the best possible performance from your web infrastructure, you'll need a complete view. don't neglect the big picture because you're too busy concentrating on details. the increasing complexity of web sites and the content they provide has consequently increased the complexity of the infrastructure that supports them. but with some knowledge of networking, a handful of useful tools, and the insight that those tools provide, designing and operating for optimal performance and reliability is within your grasp ","What you get is what you see [Web performance monitoring] To get the best possible performance from your Web infrastructure, you'll need a complete view. Don't neglect the big picture because you're too busy concentrating on details. The increasing complexity of Web sites and the content they provide has consequently increased the complexity of the infrastructure that supports them. But with some knowledge of networking, a handful of useful tools, and the insight that those tools provide, designing and operating for optimal performance and reliability is within your grasp","['Web performance', 'Web sites', 'Web infrastructure', 'networking', 'reliability', 'file servers', 'information resources', 'Internet', 'monitoring', 'performance evaluation']","['web performance monitoring', 'best possible performance', 'Web infrastructure doull', 'optimal performance', 'useful tools', 'performance', 'Web sites', 'web', 'Web', 'best']",490,90,10,490,89,10,0,0,2
"storage functionals and lyapunov functions for passive dynamical systems for nonlinear time-invariant input-output dynamical systems the passivity conditions are obtained under some restrictions. the conditions imply storage functions satisfying a dissipation inequality. a class of storage functions allowing unique reconstruction of a passive dynamical system is defined. these results are illustrated by an example of a linear system with fading memory. an important, for practical application, class of the linear relaxation systems without direct input-output interaction is considered. a necessary condition for dynamical systems to be of the relaxation type is obtained for this class. the condition is connected with the existence of a unique quadratic lyapunov function satisfying the complete monotonicity condition. this unique lyapunov function corresponds to a ""standard"" thermodynamic potential in a compact family of potentials in the nonequilibrium thermodynamics. the results obtained can be useful in automatic control, mechanics of viscoelastic materials, and various applications in physics and the system theory ","Storage functionals and Lyapunov functions for passive dynamical systems For nonlinear time-invariant input-output dynamical systems the passivity conditions are obtained under some restrictions. The conditions imply storage functions satistying a dissipation inequality. A class of storage functions allowing unique reconstruction of a passive dynamical system is defined. These results are illustrated by an example of a linear system with fading memory. An important, for practical application, class of the linear relaxation systems without direct input-output interaction is considered. A necessary condition for ‘dynamical systems to be of the relaxation type is obtained for this class. The condition is connected with the existence of a unique quadratic Lyapunov function satisfying the complete monotonicity condition. This unique Lyapunov function corresponds to a ""standard"" thermodynamic potential in a compact family of potentials in the nonequilibrium thermodynamics. The results obtained can be useful in automatic control, mechanics of viscoelastic materials, and various applications in physics and the system theory","['storage functionals', 'passive dynamical systems', 'nonlinear time-invariant input-output dynamical systems', 'passivity conditions', 'dissipation inequality', 'linear system', 'fading memory', 'necessary condition', 'unique quadratic Lyapunov function', 'complete monotonicity condition', 'thermodynamic potential', 'nonequilibrium thermodynamics', 'automatic control', 'mechanics', 'viscoelastic materials', 'functional equations', 'functions', 'linear systems', 'Lyapunov methods', 'nonlinear systems']","['dynamical systems', 'passive dynamical system', 'storage functions', 'complete monotonicity conditions', 'linear relaxation systems', 'unique Lyapunov function', 'passivity conditions', 'Lyapunov functions', 'linear system', 'functions']",978,156,20,979,155,10,10,2,8
"from the dos dog days to e-filing [law firms] the poster child for a successful e-filing venture is the case management and electronic case file system now rolling through the district and bankruptcy courts. a project of the administrative office of the united states courts, cm/ecf is a loud proponent of the benefits of the pdf approach and it has a full head of steam. present plans are for all federal courts to implement cm/ecf by 2005. that means a radical shift in methodology and tools for a lot of lawyers. it also means that you should get cozy with acrobat real soon ","From the DOS dog days to e-filing [law firms] The poster child for a successful e-filing venture is the Case Management and Electronic Case File system now rolling through the district and bankruptcy courts. A project of the Administrative Office of the United States Courts, CM/ECF is a loud proponent of the benefits of the PDF approach and it has a full head of steam. Present plans are for all federal courts to implement CM/ECF by 2005. That means a radical shift in methodology and tools for a lot of lawyers. It also means that you should get cozy with Acrobat real soon","['e-filing', 'Case Management and Electronic Case File system', 'United States Courts', 'Adobe Acrobat', 'PDF', 'document handling', 'law administration']","['successful filing venture', 'filing law firms', 'Case Management', 'poster child', 'DOS dog days', 'filing', 'dog', 'DOS', 'law', 'days']",475,104,7,475,103,10,0,0,3
"abacus, efi and anti-virus the extensible firmware interface (efi) standard emerged as a logical step to provide flexibility and extensibility to boot sequence processes, enabling the complete abstraction of a system's bios interface from the system's hardware. in doing so, this provided the means of standardizing a boot-up sequence, extending device drivers and boot time applications' portability to non pc-at-based architectures, including embedded systems like internet appliances, tv internet set-top boxes and 64-bit itanium platforms ","‘Abacus, EFI and anti-virus The Extensible Firmware Interface (EFI) standard emerged as a logical step to provide flexibility and extensibility to boot sequence processes, enabling the complete abstraction of a system's BIOS interface from the system's hardware. In doing so, this provided the means of standardizing a boot-up sequence, extending device drivers and boot time applications’ portability to non PC-AT-based architectures, including embedded systems like Internet appliances, TV Internet set-top boxes and 64-bit Itanium platforms","['Extensible Firmware Interface standard', 'anti-virus', 'embedded systems', 'computer viruses', 'security of data', 'software standards']","['Extensible Firmware Interface', 'boot sequence processes', 'systems BIOS interface', 'logical step', 'anti-virus', 'abacus EFI', 'EFI', 'abacus', 'Firmware', 'Extensible']",468,76,6,469,75,10,8,2,0
"simultaneous iterative reconstruction of emission and attenuation images in positron emission tomography from emission data only for quantitative image reconstruction in positron emission tomography attenuation correction is mandatory. in case that no data are available for the calculation of the attenuation correction factors one can try to determine them from the emission data alone. however, it is not clear if the information content is sufficient to yield an adequate attenuation correction together with a satisfactory activity distribution. therefore, we determined the log likelihood distribution for a thorax phantom depending on the choice of attenuation and activity pixel values to measure the crosstalk between both. in addition an iterative image reconstruction (one-dimensional newton-type algorithm with a maximum likelihood estimator), which simultaneously reconstructs the images of the activity distribution and the attenuation coefficients is used to demonstrate the problems and possibilities of such a reconstruction. as result we show that for a change of the log likelihood in the range of statistical noise, the associated change in the activity value of a structure is between 6% and 263%. in addition, we show that it is not possible to choose the best maximum on the basis of the log likelihood when a regularization is used, because the coupling between different structures mediated by the (smoothing) regularization prevents an adequate solution due to crosstalk. we conclude that taking into account the attenuation information in the emission data improves the performance of image reconstruction with respect to the bias of the activities, however, the reconstruction still is not quantitative ","Simultaneous iterative reconstruction of emission and attenuation images in Positron emission tomography from emission data only For quantitative image reconstruction in positron emission tomography attenuation correction is mandatory. In case that no data are available for the calculation of the attenuation correction factors one can try to determine them from the emission data alone. However, it is not lear if the information content is sufficient to yield an adequate attenuation correction together with a satisfactory activity distribution. Therefore, we determined the log likelihood distribution for a thorax phantom depending on the choice of attenuation and activity pixel values to measure the crosstalk between both. In addition an iterative image reconstruction (one-dimensional Newton-type algorithm with a maximum likelihood estimator), which simultaneously reconstructs the images of the activity distribution and the attenuation coefficients is used to demonstrate the problems and possibilities of such a reconstruction. As result we show that for a change of the log likelihood in the range of statistical noise, the associated change in the activity value of a structure is between 6% and 263%. In addition, we show that it is not possible to choose the best maximum on the basis of the log likelihood when a regularization is used, because the coupling between different structures mediated by the (smoothing) regularization prevents an adequate solution due to crosstalk. We conclude that taking into account the attenuation information in the emission data improves the performance of image reconstruction with respect to the bias of the activities, however, the reconstruction still s not quantitative","['image reconstruction', 'positron emission tomography attenuation correction', 'attenuation correction factors', 'log likelihood distribution', 'thorax phantom', 'activity pixel values', 'crosstalk', 'iterative image reconstruction', 'one-dimensional Newton-type algorithm', 'maximum likelihood estimator', 'activity distribution', 'attenuation coefficients', 'statistical noise', 'smoothing', 'attenuation information', 'eigenvalues and eigenfunctions', 'image reconstruction', 'iterative methods', 'maximum likelihood estimation', 'medical image processing', 'optimisation', 'positron emission tomography']","['emission data', 'Positron emission tomography', 'Simultaneous iterative reconstruction', 'quantitative image reconstruction', 'adequate attenuation correction', 'iterative image reconstruction', 'attenuation correction factors', 'attenuation information', 'attenuation images', 'image reconstruction']",1477,255,22,1475,254,10,5,2,9
"a fuzzy-soft learning vector quantization for control chart pattern recognition this paper presents a supervised competitive learning network approach, called a fuzzy-soft learning vector quantization, for control chart pattern recognition. unnatural patterns in control charts mean that there are some unnatural causes for variations in statistical process control (spc). hence, control chart pattern recognition becomes more important in spc. in order to detect effectively the patterns for the six main types of control charts, pham and oztemel (1994) described a class of pattern recognizers for control charts based on the learning vector quantization (lvq) such as lvq, lvq2 and lvq-x etc. in this paper, we propose a new supervised lvq for control charts based on a fuzzy-soft competitive learning network. the proposed fuzzy-soft lvq (fs-lvq) uses a fuzzy relaxation technique and simultaneously updates all neurons. it can increase correct recognition accuracy and also decrease the learning time. comparisons between lvq, lvq-x and fs-lvq are made ","A uzzy-soft learning vector quantization for control chart pattern recognition This paper presents a supervised competitive learning network approach, called a fuzzy-soft learning vector quantization, for control chart pattern recognition. Unnatural patterns in control charts mean that there are some unnatural causes for variations in statistical process control (SPC). Hence, control chart pattern recognition becomes more important in SPC. In order to detect effectively the patterns for the six main types of control charts, Pham and Oztemel (1994) described a class of pattern recognizers for control charts based on the learning vector quantization (LVQ) such as LVQ, LVQ2 and LVQ-x etc. In this paper, we propose a new supervised LVQ for control charts based on a fuzzy-soft competitive learning network. The proposed fuzzy-soft LVQ (FS-LVQ) uses a fuzzy relaxation technique and simultaneously updates all neurons. It can increase correct recognition accuracy and also decrease the learning time. Comparisons between LVQ, LVQ-X and FS-LVQ are made","['control chart pattern recognition', 'fuzzy-soft learning vector quantization', 'supervised competitive learning network approach', 'unnatural patterns', 'statistical process control', 'SPC', 'supervised LVQ', 'fuzzy relaxation technique', 'simultaneous neuron update', 'correct recognition accuracy', 'learning time', 'numerical results', 'manufacturing process', 'fuzzy neural nets', 'learning (artificial intelligence)', 'manufacturing processes', 'pattern recognition', 'statistical process control', 'vector quantisation']","['learning vector quantization', 'control', 'statistical process control', 'learning time Comparisons', 'control charts Pham', 'Unnatural patterns', 'learning', 'charts', 'patterns', 'control charts']",903,156,19,902,155,10,8,1,5
hidden markov model-based tool wear monitoring in turning this paper presents a new modeling framework for tool wear monitoring in machining processes using hidden markov models (hmms). feature vectors are extracted from vibration signals measured during turning. a codebook is designed and used for vector quantization to convert the feature vectors into a symbol sequence for the hidden markov model. a series of experiments are conducted to evaluate the effectiveness of the approach for different lengths of training data and observation sequence. experimental results show that successful tool state detection rates as high as 97% can be achieved by using this approach ,Hidden Markov model-based tool wear monitoring in turning This paper presents a new modeling framework for tool wear monitoring in machining processes using hidden Markov models (HMMs). Feature vectors are extracted from vibration signals measured during turning. A ‘codebook is designed and used for vector quantization to convert the feature vectors into a symbol sequence for the hidden Markov model. A series of experiments are conducted to evaluate the effectiveness of the approach for different lengths of training data and observation sequence. Experimental results show that successful tool state detection rates as high as 97% can be achieved by using this approach,"['tool wear monitoring', 'machining processes', 'hidden Markov models', 'vibration signals', 'codebook', 'vector quantization', 'feature extraction', 'tool state detection', 'turning process', 'HMM training', 'discrete wavelet transform', 'condition monitoring', 'discrete wavelet transforms', 'feature extraction', 'hidden Markov models', 'learning systems', 'machine tools', 'machining', 'vector quantisation', 'wear']","['hidden Markov model', 'tool wear monitoring', 'new modelling framework', 'model', 'Markov', 'wear', 'tool', 'feature vectors', 'Feature vectors', 'Hidden Markov']",573,103,20,574,102,10,7,1,5
"prospects for quantitative computed tomography imaging in the presence of foreign metal bodies using statistical image reconstruction x-ray computed tomography (ct) images of patients bearing metal intracavitary applicators or other metal foreign objects exhibit severe artifacts including streaks and aliasing. we have systematically evaluated via computer simulations the impact of scattered radiation, the polyenergetic spectrum, and measurement noise on the performance of three reconstruction algorithms: conventional filtered backprojection (fbp), deterministic iterative deblurring, and a new iterative algorithm, alternating minimization (am), based on a ct detector model that includes noise, scatter, and polyenergetic spectra. contrary to the dominant view of the literature, fbp streaking artifacts are due mostly to mismatches between fbp's simplified model of ct detector response and the physical process of signal acquisition. artifacts on am images are significantly mitigated as this algorithm substantially reduces detector-model mismatches. however, metal artifacts are reduced to acceptable levels only when prior knowledge of the metal object in the patient, including its pose, shape, and attenuation map, are used to constrain am's iterations. am image reconstruction, in combination with object-constrained ct to estimate the pose of metal objects in the patient, is a promising approach for effectively mitigating metal artifacts and making quantitative estimation of tissue attenuation coefficients a clinical possibility ","Prospects for quantitative computed tomography imaging in the presence of foreign metal bodies using statistical image reconstruction X-ray computed tomography (CT) images of patients bearing metal intracavitary applicators or other metal foreign objects exhibit severe artifacts including streaks and aliasing. We have systematically evaluated via ‘computer simulations the impact of scattered radiation, the polyenergetic spectrum, and measurement noise on the performance of three reconstruction algorithms: conventional filtered backprojection (FBP), deterministic iterative deblurring, and a new iterative algorithm, alternating minimization (AM), based on a CT detector model that includes noise, scatter, and polyenergetic spectra. Contrary to the dominant view of the literature, FBP streaking artifacts are due mostly to mismatches between FBP's simplified model of CT detector response and the physical process of signal acquisition. Artifacts on AM images are significantly mitigated as this algorithm substantially reduces detector-model mismatches. However, metal artifacts are reduced to acceptable levels only when prior knowledge of the metal object in the patient, including its pose, shape, and attenuation map, are used to constrain AM's iterations. AM image reconstruction, in combination with object-constrained CT to estimate the pose of metal objects in the Patient, is a promising approach for effectively mitigating metal artifacts and making quantitative estimation of tissue attenuation coefficients a clinical possibility","['quantitative computed tomography imaging', 'foreign metal bodies', 'statistical image reconstruction', 'metal artifact reduction', 'brachytherapy', 'medical diagnostic imaging', 'signal acquisition physical process', 'object-constrained CT', 'iterative algorithm', 'alternating minimization', 'CT detector model', 'noise', 'scatter', 'polyenergetic spectra', 'clinical possibility', 'deterministic iterative deblurring', 'filtered backprojection', 'computerised tomography', 'digital simulation', 'image reconstruction', 'iterative methods', 'medical image processing', 'X-ray absorption', 'X-ray scattering']","['metal', 'image reconstruction', 'metal artifacts', 'tomography act images', 'foreign metal bodies', 'computer simulations', 'metal objects', 'other metal', 'AM images', 'image']",1341,209,24,1342,208,10,8,1,10
"a nonlinear modulation strategy for hybrid ac/dc power systems a nonlinear control strategy to improve transient stability of a multi-machine ac power system with several dc links terminated in the presence of large disturbances is presented. the approach proposed in this paper is based on differential geometric theory, and the hvdc systems are taken as a variable admittance connected at the inverter or rectifier ac bus. after deriving the analytical description of the relationship between the variable admittance and active power flows of each generator, the traditional generator dynamic equations can thus be expressed with the variable admittance of hvdc systems as an additional state variable and changed to an affine form, which is suitable for global linearization method being used to determine its control variable. an important feature of the proposed method is that, the modulated dc power is an adaptive and non-linear function of ac system states, and it can be realized by local feedback and less transmitted data from, adjacent generators. the design procedure is tested on a dual-infeed hybrid ac/dc system ","nonlinear modulation strategy for hybrid AC/DC power systems nonlinear control strategy to improve transient stability of a multi-machine AC power system with several DC links terminated in the presence of large disturbances is presented. The approach proposed in this paper is based on differential geometric theory, and the HVDC systems are taken as a variable admittance connected at the inverter or rectifier AC bus. After deriving the analytical description of the relationship between the variable admittance and active power flows of each generator, the traditional generator dynamic equations can thus be expressed with the variable admittance of HVDC systems as an additional state variable and changed to an affine form, which is suitable for global linearization method being used to determine its control variable. An important feature of the proposed method is that, the modulated DC power is an adaptive and non-linear function of AC system states, and it can be realized by local feedback and less transmitted data from, adjacent generators. The design procedure is tested on a dual-infeed hybrid ACIDC system","['nonlinear control strategy', 'transient stability', 'multi-machine AC power system', 'DC links', 'nonlinear modulation strategy', 'hybrid AC/DC power systems', 'differential geometric theory', 'HVDC systems', 'variable admittance', 'inverter', 'rectifier AC bus', 'active power flows', 'generator dynamic equations', 'affine form', 'global linearization method', 'local feedback', 'adjacent generators', 'dual-infeed hybrid AC/DC system', 'electric admittance', 'feedback', 'HVDC power transmission', 'invertors', 'linearisation techniques', 'load flow', 'nonlinear control systems', 'power system control', 'rectifying circuits']","['variable admittance', 'systems', 'DC power', 'nonlinear modulation strategy', 'nonlinear control strategy', 'additional state variable', 'active power flows', 'AC system states', 'power', 'nonlinear']",954,176,27,952,173,10,572,173,9
"the fully entangled fraction as an inclusive measure of entanglement applications characterizing entanglement in all but the simplest case of a two qubit pure state is a hard problem, even understanding the relevant experimental quantities that are related to entanglement is difficult. it may not be necessary, however, to quantify the entanglement of a state in order to quantify the quantum information processing significance of a state. it is known that the fully entangled fraction has a direct relationship to the fidelity of teleportation maximized under the actions of local unitary operations. in the case of two qubits we point out that the fully entangled fraction can also be related to the fidelities, maximized under the actions of local unitary operations, of other important quantum information tasks such as dense coding, entanglement swapping and quantum cryptography in such a way as to provide an inclusive measure of these entanglement applications. for two qubit systems the fully entangled fraction has a simple known closed-form expression and we establish lower and upper bounds of this quantity with the concurrence. this approach is readily extendable to more complicated systems ","The fully entangled fraction as an inclusive measure of entanglement applications Characterizing entanglement in all but the simplest case of a two qubit pure state is a hard problem, even understanding the relevant experimental quantities that are related to entanglement is difficult. It may not be necessary, however, to quantity the entanglement of a state in order to quantify the quantum information processing significance of a state. It is known that the fully entangled fraction has a direct relationship to the fidelity of teleportation maximized under the actions of local unitary operations. In the case of two qubits we point out that the fully entangled fraction can also be related to the fidelities, maximized under the actions of local unitary operations, of other important quantum information tasks such as dense coding, entanglement swapping and quantum cryptography in such a way as to provide an inclusive measure of these entanglement applications. For two qubit systems the fully entangled fraction has a simple known closed-form expression and we establish lower and upper bounds of this quantity with the concurrence. This approach is readily extendable to more complicated systems","['entanglement', 'two qubit pure state', 'quantum information processing', 'fully entangled fraction', 'fidelity', 'teleportation', 'entanglement swapping', 'quantum cryptography', 'information theory', 'quantum communication', 'quantum cryptography', 'quantum interference phenomena']","['entangled fraction', 'entanglement applications', 'local unitary operations', 'inclusive measure', 'relevant experimental quantities', 'quantum cryptography', 'simplest case', 'pure state', 'fraction', 'entangled']",1022,187,12,1022,186,10,1,1,2
"breaking the myths of rewards: an exploratory study of attitudes about knowledge sharing many ceo and managers understand the importance of knowledge sharing among their employees and are eager to introduce the knowledge management paradigm in their organizations. however little is known about the determinants of the individual's knowledge sharing behavior. the purpose of this study is to develop an understanding of the factors affecting the individual's knowledge sharing behavior in the organizational context. the research model includes various constructs based on social exchange theory, self-efficacy, and theory of reasoned action. research results from the field survey of 467 employees of four large, public organizations show that expected associations and contribution are the major determinants of the individual's attitude toward knowledge sharing. expected rewards, believed by many to be the most important motivating factor for knowledge sharing, are not significantly related to the attitude toward knowledge sharing. as expected, positive attitude toward knowledge sharing is found to lead to positive intention to share knowledge and, finally, to actual knowledge sharing behaviors ","Breaking the myths of rewards: an exploratory study of attitudes about knowledge sharing Many CEO and managers understand the importance of knowledge sharing among their employees and are eager to introduce the knowledge management paradigm in their organizations. However little is known about the determinants of the individual's knowledge sharing behavior. The purpose of this study is to develop an understanding of the factors affecting the individual's knowledge sharing behavior in the ‘organizational context. The research model includes various constructs based on social exchange theory, self-efficacy, and theory of reasoned action. Research results from the field survey of 467 employees of four large, public organizations show that expected associations and contribution are the major determinants of the individual's attitude toward knowledge sharing. Expected rewards, believed by many to be the most important motivating factor for knowledge sharing, are not significantly related to the attitude toward knowledge sharing. AS expected, positive attitude toward knowledge sharing is found to lead to positive intention to share knowledge and, finally, to actual knowledge sharing behaviors","['knowledge sharing', 'knowledge management', 'social exchange theory', 'self-efficacy', 'theory of reasoned action', 'public organizations', 'rewards', 'strategic management', 'information resources', 'information systems', 'social aspects of automation', 'strategic planning']","['knowledge sharing', 'knowledge', 'individuals knowledge', 'knowledge management paradigm', 'individuals attitude', 'positive attitude', 'exploratory study', 'actual knowledge', 'attitude', 'sharing']",1034,172,12,1035,171,10,14,1,5
"industry insiders loading up on cheap company stock a surge of telecom executives and directors purchasing their own companies, stock in the last two months points toward a renewed optimism in the beleaguered sector, say some observers, who view the rash of insider buying as a vote of confidence from management. airgate pcs, charter communications, cox communications, crown castle international, nextel communications and nortel networks all have seen infusions of insider investment this summer, echoing trends in both the telecom industry and the national economy ","Industry insiders loading up on cheap company stock A surge of telecom executives and directors purchasing their own companies, stock in the last two months points toward a renewed optimism in the beleaguered sector, say some observers, who view the rash of insider buying as a vote of confidence from management. Airgate PCS, Charter Communications, Cox Communications, Crown Castle International, Nextel Communications and Norte! Networks all have seen infusions of insider investment this summer, echoing trends in both the telecom industry and the national economy","['telecom industry', 'insider investment', 'telecommunication']","['own companies stock', 'telecom executives', 'insider investment', 'Industry insiders', 'telecom industry', 'insider buying', 'company', 'Industry', 'stock', 'insiders']",484,86,3,484,85,10,1,1,0
"union outreach - a pilgrim's progress as the american labor movement continues on its path toward reorganization and rejuvenation, archivists are challenged to ensure that the organizational, political, and cultural changes labor unions are experiencing are fully documented. the article examines the need for labor archivists to reach out actively to unions and the problems they face in getting their message across, not only to union leadership but also to union members. outreach by labor archivists is vital on three critical fronts: the need to secure union funding in support of labor archival programs; obtaining union cooperation in reviewing and amending obsolete deposit agreements; and coordinating efforts with unions to save the records of closing district and local union offices. attempting to resolve these outstanding issues, labor archivists are pulled between two distinct institutional cultures (one academic in nature, the other enmeshed in a union bureaucracy) and often have their own labor archival programs compromised by the internal dynamics and politics inherent in administering large academic libraries and unions. if labor archivists are to be successful, they must find their collective voice within the labor movement and establish their relevancy to unions during a period of momentous change and restructuring. moreover, archivists need to give greater thought to designing and implementing outreach programs that bridge the fundamental ""disconnect"" between union bureaucracies and the rank and file, and unions and the public ","Union outreach - a pilgrim's progress As the American labor movement continues on its path toward reorganization and rejuvenation, archivists are challenged to ensure that the organizational, political, and cultural changes labor unions are experiencing are fully documented. The article examines the need for labor archivists to reach out actively to unions and the problems they face in getting their message across, not only to union leadership but also to union members. Outreach by labor archivists is vital on three critical fronts: the need to secure union funding in support of labor archival programs; obtaining union cooperation in reviewing and amending obsolete deposit agreements; and coordinating efforts with unions to save the records of closing district and local union offices. Attempting to resolve these outstanding issues, labor archivists are pulled between two distinct institutional cultures (one academic in nature, the other enmeshed in a union bureaucracy) and often have their ‘own labor archival programs compromised by the internal dynamics and Politics inherent in administering large academic libraries and unions. If labor archivists are to be successful, they must find their collective voice within the labor movement and establish their relevancy to unions during a period of momentous change and restructuring. Moreover, archivists need to give greater thought to designing and implementing outreach programs that bridge the fundamental ""disconnect"" between union bureaucracies and the rank and file, and unions and the public","['American labor movement', 'archivists', 'political changes', 'cultural changes', 'labor unions', 'labor archivists', 'union leadership', 'union members', 'union funding', 'labor archival programs', 'union cooperation', 'obsolete deposit agreements', 'union offices', 'institutional cultures', 'union bureaucracy', 'internal dynamics', 'large academic libraries', 'collective voice', 'information retrieval systems', 'politics', 'records management', 'service industries']","['union', 'labor archivists', 'American labor movement', 'union members Outreach', 'union bureaucracies', 'local union offices', 'union cooperation', 'union leadership', 'union funding', 'labor movement']",1332,232,22,1333,231,10,3,1,7
"spam solution? the author describes a solution to spam e-mails: disposable e-mail addresses (dea). mailshell's free trial web-based e-mail service allows you, if you start getting spammed on that dea, just to delete the dea in mailshell, and all e-mail thereafter sent to that address will automatically be junked (though you can later restore that address if you want). mailshell allows any number of dea ","Spam solution? The author describes a solution to spam E-mails: disposable E-mail addresses (DEA). Mailshell's free trial Web-based E-mail service allows you, if you start getting spammed on that DEA, just to delete the DEA in Mailshell, and all E-mail thereafter sent to that address will automatically be junked (though you can later restore that address if you want). Mailshell allows any number of DEA","['spam E-mails', 'disposable E-mail addresses', 'Mailshell', 'Web-based E-mail', 'data privacy', 'electronic mail', 'information resources']","['Mailshell', 'address', 'DEA', 'pen-based E-mail service', 'Spam solution', 'author', 'e-mail', 'E-mail', 'Spam', 'solution']",341,66,7,341,65,10,0,0,0
"computational challenges in cell simulation: a software engineering approach molecular biology's advent in the 20th century has exponentially increased our knowledge about the inner workings of life. we have dozens of completed genomes and an array of high-throughput methods to characterize gene encodings and gene product operation. the question now is how we will assemble the various pieces. in other words, given sufficient information about a living cell's molecular components, can we predict its behavior? we introduce the major classes of cellular processes relevant to modeling, discuss software engineering's role in cell simulation, and identify cell simulation requirements. our e-cell project aims to develop the theories, techniques, and software platforms necessary for whole-cell-scale modeling, simulation, and analysis. since the project's launch in 1996, we have built a variety of cell models, and we are currently developing new models that vary with respect to species, target subsystem, and overall scale ","Computational challenges in cell simulation: a software engineering approach Molecular biology's advent in the 20th century has exponentially increased our knowledge about the inner workings of life. We have dozens of completed genomes and an array of high-throughput methods to characterize gene encodings and gene product operation. The question now is how we will assemble the various pieces. In other words, given sufficient information about a living cell's molecular components, can we predict its behavior? We introduce the major classes of cellular processes relevant to modeling, discuss software engineering's role in cell simulation, and identify cell simulation requirements. Our E-Cell project aims to develop the theories, techniques, and software platforms necessary for whole-cell-scale modeling, simulation, and analysis. Since the project's launch in 1996, we have built a variety of cell models, and we are currently developing new models that vary with respect to species, target subsystem, and overall scale","['cell simulation', 'software engineering', 'object-oriented design', 'molecular biology', 'E-Cell project', 'whole-cell-scale modeling', 'biology computing', 'cellular biophysics', 'digital simulation', 'molecular biophysics', 'object-oriented methods', 'software engineering']","['cell', 'whole-cell-scale modelling simulation', 'software engineering approach', 'cell simulation requirements', 'software engineering role', 'Computational challenges', 'living cells', 'cell models', 'simulation', 'cell simulation']",880,150,12,880,149,10,0,0,2
"the impact of the internet on public library use: an analysis of the current consumer market for library and internet services the potential impact of the internet on the public's demand for the services and resources of public libraries is an issue of critical importance. the research reported in this article provides baseline data concerning the evolving relationship between the public's use of the library and its use of the internet. the authors developed a consumer model of the american adult market for information services and resources, segmented by use (or nonuse) of the public library and by access (or lack of access) to, and use (or nonuse) of, the internet. a national random digit dialing telephone survey collected data to estimate the size of each of six market segments, and to describe their usage choices between the public library and the internet. the analyses presented in this article provide estimates of the size and demographics of each of the market segments; describe why people are currently using the public library and the internet; identify the decision criteria people use in their choices of which provider to use; identify areas in which libraries and the internet appear to be competing and areas in which they appear to be complementary; and identify reasons why people choose not to use the public library and/or the internet. the data suggest that some differentiation between the library and the internet is taking place, which may very well have an impact on consumer choices between the two. longitudinal research is necessary to fully reveal trends in these usage choices, which have implications for all types of libraries in planning and policy development ","The impact of the Internet on public library use: an analysis of the current consumer market for library and Internet services, The potential impact of the Internet on the public's demand for the services and resources of public libraries is an issue of critical importance. The research reported in this article provides baseline data concerning the evolving relationship between the public's use of the library and its use of the Internet. The authors developed a consumer model of the. ‘American adult market for information services and resources, segmented by use (or nonuse) of the public library and by access (or lack of access) to, and use (or nonuse) of, the Internet. A national Random Digit Dialing telephone survey collected data to estimate the size of each of six market segments, and to describe their usage choices between the public library and the Internet. The analyses presented in this article provide estimates of the size and demographics of each of the market segments; describe why people are currently using the public library and the Internet; identify the decision criteria people use in their choices of which provider to use; identify areas in which libraries and the Internet appear to be competing and areas in which they appear to be complementary; and identify reasons why people choose not to use the public library and/or the Internet. The data suggest that some differentiation between the library and the Internet is taking place, which may very well have an impact on consumer choices between the two. Longitudinal research is necessary to fully reveal trends in these usage choices, which have implications for all types of libraries in planning and policy development","['Internet', 'public libraries', 'baseline data', 'consumer model', 'American adult market', 'national Random Digit Dialing telephone survey', 'decision criteria', 'public library', 'longitudinal research', 'Internet', 'public libraries']","['public library', 'public', 'current consumer market', 'Internet services', 'potential impact', 'consumer choices', 'publics demand', 'publics use', 'Internet', 'library']",1430,278,11,1433,277,10,8,3,4
"an efficient retrieval selection algorithm for video servers with random duplicated assignment storage technique random duplicated assignment (rda) is an approach in which video data is stored by assigning a number of copies of each data block to different, randomly chosen disks. it has been shown that this approach results in smaller response times and lower disk and ram costs compared to the well-known disk stripping techniques. based on this storage approach, one has to determine, for each given batch of data blocks, from which disk each of the data blocks is to be retrieved. this is to be done in such a way that the maximum load of the disks is minimized. the problem is called the retrieval selection problem (rsp). in this paper, we propose a new efficient algorithm for rsp. this algorithm is based on the breadth-first search approach and is able to guarantee optimal solutions for rsp in o(n/sup 2/+mn), where m and n correspond to the number of data blocks and the number of disks, respectively. we show that our proposed algorithm has a lower time complexity than an existing algorithm, called the mfs algorithm ","An efficient retrieval selection algorithm for video servers with random duplicated assignment storage technique Random duplicated assignment (RDA) is an approach in which video data is stored by assigning a number of copies of each data block to different, randomly chosen disks. It has been shown that this approach results in smaller response times and lower disk and RAM costs compared to the well-known disk stripping techniques. Based on this storage approach, ‘one has to determine, for each given batch of data blocks, from which disk each of the data blocks is to be retrieved. This is to be done in such a way that the maximum load of the disks is minimized. The problem is called the retrieval selection problem (RSP). In this paper, we propose a new efficient algorithm for RSP. This algorithm is based on the breadth-first search approach and is able to guarantee optimal solutions for RSP in O(n/sup 2/+mn), where m and n correspond to the number of data blocks and the number of disks, respectively. We show that our proposed algorithm has a lower time complexity than an existing algorithm, called the MFS algorithm","['efficient retrieval selection algorithm', 'video servers', 'random duplicated assignment storage technique', 'copies', 'data block', 'randomly chosen disks', 'response times', 'RAM costs', 'disk costs', 'maximum load', 'breadth-first search', 'optimal solutions', 'time complexity', 'computational complexity', 'magnetic disc storage', 'random-access storage', 'resource allocation', 'storage allocation', 'tree searching', 'video on demand', 'video servers']","['data blocks', 'algorithm', 'retrieval selection problem', 'duplicated assignment drda', 'new efficient algorithm', 'storage approach', 'well-known disk', 'MFS algorithm', 'video data', 'lower disk']",941,191,21,942,190,10,3,1,8
"self-calibration from image derivatives this study investigates the problem of estimating camera calibration parameters from image motion fields induced by a rigidly moving camera with unknown parameters, where the image formation is modeled with a linear pinhole-camera model. the equations obtained show the flow to be separated into a component due to the translation and the calibration parameters and a component due to the rotation and the calibration parameters. a set of parameters encoding the latter component is linearly related to the flow, and from these parameters the calibration can be determined. however, as for discrete motion, in general it is not possible to decouple image measurements obtained from only two frames into translational and rotational components. geometrically, the ambiguity takes the form of a part of the rotational component being parallel to the translational component, and thus the scene can be reconstructed only up to a projective transformation. in general, for full calibration at least four successive image frames are necessary, with the 3d rotation changing between the measurements. the geometric analysis gives rise to a direct self-calibration method that avoids computation of optical flow or point correspondences and uses only normal flow measurements. new constraints on the smoothness of the surfaces in view are formulated to relate structure and motion directly to image derivatives, and on the basis of these constraints the transformation of the viewing geometry between consecutive images is estimated. the calibration parameters are then estimated from the rotational components of several flow fields. as the proposed technique neither requires a special set up nor needs exact correspondence it is potentially useful for the calibration of active vision systems which have to acquire knowledge about their intrinsic parameters while they perform other tasks, or as a tool for analyzing image sequences in large video databases ","Self-calibration from image derivatives This study investigates the problem of estimating camera calibration parameters from image motion fields induced by a rigidly moving camera with unknown parameters, where the image formation is modeled with a linear pinhole-camera model. The equations obtained show the flow to be separated into a component due to the translation and the calibration parameters and a component due to the rotation and the calibration parameters. A set of parameters encoding the latter component is linearly related to the flow, and from these parameters the calibration can be determined. However, as for discrete motion, in general it is not possible to decouple image measurements obtained from only two frames into translational and rotational components. Geometrically, the ambiguity takes the form of a part of the rotational component being parallel to the translational component, and thus the scene can be reconstructed only up to a projective transformation. In general, for full calibration at least four successive image frames are necessary, with the 3D rotation changing between the measurements. The geometric analysis gives rise to a direct self-calibration method that avoids ‘computation of optical flow or point correspondences and uses only normal flow measurements. New constraints on the smoothness of the surfaces in view are formulated to relate structure and motion directly to image derivatives, and on the basis of these constraints the transformation of the viewing geometry between consecutive images is estimated. The calibration parameters are then estimated from the rotational components of several flow fields. As the proposed technique neither requires a special set up nor needs exact correspondence it is potentially useful for the calibration of active vision systems which have to acquire knowledge about their intrinsic parameters while they perform other tasks, or as a tool for analyzing image sequences in large video databases","['camera calibration parameters', 'image motion fields', 'rigidly moving camera', 'image formation', 'linear pinhole-camera model', 'calibration parameters', 'image measurements', 'translational components', 'rotational components', 'direct self-calibration method', 'optical flow', 'point correspondences', 'normal flow measurements', 'active vision systems', 'image sequences', 'large video databases', 'depth distortion', 'active vision', 'calibration', 'image motion analysis', 'image sequences', 'motion estimation']","['image', 'image derivatives', 'camera calibration parameters', 'decoupled image measurements', 'successive image frames', 'image motion fields', 'consecutive images', 'image sequences', 'image formation', 'calibration parameters']",1695,300,22,1696,299,10,11,1,8
"perspectives on scholarly online books: the columbia university online books evaluation project the online books evaluation project at columbia university studied the potential for scholarly online books from 1995 to 1999. issues included scholars' interest in using online books, the role they might play in scholarly life, features that scholars and librarians sought in online books, the costs of producing and owning print and online books, and potential marketplace arrangements. scholars see potential for online books to make their research, learning, and teaching more efficient and effective. librarians see potential to serve their scholars better. librarians may face lower costs if they can serve their scholars with online books instead of print books. publishers may be able to offer scholars greater opportunities to use their books while enhancing their own profitability ","Perspectives on scholarly online books: the Columbia University Online Books Evaluation Project The Online Books Evaluation Project at Columbia University studied the potential for scholarly online books from 1995 to 1999. Issues included scholars’ interest in using online books, the role they might play in scholarly life, features that scholars and librarians sought in online books, the costs of producing and owning print and online books, and potential marketplace arrangements. Scholars see potential for online books to make their research, learning, and teaching more efficient and effective. Librarians see potential to serve their scholars better. Librarians may face lower costs if they can serve their scholars with online books instead of print books. Publishers may be able to offer scholars greater opportunities to use their books while enhancing their ‘own profitability","['Columbia University Online Books Evaluation Project', 'scholarly online books', 'print books', 'costs', 'marketplace arrangements', 'research', 'learning', 'academic libraries', 'economics', 'electronic publishing', 'research libraries']","['scholarly online books', 'Columbia University', 'books', 'potential marketplace arrangements', 'scholarly life features', 'print books Publishers', 'scholars interest', 'online', 'scholars', 'online books']",757,132,11,758,131,10,4,2,3
"from continuous recovery to discrete filtering in numerical approximations of conservation laws modern numerical approximations of conservation laws rely on numerical dissipation as a means of stabilization. the older, alternative approach is the use of central differencing with a dose of artificial dissipation. in this paper we review the successful class of weighted essentially non-oscillatory finite volume schemes which comprise sophisticated methods of the first kind. new developments in image processing have made new devices possible which can serve as highly nonlinear artificial dissipation terms. we view artificial dissipation as discrete filter operation and introduce several new algorithms inspired by image processing ","From continuous recovery to discrete filtering in numerical approximations of conservation laws Modern numerical approximations of conservation laws rely on numerical dissipation as a means of stabilization. The older, alternative approach is the use of central differencing with a dose of artificial dissipation. In this paper we review the successful class of weighted essentially non-oscillatory finite volume schemes which comprise sophisticated methods of the first kind. New developments in image processing have made new devices possible which can serve as highly nonlinear artificial dissipation terms. We view artificial dissipation as discrete filter operation and introduce several new algorithms inspired by image processing","['continuous recovery', 'discrete filtering', 'numerical approximations', 'conservation laws', 'numerical dissipation', 'central differencing', 'artificial dissipation', 'finite volume schemes', 'image processing', 'highly nonlinear artificial dissipation terms', 'discrete filter operation', 'conservation laws', 'finite volume methods', 'fluid mechanics', 'image processing', 'polynomials']","['artificial dissipation', 'numerical approximations', 'discrete filter operation', 'conservation laws Modern', 'numerical dissipation', 'continuous recovery', 'numerical', 'dissipation', 'conservation laws', 'approximations']",635,103,16,635,102,10,0,0,4
"packetvideo. one step ahead of the streaming wireless market go beyond the hype, however, and it's clear that packetvideo is making strides in delivering streaming multimedia content to wireless devices. for one thing, its technology, based on the industry-standard motion pictures expert group 4 (mpeg-4) video encoder/decoder, actually works as promised. secondly, the company has forged a broad-based band of alliances that not only will eventually help it reach potential customers down the road, but provides it financial support until the company can ramp up sales. the list of packetvideo's technology partners who are also investors-and who have pumped more than $121 million into the company-includes not just wireless device manufacturers, but content providers and semiconductor vendors, all of whom stand to benefit by increased sales of handheld wireless terminals ","PacketVideo. One step ahead of the streaming wireless market Go beyond the hype, however, and it's clear that PacketVideo is making strides in delivering streaming multimedia content to wireless devices. For one thing, its technology, based on the industry-standard Motion Pictures, Expert Group 4 (MPEG-4) video encoder/decoder, actually works as promised. Secondly, the company has forged a broad-based band of alliances that not only will eventually help it reach potential customers down the road, but provides it financial support until the ‘company can ramp up sales. The list of PacketVideo's technology partners who are also investors-and who have pumped more than $121 million into the company-includes not just wireless device manufacturers, but content providers and semiconductor vendors, all of whom stand to benefit by increased sales of handheld wireless terminals","['PacketVideo', 'multimedia content streaming', 'wireless devices', 'MPEG-4', 'wireless device manufacturers', 'content providers', 'semiconductor vendors', 'handheld wireless terminals', 'multimedia communication', 'notebook computers', 'radio access networks']","['PacketVideo', 'wireless', 'wireless device manufacturers', 'streaming multimedia content', 'handheld wireless terminals', 'technology partners', 'content providers', 'wireless devices', 'wireless market', 'streaming']",748,131,11,750,130,10,7,2,2
"what do you say? open letters to women considering a computer science major in the last decade we have both monitored with great interest the ratio of female to male computer science majors at our respective institutions. with each entering class, we think: ""surely, now is the time when the numbers will become more balanced."" logic tells us that this must eventually happen, because the opportunities in computing are simply too attractive for an entire segment of our population to routinely pass up. but each year we are again disappointed in the number of women students, as they continue to be woefully under-represented among computer science majors. so, what do you say to a young woman who is considering a college choice and a choice of major in order to make computer science a more attractive option? we have organized some thoughts on that subject into open letters ","What do you say? Open letters to women considering a computer science major In the last decade we have both monitored with great interest the ratio of female to male computer science majors at our respective institutions. With each entering class, we think: ""Surely, now is the time when the numbers will become more balanced."" Logic tells us that this must eventually happen, because the opportunities in computing are simply too attractive for an entire segment of our population to routinely pass up. But each year we are again disappointed in the number of women students, as they continue to be woefully under-represented among ‘computer science majors. So, what do you say to a young woman who is considering a college choice and a choice of major in order to make ‘computer science a more attractive option? We have organized some thoughts on that subject into open letters","['women', 'computer science education', 'female', 'male', 'computer science majors', 'gender issues', 'computer science education', 'gender issues', 'social aspects of automation']","['computer science majors', 'open letters', 'attractive option', 'women students', 'women', 'science', 'computer', 'majors', 'letters', 'computer science']",731,149,9,733,148,10,16,2,2
"image reconstruction of simulated specimens using convolution back projection this paper reports the reconstruction of cross-sections of composite structures. the convolution back projection (cbp) algorithm has been used to capture the attenuation field over the specimen. five different test cases have been taken up for evaluation. these cases represent varying degrees of complexity. in addition, the role of filters on the nature of the reconstruction errors has also been discussed. numerical results obtained in the study reveal that cbp algorithm is a useful tool for qualitative as well as quantitative assessment of composite regions encountered in engineering applications ","Image reconstruction of simulated specimens using convolution back projection This paper reports the reconstruction of cross-sections of composite structures. The convolution back projection (CBP) algorithm has been used to capture the attenuation field over the specimen. Five different test cases have been taken up for evaluation. These cases represent varying degrees of complexity. In addition, the role of filters on the nature of the reconstruction errors has also been discussed. Numerical results obtained in the study reveal that CBP algorithm is a useful tool for qualitative as well as quantitative assessment of composite regions encountered in engineering applications","['image reconstruction', 'simulated specimens', 'convolution back projection', 'composite structures', 'attenuation field', 'filters', 'reconstruction errors', 'CBP algorithm', 'composite regions', 'engineering applications', 'computerised tomography', 'computerised tomography', 'convolution', 'image reconstruction', 'nondestructive testing']","['convolution', 'projection', 'reconstruction errors', 'different test cases', 'composite structures', 'Image reconstruction', 'simulated specimens', 'composite regions', 'reconstruction', 'specimens']",585,99,15,585,98,10,0,0,4
"webcad: a computer aided design tool constrained with explicit 'design for manufacturability' rules for computer numerical control milling a key element in the overall efficiency of a manufacturing enterprise is the compatibility between the features that have been created in a newly designed part, and the capabilities of the downstream manufacturing processes. with this in mind, a process-aware computer aided design (cad) system called webcad has been developed. the system restricts the freedom of the designer in such a way that the designed parts can be manufactured on a three-axis computer numerical control milling machine. this paper discusses the vision of webcad and explains the rationale for its development in comparison with commercial cad/cam (computer aided design/manufacture) systems. the paper then goes on to describe the implementation issues that enforce the manufacturability rules. finally, certain design tools are described that aid a user during the design process. some examples are given of the parts designed and manufactured with webcad ","WebCAD: A computer aided design tool constrained with explicit ‘design for manufacturabilty' rules for computer numerical control milling A key element in the overall efficiency of a manufacturing enterprise is the ‘compatibility between the features that have been created in a newly designed part, and the capabilities of the downstream manufacturing processes. With this in mind, a process-aware computer aided design (CAD) system called WebCAD has been developed. The system restricts the freedom of the designer in such a way that the designed parts can be manufactured on a three-axis computer numerical control milling machine. This paper discusses the vision of WebCAD and explains the rationale for its development in comparison with commercial CAD/CAM (computer aided design/manutacture) systems. The paper then goes on to describe the implementation issues that enforce the manufacturability rules. Finally, certain design tools are described that aid a user during the design process. Some examples are given of the parts designed and manufactured with WebCAD","['WebCAD', 'computer aided design tool', 'design for manufacturability rules', 'computer numerical control milling', 'manufacturing enterprise efficiency', 'process-aware CAD system', 'three-axis CNC milling machine', 'CAD/CAM systems', 'manufacturability rules', 'design tools', 'Internet-based CAD/CAM', 'CAD/CAM', 'computerised numerical control', 'design for manufacture', 'Internet', 'machining']","['commercial CAD/CAM computer', 'process-aware computer', 'certain design tools', 'three-axis computer', 'design cad system', 'explicit design', 'design process', 'A computer', 'designer', 'design tool']",913,160,16,913,159,10,18,4,2
"gender, software design, and occupational equity after reviewing the work on gender bias in software design, a model of gender-role influenced achievement choice taken from eccles (1994) is presented. the paper concludes that (1) though laudable, reduction of gender bias in software design is not the most straightforward way to reduce gender inequity in the choice of computing as a career, (2) the model itself makes more clear some of the ethical issues involved in attempting to achieve gender equity on computing, and (3) efforts to reduce gender inequity in the choice of computing as a career need to be evaluated in the light of this model ","Gender, software design, and occupational equity After reviewing the work on gender bias in software design, a model of gender-role influenced achievement choice taken from Eccles (1994) is presented. The paper concludes that (1) though laudable, reduction of gender bias in software design is not the most straightforward way to reduce gender inequity in the choice of computing as a career, (2) the model itself makes more clear some of the ethical issues involved in attempting to achieve gender equity on computing, and (3) efforts to reduce gender inequity in the choice of computing as a career need to be evaluated in the light of this model","['gender bias', 'software design', 'gender-role influenced achievement choice model', 'computing career', 'ethical issues', 'occupational equity', 'employment', 'gender issues', 'software engineering']","['gender', 'gender inequity', 'gender bias', 'gender software design', 'occupational equity', 'achievement choice', 'gender equity', 'design', 'software', 'software design']",542,108,9,542,107,10,0,0,1
"copyright of electronic publishing with the spreading of the internet and the wide use of computers, electronic publishing is becoming an indispensable measure to gain knowledge and skills. meanwhile, copyright is facing much more infringement than ever in this electronic environment. so, it is a key factor to effectively protect copyright of electronic publishing to foster the new publication fields. the paper analyzes the importance of copyright, the main causes for copyright infringement in electronic publishing, and presents viewpoints on the definition and application of fair use of a copyrighted work and thinking of some means to combat breach of copyright ","Copyright of electronic publishing With the spreading of the Internet and the wide use of computers, electronic publishing is becoming an indispensable measure to gain knowledge and skills. Meanwhile, copyright is facing much more infringement than ever in this electronic environment. So, it is a key factor to effectively protect copyright of electronic publishing to foster the new publication fields. The paper analyzes the importance of copyright, the main causes for copyright infringement in electronic publishing, and presents viewpoints on the definition and application of fair use of a copyrighted work and thinking of some means to combat breach of copyright","['electronic publishing copyright', 'Internet', 'copyright infringement', 'electronic environment', 'copyright protection', 'fair use', 'copyrighted work', 'copyright', 'electronic publishing', 'Internet']","['electronic publishing', 'new publication fields', 'electronic environment', 'copyright infringement', 'wide use', 'fair use', 'electronic', 'copyright', 'Copyright', 'publishing']",570,102,10,570,101,10,0,0,3
"maclp: multi agent constraint logic programming multi agent systems (mas) have become the key technology for decomposing complex problems in order to solve them more efficiently, or for problems distributed in nature. however, many industrial applications, besides their distributed nature, also involve a large number of parameters and constraints, i.e. they are combinatorial. solving such particularly hard problems efficiently requires programming tools that combine mas technology with a programming schema that facilitates the modeling and solution of constraints. this paper presents maclp (multi agent constraint logic programming), a logic programming platform for building, in a declarative way, multi agent systems with constraint-solving capabilities. maclp extends cspcons, a logic programming system that permits distributed program execution through communicating sequential prolog processes with constraints, by providing all the necessary facilities for communication between agents. these facilities abstract from the programmer all the low-level details of the communication and allow him to focus on the development of the agent itself ","MACLP: multi agent constraint logic programming Multi agent systems (MAS) have become the key technology for decomposing complex problems in order to solve them more efficiently, or for problems distributed in nature. However, many industrial applications, besides their distributed nature, also involve a large number of parameters and constraints, i.e. they are combinatorial. Solving such particularly hard problems efficiently requires programming tools that ‘combine MAS technology with a programming schema that facilitates the modeling and solution of constraints. This paper presents MACLP (multi agent constraint logic programming), a logic programming platform for building, in a declarative way, multi agent systems with constraint-solving capabilities. MACLP extends CSPCONS, a logic programming system that permits distributed program execution through ‘communicating sequential Prolog processes with constraints, by providing all the necessary facilities for communication between agents. These facilities abstract from the programmer all the low-level details of the communication and allow him to focus on the development of the agent itself","['multi agent constraint logic programming', 'multi agent systems', 'parameters', 'combinatorial problems', 'hard problems', 'constraint solving', 'distributed program execution', 'communicating sequential Prolog processes', 'constraint handling', 'distributed programming', 'multi-agent systems', 'PROLOG']","['program', 'agent', 'constraints', 'multi', 'logic programming platform', 'logic programming system', 'programming schema', 'programming tools', 'program execution', 'logic']",999,158,12,1001,157,10,19,2,4
"mapping ccf to marc21: an experimental approach the purpose of this article is to raise and address a number of issues pertaining to the conversion of common communication format (ccf) into marc21. in this era of global resource sharing, exchange of bibliographic records from one system to another is imperative in today's library communities. instead of using a single standard to create machine-readable catalogue records, more than 20 standards have emerged and are being used by different institutions. because of these variations in standards, sharing of resources and transfer of data from one system to another among the institutions locally and globally has become a significant problem. addressing this problem requires keeping in mind that countries such as india and others in southeast asia are using the ccf as a standard for creating bibliographic cataloguing records. this paper describes a way to map the bibliographic catalogue records from ccf to marc21, although 100% mapping is not possible. in addition, the paper describes an experimental approach that enumerates problems that may occur during the mapping of records/exchanging of records and how these problems can be overcome ","Mapping CCF to MARC21: an experimental approach The purpose of this article is to raise and address a number of issues pertaining to the conversion of Common Communication Format (CCF) into MARC21. In this era of global resource sharing, exchange of bibliographic records from one system to another is imperative in today's library communities. Instead of using a single standard to create machine-readable catalogue records, more than 20 standards have emerged and are being used by different institutions. Because of these variations in standards, sharing of resources and transfer of data from ‘one system to another among the institutions locally and globally has become a significant problem. Addressing this problem requires keeping in mind that countries such as India and others in southeast Asia are using the CCF as a standard for creating bibliographic cataloguing records. This paper describes a way to map the bibliographic catalogue records from CCF to MARC21, although 100% mapping is not possible. In addition, the paper describes an experimental approach that enumerates problems that may occur during the mapping of records/exchanging of records and how these problems can be overcome","['Common Communication Format conversion', 'MARC21', 'global resource sharing', 'bibliographic records exchange', 'library communities', 'machine-readable catalogue records', 'standards', 'data transfer', 'India', 'southeast Asia', 'CCF to MARC21 mapping', 'cataloguing', 'electronic data interchange', 'library automation', 'records management', 'standards']","['experimental approach', 'records', 'MARC21', 'machine-readable catalogue records', 'bibliographic cataloguing records', 'bibliographic records', 'single standard', 'Mapping cf', 'cf', 'experimental']",1017,186,16,1018,185,10,3,1,6
"questioning the rfp process [telecom] in the current climate, the most serious concern about the purchasing habits of telecom carriers is obviously the lack of spending. even against a backdrop of economic constraints and financial struggles, however, genuine concerns about the purchasing process itself are being raised by some of those closest to it ","Questioning the RFP process [telecom] In the current climate, the most serious concern about the purchasing habits of telecom carriers is obviously the lack of spending. Even against a backdrop of economic constraints and financial struggles, however, genuine concerns about the purchasing process itself are being raised by some of those closest to it","['telecom carriers', 'purchasing process', 'sales cycle', 'request for information', 'request for proposal', 'purchasing', 'telecommunication']","['rip process telecom', 'financial struggles', 'purchasing habits', 'telecom carriers', 'genuine concerns', 'serious concern', 'current climate', 'telecom', 'concern', 'process']",299,55,7,299,54,10,0,0,0
"completion to involution and semidiscretisations we discuss the relation between the completion to involution of linear over-determined systems of partial differential equations with constant coefficients and the properties of differential algebraic equations obtained by their semidiscretisation. for a certain class of ""weakly over-determined"" systems, we show that the differential algebraic equations do not contain hidden constraints, if and only if the original partial differential system is involutive. we also demonstrate how the formal theory can be used to obtain an existence and uniqueness theorem for smooth solutions of strongly hyperbolic systems and to estimate the drift off the constraints, if an underlying equation is numerically solved. finally, we show for general linear systems how the index of differential algebraic equations obtained by semidiscretisations can be predicted from the result of a completion of the partial differential system ","Completion to involution and semidiscretisations We discuss the relation between the completion to involution of linear over-determined systems of partial differential equations with constant coefficients and the properties of differential algebraic equations obtained by their semidiscretisation. For a certain class of ""weakly over-determined” systems, we show that the differential algebraic equations do not contain hidden constraints, if and only if the original partial differential system is involutive. We also demonstrate how the formal theory can be used to obtain an existence and uniqueness theorem for smooth solutions of strongly hyperbolic systems and to estimate the drift off the constraints, if an underlying equation is numerically solved. Finally, we show for general linear systems how the index of differential algebraic equations obtained by semidiscretisations can be predicted from the result of a completion of the partial differential system","['completion', 'involution', 'linear over-determined systems', 'partial differential equations', 'matrices', 'semidiscretisations', 'constant coefficients', 'index', 'differential algebraic equations', 'uniqueness theorem', 'strongly hyperbolic systems', 'hyperbolic equations', 'matrix algebra', 'partial differential equations']","['differential algebraic equations', 'partial differential system', 'system', 'differential', 'partial differential equations', 'linear overdetermined systems', 'general linear systems', 'underlying equation', 'hyperbolic systems', 'involutive']",832,138,14,832,137,10,1,1,6
"embedding the outer automorphism group out(f/sub n/) of a free group of rank n in the group out(f/sub m/) for m > n it is proved that for every n >or= 1, the group out(f/sub n/) is embedded in the group out(f/sub m/) with m = 1 + (n - 1)k/sup n/, where k is an arbitrary natural number coprime to n - 1 ","Embedding the outer automorphism group Out(F/sub n/) of a free group of rank n in the group Out(F/sub m/) for m >n It is proved that for every n >or= 1, the group Out(F/sub n/) is embedded in the group Out(F/sub m/) with m = 1 + (n - 1)k/sup n/, where k is an arbitrary natural number coprime ton - 1","['outer automorphism group embedding', 'free group', 'arbitrary natural number coprime', 'formal logic', 'group theory']","['F/sub n/n', 'F/sub m/s', 'outer automorphism group', 'group', 'free group', 'k/sup n/n', 'rank n', 'n', 'n/n', 'F/sub']",239,65,5,239,62,10,93,41,1
"himalayan information system: a proposed model the information explosion and the development in information technology force us to develop information systems in various fields. the research on himalaya has achieved phenomenal growth in recent years in india. the information requirements of himalayan researchers are divergent in nature. in order to meet these divergent needs, all information generated in various himalayan research institutions has to be collected and organized to facilitate free flow of information. this paper describes the need for a system for himalayan information. it also presents the objectives of himalayan information system (himis). it discusses in brief the idea of setting up a himis and explains its utility to the users. it appeals to the government for supporting the development of such system ","Himalayan information system: a proposed model The information explosion and the development in information technology force us to develop information systems in various fields. The research on Himalaya has achieved phenomenal growth in recent years in India. The information requirements of Himalayan researchers are divergent in nature. In order to meet these divergent needs, all information generated in various Himalayan research institutions has to be collected and organized to facilitate free flow of information. This paper describes the need for a system for Himalayan information. It also presents the objectives of Himalayan information system (HIMIS) It discusses in brief the idea of setting up a HIMIS and explains its utility to the users. It appeals to the government for supporting the development of such system","['Himalayan information system model', 'information explosion', 'information technology', 'India', 'information requirements', 'HIMIS', 'government', 'information network', 'geographic information systems', 'information needs', 'information networks', 'information resources']","['information', 'Himalayan information system', 'information requirements', 'information technology', 'information explosion', 'Himalayan researchers', 'information systems', 'Himalayan', 'such system', 'Himalayan information']",707,126,12,706,125,10,0,1,3
"time-integration of multiphase chemistry in size-resolved cloud models the existence of cloud drops leads to a transfer of chemical species between the gas and aqueous phases. species concentrations in both phases are modified by chemical reactions and by this phase transfer. the model equations resulting from such multiphase chemical systems are nonlinear, highly coupled and extremely stiff. in the paper we investigate several numerical approaches for treating such processes. the droplets are subdivided into several classes. this decomposition of the droplet spectrum into classes is based on their droplet size and the amount of scavenged material inside the drops, respectively. the very fast dissociations in the aqueous phase chemistry are treated as forward and backward reactions. the aqueous phase and gas phase chemistry, the mass transfer between the different droplet classes among themselves and with the gas phase are integrated in an implicit and coupled manner by the second order bdf method. for this part we apply a modification of the code lsode with special linear system solvers. these direct sparse techniques exploit the special block structure of the corresponding jacobian. furthermore we investigate an approximate matrix factorization which is related to operator splitting at the linear algebra level. the sparse jacobians are generated explicitly and stored in a sparse form. the efficiency and accuracy of our time-integration schemes is discussed for four multiphase chemistry systems of different complexity and for a different number of droplet classes ","Time-integration of multiphase chemistry in size-resolved cloud models The existence of cloud drops leads to a transfer of chemical species between the gas and aqueous phases. Species concentrations in both phases are modified by chemical reactions and by this phase transfer. The model equations resulting from such multiphase chemical systems are nonlinear, highly coupled and extremely stiff. In the paper we investigate several numerical approaches for treating Such processes. The droplets are subdivided into several classes. This decomposition of the droplet spectrum into classes is based on their droplet size and the amount of scavenged material inside the drops, respectively. The very fast dissociations in the aqueous phase chemistry are treated as forward and backward reactions. The aqueous phase and gas phase chemistry, the mass transfer between the different droplet classes among themselves and with the gas phase are integrated in an implicit and coupled manner by the second order BDF method. For this part we apply a modification of the code LSODE with special linear system solvers. These direct sparse techniques exploit the special block structure of the corresponding Jacobian. Furthermore we investigate an approximate matrix factorization which is related to operator splitting at the linear algebra level. The sparse Jacobians are generated explicitly and stored in a sparse form. The efficiency and accuracy of our time-integration schemes is discussed for four multiphase chemistry systems of different complexity and for a different number of droplet classes","['multiphase chemistry', 'size-resolved cloud models', 'cloud drops', 'chemical species', 'chemical reactions', 'multiphase chemical systems', 'aqueous phase chemistry', 'gas phase chemistry', 'approximate matrix factorization', 'operator splitting', 'linear algebra', 'sparse Jacobians', 'time-integration schemes', 'air pollution modelling', 'atmospheric chemistry', 'clouds', 'environmental science computing', 'geophysics computing', 'Jacobian matrices', 'matrix decomposition', 'multiphase flow', 'sparse matrices']","['phases', 'multi-phase chemistry systems', 'size-resolved cloud models', 'different droplet classes', 'aqueous phase chemistry', 'gas phase chemistry', 'phase transfer', 'gas phase', 'aqueous phase', 'multi-phase chemistry']",1354,238,22,1354,237,10,0,0,7
"completeness of timed mu crl previously a straightforward extension of the process algebra mu crl was proposed to explicitly deal with time. the process algebra mu crl has been especially designed to deal with data in a process algebraic context. using the features for data, only a minor extension of the language was needed to obtain a very expressive variant of time. previously it contained syntax, operational semantics and axioms characterising timed mu crl. it did not contain an in depth analysis of theory of timed mu crl. this paper fills this gap, by providing soundness and completeness results. the main tool to establish these is a mapping of timed to untimed mu crl and employing the completeness results obtained for untimed mu crl ","Completeness of timed mu CRL Previously a straightforward extension of the process algebra mu CRL was proposed to explicitly deal with time. The process algebra mu CRL has been especially designed to deal with data in a process algebraic context. Using the features for data, only a minor extension of the language was needed to obtain a very expressive variant of time. Previously it contained syntax, operational semantics and axioms characterising timed mu CRL. It did not contain an in depth analysis of theory of timed mu CRL. This paper fills this gap, by providing soundness and completeness results. The main tool to establish these is ‘a mapping of timed to untimed mu CRL and employing the completeness results obtained for untimed mu CRL","['timed mu CRL', 'completeness', 'process algebra', 'operational semantics', 'bisimulation equivalence', 'process algebra']","['process', 'untied mu CRL', 'mu col.', 'straightforward extension', 'algebraic context', 'minor extension', 'mu', 'CRL', 'algebraic', 'mu CRL']",624,125,6,625,124,10,1,1,2
"reachability in contextual nets contextual nets, or petri nets with read arcs, are models of concurrent systems with context dependent actions. the problem of reachability in such nets consists in finding a sequence of transitions that leads from the initial marking of a given contextual net to a given goal marking. the solution to this problem that is presented in this paper consists in constructing a finite complete prefix of the unfolding of the given contextual net, that is a finite prefix in which all the markings that are reachable from the initial marking are present, and in searching in each branch of this prefix for the goal marking by solving an appropriate linear programming problem ","Reachability in contextual nets Contextual nets, or Petri nets with read arcs, are models of concurrent systems with context dependent actions. The problem of reachability in such nets consists in finding a sequence of transitions that leads from the initial marking of a given contextual net to a given goal marking. The solution to this problem that is presented in this paper consists in constructing a finite complete prefix of the unfolding of the given contextual net, that is a finite prefix in which all the markings that are reachable from the initial marking are present, and in searching in each branch of this prefix for the goal marking by solving an appropriate linear programming problem","['contextual nets reachability', 'Petri nets', 'concurrent systems', 'context dependent actions', 'finite prefix', 'goal marking', 'linear programming', 'formal specification', 'linear programming', 'Petri nets', 'process algebra']","['Contextual nets', 'readability', 'finite complete prefix', 'finite prefix', 'Petri nets', 'such nets', 'read arcs', 'nets', 'contextual', 'contextual net']",587,117,11,587,116,10,0,0,5
"mobile commerce: transforming the vision into reality this editorial preface investigates current developments in mobile commerce (m-commerce) and proposes an integrated architecture that supports business and consumer needs in an optimal way to successfully implement m-commerce business processes. the key line of thought is based on the heuristic observation that customers will not want to receive m-commerce offerings to their mobile telephones. as a result, a pull as opposed to a push approach becomes a necessary requirement to conduct m-commerce. in addition, m-commerce has to rely on local, regional, demographic and many other variables to be truly effective. both observations necessitate an m-commerce architecture that allows the coherent integration of enterprise-level systems as well as the aggregation of product and service offerings from many different and partially competing parties into a collaborative m-commerce platform. the key software component within this integrated architecture is an event management engine to monitor, detect, store, process and measure information about outside events that are relevant to all participants in m-commerce ","Mobile commerce: transforming the vision into reality This editorial preface investigates current developments in mobile commerce (M-commerce) and proposes an integrated architecture that supports business and consumer needs in an optimal way to successfully implement M-commerce business processes. The key line of thought is based on the heuristic observation that customers will not want to receive M-commerce offerings to their mobile telephones. As a result, a pull as, ‘opposed to a push approach becomes a necessary requirement to conduct M-commerce. In addition, M-commerce has to rely on local, regional, demographic and many other variables to be truly effective. Both observations necessitate an M-commerce architecture that allows the coherent integration of enterprise-level systems as well as the aggregation of product and service offerings from many different and Partially competing parties into a collaborative M-commerce platform The key software component within this integrated architecture is an ‘event management engine to monitor, detect, store, process and measure information about outside events that are relevant to all participants in M-commerce","['M-commerce', 'mobile commerce', 'integrated architecture', 'consumer needs', 'business needs', 'mobile telephones', 'pull approach', 'collaborative platform', 'event management engine', 'cellular radio', 'electronic commerce', 'integrated software', 'mobile computing']","['commerce', 'integrated architecture', 'collaborative commerce platform', 'commerce business processes', 'commerce architecture', 'commerce offerings', 'editorial preface', 'addition commerce', 'mobile commerce', 'Mobile commerce']",1007,167,13,1009,166,10,11,4,3
"maple 8 keeps everyone happy the author is impressed with the upgrade to the mathematics package maple 8, finding it genuinely useful to scientists and educators. the developments waterloo maple class as revolutionary include a student calculus package, and maplets. the first provides a high-level command set for calculus exploration and plotting (removing the need to work with, say, plot primitives). the second is a package for hand-coding custom graphical user interfaces (guis) using elements such as check boxes, radio buttons, slider bars and pull-down menus. when called, a maplet launches a runtime java environment that pops up a window-analogous to a java applet-to perform a programmed routine, if required passing the result back to the maple worksheet ","Maple 8 keeps everyone happy The author is impressed with the upgrade to the mathematics package Maple 8, finding it genuinely useful to scientists and educators. The developments Waterloo Maple class as revolutionary include a student calculus package, and Maplets. The first provides a high-level command set for calculus exploration and plotting (removing the need to work with, say, plot primitives). The second is a package for hand-coding custom graphical user interfaces (GUIs) using elements such as check boxes, radio buttons, slider bars and pull-down menus. When called, a Maplet launches a runtime Java environment that pops up a window-analogous to a Java applet-to perform a programmed routine, if required passing the result back to the Maple worksheet","['Maple 8 mathematics package', 'student calculus package', 'high-level command set', 'calculus exploration', 'calculus plotting', 'GUIs', 'Maplet', 'runtime Java environment', 'software reviews', 'symbol manipulation']","['Maple', 'mathematics package Maple', 'student calculus package', 'runtime Java environment', 'calculus exploration', 'Maple worksheet', 'upgrade', 'author', 'happy', 'package']",650,119,10,650,118,10,0,0,3
"quantum-information processing by nuclear magnetic resonance: experimental implementation of half-adder and subtractor operations using an oriented spin-7/2 system the advantages of using quantum systems for performing many computational tasks have already been established. several quantum algorithms have been developed which exploit the inherent property of quantum systems such as superposition of states and entanglement for efficiently performing certain tasks. the experimental implementation has been achieved on many quantum systems, of which nuclear magnetic resonance has shown the largest progress in terms of number of qubits. this paper describes the use of a spin-7/2 as a three-qubit system and experimentally implements the half-adder and subtractor operations. the required qubits are realized by partially orienting /sup 133/cs nuclei in a liquid-crystalline medium, yielding a quadrupolar split well-resolved septet. another feature of this paper is the proposal that labeling of quantum states of system can be suitably chosen to increase the efficiency of a computational task ","Quantum-information processing by nuclear magnetic resonance: Experimental implementation of half-adder and subtractor operations using an oriented spin-7/2 system The advantages of using quantum systems for performing many computational tasks have already been established. Several quantum algorithms have been developed which exploit the inherent property of quantum systems such as superposition of states and entanglement for efficiently performing certain tasks. The experimental implementation has been achieved on many quantum systems, of which nuclear magnetic resonance has shown the largest progress in terms of number of qubits. This paper describes the use of a spin-7/2 as a three-qubit system and experimentally implements the half-adder and subtractor operations. The required qubits are realized by partially orienting /sup 133/Cs nuclei ina liquid-crystalline medium, yielding a quadrupolar split well-resolved septet. Another feature of this paper is the proposal that labeling of quantum states of system can be suitably chosen to increase the efficiency of a computational task","['quantum-information processing', 'nuclear magnetic resonance', 'half-adder operations', 'subtractor operations', 'oriented spin-7/2 system', 'quantum systems', 'computational tasks', 'quantum algorithms', 'state superposition', 'entanglement', 'qubits', 'three-qubit system', '/sup 133/Cs nuclei', 'liquid-crystalline medium', 'quadrupolar split well-resolved septet', 'quantum states', 'computational task', '/sup 133/Cs', 'adders', 'computation theory', 'nuclear magnetic resonance', 'quantum gates', 'spin systems']","['quantum', 'nuclear magnetic resonance', 'system', 'Quantum-information processing', 'Several quantum algorithms', 'many computational tasks', 'many quantum systems', 'three-qubit system', 'quantum states', 'quantum systems']",946,154,23,946,152,10,110,36,7
"packet spacing: an enabling mechanism for delivering multimedia content in computational grids streaming multimedia with udp has become increasingly popular over distributed systems like the internet. scientific applications that stream multimedia include remote computational steering of visualization data and video-on-demand teleconferencing over the access grid. however, udp does not possess a self-regulating, congestion-control mechanism; and most best-effort traffic is served by congestion-controlled tcp. consequently, udp steals bandwidth from tcp such that tcp flows starve for network resources. with the volume of internet traffic continuing to increase, the perpetuation of udp-based streaming will cause the internet to collapse as it did in the mid-1980's due to the use of non-congestion-controlled tcp. to address this problem, we introduce the counter-intuitive notion of inter-packet spacing with control feedback to enable udp-based applications to perform well in the next-generation internet and computational grids. when compared with traditional udp-based streaming, we illustrate that our approach can reduce packet loss over 50% without adversely affecting delivered throughput ","Packet spacing: an enabling mechanism for delivering multimedia content in ‘computational grids Streaming multimedia with UDP has become increasingly popular over distributed systems like the Internet. Scientific applications that stream multimedia include remote computational steering of visualization data and video-on-demand teleconferencing over the Access Grid. However, UDP does not possess a self-regulating, congestion-control mechanism; and most best-effort traffic is served by congestion-controlled TCP. Consequently, UDP steals bandwidth from TCP such that TCP flows starve for network resources. With the volume of Internet traffic continuing to increase, the perpetuation of UDP-based streaming will cause the Internet to collapse as it did in the mid-1980's due to the use of non-congestion-controlled TCP. To address this problem, we introduce the counter-intuitive notion of inter-packet spacing with control feedback to enable UDP-based applications to perform well in the next-generation Internet and computational grids. When compared with traditional UDP-based streaming, we illustrate that our approach can reduce packet loss over 50% without adversely affecting delivered throughput","['streaming multimedia', 'UDP', 'distributed systems', 'Internet', 'remote computational steering', 'visualization data', 'inter-packet spacing', 'UDP-based streaming', 'network protocol', 'transport protocols', 'multimedia communication', 'transport protocols']","['computational grids', 'self-regulating congestion-control mechanism', 'traditional uk-based streaming', 'remote computational steering', 'inter-packet spacing', 'multimedia content', 'stream multimedia', 'multimedia', 'Packet spacing', 'uk-based streaming']",1045,162,12,1046,161,10,13,1,4
the uk's national electronic site licensing initiative (nesli) in 1998 the uk created the national electronic site licensing initiative (nesli) to increase and improve access to electronic journals and to negotiate license agreements on behalf of academic libraries. the use of a model license agreement and the success of site licensing is discussed. highlights from an interim evaluation by the joint information systems committee (jisc) are noted and key issues and questions arising from the evaluation are identified ,The UK's National Electronic Site Licensing Initiative (NESLI) In 1998 the UK created the National Electronic Site Licensing Initiative (NESLI) to increase and improve access to electronic journals and to negotiate license agreements on behalf of academic libraries. The use of a model license agreement and the success of site licensing is. discussed. Highlights from an interim evaluation by the Joint Information Systems Committee (JISC) are noted and key issues and questions arising from the evaluation are identified,"['National Electronic Site Licensing Initiative', 'NESLI', 'electronic journals', 'license agreements', 'academic libraries', 'Joint Information Systems Committee', 'usage statistics', 'JISC', 'ICOLC', 'academic libraries', 'contracts', 'electronic publishing', 'information resources', 'library automation']","['site licensing', 'Electronic', 'Initiative', 'National', 'NESLI', 'model license agreement', 'license agreements', 'k', 'Site', 'Licensing']",444,79,14,445,78,10,0,1,4
"efficient feasibility testing for dial-a-ride problems dial-a-ride systems involve dispatching a vehicle to satisfy demands from a set of customers who call a vehicle-operating agency requesting that an item tie picked up from a specific location and delivered to a specific destination. dial-a-ride problems differ from other routing and scheduling problems, in that they typically involve service-related constraints. it is common to have maximum wait time constraints and maximum ride time constraints. in the presence of maximum wait time and maximum ride time restrictions, it is not clear how to efficiently determine, given a sequence of pickups and deliveries, whether a feasible schedule exists. we demonstrate that this, in fact, can be done in linear time ","Efficient feasibility testing for dial-a-ride problems Dial-a-ride systems involve dispatching a vehicle to satisfy demands from a set of customers who call a vehicle-operating agency requesting that an item tie picked up from a specific location and delivered to a specific destination. Dial-a-ride problems differ from other routing and scheduling problems, in that they typically involve service-related constraints. It is common to have maximum wait time constraints and maximum ride time constraints. In the presence of maximum wait time and maximum ride time restrictions, it is not clear how to efficiently determine, given a sequence of pickups and deliveries, whether a feasible schedule exists. We demonstrate that this, in fact, can be done in linear time","['feasibility testing', 'dial-a-ride problems', 'dispatching', 'vehicle-operating agency', 'routing', 'scheduling', 'service-related constraints', 'maximum wait time constraints', 'maximum ride time constraints', 'computational complexity', 'dispatching', 'scheduling', 'transportation']","['dial-a-ride problems', 'maximum wait time', 'Efficient feasibility testing', 'scheduling problems', 'Dial-a-ride systems', 'specific location', 'feasible schedule', 'linear time', 'problems', 'feasibility']",651,117,13,651,116,10,0,0,5
"quantized-state systems: a devs-approach for continuous system simulation a new class of dynamical systems, quantized state systems or qss, is introduced in this paper. qss are continuous time systems where the input trajectories are piecewise constant functions and the state variable trajectories - being themselves piecewise linear functions - are converted into piecewise constant functions via a quantization function equipped with hysteresis. it is shown that qss can be exactly represented and simulated by a discrete event model, within the framework of the devs-approach. further, it is shown that qss can be used to approximate continuous systems, thus allowing their discrete-event simulation in opposition to the classical discrete-time simulation. it is also shown that in an approximating qss, some stability properties of the original system are conserved and the solutions of the qss go to the solutions of the original system when the quantization goes to zero ","Quantized-State Systems: A DEVS-approach for continuous system simulation Anew class of dynamical systems, Quantized State Systems or QSS, is introduced in this paper. QSS are continuous time systems where the input trajectories are piecewise constant functions and the state variable trajectories - being themselves piecewise linear functions - are converted into piecewise constant functions via a quantization function equipped with hysteresis. It is shown that QSS can be exactly represented and simulated by a discrete event model, within the framework of the DEVS-approach. Further, it is shown that QSS can be used to approximate continuous systems, thus allowing their discrete-event simulation in opposition to the classical discrete-time simulation. It is also shown that in an approximating QSS, some stability properties of the original system are conserved and the solutions of the QSS go to the solutions of the original system when the quantization goes to zero","['dynamical systems', 'Quantized State Systems', 'continuous time systems', 'piecewise constant functions', 'discrete event model', 'discrete-event simulation', 'continuous time systems', 'discrete event simulation', 'piecewise constant techniques']","['system', 'piecewise constant functions', 'original system', 'classical discrete-time simulation', 'approximate continuous systems', 'continuous system simulation', 'piecewise linear functions', 'continuous time systems', 'Quantized-State systems', 'dynamical systems']",831,148,9,831,146,10,457,138,1
"characterization of sheet buckling subjected to controlled boundary constraints a wedge strip test is designed to study the onset and post-buckling behavior of a sheet under various boundary constraints. the device can be easily incorporated into a conventional tensile test machine, and material resistance to buckling is measured as the buckling height versus the in-plane strain state. the design yields different but consistent buckling modes with easy changes of boundary conditions (either clamped or freed) and sample geometry. experimental results are then used to verify a hybrid approach to buckling prediction, i.e., the combination of the fem analysis and an energy-based analytical wrinkling criterion. the fem analysis is used to obtain the stress field and deformed geometry in a complex forming condition, while the analytical solution is to provide the predictions less sensitive to artificial numerical parameters. a good agreement between experimental data and numerical predictions is obtained ","Characterization of sheet buckling subjected to controlled boundary constraints Awedge strip test is designed to study the onset and post-buckling behavior of a sheet under various boundary constraints. The device can be easily incorporated into a conventional tensile test machine, and material resistance to buckling is measured as the buckling height versus the in-plane strain state. The design yields different but consistent buckling modes with easy changes of boundary conditions (either clamped or freed) and sample geometry. Experimental results are then used to verify a hybrid approach to buckling prediction, i., the combination of the FEM analysis and an energy-based analytical wrinkling criterion The FEM analysis is used to obtain the stress field and deformed geometry in a complex forming condition, while the analytical solution is to provide the predictions less sensitive to artificial numerical parameters. A good agreement between experimental data and numerical predictions is obtained","['wedge strip test', 'boundary constraints', 'sheet buckling', 'forming processes', 'tensile test machine', 'strain state', 'energy-based analytical wrinkling criterion', 'stress field', 'deformed geometry', 'finite element analysis', 'buckling', 'deformation', 'finite element analysis', 'forming processes', 'stress effects']","['controlled boundary constraints', 'various boundary constraints', 'sheet', 'post-buckling behavior', 'boundary conditions', 'Characterization', 'strip test', 'boundary', 'constraints', 'test']",866,149,15,863,147,10,480,138,5
"tcp explicit congestion notification over atm-ubr: a simulation study the enhancement of transmission control protocol's (tcp's) congestion control mechanisms using explicit congestion notification (ecn) over asynchronous transfer mode (atm) networks is overviewed. tcp's congestion control is enhanced so that congestion is indicated by not only packet losses as is currently the case but an agent implemented at the atm network's edge as well. the novel idea uses efci (explicit forward congestion indication) bits (available in every atm cell header) to generalize the ecn response to the ubr (unspecified bit rate) service, notify congestion, and adjust the credit-based window size of the tcr. the authors' simulation experiments show that tcp ecn achieves significantly lower cell loss, packet retransmissions, and buffer utilization, and exhibits better throughput than (non-ecn) tcp reno ","TCP explicit congestion notification over ATM-UBR: a simulation study The enhancement of transmission control protocol's (TCP's) congestion control mechanisms using explicit congestion notification (ECN) over asynchronous transfer mode (ATM) networks is overviewed. TCP's congestion control is enhanced so that congestion is indicated by not only packet losses as is currently the case but an agent implemented at the ATM network's edge as well. The novel idea uses EFCI (explicit forward congestion indication) bits (available in every ATM cell header) to generalize the ECN response to the UBR (unspecified bit rate) service, notify congestion, and adjust the credit-based window size of the TCR. The authors' simulation experiments show that TCP ECN achieves significantly lower cell loss, packet retransmissions, and buffer utilization, and exhibits better throughput than (non-ECN) TCP Reno","['TCP explicit congestion notification', 'ATM-UBR', 'simulation', 'congestion control mechanisms', 'ATM networks', 'packet losses', 'agent', 'explicit forward congestion indication bits', 'unspecified bit rate service', 'credit-based window size', 'cell loss', 'packet retransmissions', 'buffer utilization', 'throughput', 'asynchronous transfer mode', 'digital simulation', 'local area networks', 'telecommunication congestion control', 'transport protocols', 'wide area networks']","['congestion', 'explicit congestion notification', 'transmission control protocols', 'authors simulation experiments', 'congestion control mechanisms', 'congestion indication bits', 'types congestion control', 'TCP Reno', 'TCP ec', 'TCP']",768,129,20,768,128,10,0,0,9
"wired right [accounting] from business intelligence to wireless networking to service providers, here is what you need to know to keep up to speed with a changing landscape ","Wired right [accounting] From business intelligence to wireless networking to service providers, here is what you need to know to keep up to speed with a changing landscape","['accounting', 'business intelligence', 'wireless networking', 'service providers', 'accounting', 'computer based training', 'document handling', 'information resources', 'management information systems', 'marketing', 'outsourcing', 'teleconferencing', 'wireless LAN']","['business intelligence', 'wireless networking', 'service providers', 'right accounting', 'right', 'wireless', 'business', 'networking', 'accounting', 'intelligence']",145,29,13,145,28,10,0,0,0
"design of 1-d and 2-d variable fractional delay allpass filters using weighted least-squares method in this paper, a weighted least-squares method is presented to design one-dimensional and two-dimensional variable fractional delay allpass filters. first, each coefficient of the variable allpass filter is expressed as the polynomial of the fractional delay parameter. then, the nonlinear phase error is approximated by a weighted equation error such that the cost function can be converted into a quadratic form. next, by minimizing the weighted equation error, the optimal polynomial coefficients can be obtained iteratively by solving a set of linear simultaneous equations at each iteration. finally, the design examples are demonstrated to illustrate the effectiveness of the proposed approach ","Design of 1-D and 2-D variable fractional delay allpass filters using weighted least-squares method In this paper, a weighted least-squares method is presented to design ‘one-dimensional and two-dimensional variable fractional delay allpass filters. First, each coefficient of the variable allpass filter is expressed as the polynomial of the fractional delay parameter. Then, the nonlinear phase error is approximated by a weighted equation error such that the cost function can be converted into a quadratic form. Next, by minimizing the weighted equation error, the optimal polynomial coefficients can be obtained iteratively by solving a set of linear simultaneous equations at each iteration. Finally, the design examples are demonstrated to illustrate the effectiveness of the proposed approach","['weighted least-squares method', 'variable fractional delay allpass filters', '1D allpass filters', '2D allpass filters', 'fractional delay parameter', 'nonlinear phase error approximation', 'weighted equation error', 'cost function', 'optimal polynomial coefficients', 'linear simultaneous equations', 'all-pass filters', 'delays', 'filtering theory', 'iterative methods', 'least squares approximations', 'polynomials']","['weighted least-squares method', 'weighted equation error', 'optimal polynomial coefficients', 'fractional delay parameters', 'variable malpass filter', 'nonlinear phase error', 'd-d', 'delay', 'variable', 'fractional']",685,116,16,686,115,10,15,1,2
"tracking control of the flexible slider-crank mechanism system under impact the variable structure control (vsc) and the stabilizer design by using the pole placement technique are applied to the tracking control of the flexible slider-crank mechanism under impact. the vsc strategy is employed to track the crank angular position and speed, while the stabilizer design is involved to suppress the flexible vibrations simultaneously. from the theoretical impact consideration, three approaches including the generalized momentum balance (gmb), the continuous force model (cfm), and the cfm associated with the effective mass compensation emc are adopted, and are derived on the basis of the energy and impulse-momentum conservations. simulation results are provided to demonstrate the performance of the motor-controller flexible slider-crank mechanism not only accomplishing good tracking trajectory of the crank angle, but also eliminating vibrations of the flexible connecting rod ","Tracking control of the flexible slider-crank mechanism system under impact The variable structure control (VSC) and the stabilizer design by using the pole placement technique are applied to the tracking control of the, flexible slider-crank mechanism under impact. The VSC strategy is ‘employed to track the crank angular position and speed, while the stabilizer design is involved to suppress the flexible vibrations simultaneously. From the theoretical impact consideration, three approaches including the generalized momentum balance (GMB), the continuous force model (CFM), and the CFM associated with the effective mass compensation EMC are adopted, and are derived on the basis of the energy and impulse-momentum conservations. Simulation results are provided to demonstrate the performance of the motor-controller flexible slider-crank mechanism not only accomplishing good tracking trajectory of the crank angle, but also eliminating vibrations of the flexible connecting rod","['tracking control', 'flexible slider-crank mechanism system', 'impact', 'variable structure control', 'stabilizer design', 'crank angular position', 'flexible vibrations', 'generalized momentum balance', 'continuous force model', 'effective mass compensation', 'conservation laws', 'tracking trajectory', 'flexible connecting rod', 'multibody dynamics', 'pole placement technique', 'flexible structures', 'impact (mechanical)', 'position control', 'tracking', 'variable structure systems', 'vibration control']","['flexible slider-crank mechanism', 'stabilizer design', 'tracking control', 'theoretical impact consideration', 'variable structure control', 'good tracking trajectory', 'flexible vibrations', 'flexible', 'control', 'mechanism']",846,139,21,848,138,10,8,2,5
"using devs formalism to operationalize elp models for diagnosis in sachem this paper describes an original approach to discrete event control of continuous processes by means of expert knowledge. we present an application of this approach on the sachem diagnosis subsystem. the sachem system is a large-scale knowledge-based system that aims in helping a set of operators to control the dynamics of complex continuous systems (e.g., blast furnaces). the proposed method is based on: (i) the definition of a language facilitating the acquisition and representation of expert knowledge, called elp (expert language process); (ii) the use of the devs formalism to make elp models operational; (iii) algorithms for exploiting operational models ","Using DEVS formalism to operationalize ELP models for diagnosis in SACHEM This paper describes an original approach to discrete event control of continuous processes by means of expert knowledge. We present an application of this approach on the SACHEM diagnosis subsystem. The SACHEM system is a large-scale knowledge-based system that aims in helping a set of operators to control the dynamics of complex continuous systems (e.g., blast furnaces). The proposed method is based ‘on: (i) The definition of a language facilitating the acquisition and representation of expert knowledge, called ELP (Expert Language Process); (ii) The use of the DEVS formalism to make ELP models ‘operational; (iil) Algorithms for exploiting operational models","['SACHEM', 'ELP models', 'discrete event control', 'continuous processes', 'SACHEM system', 'expert knowledge', 'complex continuous systems', 'knowledge representation', 'knowledge acquisition', 'DEVS', 'discrete event simulation', 'knowledge acquisition', 'knowledge representation', 'process control']","['expert knowledge', 'help models', 'large-scale knowledge-based system', 'sachet diagnosis subsystem', 'complex continuous systems', 'continuous processes', 'operational models', 'original approach', 'sachet system', 'help']",630,112,14,632,111,10,16,3,3
"lr parsing for conjunctive grammars the generalized lr parsing algorithm for context-free grammars, introduced by tomita in 1986, is a polynomial-time implementation of nondeterministic lr parsing that uses graph-structured stack to represent the contents of the nondeterministic parser's pushdown for all possible branches of computation at a single computation step. it has been specifically developed as a solution for practical parsing tasks arising in computational linguistics, and indeed has proved itself to be very suitable for natural language processing. conjunctive grammars extend context-free grammars by allowing the use of an explicit intersection operation within grammar rules. this paper develops a new lr-style parsing algorithm for these grammars, which is based on the very same idea of a graph-structured pushdown, where the simultaneous existence of several paths in the graph is used to perform the mentioned intersection operation. the underlying finite automata are treated in the most general way: instead of showing the algorithm's correctness for some particular way of constructing automata, the paper defines a wide class of automata usable with a given grammar, which includes not only the traditional lr(k) automata, but also, for instance, a trivial automaton with a single reachable state. a modification of the slr(k) table construction method that makes use of specific properties of conjunctive grammars is provided as one possible way of making finite automata to use with the algorithm ","LR parsing for conjunctive grammars The generalized LR parsing algorithm for context-free grammars, introduced by Tomita in 1986, is a polynomial-time implementation of nondeterministic LR parsing that uses graph-structured stack to represent the contents of the nondeterministic parser's pushdown for all possible branches of computation at a single computation step. It has been specifically developed as a solution for practical parsing tasks arising in computational linguistics, and indeed has proved itself to be very suitable for natural language processing. Conjunctive grammars extend context-free grammars by allowing the use of an explicit intersection operation within grammar rules. This paper develops a new LR-style Parsing algorithm for these grammars, which is based on the very same idea of a graph-structured pushdown, where the simultaneous existence of several paths in the graph is used to perform the mentioned intersection operation. The underlying finite automata are treated in the most general way: instead of showing the algorithm's correctness for some particular way of constructing automata, the paper defines a wide class of automata usable with a given grammar, which includes not only the traditional LR(k) automata, but also, for instance, a trivial automaton with a single reachable state. A modification of the SLR(k) table construction method that makes use of specific properties of conjunctive grammars is provided as one possible way of making finite automata to use with the algorithm","['conjunctive grammars', 'generalized LR parsing algorithm', 'graph-structured stack', 'nondeterministic parser pushdown', 'computation', 'computational linguistics', 'natural language processing', 'context-free grammars', 'explicit intersection operation', 'grammar rules', 'finite automata', 'trivial automaton', 'single reachable state', 'Boolean closure', 'deterministic context-free languages', 'computational linguistics', 'context-free grammars', 'context-free languages', 'pushdown automata']","['context-free grammars', 'nondeterministic LR parsing', 'practical parsing tasks', 'grammars', 'traditional LR', 'grammar rules', 'LR', 'LR parsing', 'conjunctive grammars', 'Conjunctive grammars']",1301,227,19,1301,226,10,0,0,9
"hybrid simulation of space plasmas: models with massless fluid representation of electrons. iv. kelvin-helmholtz instability for pt.iii. see prikl. mat. informatika, maks press, no. 4, p. 5-56 (2000). this is a survey of the literature on hybrid simulation of the kelvin-helmholtz instability. we start with a brief review of the theory: the simplest model of the instability - a transition layer in the form of a tangential discontinuity; compressibility of the medium; finite size of the velocity shear region; pressure anisotropy. we then describe the electromagnetic hybrid model (ions as particles and electrons as a massless fluid) and the main numerical schemes. we review the studies on two-dimensional and three-dimensional hybrid simulation of the process of particle mixing across the magnetopause shear layer driven by the onset of a kelvin-helmholtz instability. the article concludes with a survey of literature on hybrid simulation of the kelvin-helmholtz instability in finite-size objects: jets moving across the magnetic field in the middle of the field reversal layer; interaction between a magnetized plasma flow and a cylindrical plasma source with zero own magnetic field ","Hybrid simulation of space plasmas: models with massless fluid representation of electrons. IV. Kelvin-Helmholtz instability For ptill. see Prikl. Mat. Informatika, MAKS Press, no. 4, p. 5-56 (2000) This is a survey of the literature on hybrid simulation of the Kelvin-Helmholtz instability. We start with a brief review of the theory: the simplest model of the instability - a transition layer in the form of a tangential discontinuity; compressibility of the medium: finite size of the velocity shear region; pressure anisotropy. We then describe the electromagnetic hybrid model (ions as particles and electrons as a massless fluid) and the main numerical schemes. We review the studies on two-dimensional and three-dimensional hybrid simulation of the process of particle mixing across the magnetopause shear layer driven by the onset of a Kelvin-Helmholtz instability. The article concludes with a survey of literature on hybrid simulation of the Kelvin-Helmholtz instability in finite-size objects: jets moving across the magnetic field in the middle of the field reversal layer: interaction between a magnetized plasma flow and a cylindrical plasma source with zero own magnetic field","['hybrid simulation', 'space plasmas', 'massless fluid representation', 'Kelvin-Helmholtz instability', 'transition layer', 'tangential discontinuity', 'pressure anisotropy', 'electromagnetic hybrid model', 'three-dimensional hybrid simulation', 'magnetopause shear layer', 'field reversal layer', 'magnetized plasma flow', 'cylindrical plasma source', 'flow instability', 'flow simulation', 'hybrid simulation', 'magnetohydrodynamics', 'plasma instability', 'stability']","['Kelvin-Helmholtz instability', 'three-dimensional hybrid simulation', 'massless fluid representation', 'cylindrical plasma source', 'space plasmas models', 'plasma flow', 'simulation', 'hybrid simulation', 'Hybrid simulation', 'massless fluid']",1015,180,19,1013,179,10,6,4,6
diagnostic expert system using non-monotonic reasoning the objective of this work is to develop an expert system for cucumber disorder diagnosis using non-monotonic reasoning to handle the situation when the system cannot reach a conclusion. one reason for this situation is when the information is incomplete. another reason is when the domain knowledge itself is incomplete. another reason is when the information is inconsistent. this method maintains the truth of the system in case of changing a piece of information. the proposed method uses two types of non-monotonic reasoning namely: default reasoning and reasoning in the presence of inconsistent information to achieve its goal ,Diagnostic expert system using non-monotonic reasoning The objective of this work is to develop an expert system for cucumber disorder diagnosis using non-monotonic reasoning to handle the situation when the system cannot reach a conclusion. One reason for this situation is when the information is incomplete. Another reason is when the domain knowledge itself is incomplete. Another reason is when the information is inconsistent. This method maintains the truth of the system in case of changing a piece of information. The proposed method uses two types ‘of non-monotonic reasoning namely: default reasoning and reasoning in the presence of inconsistent information to achieve its goal,"['diagnostic expert system', 'nonmonotonic reasoning', 'cucumber disorder diagnosis', 'incomplete information', 'inconsistent information', 'truth maintenance', 'default reasoning', 'agriculture', 'agriculture', 'diagnostic expert systems', 'nonmonotonic reasoning', 'truth maintenance', 'uncertainty handling']","['non-monotonic reasoning', 'reason', 'cucumber disorder diagnosis', 'inconsistent information', 'Diagnostic expert system', 'default reasoning', 'system cannon', 'system', 'expert system', 'non-monotonic']",585,105,13,586,104,10,2,1,0
"the effect of voxel size on the accuracy of dose-volume histograms of prostate /sup 125/i seed implants cumulative dose-volume histograms (dvh) are crucial in evaluating the quality of radioactive seed prostate implants. when calculating dvhs, the choice of voxel size is a compromise between computational speed (larger voxels) and accuracy (smaller voxels). we quantified the effect of voxel size on the accuracy of dvhs using an in-house computer program. the program was validated by comparison with a hand-calculated dvh for a single 0.4-u iodine-125 model 6711 seed. we used the program to find the voxel size required to obtain accurate dvhs of five iodine-125 prostate implant patients at our institution. one-millimeter cubes were sufficient to obtain dvhs that are accurate within 5% up to 200% of the prescription dose. for the five patient plans, we obtained good agreement with the variseed (version 6.7, varian, usa) treatment planning software's dvh algorithm by using voxels with a sup-inf dimension equal to the spacing between successive transverse seed implant planes (5 mm). the volume that receives at least 200% of the target dose, v/sub 200/, calculated by variseed was 30% to 43% larger than that calculated by our program with small voxels. the single-seed dvh calculated by variseed fell below the hand calculation by up to 50% at low doses (30 gy), and above it by over 50% at high doses (>250 gy) ","The effect of voxel size on the accuracy of dose-volume histograms of prostate Jsup 125/1 seed implants Cumulative dose-volume histograms (DVH) are crucial in evaluating the quality of radioactive seed prostate implants. When calculating DVHs, the choice of voxel size is a compromise between computational speed (larger voxels) and accuracy (smaller voxels). We quantified the effect of voxel size on the accuracy of DVHs using an in-house computer program. The program was validated by comparison with a hand-calculated DVH for a single 0.4-U iodine-125 model 6711 seed. We used the program to find the voxel size required to obtain accurate DVHs of five iodine-125 prostate implant patients at our institution. One-millimeter cubes were sufficient to obtain DVHs that are accurate within 5% up to 200% of the prescription dose. For the five patient plans, we obtained good agreement with the VariSeed (version 6.7, Varian, USA) treatment planning software's DVH algorithm by using voxels with a sup-int dimension equal to the spacing between successive transverse seed implant planes (5 mm). The volume that receives at least 200% of the target dose, V/sub 200/, calculated by VariSeed was 30% to 43% larger than that calculated by our program with small voxels. The single-seed DVH calculated by VariSeed fell below the hand calculation by up to 50% at low doses (30 Gy), and above it by over 50% at high doses (>250 cy)","['cumulative dose-volume histograms', 'prostate /sup 125/I seed implants', 'radioactive seed prostate implants', 'voxel size', 'computational speed', 'in-house computer program', 'hand-calculated dose-volume histograms', 'single-seed dose-volume histograms', '/sup 125/I model', '/sup 125/I prostate implant patients', ""VariSeed treatment planning software's dose-volume histogram algorithm"", 'I', 'dosimetry', 'iodine', 'medical computing', 'radiation therapy', 'radioactive sources', 'radioisotopes']","['vowel size', 'Cumulative dose-volume histograms', 'successive transverse seed', 'in-house computer program', 'iodine-131 prostate', 'larger vowels', 'small vowels', 'vowels', 'dose-volume histograms', 'size']",1195,231,18,1195,230,10,4,4,3
"rank tests of association for exchangeable paired data we describe two rank tests of association for paired exchangeable data motivated by the study of lifespans in twins. the pooled sample is ranked. the nonparametric test of association is based on r/sup +/, the sum of the smaller within-pair ranks. a second measure l/sup +/ is the sum of within-pair rank products. under the null hypothesis of within-pair independence, the two test statistics are approximately normally distributed. expressions for the exact means and variances of r/sup +/ and l/sup +/ are given. we describe the power of these two statistics under a close alternative hypothesis to that of independence. both the r/sup +/ and l/sup +/ tests indicate nonparametric statistical evidence of positive association of longevity in identical twins and a negligible relationship between the lifespans of fraternal twins listed in the danish twin registry. the statistics are also applied to the analysis of a clinical trial studying the time to failure of ventilation tubes in children with bilateral otitis media ","Rank tests of association for exchangeable paired data We describe two rank tests of association for paired exchangeable data motivated by the study of lifespans in twins. The pooled sample is ranked. The nonparametric test of association is based on R/sup +/, the sum of the smaller within-pair ranks. A second measure L/sup +/ is the sum of within-pair rank products. Under the null hypothesis of within-pair independence, the two test statistics are approximately normally distributed. Expressions for the exact means and variances of Risup +/ and L/sup #/ are given. We describe the power of these two statistics under a close alternative hypothesis to that of independence. Both the R/sup +/ and L/sup +/ tests indicate nonparametric statistical evidence of positive association of longevity in identical twins and a negligible relationship between the lifespans of fraternal twins listed in the Danish twin registry. The statistics are also applied to the analysis of a clinical trial studying the time to failure of ventilation tubes in children with bilateral otitis media","['rank tests', 'association', 'paired exchangeable data', 'twin lifespans', 'pooled sample', 'nonparametric test', 'within-pair ranks', 'within-pair rank products', 'null hypothesis', 'within-pair independence', 'test statistics', 'exact means', 'exact variances', 'nonparametric statistical evidence', 'longevity', 'identical twins', 'fraternal twins', 'Danish twin registry', 'clinical trial', 'ventilation tube failure time', 'bilateral otitis media', 'data analysis', 'medical computing', 'paediatrics', 'statistical analysis']","['within-pair rank products', 'smaller within-pair ranks', 'paired exchangeable data', 'exchangeable paired data', 'positive association', 'non-parametric test', 'test statistics', 'tests', 'rank tests', 'Rank tests']",911,171,25,911,170,10,2,2,10
"designing human-centered distributed information systems many computer systems are designed according to engineering and technology principles and are typically difficult to learn and use. the fields of human-computer interaction, interface design, and human factors have made significant contributions to ease of use and are primarily concerned with the interfaces between systems and users, not with the structures that are often more fundamental for designing truly human-centered systems. the emerging paradigm of human-centered computing (hcc)-which has taken many forms-offers a new look at system design. hcc requires more than merely designing an artificial agent to supplement a human agent. the dynamic interactions in a distributed system composed of human and artificial agents-and the context in which the system is situated-are indispensable factors. while we have successfully applied our methodology in designing a prototype of a human-centered intelligent flight-surgeon console at nasa johnson space center, this article presents a methodology for designing human-centered computing systems using electronic medical records (emr) systems ","Designing human-centered distributed information systems Many computer systems are designed according to engineering and technology principles and are typically difficult to learn and use. The fields of human-computer interaction, interface design, and human factors have made significant contributions to ease of use and are primarily concerned with the interfaces between systems and users, not with the structures that are often more fundamental for designing truly human-centered systems. The emerging paradigm of human-centered ‘computing (HCC)-which has taken many forms-offers a new look at system design. HCC requires more than merely designing an artificial agent to supplement a human agent. The dynamic interactions in a distributed system composed of human and artificial agents-and the context in which the system is situated-are indispensable factors. While we have successfully applied our methodology in designing a prototype of a human-centered intelligent flight-surgeon console at NASA Johnson Space Center, this article presents a methodology for designing human-centered computing systems using electronic medical records (EMR) systems","['human-centered distributed information systems design', 'distributed cognition', 'artificial agents', 'human agents', 'multiple analysis levels', 'human-computer interaction', 'interface design', 'human factors', 'human-centered computing systems', 'human-centered intelligent flight surgeon console', 'NASA Johnson Space Center', 'electronic medical records systems', 'human factors', 'information systems', 'medical information systems', 'records management', 'software agents', 'task analysis', 'user centred design', 'user interfaces']","['systems', 'human-centered', 'human-centered computing systems', 'human-centered systems', 'Many computer systems', 'information systems', 'system design cc', 'human factors', 'human agent', 'human-centered computing']",997,160,20,998,159,10,9,1,3
"novel line conditioner with voltage up/down capability in this paper, a novel pulsewidth-modulated line conditioner with fast output voltage control is proposed. the line conditioner is made up of an ac chopper with reversible voltage control and a transformer for series voltage compensation. in the ac chopper, a proper switching operation is achieved without the commutation problem. to absorb energy stored in line stray inductance, a regenerative dc snubber can be utilized which has only one capacitor without discharging resistors or complicated regenerative circuit for snubber energy. therefore, the proposed ac chopper gives high efficiency and reliability. the output voltage of the line conditioner is controlled using a fast sensing technique of the output voltage. it is also shown via some experimental results that the presented line conditioner gives good dynamic and steady-state performance for high quality of the output voltage ","Novel line conditioner with voltage up/down capability In this paper, a novel pulsewidth-modulated line conditioner with fast output voltage control is proposed. The line conditioner is made up of an AC chopper with reversible voltage control and a transformer for series voltage compensation. In the AC chopper, a proper switching operation is achieved without the commutation problem. To absorb energy stored in line stray inductance, a regenerative DC snubber can be utilized which has only one capacitor without discharging resistors or complicated regenerative circuit for snubber energy. Therefore, the proposed AC chopper gives high efficiency and reliability. The output voltage of the line conditioner is controlled using a fast sensing technique of the output voltage. It is also shown via some experimental results that the presented line conditioner gives good dynamic and steady-state performance for high quality of the output voltage","['pulsewidth-modulated line conditioner', 'output voltage control', 'AC chopper', 'reversible voltage control', 'series voltage compensation transformer', 'switching operation', 'commutation', 'line stray inductance', 'regenerative DC snubber', 'dynamic performance', 'steady-state performance', 'choppers (circuits)', 'commutation', 'power transformers', 'PWM power convertors', 'snubbers', 'voltage control']","['output voltage', 'AC chopper', 'voltage', 'series voltage compensation', 'voltage up/down capability', 'reversible voltage control', 'Novel line conditioner', 'line', 'conditioner', 'line conditioner']",808,142,17,808,141,10,0,0,5
"modeling and simulation of an abr flow control algorithm using a virtual source/virtual destination switch the available bit rate (abr) service class of asynchronous transfer mode networks uses a feedback control mechanism to adapt to varying link capacities. the virtual source/virtual destination (vs/vd) technique offers the possibility of segmenting the otherwise end-to-end abr control loop into separate loops. the improved feedback delay and control of abr traffic inside closed segments provide a better performance for abr connections. this article presents the use of classical linear control theory to model and develop an abr vs/vd flow control algorithm. discrete event simulations are used to analyze the behavior of the algorithm with respect to transient behavior and correctness of the control model. linear control theory offers the means to derive correct choices of parameters and to assess performance issues, such as stability of the system, during the design phase. the performance goals are high link utilization, fair bandwidth distribution, and robust operation in various environments, which are verified by discrete event simulations. the major contribution of this work is the use of analytic methods (linear control theory) to model and design an abr flow control algorithm tailored for the special layout of a vs/vd switch, and the use of simulation techniques to verify the result ","Modeling and simulation of an ABR flow control algorithm using a virtual source/virtual destination switch The available bit rate (ABR) service class of asynchronous transfer mode networks uses a feedback control mechanism to adapt to varying link capacities. The virtual source/virtual destination (VS/VD) technique offers the possibility of segmenting the otherwise end-to-end ABR control loop into separate loops. The improved feedback delay and control of ABR traffic inside closed segments provide a better performance for ABR connections. This article presents the use of classical linear control theory to model and develop an ABR VSIVD flow control algorithm. Discrete event simulations are used to analyze the behavior of the algorithm with respect to transient behavior and correctness of the control model. Linear control theory offers the means to derive correct choices of parameters and to assess performance issues, such as stability of the system, during the design phase. The performance goals are high link utilization, fair bandwidth distribution, and robust operation in various environments, which are verified by discrete event simulations. The major contribution of this work is the use of analytic methods (linear control theory) to model and design an ABR flow control algorithm tailored for the special layout of a VS/VD switch, and the use of simulation techniques to verify the result","['modeling', 'ABR flow control algorithm', 'virtual source/virtual destination switch', 'ATM networks', 'available bit rate service class', 'feedback control mechanism', 'link capacities', 'control loop', 'feedback delay', 'traffic control', 'closed segments', 'classical linear control theory', 'discrete event simulations', 'transient behavior', 'control model', 'performance issues', 'stability', 'high link utilization', 'fair bandwidth distribution', 'robust operation', 'asynchronous transfer mode', 'discrete event simulation', 'feedback', 'telecommunication congestion control', 'telecommunication traffic']","['control', 'virtual source/virtual destination', 'feedback control mechanism', 'air connections', 'control model', 'air traffic', 'discrete event simulations', 'Discrete event simulations', 'linear control theory', 'Linear control theory']",1200,214,25,1200,213,10,1,1,10
"minimised geometric buchberger algorithm for integer programming recently, various algebraic integer programming (ip) solvers have been proposed based on the theory of grobner bases. the main difficulty of these solvers is the size of the grobner bases generated. in algorithms proposed so far, large grobner bases are generated by either introducing additional variables or by considering the generic ip problem ip/sub a,c/. some improvements have been proposed such as hosten and sturmfels' method (grin) designed to avoid additional variables and thomas' truncated grobner basis method which computes the reduced grobner basis for a specific ip problem ip/sub a,c/(b) (rather than its generalisation ipa,c). in this paper we propose a new algebraic algorithm for solving ip problems. the new algorithm, called minimised geometric buchberger algorithm, combines hosten and sturmfels' grin and thomas' truncated grobner basis method to compute the fundamental segments of an ip problem ip/sub a,c/ directly in its original space and also the truncated grobner basis for a specific ip problem ip/sub a,c/ (b). we have carried out experiments to compare this algorithm with others such as the geometric buchberger algorithm, the truncated geometric buchberger algorithm and the algorithm in grin. these experiments show that the new algorithm offers significant performance improvement ","Minimised geometric Buchberger algorithm for integer programming Recently, various algebraic integer programming (IP) solvers have been proposed based on the theory of Grobner bases. The main difficulty of these solvers is the size of the Grobner bases generated. In algorithms proposed so far, large Grobner bases are generated by either introducing additional variables or by considering the generic IP problem IP/sub A.C/. Some improvements have been proposed such as Hosten and Sturmfels' method (GRIN) designed to avoid additional variables and Thomas’ truncated Grobner basis method which computes the reduced Grobner basis for a specific IP problem IP/sub A,C/(b) (rather than its generalisation IPA,C). In this paper we propose a new algebraic algorithm for solving IP problems. The new algorithm, called Minimised Geometric Buchberger Algorithm, combines Hosten and Sturmfels' GRIN and Thomas’ truncated Grobner basis method to compute the fundamental segments of an IP problem IP/sub A,C/ directly in its original space and also the truncated Grobner basis for a specific IP problem IP/sub A,C/ (b). We have carried out experiments to compare this algorithm with others such as the geometric Buchberger algorithm, the truncated geometric Buchberger algorithm and the algorithm in GRIN. These experiments show that the new algorithm offers significant performance improvement","['algebraic integer programming', 'minimised geometric Buchberger algorithm', 'Grobner bases', 'GRIN algorithm', 'truncated Grobner basis method', 'reduced Grobner basis', 'fundamental segments', 'geometric Buchberger algorithm', 'truncated geometric Buchberger algorithm', 'performance improvement', 'integer programming', 'minimisation']","['geometric Buchberger algorithm', 'IP problem IP/sub', 'algorithm', 'integer programming', 'new algorithm', 'basis method', 'new algebraic algorithm', 'IP problems', 'geometric', 'Buchberger']",1182,204,12,1182,203,10,3,3,3
"post-haste. 100th robotic containerization system installed in us mail sorting center spot welding, machine tending, material handling, picking, packing, painting, palletizing, assembly...the list of tasks being performed by abb robots keeps on growing. adding to this portfolio is a new robot containerization system (rcs) that abb developed specifically for the united states postal service (usps). the rcs has brought new levels of speed, accuracy, efficiency and productivity to the process of sorting and containerizing mail and packages. recently, the 100th abb rcs was installed at the usps processing and distribution center in columbus, ohio ","Post-haste. 100th robotic containerization system installed in US mail sorting center Spot welding, machine tending, material handling, picking, packing, painting, palletizing, assembly...the list of tasks being performed by ABB robots keeps on growing. Adding to this portfolio is a new robot containerization system (RCS) that ABB developed specifically for the United States Postal Service (USPS). The RCS has brought new levels of speed, accuracy, efficiency and productivity to the process of sorting and containerizing mail and packages. Recently, the 100th ABB RCS was installed at the USPS processing and distribution center in Columbus, Ohio","['mail sorting center', 'robotic containerization system', 'USA', 'ABB robots', 'United States Postal Service', 'mail sorting', 'packages sorting', 'industrial robots', 'materials handling', 'postal services', 'sorting']","['containerization', 'packing painting palletizing', 'distribution center', 'uses processing', '100th ABB RCS', 'ABB robots', 'US mail', 'ABB', '100th', 'robots']",557,95,11,557,94,10,0,0,2
"a 120-mw 3-d rendering engine with 6-mb embedded dram and 3.2-gb/s runtime reconfigurable bus for pda chip a low-power three-dimensional (3-d) rendering engine is implemented as part of a mobile personal digital assistant (pda) chip. six-megabit embedded dram macros attached to 8-pixel-parallel rendering logic are logically localized with a 3.2-gb/s runtime reconfigurable bus, reducing the area by 25% compared with conventional local frame-buffer architectures. the low power consumption is achieved by polygon-dependent access to the embedded dram macros with line-block mapping providing read-modify-write data transaction. the 3-d rendering engine with 2.22-mpolygons/s drawing speed was fabricated using 0.18- mu m cmos embedded memory logic technology. its area is 24 mm/sup 2/ and its power consumption is 120 mw ","‘A120-mW 3-D rendering engine with 6-Mb embedded DRAM and 3.2-GB/s runtime reconfigurable bus for PDA chip A low-power three-dimensional (3-D) rendering engine is implemented as part of a mobile personal digital assistant (PDA) chip. Six-megabit embedded DRAM macros attached to 8-pixel-parallel rendering logic are logically localized with a 3.2-GB/s runtime reconfigurable bus, reducing the area by 25% compared with conventional local frame-buffer architectures. The low power consumption is achieved by polygon-dependent access to the embedded DRAM macros with line-block mapping providing read-modify-write data transaction. The 3-D rendering engine with 2.22-Mpolygons/s drawing speed was fabricated using 0.18- mu m CMOS embedded memory logic technology. Its area is 24 mm/sup 2/ and its power consumption is 120 mW","['low-power 3D rendering engine', 'three-dimensional rendering engine', 'mobile PDA chip', 'mobile personal digital assistant chip', 'embedded DRAM macros', '8-pixel-parallel rendering logic', 'reconfigurable bus', 'low power consumption', 'polygon-dependent access', 'line-block mapping', 'read-modify-write data transaction', 'CMOS embedded memory logic technology', '3D graphics rendering', '120 mW', '6 Mbit', '3.2 GB/s', '0.18 micron', 'CMOS digital integrated circuits', 'computer graphic equipment', 'low-power electronics', 'microprocessor chips', 'mobile computing', 'notebook computers', 'random-access storage', 'rendering (computer graphics)']","['embedded DRAM macros', 'configurable bus', 'engine', 'PDA chip', 'read-modify-write data transaction', 'd-d', 'A120-mW', 'embedded', 'DRAM', 'embedded DRAM']",706,118,25,707,116,10,430,116,5
"global action rules in distributed knowledge systems previously z. ras and j.m. zytkow (2000) introduced and investigated query answering system based on distributed knowledge mining. the notion of an action rule was introduced by z. ras and a. wieczorkowska (2000) and its application domain e-business was taken. in this paper, we generalize the notion of action rules in a similar way to handling global queries. mainly, when values of attributes for a given customer, used in action rules, can not be easily changed by business user, definitions of these attributes are extracted from other sites of a distributed knowledge system. to be more precise, attributes at every site of a distributed knowledge system are divided into two sets: stable and flexible. values of flexible attributes, for a given consumer, sometime can be changed and this change can be influenced and controlled by a business user. however, some of these changes (for instance to the attribute ""profit') can not be done directly to a chosen attribute. in this case, definitions of such an attribute in terms of other attributes have to be learned. these new definitions are used to construct action rules showing what changes in values of flexible attributes, for a given consumer, are needed in order to re-classify this consumer the way business user wants. but, business user may be either unable or unwilling to proceed with actions leading to such changes. in all such cases we may search for definitions of these flexible attributes looking at either local or remote sites for help ","Global action rules in distributed knowledge systems. Previously Z. Ras and J.M. Zytkow (2000) introduced and investigated query answering system based on distributed knowledge mining. The notion of an action rule was introduced by Z. Ras and A. Wieczorkowska (2000) and its application domain e-business was taken. In this paper, we generalize the notion of action rules in a similar way to handling global queries. Mainly, when values of attributes for a given customer, used in action rules, can not be easily changed by business user, definitions of these attributes are extracted from other sites of a distributed knowledge system. To be more precise, attributes at every site of a distributed knowledge system are divided into two sets: stable and flexible. Values of flexible attributes, for a given ‘consumer, sometime can be changed and this change can be influenced and controlled by a business user. However, some of these changes (for instance to the attribute ""profit}) can not be done directly to a chosen attribute. In this case, definitions of such an attribute in terms of other attributes have to be learned. These new definitions are used to construct action rules showing what changes in values of flexible attributes, for a given consumer, are needed in order to re-classify this consumer the way business user wants. But, business user may be either unable or unwilling to proceed with actions leading to such changes. In all such cases we may search for definitions of these flexible attributes looking at either local or remote sites for help","['global action rules', 'query answering system', 'action rules', 'attributes', 'e-commerce', 'distributed knowledge mining', 'data mining', 'electronic commerce', 'knowledge based systems']","['flexible attributes', 'knowledge system', 'business users definitions', 'query answering system', 'Global action rules', 'precise attributes', 'way business user', 'other attributes', 'action', 'action rules']",1310,256,9,1312,255,10,10,3,0
"relationship between strong monotonicity property, p/sub 2/-property, and the gus-property in semidefinite linear complementarity problems in a recent paper on semidefinite linear complementarity problems, gowda and song (2000) introduced and studied the p-property, p/sub 2/-property, gus-property, and strong monotonicity property for linear transformation l: s/sup n/ to s/sup n/, where s/sup n/ is the space of all symmetric and real n * n matrices. in an attempt to characterize the p/sub 2/-property, they raised the following two questions: (i) does the strong monotonicity imply the p/sub 2/-property? (ii) does the gus-property imply the p/sub 2/-property? in this paper, we show that the strong monotonicity property implies the p/sub 2/-property for any linear transformation and describe an equivalence between these two properties for lyapunov and other transformations. we show by means of an example that the gus-property need not imply the p/sub 2/-property, even for lyapunov transformations ","Relationship between strong monotonicity property, P/sub 2/-property, and the GUS- property in semidefinite linear complementarity problems. Ina recent paper on semidefinite linear complementarity problems, Gowda and Song (2000) introduced and studied the P-property, P/sub 2/-property, GUS- property, and strong monotonicity property for linear transformation L: S/sup n/ to S/sup n/, where S/sup n/ is the space of all symmetric and real n * n matrices. In an attempt to characterize the P/sub 2/-property, they raised the following two questions: (i) Does the strong monotonicity imply the P/sub 2/-property? (ii) Does the GUS-property imply the P/sub 2/-property? In this paper, we show that the strong monotonicity property implies the P/sub 2/-property for any linear transformation and describe an equivalence between these two properties for Lyapunov and other transformations. We show by means of an example that the GUS-property need not imply the P/sub 2/-property, even for Lyapunov transformations","['semidefinite linear complementarity problems', 'strong monotonicity property', 'P/sub 2/-property', 'GUS-property', 'linear transformation', 'symmetric real matrices', 'Lyapunov transformations', 'complementarity', 'Lyapunov matrix equations', 'matrix algebra']","['property', 'strong monotonicity property', 'pub', 'linear transformation', 'gus property', 'Lyapunov transformations', 'strong', 'linear', 'monotonicity', 'strong monotonicity']",863,147,10,864,147,10,357,120,3
"the 3d visibility complex visibility problems are central to many computer graphics applications. the most common examples include hidden-part removal for view computation, shadow boundaries, mutual visibility of objects for lighting simulation. in this paper, we present a theoretical study of 3d visibility properties for scenes of smooth convex objects. we work in the space of light rays, or more precisely, of maximal free segments. we group segments that ""see"" the same object; this defines the 3d visibility complex. the boundaries of these groups of segments correspond to the visual events of the scene (limits of shadows, disappearance of an object when the viewpoint is moved, etc.). we provide a worst case analysis of the complexity of the visibility complex of 3d scenes, as well as a probabilistic study under a simple assumption for ""normal"" scenes. we extend the visibility complex to handle temporal visibility. we give an output-sensitive construction algorithm and present applications of our approach ","The 3D visibility complex Visibility problems are central to many computer graphics applications. The most common examples include hidden-part removal for view computation, shadow boundaries, mutual visibility of objects for lighting simulation. In this paper, we present a theoretical study of 3D visibility properties for scenes of smooth convex objects. We work in the space of light rays, or more precisely, of maximal free segments. We group segments that ""see"" the same object; this defines the 3D visibility complex. The boundaries of these groups of segments correspond to the visual events of the scene (limits of shadows, disappearance of an object when the viewpoint is moved, etc.). We provide a worst case analysis of the complexity of the visibility ‘complex of 3D scenes, as well as a probabilistic study under a simple assumption for ""normal"" scenes. We extend the visibility complex to handle temporal visibility. We give an output-sensitive construction algorithm and present applications of our approach","['3D visibility complex', 'computer graphics', 'hidden-part removal', 'view computation', 'shadow boundaries', 'mutual object visibility', 'lighting simulation', 'smooth convex objects', 'light rays', 'maximal free segments', 'visual events', 'worst case complexity analysis', 'probabilistic study', 'normal scenes', 'temporal visibility', 'output-sensitive construction algorithm', 'computational complexity', 'computer graphics', 'visibility']","['visibility', 'd visibility complex', 'd visibility properties', 'smooth convex objects', 'temporal visibility', 'mutual visibility', 'complexity', 'd scenes', 'd', 'visibility complex']",865,158,19,866,157,10,7,1,11
"uniform supersaturated design and its construction supersaturated designs are factorial designs in which the number of main effects is greater than the number of experimental runs. in this paper, a discrete discrepancy is proposed as a measure of uniformity for supersaturated designs, and a lower bound of this discrepancy is obtained as,a benchmark of design uniformity. a construction method for uniform supersaturated designs via resolvable balanced incomplete block designs is also presented along with the investigation of properties of the resulting designs. the construction method shows a strong link between these two different kinds of designs ","Uniform supersaturated design and its construction Supersaturated designs are factorial designs in which the number of main effects is greater than the number of experimental runs. In this paper, a discrete discrepancy is proposed as a measure of uniformity for supersaturated designs, and a lower bound of this discrepancy is obtained as,a benchmark of design uniformity. A construction method for uniform supersaturated designs via resolvable balanced incomplete block designs is also presented along with the investigation of properties of the resulting designs. The construction method shows a strong link between these two different kinds of designs","['uniform supersaturated design', 'factorial designs', 'experimental runs', 'discrete discrepancy', 'resolvable balanced incomplete block designs', 'CAD']","['construction method', 'designs', 'uniform supersaturated designs', 'discrete discrepancy', 'factorial designs', 'supersaturated design', 'supersaturated', 'uniform', 'Supersaturated designs', 'construction']",559,97,6,559,96,10,0,0,1
"a friction compensator for pneumatic control valves a procedure that compensates for static friction (stiction) in pneumatic control valves is presented. the compensation is obtained by adding pulses to the control signal. the characteristics of the pulses are determined from the control action. the compensator is implemented in industrial controllers and control systems, and the industrial experiences show that the procedure reduces the control error during stick-slip motion significantly compared to standard control without stiction compensation ","Atriction compensator for pneumatic control valves A procedure that compensates for static friction (stiction) in pneumatic control valves is presented. The compensation is obtained by adding Pulses to the control signal. The characteristics of the pulses are determined from the control action. The compensator is implemented in industrial controllers and control systems, and the industrial experiences show that the procedure reduces the control error during stick-slip motion significantly compared to standard control without stiction compensation","['friction compensator', 'pneumatic control valves', 'static friction compensation', 'stiction compensation', 'industrial controllers', 'control error reduction', 'stick-slip motion', 'standard control', 'compensation', 'pneumatic control equipment', 'process control', 'stiction', 'valves']","['control', 'pneumatic control valves', 'industrial experiences', 'industrial controllers', 'standard control', 'control systems', 'control signal', 'control action', 'control error', 'pneumatic']",478,77,13,478,75,10,283,75,5
"hours of operation and service in academic libraries: toward a national standard in an effort toward establishing a standard for academic library hours, the article surveys and compares hours of operation and service for arl libraries and ipeds survey respondents. the article ranks the arl (association for research libraries) libraries according to hours of operation and reference hours and then briefly discusses such issues as libraries offering twenty-four access and factors affecting service hour decisions ","Hours of operation and service in academic libraries: toward a national standard In an effort toward establishing a standard for academic library hours, the article surveys and compares hours of operation and service for ARL libraries and IPEDS survey respondents. The article ranks the ARL (Association for Research Libraries) libraries according to hours of operation and reference hours and then briefly discusses such issues as libraries offering twenty-four access and factors affecting service hour decisions","['academic library hours', 'operation/service hours', 'ARL libraries', 'IPEDS survey respondents', 'Integrated Post Secondary Education Data System', 'Association for Research Libraries', 'academic libraries', 'management', 'research libraries', 'standards']","['Research libraries libraries', 'pieds survey respondents', 'service hour decisions', 'academic library hours', 'academic libraries', 'national standard', 'reference hours', 'article surveys', 'hours', 'library']",440,76,10,440,75,10,0,0,0
"decentralized adaptive output feedback stabilization for a class of interconnected systems with unknown bound of uncertainties the problem of adaptive decentralized stabilization for a class of linear time-invarying large-scale systems with nonlinear interconnectivity and uncertainties is discussed. the bounds of uncertainties are assumed to be unknown. for such uncertain dynamic systems, an adaptive decentralized controller is presented. the resulting closed-loop systems are asymptotically stable in theory. moreover, an adaptive decentralized control scheme is given. the scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering. finally, simulations show that the control scheme is effective ","Decentralized adaptive output feedback stabilization for a class of interconnected systems with unknown bound of uncertainties The problem of adaptive decentralized stabilization for a class of linear time-invarying large-scale systems with nonlinear interconnectivity and uncertainties is discussed. The bounds of uncertainties are assumed to be unknown. For such uncertain dynamic systems, an adaptive decentralized controller is presented. The resulting closed-loop systems are asymptotically stable in theory. Moreover, an adaptive decentralized control scheme is given. The scheme ensures the closed-loop systems exponentially practically stable and can be used in practical engineering, Finally, simulations show that the control scheme is effective","['adaptive decentralized stabilization', 'closed-loop systems', 'uncertain dynamic systems', 'robust control', 'large scale systems', 'adaptive systems', 'closed loop systems', 'interconnected systems', 'large-scale systems', 'state feedback', 'uncertain systems']","['uncertainties', 'closed-loop systems', 'control scheme', 'stabilization', 'adaptive decentralized controller', 'nonlinear interconnectivity', 'class', 'interconnected systems', 'systems', 'adaptive']",657,100,11,657,99,10,1,1,0
"commerce department plan eases 3g spectrum crunch the federal government made its first move last week toward cleaning up a spectrum allocation system that was in shambles just a year ago and had some, spectrum-starved wireless carriers fearing they wouldn't be able to compete in third-generation services. the move, however, is far from complete and leaves numerous details unsettled ","Commerce Department plan eases 3G spectrum crunch The federal government made its first move last week toward cleaning up a spectrum allocation system that was in shambles just a year ago and had some, spectrum-starved wireless carriers fearing they wouldn't be able to compete in third-generation services. The move, however, is far from complete and leaves numerous details unsettled","['3G spectrum', 'federal government', 'spectrum allocation system', 'wireless carriers', 'cellular radio', 'frequency allocation', 'mobile communication']","['spectrum allocation system', 'Commerce Department plan', 'federal government', 'g spectrum crunch', 'first move', 'spectrum', 'g', 'plan', 'Commerce', 'Department']",327,60,7,327,59,10,0,0,1
"emarketing: restaurant web sites that click a number of global companies have adopted electronic commerce as a means of reducing transaction related expenditures, connecting with current and potential customers, and enhancing revenues and profitability. if a restaurant is to have an internet presence, what aspects of the business should be highlighted? food service companies that have successfully ventured onto the web have employed assorted web-based technologies to create a powerful marketing tool of unparalleled strength. historically, it has been difficult to create a set of criteria against which to evaluate website effectiveness. as practitioners consider additional resources for website development, the effectiveness of e-marketing investment becomes increasingly important. care must be exercised to ensure that the quality of the site adheres to high standards and incorporates evolving technology, as appropriate. developing a coherent website strategy, including an effective website design, are proving critical to an effective web presence ","‘eMarketing: restaurant Web sites that click A number of global companies have adopted electronic commerce as a means of reducing transaction related expenditures, connecting with current and potential customers, and enhancing revenues and profitability. If'a restaurant is to have an Internet presence, what aspects of the business should be highlighted? Food service companies that have successfully ventured onto the web have employed assorted web-based technologies to create a powerful marketing tool of unparalleled strength. Historically, it has been difficult to create a set of criteria against which to evaluate website effectiveness. AS practitioners consider additional resources for website development, the effectiveness of e-marketing investment becomes increasingly important. Care must be exercised to ensure that the quality of the site adheres to high standards and incorporates evolving technology, as appropriate. Developing a coherent website strategy, including an effective website design, are proving critical to an effective web presence","['e-marketing', 'restaurant Web sites', 'electronic commerce', 'Internet presence', 'food service companies', 'revenues', 'profitability', 'catering industry', 'electronic commerce', 'information resources', 'Internet', 'marketing data processing']","['profitability ifa restaurant', 'effective webster design', 'effective web presence', 'Food service companies', 'webster effectiveness', 'restaurant Web sites', 'global companies', 'eMarketing', 'restaurant', 'sites']",915,149,12,917,147,10,434,114,5
"what's best practice for open access? the business of publishing journals is in transition. nobody knows exactly how it will work in the future, but everybody knows that the electronic publishing revolution will ensure it won't work as it does now. this knowledge has provoked a growing sense of nervous anticipation among those concerned, some edgy and threatened by potential changes to their business, others excited by the prospect of change and opportunity. the paper discusses the open publishing model for dissemination of research ","What's best practice for open access? The business of publishing journals is in transition. Nobody knows exactly how it will work in the future, but everybody knows that the electronic publishing revolution will ensure it won't work as it does now. This knowledge has provoked a growing sense of nervous anticipation among those concerned, some edgy and threatened by potential changes to their business, others excited by the prospect of change and opportunity. The paper discusses the open publishing model for dissemination of research","['open access', 'journal publishing', 'electronic publishing', 'business', 'open publishing model', 'research dissemination', 'electronic publishing']","['electronic publishing revolution', 'open publishing model', 'publishing journals', 'potential changes', 'business others', 'best practice', 'open access', 'publishing', 'whats', 'open']",455,85,7,455,84,10,0,0,0
"two quantum analogues of fisher information from a large deviation viewpoint of quantum estimation we discuss two quantum analogues of the fisher information, the symmetric logarithmic derivative fisher information and kubo-mori-bogoljubov fisher information from a large deviation viewpoint of quantum estimation and prove that the former gives the true bound and the latter gives the bound of consistent superefficient estimators. as another comparison, it is shown that the difference between them is characterized by the change of the order of limits ","‘Two quantum analogues of Fisher information from a large deviation viewpoint of quantum estimation We discuss two quantum analogues of the Fisher information, the symmetric logarithmic derivative Fisher information and Kubo-Mori-Bogoljubov Fisher information from a large deviation viewpoint of quantum estimation and prove that the former gives the true bound and the latter gives the bound of consistent superefficient estimators. AS another comparison, it is shown that the difference between them is characterized by the change of the order of limits","['quantum analogues', 'quantum estimation', 'Kubo-Mori-Bogoljubov Fisher information', 'consistent superefficient estimators', 'statistical inference', 'large deviation viewpoint', 'symmetric logarithmic derivative Fisher information', 'information theory', 'parameter estimation', 'probability', 'quantum theory']","['large deviation viewpoint', 'quantum estimation', 'quantum analogues', 'Kubo-Mori-Bogoljubov Fisher information', 'consistent super-efficient estimators', 'quantum', 'Fisher', 'information', 'estimators', 'Fisher information']",474,82,11,475,81,10,3,1,0
"enlisting on-line residents: expanding the boundaries of e-government in a japanese rural township the purpose of this article is to analyze and learn from an unusual way in which local bureaucrats in a japanese rural township are using the internet to serve their constituents by enlisting the support of ""on-line residents."" successful e-government requires not only rethinking the potential uses of computer technology, but in adopting new patterns of decision-making, power sharing, and office management that many bureaucrats may not be predisposed to make. the main thesis of this article is that necessity and practicality can play a powerful motivational role in facilitating the incorporation of information technology (it) at the level of local government. this case study of how bureaucrats in towa-cho, a small, agricultural town in northeastern japan, have harnessed the internet demonstrates clearly the fundamentals of building a successful e-government framework in this rural municipality, similar to many communities in europe and north america today ","Enlisting on-line residents: Expanding the boundaries of e-government in a Japanese rural township The purpose of this article is to analyze and learn from an unusual way in which local bureaucrats in a Japanese rural township are using the Internet to serve their constituents by enlisting the support of “on-line residents."" Successful e-government requires not only rethinking the potential uses of computer technology, but in adopting new patterns of decision-making, power sharing, and office management that many bureaucrats may not be predisposed to make. The main thesis of this article is that necessity and practicality can play a powerful motivational role in facilitating the incorporation of information technology (IT) at the level of local government. This case study of how bureaucrats in Towa-cho, a small, agricultural town in Northeastern Japan, have harnessed the Internet demonstrates clearly the fundamentals of building a successful e-government framework in this Tural municipality, similar to many communities in Europe and North ‘America today","['on-line residents', 'e-government', 'Japanese rural township', 'local bureaucrats', 'Internet', 'decision-making', 'power sharing', 'office management', 'Towa-cho', 'rural municipality', 'government data processing', 'Internet']","['Japanese rural township', 'on-line residents', 'successful government framework', 'government', 'rural municipality', 'local bureaucrats', 'many bureaucrats', 'local government', 'Successful government', 'on-line']",911,159,12,912,158,10,9,3,7
"a unified view for vector rotational cordic algorithms and architectures based on angle quantization approach vector rotation is the key operation employed extensively in many digital signal processing applications. in this paper, we introduce a new design concept called angle quantization (aq). it can be used as a design index for vector rotational operation, where the rotational angle is known in advance. based on the aq process, we establish a unified design framework for cost-effective low-latency rotational algorithms and architectures. several existing works, such as conventional coordinate rotational digital computer (cordic), ar-cordic, mvr-cordic, and eeas-based cordic, can be fitted into the design framework, forming a vector rotational cordic family. moreover, we address four searching algorithms to solve the optimization problem encountered in the proposed vector rotational cordic family. the corresponding scaling operations of the cordic family are also discussed. based on the new design framework, we can realize high-speed/low-complexity rotational vlsi circuits, whereas without degrading the precision performance in fixed-point implementations ","A unified view for vector rotational CORDIC algorithms and architectures based ‘on angle quantization approach Vector rotation is the key operation employed extensively in many digital signal processing applications. In this paper, we introduce a new design concept called Angle Quantization (AQ). It can be used as a design index for vector rotational operation, where the rotational angle is known in advance. Based on the AQ process, we establish a Unified design framework for cost-effective low-latency rotational algorithms and architectures. Several existing works, such as conventional COordinate Rotational Digital Computer (CORDIC), AR-CORDIC, MVR-CORDIC, and EEAS-based CORDIC, can be fitted into the design framework, forming a Vector Rotational CORDIC Family. Moreover, we address four searching algorithms to solve the optimization problem encountered in the proposed vector rotational CORDIC family. The corresponding scaling operations of the CORDIC family are also discussed. Based on the new design framework, we can realize high-speed/low-complexity rotational VLSI circuits, whereas without degrading the precision performance in fixed-point implementations.","['vector rotational CORDIC algorithms', 'digital signal processing applications', 'DSP applications', 'angle quantization', 'design index', 'vector rotational operation', 'unified design framework', 'low-latency rotational algorithms', 'greedy searching algorithm', 'low-latency rotational architectures', 'searching algorithms', 'optimization problem', 'scaling operations', 'high-speed rotational VLSI circuits', 'low-complexity rotational VLSI circuits', 'fixed-point implementations', 'trellis-based searching algorithm', 'digital signal processing chips', 'fixed point arithmetic', 'optimisation', 'parallel architectures', 'pipeline arithmetic', 'search problems', 'signal processing', 'VLSI']","['rotational', 'rotational nordic family', 'rotational nordic algorithms', 'Unified design framework', 'rotational algorithms', 'rotational operation', 'new design framework', 'new design concept', 'rotational angle', 'nordic family']",1016,162,25,1018,161,10,2,2,6
a hybrid-neural network and population learning algorithm approach to solving reliability optimization problem proposes a hybrid approach integrating a dedicated artificial neural network and population learning algorithm applied to maximising system reliability under cost and technical feasibility constraints. the paper includes a formulation of the system reliability optimisation (sro) problem and a description of the dedicated neural network trained by applying the population learning algorithm. a solution to the example sro problem is shown and results of the computational experiment are presented and discussed ,A hybrid-neural network and population learning algorithm approach to solving reliability optimization problem Proposes a hybrid approach integrating a dedicated artificial neural network and population learning algorithm applied to maximising system reliability under cost and technical feasibility constraints. The paper includes a formulation of the system reliability optimisation (SRO) problem and a description of the dedicated neural network trained by applying the population learning algorithm. A solution to the example SRO problem is shown and results of the computational experiment are presented and discussed,"['hybrid approach', 'dedicated artificial neural network', 'population learning algorithm', 'system reliability', 'cost constraints', 'technical feasibility constraints', 'reliability optimization problem', 'learning (artificial intelligence)', 'neural nets', 'optimisation', 'reliability theory']","['population', 'system reliability', 'reliability optimization problem', 'dedicated neural network', 'hybrid-neural network', 'algorithm A solution', 'example SRO problem', 'hybrid approach', 'network', 'algorithm']",539,85,11,539,84,10,0,0,2
"general solution of a density functionally gradient piezoelectric cantilever and its applications we have used the plane strain theory of transversely isotropic bodies to study a piezoelectric cantilever. in order to find the general solution of a density functionally gradient piezoelectric cantilever, we have used the inverse method (i.e. the airy stress function method). we have obtained the stress and induction functions in the form of polynomials as well as the general solution of the beam. based on this general solution, we have deduced the solutions of the cantilever under different loading conditions. furthermore, as applications of this general solution in engineering, we have studied the tip deflection and blocking force of a piezoelectric cantilever actuator. finally, we have addressed a method to determine the density distribution profile for a given piezoelectric material ","General solution of a density functionally gradient piezoelectric cantilever and its applications We have used the plane strain theory of transversely isotropic bodies to study a piezoelectric cantilever. In order to find the general solution of a density functionally gradient piezoelectric cantilever, we have used the inverse method (i. the Airy stress function method). We have obtained the stress and induction functions in the form of polynomials as well as the general solution of the beam. Based on this general solution, we have deduced the solutions of the cantilever under different loading conditions. Furthermore, as applications of this general solution in engineering, we have studied the tip deflection and blocking force of a piezoelectric cantilever actuator. Finally, we have addressed a method to determine the density distribution profile for a given piezoelectric material","['plane strain theory', 'transversely isotropic bodies', 'inverse method', 'Airy stress function', 'polynomials', 'loading conditions', 'piezoelectric cantilever actuator', 'density distribution profile', 'piezoelectric material', 'inverse problems', 'piezoelectric actuators', 'piezoelectric materials', 'polynomials']","['piezoelectric cantilever actuators', 'piezoelectric', 'density distribution profile', 'piezoelectric material', 'cantilever', 'density', 'piezoelectric cantilever', 'solution', 'general solution', 'General solution']",764,134,13,762,133,10,0,1,7
simulation of ecological and economical structural-type functions an optimization approach to the simulation of ecological and economical structural-type functions is proposed. a methodology for construction of such functions is created in an explicit analytical form ,Simulation of ecological and economical structural-type functions. An optimization approach to the simulation of ecological and economical structural-type functions is proposed. A methodology for construction of such functions is created in an explicit analytical form,"['economical structural-type functions', 'optimisation approach', 'simulation', 'explicit analytical form', 'natural resources', 'pollution control']","['economical structural-type functions', 'ecological', 'Simulation', 'explicit analytical form', 'optimization approach', 'such functions', 'functions', 'optimization', 'economical', 'structural-type']",233,36,6,234,35,10,0,1,0
"on a general constitutive description for the inelastic and failure behavior of fibrous laminates. ii. laminate theory and applications for pt. i see ibid., pp. 1159-76. the two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms. the constitutive relationship is established micromechanically, through layer-by-layer analysis. namely, only the properties of the constituent fiber and matrix materials of the composites are required as input data. in the previous part lamina theory was presented. three fundamental quantities of the laminae, i.e. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix, with different fiber preform (including woven, braided, and knitted fabric) reinforcements were explicitly obtained by virtue of the bridging micromechanics model. in this paper, the laminate stress analysis is shown. the purpose of this analysis is to determine the load shared by each lamina in the laminate, so that the lamina theory can be applied. incorporation of the constitutive equations into an fem software package is illustrated. a number of application examples are given to demonstrate the efficiency of the constitutive theory. the predictions made include: failure envelopes of multidirectional laminates subjected to biaxial in-plane loads, thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate, s-n curves of multilayer knitted fabric reinforced laminates under tensile fatigue, and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads ","On a general constitutive description for the inelastic and failure behavior of fibrous laminates. I. Laminate theory and applications For pt. I see ibid., pp. 1159-76. The two papers report systematically a constitutive description for the inelastic and strength behavior of laminated composites reinforced with various fiber preforms. The constitutive relationship is established micromechanically, through layer-by-layer analysis. Namely, only the properties of the constituent fiber and matrix materials of the composites are required as input data. In the previous part lamina theory was presented. Three fundamental quantities of the laminae, i. the internal stresses generated in the constituent fiber and matrix materials and the instantaneous compliance matrix, with different fiber preform (including woven, braided, and knitted fabric) reinforcements were explicitly obtained by virtue of the bridging micromechanics model. In this paper, the laminate stress analysis is shown. The purpose of this analysis is to determine the load shared by each lamina in the laminate, so that the lamina theory can be applied. Incorporation of the constitutive equations into an FEM software package is illustrated A number of application examples are given to demonstrate the efficiency of the constitutive theory. The predictions made include failure envelopes of multidirectional laminates subjected to biaxial in-plane loads, thermomechanical cycling stress-strain curves of a titanium metal matrix composite laminate, S-N curves of multilayer knitted fabric reinforced laminates under tensile fatigue, and bending load-deflection plots and ultimate bending strengths of laminated braided fabric reinforced beams subjected to lateral loads","['general constitutive description', 'inelastic behavior', 'failure behavior', 'fibrous laminates', 'laminate theory', 'strength behavior', 'composites', 'fiber preforms', 'micromechanics', 'layer-by-layer analysis', 'internal stresses', 'matrix materials', 'instantaneous compliance matrix', 'stress analysis', 'load', 'FEM software package', 'failure envelopes', 'multidirectional laminates', 'biaxial in-plane loads', 'thermomechanical cycling stress-strain curves', 'titanium metal matrix composite laminate', 'S-N curves', 'multilayer knitted fabric reinforced laminates', 'tensile fatigue', 'bending load deflection plots', 'ultimate bending strengths', 'laminated braided fabric reinforced beams', 'lateral loads', 'bending', 'bending strength', 'fatigue', 'fibre reinforced composites', 'finite element analysis', 'internal stresses', 'laminates', 'mechanical engineering computing', 'stress analysis', 'stress-strain relations', 'thermomechanical treatment']","['constitutive', 'general constitutive description', 'multi-directional laminates', 'constitutive relationship', 'laminated braided fabric', 'laminate stress analysis', 'constitutive equations', 'laminated composites', 'constitutive theory', 'constitutive description']",1503,244,39,1498,243,10,1,4,20
"ecg-gated /sup 18/f-fdg positron emission tomography. single test evaluation of segmental metabolism, function and contractile reserve in patients with coronary artery disease and regional dysfunction /sup 18/f-fluorodeoxyglucose (/sup 18/f-fdg)-positron emission tomography (pet) provides information about myocardial glucose metabolism to diagnose myocardial viability. additional information about the functional status is necessary. comparison of tomographic metabolic pet with data from other imaging techniques is always hampered by some transfer uncertainty and scatter. we wanted to evaluate a new fourier-based ecg-gated pet technique using a high resolution scanner providing both metabolic and functional data with respect to feasibility in patients with diseased left ventricles. forty-five patients with coronary artery disease and at least one left ventricular segment with severe hypokinesis or akinesis at biplane cineventriculography were included. a new fourier-based ecg-gated metabolic /sup 18/f-fdg-pet was performed in these patients. function at rest and /sup 18/f-fdg uptake were examined in the pet study using a 36-segment model. segmental comparison with ventriculography revealed a high reliability in identifying dysfunctional segments (>96%). /sup 18/f-fdg uptake of normokinetic/hypokinetic/akinetic segments was 75.4+or-7.5, 65.3+or-10.5, and 35.9+or-15.2% (p<0.001). in segments >or=70% /sup 18/f-fdg uptake no akinesia was observed. no residual function was found below 40% /sup 18/f-fdg uptake. an additional dobutamine test was performed and revealed inotropic reserve (viability) in 42 akinetic segments and 45 hypokinetic segments. ecg-gated metabolic pet with pixel-based fourier smoothing provides reliable data on regional function. assessment of metabolism and function makes complete judgement of segmental status feasible within a single study without any transfer artefacts or test-to-test variability. the results indicate the presence of considerable amounts of viable myocardium in regions with an uptake of 40-50% /sup 18/f-fdg ","ECG-gated /sup 18/F-FDG positron emission tomography. Single test evaluation of segmental metabolism, function and contractile reserve in patients with coronary artery disease and regional dysfunction Jsup 18/F-fluorodeoxyglucose (/sup 18/F-FDG)-positron emission tomography (PET) provides information about myocardial glucose metabolism to diagnose myocardial viability. Additional information about the functional status is necessary. Comparison of tomographic metabolic PET with data from other imaging techniques is always hampered by some transfer uncertainty and scatter. We wanted to evaluate a new Fourier-based ECG-gated PET technique using a high resolution scanner providing both metabolic and functional data with respect to feasibility in patients with diseased left ventricles. Forty-five patients with coronary artery disease and at least one left ventricular segment with severe hypokinesis or akinesis at biplane cineventriculography were included Anew Fourier-based ECG-gated metabolic /sup 18/F-FDG-PET was performed in these patients. Function at rest and /sup 18/F-FDG uptake were examined in the PET study using a 36-segment model. Segmental ‘comparison with ventriculography revealed a high reliability in identifying dysfunctional segments (>96%). /sup 18/F-FDG uptake of normokinetic/hypokinetic/akinetic segments was 75.4+0r-7.5, 65.3+01-10.5, and 35.9+0r-15.2% (p<0.001). In segments >or=70% Jsup 18/F-FDG uptake no akinesia was observed. No residual function was found below 40% /sup 18/F-FDG uptake. An additional dobutamine test was performed and revealed inotropic reserve (viability) in 42 akinetic segments and 45 hypokinetic segments. ECG-gated metabolic PET with pixel-based Fourier smoothing provides reliable data on regional function. Assessment of metabolism and function makes complete judgement of segmental status feasible within a single study without any transfer artefacts or test-to-test variability. The results indicate the presence of considerable amounts of viable myocardium in regions with an uptake of 40-50% /sup 18/F-FDG","['Fourier-based ECG-gated metabolic /sup 18/F-fluorodeoxyglucose-positron emission tomography', '/sup 18/F-fluorodeoxyglucose uptake', 'thirty six-segment model', 'ventriculography', 'dysfunctional segments', 'normokinetic/hypokinetic/akinetic segments', 'residual function', 'dobutamine test', 'inotropic reserve', 'akinetic segments', 'hypokinetic segments', 'pixel-based Fourier smoothing', 'regional function', 'segmental status', 'transfer artefacts', 'viable myocardium', 'regional dysfunction', 'myocardial glucose metabolism', 'myocardial viability', 'functional', 'transfer uncertainty', 'Fourier-based ECG-gated PET technique', 'high resolution scanner', 'patients', 'diseased left ventricles', 'coronary artery disease', 'left ventricular segment', 'severe hypokinesis', 'akinesis', 'biplane cineventriculography', 'cardiology', 'diseases', 'electrocardiography', 'Fourier analysis', 'medical image processing', 'physiological models', 'positron emission tomography']","['sup 18/F-FDG uptake', '% sup 18/F-FDG', 'segmental metabolism function', 'myocardial glucose metabolism', 'regional function Assessment', 'tomographic metabolic PET', 'ECG-gated metabolic PET', 'dysfunctional segments', 'segmental status', 'sup']",1804,274,37,1804,272,10,596,150,17
"estimation of blocking probabilities in cellular networks with dynamic channel assignment blocking probabilities in cellular mobile communication networks using dynamic channel assignment are hard to compute for realistic sized systems. this computational difficulty is due to the structure of the state space, which imposes strong coupling constraints amongst components of the occupancy vector. approximate tractable models have been proposed, which have product form stationary state distributions. however, for real channel assignment schemes, the product form is a poor approximation and it is necessary to simulate the actual occupancy process in order to estimate the blocking probabilities. meaningful estimates of the blocking probability typically require an enormous amount of cpu time for simulation, since blocking events are usually rare. advanced simulation approaches use importance sampling (is) to overcome this problem. we study two regimes under which blocking is a rare event: low-load and high cell capacity. our simulations use the standard clock (sc) method. for low load, we propose a change of measure that we call static issc, which has bounded relative error. for high capacity, we use a change of measure that depends on the current state of the network occupancy. this is the dynamic issc method. we prove that this method yields zero variance estimators for single clique models, and we empirically show the advantages of this method over naive simulation for networks of moderate size and traffic loads ","Estimation of blocking probabilities in cellular networks with dynamic channel assignment Blocking probabilities in cellular mobile communication networks using dynamic channel assignment are hard to compute for realistic sized systems. This computational difficulty is due to the structure of the state space, which imposes strong coupling constraints amongst components of the occupancy vector. Approximate tractable models have been proposed, which have product form stationary state distributions. However, for real channel assignment schemes, the product form is a poor approximation and it is necessary to simulate the actual occupancy process in order to estimate the blocking probabilities. Meaningful estimates of the blocking probability typically require an enormous, amount of CPU time for simulation, since blocking events are usually rare. Advanced simulation approaches use importance sampling (IS) to ‘overcome this problem. We study two regimes under which blocking is a rare event: low-load and high cell capacity. Our simulations use the standard clock (SC) method. For low load, we propose a change of measure that we call static ISSC, which has bounded relative error. For high capacity, we use a change of measure that depends on the current state of the network occupancy. This is the dynamic ISSC method. We prove that this method yields zero variance estimators for single clique models, and we empirically show the advantages of this method ‘over naive simulation for networks of moderate size and traffic loads","['blocking probability estimation', 'dynamic channel assignment', 'cellular mobile communication networks', 'strong coupling constraints', 'occupancy vector', 'approximate tractable models', 'product form stationary state distributions', 'CPU time', 'simulation', 'importance sampling', 'low-load', 'high cell capacity', 'standard clock method', 'static ISSC method', 'bounded relative error', 'quality of service', 'dynamic ISSC method', 'zero variance estimators', 'single clique models', 'network traffic load', 'cellular radio', 'channel allocation', 'digital simulation', 'importance sampling', 'probability', 'quality of service', 'telecommunication computing', 'telecommunication traffic']","['probabilities', 'dynamic channel assignment', 'stationary state distributions', 'dynamic iss method', 'network occupancy', 'cellular networks', 'dynamic', 'networks', 'cellular', 'channel']",1305,231,28,1308,230,10,12,3,12
"pulmonary perfusion patterns and pulmonary arterial pressure uses artificial intelligence methods to determine whether quantitative parameters describing the perfusion image can be synthesized to make a reasonable estimate of the pulmonary arterial (pa) pressure measured at angiography. radionuclide perfusion images were obtained in 120 patients with normal chest radiographs who also underwent angiographic pa pressure measurement within 3 days of the radionuclide study. an artificial neural network (ann) was constructed from several image parameters describing statistical and boundary characteristics of the perfusion images. with use of a leave-one-out cross-validation technique, this method was used to predict the pa systolic pressure in cases on which the ann had not been trained. a pearson correlation coefficient was determined between the predicted and measured pa systolic pressures. ann predictions correlated with measured pulmonary systolic pressures (r=0.846, p<.001). the accuracy of the predictions was not influenced by the presence of pulmonary embolism. none of the 51 patients with predicted pa pressures of less than 29 mm hg had pulmonary hypertension at angiography. all 13 patients with predicted pa pressures greater than 48 mm hg had pulmonary hypertension at angiography. meaningful information regarding pa pressure can be derived from noninvasive radionuclide perfusion scanning. the use of image analysis in concert with artificial intelligence methods helps to reveal physiologic information not readily apparent at visual image inspection ","Pulmonary perfusion patterns and pulmonary arterial pressure Uses artificial intelligence methods to determine whether quantitative parameters describing the perfusion image can be synthesized to make a reasonable estimate of the pulmonary arterial (PA) pressure measured at angiography. Radionuclide perfusion images were obtained in 120 Patients with normal chest radiographs who also underwent angiographic PA pressure measurement within 3 days of the radionuclide study. An artificial neural network (ANN) was constructed from several image parameters describing statistical and boundary characteristics of the perfusion images. With use of a leave-one-out cross-validation technique, this method was used to predict the PA systolic pressure in ‘cases on which the ANN had not been trained. A Pearson correlation coefficient was determined between the predicted and measured PA systolic pressures. ANN predictions correlated with measured pulmonary systolic pressures (r=0.846, P<.001). The accuracy of the predictions was not influenced by the presence of pulmonary embolism. None of the 51 patients with predicted PA pressures of less than 29 mm Hg had pulmonary hypertension at angiography. All 13 patients with predicted PA pressures greater than 48 mm Hg had pulmonary hypertension at angiography. Meaningful information regarding PA pressure can be derived from noninvasive radionuclide perfusion scanning. The use of image analysis in concert with artificial intelligence methods helps to reveal physiologic information not readily apparent at visual image inspection","['pulmonary perfusion patterns', 'angiographic pulmonary arterial pressure measurement', 'artificial neural network predictions', 'accuracy', 'pulmonary embolism', 'pulmonary hypertension', 'noninvasive radionuclide perfusion scanning', 'image analysis', 'physiologic information', 'visual image inspection', 'image parameters', 'statistical characteristics', 'boundary characteristics', 'leave-one-out cross-validation technique', 'pulmonary arterial systolic pressure', 'Pearson correlation coefficient', 'artificial intelligence methods', 'quantitative parameters', 'perfusion image', 'angiography', 'radionuclide perfusion images', 'patients', 'normal chest radiographs', '29 Pa', '48 Pa', 'haemodynamics', 'lung', 'medical image processing', 'neural nets', 'radioisotope imaging', 'statistical analysis']","['PA pressure', 'pressure', 'perfusion image', 'pulmonary hypertension', 'pulmonary systolic pressures', 'Pulmonary perfusion patterns', 'pulmonary arterial pressure', 'several image parameters', 'pulmonary embolism None', 'systolic pressure']",1357,222,31,1358,221,10,5,1,9
"mining the optimal class association rule set we define an optimal class association rule set to be the minimum rule set with the same predictive power of the complete class association rule set. using this rule set instead of the complete class association rule set we can avoid redundant computation that would otherwise be required for mining predictive association rules and hence improve the efficiency of the mining process significantly. we present an efficient algorithm for mining the optimal class association rule set using an upward closure property of pruning weak rules before they are actually generated. we have implemented the algorithm and our experimental results show that our algorithm generates the optimal class association rule set, whose size is smaller than 1/17 of the complete class association rule set on average, in significantly less time than generating the complete class association rule set. our proposed criterion has been shown very effective for pruning weak rules in dense databases ","Mining the optimal class association rule set We define an optimal class association rule set to be the minimum rule set with the same predictive power of the complete class association rule set. Using this rule set instead of the complete class association rule set we can avoid redundant computation that would otherwise be required for mining predictive association rules and hence improve the efficiency of the mining process significantly. We present an efficient algorithm for mining the optimal class association rule set using an upward closure property of pruning weak rules before they are actually generated. We have implemented the algorithm and our experimental results show that ‘our algorithm generates the optimal class association rule set, whose size is smaller than 1/17 of the complete class association rule set on average, in significantly less time than generating the complete class association rule set. Our proposed criterion has been shown very effective for pruning weak rules in dense databases","['optimal class association rule set mining', 'minimum rule set', 'predictive power', 'redundant computation', 'predictive association rules', 'relational database', 'upward closure property', 'data mining', 'weak rule pruning', 'experimental results', 'dense databases', 'data mining', 'relational databases', 'very large databases']","['class', 'rule', 'complete', 'optimal', 'weak rules', 'predictive association rules', 'same predictive power', 'efficient algorithm', 'minimum rule', 'association']",864,160,14,865,159,10,3,1,4
convolution-based global simulation technique for millimeter-wave photodetector and photomixer circuits a fast convolution-based time-domain approach to global photonic-circuit simulation is presented that incorporates a physical device model in the complete detector or mixer circuit. the device used in the demonstration of this technique is a gaas metal-semiconductor-metal (msm) photodetector that offers a high response speed for the detection and generation of millimeter waves. global simulation greatly increases the accuracy in evaluating the complete circuit performance because it accounts for the effects of the millimeter-wave embedding circuit. device and circuit performance are assessed by calculating optical responsivity and bandwidth. device-only simulations using gaas msms are compared with global simulations that illustrate the strong interdependence between device and external circuit ,Convolution-based global simulation technique for milimeter-wave photodetector and photomixer circuits A fast convolution-based time-domain approach to global photonic-circuit simulation is presented that incorporates a physical device model in the complete detector or mixer circuit. The device used in the demonstration of this technique is a GaAs metal-semiconductor-metal (MSM) photodetector that offers a high response speed for the detection and generation of millimeter waves. Global simulation greatly increases the accuracy in evaluating the complete circuit performance because it accounts for the effects of the millimeter-wave embedding circuit. Device and circuit performance are assessed by calculating optical responsivity and bandwidth. Device-only simulations using GaAs MSMs are compared with global simulations that illustrate the strong interdependence between device and external circuit,"['convolution-based time-domain approach', 'global photonic-circuit simulation', 'physical device model', 'GaAs MSM photodetector', 'millimeter-wave photodetector', 'photomixer', 'MM-wave embedding circuit', 'optical responsivity', 'bandwidth', 'convolution-based global simulation', 'GaAs', 'circuit simulation', 'convolution', 'equivalent circuits', 'gallium arsenide', 'metal-semiconductor-metal structures', 'microwave photonics', 'millimetre wave detectors', 'millimetre wave mixers', 'photodetectors', 'time-domain analysis']","['global photonic-circuit simulation', 'complete circuit performance', 'Device-only simulations', 'photomixer circuits', 'global simulations', 'external circuit', 'simulations', 'mixer circuit', 'Global simulation', 'circuit performance']",792,119,21,791,118,10,11,1,7
"the importance of continuity: a reply to chris eliasmith in his reply to eliasmith (see ibid., vol.11, p.417-26, 2001) poznanski considers how the notion of continuity of dynamic representations serves as a beacon for an integrative neuroscience to emerge. he considers how the importance of continuity has come under attack from eliasmith (2001) who claims: (i) continuous nature of neurons is not relevant to the information they process, and (ii) continuity is not important for understanding cognition because the various sources of noise introduce uncertainty into spike arrival times, so encoding and decoding spike trains must be discrete at some level ","The importance of continuity: a reply to Chris Eliasmith In his reply to Eliasmith (see ibid., vol.11, p.417-26, 2001) Poznanski considers how the notion of continuity of dynamic representations serves as a beacon for an integrative neuroscience to emerge. He considers how the importance of continuity has come under attack from Eliasmith (2001) who claims: (i) continuous nature of neurons is not relevant to the information they process, and (i) continuity is not important for understanding cognition because the various sources of noise introduce uncertainty into spike arrival times, so encoding and decoding spike trains must be discrete at some level","['continuity', 'dynamic representations', 'integrative neuroscience', 'neurons', 'cognition', 'uncertainty', 'spike arrival times', 'spike trains', 'cognitive systems', 'neural nets', 'cognitive systems', 'neural nets', 'neurophysiology']","['continuity', 'importance', 'reply', 'integrative neuroscience', 'dynamic representations', 'spike arrival times', 'continuous nature', 'various sources', 'Chris Eliasmith', 'Eliasmith']",559,102,13,558,101,10,1,1,3
"blending parametric patches with subdivision surfaces in this paper the problem of blending parametric surfaces using subdivision patches is discussed. a new approach, named removing-boundary, is presented to generate piecewise-smooth subdivision surfaces through discarding the outmost quadrilaterals of the open meshes derived by each subdivision step. then the approach is employed both to blend parametric bicubic b-spline surfaces and to fill n-sided holes. it is easy to produce piecewise-smooth subdivision surfaces with both convex and concave corners on the boundary, and limit surfaces are guaranteed to be c/sup 2/ continuous on the boundaries except for a few singular points by the removing-boundary approach. thus the blending method is very efficient and the blending surface generated is of good effect ","Blending parametric patches with subdivision surfaces In this paper the problem of blending parametric surfaces using subdivision patches is discussed. A new approach, named removing-boundary, is, presented to generate piecewise-smooth subdivision surfaces through discarding the outmost quadrilaterals of the open meshes derived by each subdivision step. Then the approach is employed both to blend parametric bicubic B-spline surfaces and to fill n-sided holes. It is easy to produce piecewise-smooth subdivision surfaces with both convex and concave corners on the boundary, and limit surfaces are guaranteed to be C/sup 2/ continuous on the boundaries except for a few singular points by the removing-boundary approach. Thus the blending method is very efficient and the blending surface generated is of good effect","['subdivision surfaces', 'parametric surfaces blending', 'subdivision patches', 'piecewise-smooth subdivision surfaces', 'quadrilaterals', 'parametric bicubic B-spline surfaces', 'piecewise smooth subdivision surfaces', 'computer graphics', 'solid modelling', 'splines (mathematics)']","['piecewise-smooth subdivision surfaces', 'surfaces', 'subdivision', 'removing-boundary approach', 'subdivision patches', 'parametric surfaces', 'parametric patches', 'subdivision step', 'limit surfaces', 'subdivision surfaces']",700,120,10,701,119,10,0,1,1
"sharpening the estimate of the stability constant in the maximum-norm of the crank-nicolson scheme for the one-dimensional heat equation this paper is concerned with the stability constant c/sub infinity / in the maximum-norm of the crank-nicolson scheme applied. to the one-dimensional heat equation. a well known result due to s.j. serdyukova is that c/sub infinity / < 23. in the present paper, by using a sharp resolvent estimate for the discrete laplacian together with the cauchy formula, it is shown that 3 <or= c/sub infinity / < 4.325. this bound also holds when the heat equation is considered on a bounded interval along with dirichlet or neumann boundary conditions ","Sharpening the estimate of the stability constant in the maximum-norm of the Crank-Nicolson scheme for the one-dimensional heat equation This paper is concerned with the stability constant C/sub infinity / in the maximum-norm of the Crank-Nicolson scheme applied. to the ‘one-dimensional heat equation. A well known result due to S.J. Serdyukova is that C/sub infinity / < 23. In the present paper, by using a sharp resolvent estimate for the discrete Laplacian together with the Cauchy formula, it is shown that 3 <or= C/sub infinity / < 4.325. This bound also holds when the heat equation is considered ‘on a bounded interval along with Dirichlet or Neumann boundary conditions","['stability constant', 'Crank-Nicolson scheme', 'one-dimensional heat equation', 'sharp resolvent estimate', 'discrete Laplacian', 'Cauchy formula', 'Neumann boundary conditions', 'Dirichlet boundary conditions', 'iterative methods', 'Laplace equations', 'stability']","['one-dimensional heat equation', 'Crank-Nicolson scheme', 'estimate', 'constant club infinity', 'present paper', 'club', 'infinity', 'heat', 'club infinity', 'heat equation']",569,110,11,571,109,10,17,2,4
"web content extraction. a whizbang! approach the extraction technology that whizbang uses consists of a unique approach to scouring the web for current, very specific forms of information. flipdog, for example, checks company web sites for hyperlinks to pages that list job opportunities. it then crawls to the deeper page and, using the whizbang! extraction framework, extracts the key elements of the postings, such as job title, name of employer, job category, and job function. click on a job and you are transferred to the company web site to view the job description as it appears there ","Web content extraction. A WhizBang! approach The extraction technology that Whizbang uses consists of a unique approach to scouring the Web for current, very specific forms of information. FlipDog, for example, checks company Web sites for hyperlinks to pages that list job opportunities. It then crawls to the deeper page and, using the WhizBang! Extraction Framework, extracts the key elements of the postings, such as job title, name of employer, job category, and Job function. Click on a job and you are transferred to the company Web site to view the job description as it appears there","['Web content extraction', 'FlipDog', 'job description', 'job-hunting site', 'company Web sites', 'WhizBang! Extraction Framework', 'human resource management', 'information resources', 'information retrieval']","['company Web site', 'job', 'employers job category', 'extraction technology', 'job opportunities', 'unique approach', 'job description', 'job title name', 'Web', 'extraction']",496,98,9,496,97,10,0,0,1
"shaping the future. bendwizard: a tool for off-line programming of robotic tending systems setting up a robot to make metal cabinets or cases for desktop computers can be a complex operation. for instance, one expert might be required to carry out a feasibility study, and then another to actually program the robot. understandably, the need for so much expertise, and the time that's required, generally limits the usefulness of automation to high-volume production. workshops producing parts in batches smaller than 50 or so, or which rely heavily on semiskilled operators, are therefore often discouraged from investing in automation, and so miss out on its many advantages. what is needed is a software tool that operators without special knowledge of robotics, or with no more than rudimentary cad skills, can use. one which allows easy offline programming and simulation of the work cell on a pc ","Shaping the future. BendWizard: a tool for off-line programming of robotic tending systems Setting up a robot to make metal cabinets or cases for desktop computers can be a complex operation. For instance, one expert might be required to carry out a feasibility study, and then another to actually program the robot. Understandably, the need for so much expertise, and the time that's required, generally limits the usefulness of automation to high-volume production. Workshops producing parts in batches smaller than 50 or so, or which rely heavily on semiskilled operators, are therefore often discouraged from investing in automation, and so miss ‘out on its many advantages. What is needed is a software tool that operators without special knowledge of robotics, or with no more than rudimentary CAD skills, can use. One which allows easy offline programming and simulation of the work cell on a PC","['robotic tending systems', 'BendWizard offline programming tool', 'metal cabinets', 'desktop computer cases', 'feasibility study', 'high-volume production', 'workshops', 'CAD skills', 'work cell simulation', 'bending', 'industrial robots', 'machining', 'robot programming']","['robot', 'easy offline programming', 'semiskilled operators', 'off-line programming', 'future BendWizard', 'complex operation', 'software tool', 'programming', 'tool', 'future']",757,146,13,758,145,10,3,1,6
"the pedagogy of on-line learning: a report from the university of the highlands and islands millennium institute authoritative sources concerned with computer-aided learning, resource-based learning and on-line learning and teaching are generally agreed that, in addition to subject matter expertise and technical support, the quality of the learning materials and the learning experiences of students are critically dependent on the application of pedagogically sound theories of learning and teaching and principles of course design. the university of the highlands and islands project (uhimi) is developing ""on-line learning"" on a large scale. these developments have been accompanied by a comprehensive programme of staff development. a major emphasis of the programme is concerned with ensuring that course developers and tutors are pedagogically aware. this paper reviews (i) what is meant by ""on-line learning"" in the uhimi context (ii) the theories of learning and teaching and principles of course design that inform the staff development programme and (iii) a review of progress to date ","The pedagogy of on-line learning: a report from the University of the Highlands and Islands Millennium institute Authoritative sources concerned with computer-aided learning, resource-based learning and on-line learning and teaching are generally agreed that, in addition to subject matter expertise and technical support, the quality of the learning materials and the learning experiences of students are critically dependent on the application of pedagogically sound theories of learning and teaching and principles of course design. The University of the Highlands and Islands Project (UHIMI) is developing ""on-line learning’ on a large scale. These developments have been accompanied by a comprehensive programme of staff development. A major emphasis of the programme is concerned with ensuring that course developers and tutors are pedagogically aware. This paper reviews (i) what is meant by “on-line learning” in the UHIMI context (i) the theories of learning and teaching and principles of course design that inform the staff development programme and (iil) a review of progress to date","['online learning', 'pedagogy', 'computer-aided learning', 'resource-based learning', 'teaching', 'technical support', 'educational course design', 'distance education', 'Internet', 'University of the Highlands and Islands Project', 'staff development', 'distance learning', 'educational computing', 'educational courses', 'Internet', 'teaching']","['learning', 'on-line learning', 'Islands Millennium institute', 'staff development programme', 'resource-based learning', 'computer-aided learning', 'learning materials', 'course developers', 'Islands Project', 'on-line']",936,162,16,935,161,10,5,5,4
"on the distribution of lachlan nonsplitting bases we say that a computably enumerable (c.e.) degree b is a lachlan nonsplitting base (lnb), if there is a computably enumerable degree a such that a>b, and for any c.e. degrees w, v<or=a, if a<or=wvvv b then either a<or=wv b or a<or=vv b. in this paper we investigate the relationship between bounding and nonbounding of lachlan nonsplitting bases and the high/low hierarchy. we prove that there is a non-low/sub 2/ c.e. degree which bounds no lachlan nonsplitting base ","On the distribution of Lachian nonspiitting bases We say that a computably enumerable (c.e.) degree b is a Lachlan nonsplitting base (LNB), if there is a computably enumerable degree a such that a>b, and for any c.e. degrees w, v<or=a, if a<or=wWWV b then either a<or=wV b or a<or=vV b. In this paper we investigate the relationship between bounding and nonbounding of Lachlan nonspiitting bases and the high/low hierarchy. We prove that there is a non-Low/sub 2/ ce. degree which bounds no Lachlan nonsplitting base","['Lachlan nonsplitting bases distribution', 'computably enumerable degree', 'Turing degrees', 'formal logic', 'Turing machines']","['nonspiitting bases', 'nonsplitting base', 'degree', 'enumerable degree', 'c.e. degrees', 'c. degree', '= wWWV b', '= wV b', '= vV b', 'bases']",433,86,5,432,85,10,7,5,0
"the variance of firm growth rates: the 'scaling' puzzle recent evidence suggests that a power-law relationship exists between a firm's size and the variance of its growth rate. the flatness of the relation is regarded as puzzling, in that it suggests that large firms are not much more stable than small firms. it has been suggested that the powerlaw nature of the relationship reflects the presence of some form of correlation of growth rates across the firm's constituent businesses. here, it is shown that a model of independent businesses which allows for the fact that these businesses vary in size, as modelled by a simple 'partitions of integers' model, provides a good representation of what is observed empirically ","The variance of firm growth rates: the ‘scaling’ puzzle Recent evidence suggests that a power-law relationship exists between a firm's size and the variance of its growth rate. The flatness of the relation is regarded as puzzling, in that it suggests that large firms are not much more stable than small firms. It has been suggested that the poweriaw nature of the relationship reflects the presence of some form of correlation of growth rates across the firm's constituent businesses. Here, it is shown that a model of independent businesses which allows for the fact that these businesses vary in size, as modelled by a simple ‘partitions of integers' model, provides a good representation of what is observed empirically","['firm growth rates', 'scaling puzzle', 'power-law', 'flatness', 'correlation', 'constituent businesses', 'partitions of integers model', 'size distribution', 'corporate growth', 'corporate modelling', 'economics', 'fluctuations', 'nonlinear dynamical systems', 'probability', 'statistical mechanics']","['poor-law relationship', 'firm growth rates', 'firms', 'scaling puzzle', 'small firms', 'large firms', 'firms size', 'rate', 'growth', 'growth rates']",606,119,15,606,118,10,4,3,4
online longitudinal survey research: viability and participation this article explores the viability of conducting longitudinal survey research using the internet in samples exposed to trauma. a questionnaire battery assessing psychological adjustment following adverse life experiences was posted online. participants who signed up to take part in the longitudinal aspect of the study were contacted 3 and 6 months after initial participation to complete the second and third waves of the research. issues of data screening and sample attrition rates are considered and the demographic profiles and questionnaire scores of those who did and did not take part in the study during successive time points are compared. the results demonstrate that it is possible to conduct repeated measures survey research online and that the similarity in characteristics between those who do and do not take part during successive time points mirrors that found in traditional pencil-and-paper trauma surveys ,Online longitudinal survey research: viability and participation This article explores the viability of conducting longitudinal survey research using the Internet in samples exposed to trauma. A questionnaire battery assessing psychological adjustment following adverse life experiences was posted online. Participants who signed up to take part in the longitudinal aspect of the study were contacted 3 and 6 months after initial participation to complete the second and third waves of the research. Issues of data screening and sample attrition rates are considered and the demographic profiles and questionnaire scores of those who did and did not take part in the study during successive time points are compared. The results demonstrate that it is possible to conduct repeated measures survey research online and that the. similarity in characteristics between those who do and do not take part during successive time points mirrors that found in traditional penciland-paper trauma surveys,"['online longitudinal survey research', 'Internet', 'trauma', 'questionnaire', 'psychological adjustment', 'data screening', 'sample attrition rates', 'demographic profiles', 'World Wide Web', 'psychology research', 'demography', 'information resources', 'Internet', 'psychology']","['longitudinal survey research', 'successive time points', 'viability', 'sample attrition rates', 'initial participation', 'longitudinal aspect', 'research Issues', 'research', 'longitudinal', 'survey']",847,148,14,847,147,10,9,2,7
"agc for autonomous power system using combined intelligent techniques in the present work two intelligent load frequency controllers have been developed to regulate the power output and system frequency by controlling the speed of the generator with the help of fuel rack position control. the first controller is obtained using fuzzy logic (fl) only, whereas the second one by using a combination of fl, genetic algorithms and neural networks. the aim of the proposed controller(s) is to restore in a very smooth way the frequency to its nominal value in the shortest time possible whenever there is any change in the load demand etc. the action of these controller(s) provides a satisfactory balance between frequency overshoot and transient oscillations with zero steady-state error. the design and performance evaluation of the proposed controller(s) structure are illustrated with the help of case studies applied (without loss of generality) to a typical single-area power system. it is found that the proposed controllers exhibit satisfactory overall dynamic performance and overcome the possible drawbacks associated with other competing techniques ","AGC for autonomous power system using combined intelligent techniques In the present work two intelligent load frequency controllers have been developed to regulate the power output and system frequency by controlling the speed of the generator with the help of fuel rack Position control. The first controller is obtained using fuzzy logic (FL) only, whereas the second one by using a combination of FL, genetic algorithms and neural networks. The aim of the proposed controller(s) is to restore in a very smooth way the frequency to its nominal value in the shortest time possible whenever there is any change in the load demand etc. The action of these controller(s) provides a satisfactory balance between frequency overshoot and transient oscillations with zero steady-state error. The design and performance evaluation of the proposed controller(s) structure are illustrated with the help of case studies applied (without loss of generality) to a typical single-area power system. It is found that the proposed controllers exhibit satisfactory overall dynamic performance and overcome the possible ‘drawbacks associated with other competing techniques","['autonomous power system', 'combined intelligent techniques', 'power output regulation', 'generator speed control', 'fuel rack position control', 'fuzzy logic', 'genetic algorithms', 'neural networks', 'load demand', 'frequency overshoot', 'transient oscillations', 'zero steady-state error', 'performance evaluation', 'single-area power system', 'overall dynamic performance', 'competing techniques', 'frequency control', 'controller design', 'frequency control', 'fuzzy control', 'genetic algorithms', 'load regulation', 'neurocontrollers', 'oscillations', 'power generation control']","['combined intelligent techniques', 'autonomous power system', 'controllers structure', 'frequency overshoot', 'system frequency', 'first controller', 'power output', 'system', 'power', 'controllers']",983,175,25,984,174,10,9,1,11
"wavelet-based image segment representation an efficient representation method for arbitrarily shaped image segments is proposed. this method includes a smart way to select a wavelet basis to approximate the given image segment, with improved image quality and reduced computational load ","Wavelet-based image segment representation An efficient representation method for arbitrarily shaped image segments is proposed. This method includes a smart way to select a wavelet basis to approximate the given image segment, with improved image quality and reduced computational load","['image segment representation', 'arbitrarily shaped image segments', 'wavelet basis', 'improved image quality', 'reduced computational load', 'discrete wavelet transform', 'DWT', 'discrete wavelet transforms', 'image representation', 'image segmentation']","['image segments', 'efficient representation method', 'wavelets basis', 'image quality', 'Wavelet-based', 'smart way', 'image', 'method', 'representation', 'segment']",247,41,10,247,40,10,0,0,1
"an optimization approach to plan for reusable software components it is well acknowledged in software engineering that there is a great potential for accomplishing significant productivity improvements through the implementation of a successful software reuse program. on the other hand, such gains are attainable only by instituting detailed action plans at both the organizational and program level. given this need, the paucity of research papers related to planning, and in particular, optimized planning is surprising. this research, which is aimed at this gap, brings out an application of optimization for the planning of reusable software components (scs). we present a model that selects a set of scs that must be built, in order to lower development and adaptation costs. we also provide implications to project management based on simulation, an approach that has been adopted by other cost models in the software engineering literature. such a prescriptive model does not exist in the literature ","An optimization approach to plan for reusable software components It is well acknowledged in software engineering that there is a great potential for accomplishing significant productivity improvements through the implementation of a successful software reuse program. On the other hand, such gains are attainable only by instituting detailed action plans at both the organizational and program level. Given this need, the paucity of research papers related to planning, and in particular, optimized planning is surprising. This research, which is aimed at this, gap, brings out an application of optimization for the planning of reusable software components (SCs). We present a model that selects a set of SCs that must be built, in order to lower development and adaptation costs. We also provide implications to project management based on simulation, an approach that has been adopted by other cost models in the software engineering literature. Such a prescriptive model does not exist in the literature","['software engineering', 'productivity improvements', 'software reuse program', 'optimization', 'action plans', 'optimized planning', 'reusable software components', 'adaptation costs', 'development costs', 'project management', 'simulation', 'object-oriented programming', 'optimisation', 'planning', 'project management', 'software development management', 'software reusability']","['reusable software components', 'planning', 'significant productivity improvements', 'software', 'software engineering literature', 'optimization approach', 'detailed action plans', 'other cost models', 'optimization', 'software engineering']",854,155,17,855,154,10,0,1,4
"bistability of harmonically forced relaxation oscillations relaxation oscillations appear in processes which involve transitions between two states characterized by fast and slow time scales. when a relaxation oscillator is coupled to an external periodic force its entrainment by the force results in a response which can include multiple periodicities and bistability. the prototype of these behaviors is the harmonically driven van der pol equation which displays regions in the parameter space of the driving force amplitude where stable orbits of periods 2n+or-1 coexist, flanked by regions of periods 2n+1 and 2n-1. the parameter regions of such bistable orbits are derived analytically for the closely related harmonically driven stoker-haag piecewise discontinuous equation. the results are valid over most of the control parameter space of the system. also considered are the reasons for the more complicated dynamics featuring regions of high multiple periodicity which appear like noise between ordered periodic regions. since this system mimics in detail the less analytically tractable forced van der pol equation, the results suggest extensions to situations where forced relaxation oscillations are a component of the operating mechanisms ","Bistability of harmonically forced relaxation oscillations Relaxation oscillations appear in processes which involve transitions between two states characterized by fast and slow time scales. When a relaxation oscillator is coupled to an external periodic force its entrainment by the force results in a response which can include multiple periodicities and bistability. The prototype of these behaviors is the harmonically driven van der Pol equation which displays regions in the parameter space of the driving force amplitude where stable orbits of periods 2n+or-1 coexist, flanked by regions of periods 2n+1 and 2n-1. The parameter regions of such bistable orbits are derived analytically for the closely related harmonically driven Stoker-Haag piecewise discontinuous equation. The results are valid ‘over most of the control parameter space of the system. Also considered are the reasons for the more complicated dynamics featuring regions of high multiple periodicity which appear like noise between ordered periodic regions. Since this system mimics in detail the less analytically tractable forced van der Pol equation, the results suggest extensions to situations where forced relaxation oscillations are a component of the operating mechanisms,","['bistability', 'harmonically forced relaxation oscillations', 'external periodic force', 'entrainment', 'van der Pol equation', 'harmonically driven Stoker-Haag piecewise discontinuous equation', 'control parameter space', 'nonlinear dynamics', 'bifurcation', 'nonlinear control systems', 'nonlinear dynamical systems', 'oscillations', 'piecewise constant techniques', 'relaxation', 'relaxation oscillators']","['relaxation oscillations', 'high multiple periodicity', 'external periodic force', 'multiple periodicities', 'slow time scales', 'periodic regions', 'periods n1', 'periods 2n', 'relaxation', 'oscillations']",1073,182,15,1075,181,10,4,2,5
"building an effective computer science student organization: the carnegie mellon women@scs action plan this paper aims to provide a practical guide for building a student organization and designing activities and events that can encourage and support a community of women in computer science. this guide is based on our experience in building women@scs, a community of women in the school of computer science (scs) at carnegie mellon university. rather than provide an abstract ""to-do"" or ""must-do"" list, we present a sampling of concrete activities and events in the hope that these might suggest possibilities for a likeminded student organization. however, since we have found it essential to have a core group of activist students at the helm, we provide a ""to-do"" list of features that we feel are essential for forming, supporting and sustaining creative and effective student leadership ","Building an effective computer science student organization: the Carnegie Mellon Women@SCS action plan This paper aims to provide a practical guide for building a student organization and designing activities and events that can encourage and support a community of women in computer science. This guide is based ‘on our experience in building Women@SCS, a community of women in the School of Computer Science (SCS) at Carnegie Melion University. Rather than provide an abstract ""to-do"" or ""must-do"" list, we present a sampling of concrete activities and events in the hope that these might suggest possibilities for a likeminded student organization. However, since we have found it essential to have a core group of activist students at the helm, we provide a ""to-do"" list of features that we feel are essential for forming, supporting and sustaining creative and effective student leadership","['computer science student organization', 'Women@SCS action plan', 'gender issues', 'women', 'computer science education', 'Carnegie Mellon University', 'student leadership', 'computer science education', 'gender issues', 'social aspects of automation']","['computer science', 'student', 'like-minded student organization', 'effective student leadership', 'Carnegie melon university', 'activist students', 'practical guide', 'organization', 'effective', 'student organization']",755,140,10,756,139,10,3,2,2
"a new subspace identification approach based on principal component analysis principal component analysis (pca) has been widely used for monitoring complex industrial processes with multiple variables and diagnosing process and sensor faults. the objective of this paper is to develop a new subspace identification algorithm that gives consistent model estimates under the errors-in-variables (eiv) situation. in this paper, we propose a new subspace identification approach using principal component analysis. pca naturally falls into the category of eiv formulation, which resembles total least squares and allows for errors in both process input and output. we propose to use pca to determine the system observability subspace, the matrices and the system order for an eiv formulation. standard pca is modified with instrumental variables in order to achieve consistent estimates of the system matrices. the proposed subspace identification method is demonstrated using a simulated process and a real industrial process for model identification and order determination. for comparison the moesp algorithm and n4sid algorithm are used as benchmarks to demonstrate the advantages of the proposed pca based subspace model identification (smi) algorithm ","‘Anew subspace identification approach based on principal component analysis Principal component analysis (PCA) has been widely used for monitoring complex industrial processes with multiple variables and diagnosing process and sensor faults. The objective of this paper is to develop a new subspace identification algorithm that gives consistent model estimates under the errors-in-variables (EIV) situation. In this paper, we propose a new subspace identification approach using principal component analysis. PCA naturally falls into the category of EIV formulation, which resembles total least squares and allows for errors in both process input and output. We propose to use PCA to determine the system ‘observability subspace, the matrices and the system order for an EIV formulation. Standard PCA is modified with instrumental variables in order to achieve consistent estimates of the system matrices. The proposed subspace identification method is demonstrated using a simulated process and a real industrial process for model identification and order determination. For comparison the MOESP algorithm and N4SID algorithm are used as benchmarks to demonstrate the advantages of the proposed PCA based subspace model identification (SMI) algorithm,","['subspace identification approach', 'principal component analysis', 'PCA', 'complex industrial process monitoring', 'process fault diagnosis', 'sensor fault diagnosis', 'errors-in-variables situation', 'EIV situation', 'total least-squares approximation', 'system observability subspace', 'consistent system matrix estimates', 'MOESP algorithm', 'N4SID algorithm', 'subspace model identification', 'SMI', 'fault diagnosis', 'identification', 'least squares approximations', 'principal component analysis', 'process monitoring']","['sub-space identification approach', 'identification', 'model identification', 'sub-space', 'sub-space identification method', 'complex industrial processes', 'consistent model estimates', 'real industrial process', 'observability sub-space', 'simulated process']",1074,180,20,1077,178,10,664,178,6
"discrete output feedback sliding mode control of second order systems - a moving switching line approach the sliding mode control systems (smcs) for which the switching variable is designed independent of the initial conditions are known to be sensitive to parameter variations and extraneous disturbances during the reaching phase. for second order systems this drawback is eliminated by using the moving switching line technique where the switching line is initially designed to pass the initial conditions and is subsequently moved towards a predetermined switching line. in this paper, we make use of the above idea of moving switching line together with the reaching law approach to design a discrete output feedback sliding mode control. the main contributions of this work are such that we do not require to use system states as it makes use of only the output samples for designing the controller. and by using the moving switching line a low sensitivity system is obtained through shortening the reaching phase. simulation results show that the fast output sampling feedback guarantees sliding motion similar to that obtained using state feedback ","Discrete output feedback sliding mode control of second order systems - a moving switching line approach The sliding mode control systems (SMCS) for which the switching variable is designed independent of the initial conditions are known to be sensitive to parameter variations and extraneous disturbances during the reaching phase. For second order systems this drawback is eliminated by using the moving switching line technique where the switching line is initially designed to pass the initial conditions and is subsequently moved towards a predetermined switching line. In this Paper, we make use of the above idea of moving switching line together with the reaching law approach to design a discrete output feedback sliding mode control. The main contributions of this work are such that we do not require to use system states as it makes use of only the ‘output samples for designing the controller. and by using the moving switching line a low sensitivity system is obtained through shortening the reaching phase. Simulation results show that the fast output sampling feedback guarantees sliding motion similar to that obtained using state feedback","['sliding mode control', 'switching variable', 'parameter variations', 'moving switching line', 'discrete output feedback', 'fast output sampling feedback', 'state feedback', 'state feedback', 'variable structure systems']","['switching line', 'second order systems', 'predetermined switching line', 'switching line technique', 'switching line approach', 'mode control systems', 'output samples', 'mode control', 'discrete output feedback', 'Discrete output feedback']",975,182,9,976,181,10,6,1,1
"design and prototype of a performance tool interface for openmp this paper proposes a performance tools interface for openmp, similar in spirit to the mpi profiling interface in its intent to define a clear and portable api that makes openmp execution events visible to runtime performance tools. we present our design using a source-level instrumentation approach based on openmp directive rewriting. rules to instrument each directive and their combination are applied to generate calls to the interface consistent with directive semantics and to pass context information (e.g., source code locations) in a portable and efficient way. our proposed openmp performance api further allows user functions and arbitrary code regions to be marked and performance measurement to be controlled using new openmp directives. to prototype the proposed openmp performance interface, we have developed compatible performance libraries for the expert automatic event trace analyzer [17, 18] and the tau performance analysis framework [13]. the directive instrumentation transformations we define are implemented in a source-to-source translation tool called opari. application examples are presented for both expert and tau to show the openmp performance interface and opari instrumentation tool in operation. when used together with the mpi profiling interface (as the examples also demonstrate), our proposed approach provides a portable and robust solution to performance analysis of openmp and mixed-mode (openmp + mpi) applications ","Design and prototype of a performance tool interface for OpenMP. This paper proposes a performance tools interface for OpenMP, similar in spirit to the MPI profiling interface in its intent to define a clear and portable API that makes OpenMP execution events visible to runtime performance tools. We present our design using a source-level instrumentation approach based on OpenMP directive rewriting. Rules to instrument each directive and their combination are applied to generate calls to the interface consistent with directive semantics and to pass context information (e.g., source code locations) in a portable and efficient way. Our proposed OpenMP performance API further allows user functions and arbitrary code regions to be marked and performance measurement to be controlled using new OpenMP directives. To prototype the proposed OpenMP performance interface, we have developed compatible performance libraries for the EXPERT automatic event trace analyzer [17, 18] and the TAU performance analysis framework [13]. The directive instrumentation transformations we define are implemented in a source-to-source translation tool called OPARI. Application examples are presented for both EXPERT and TAU to show the OpenMP performance interface and OPARI instrumentation tool in operation. When used together with the MPI profiling interface (as the examples also demonstrate), our proposed approach provides a portable and robust solution to performance analysis of OpenMP and mixed-mode (OpenMP + MPI) applications","['performance tool interface', 'MPI profiling interface', 'API', 'source-level instrumentation approach', 'OpenMP directive rewriting', 'directive semantics', 'arbitrary code regions', 'performance libraries', 'EXPERT automatic event trace analyzer', 'TAU performance analysis framework', 'source-to-source translation tool', 'OPARI', 'parallel programming', 'application program interfaces', 'message passing', 'parallel programming', 'program compilers', 'software performance evaluation']","['openup performance interface', 'performance analysis', 'compatible performance libraries', 'performance tools interface', 'performance measurement', 'openup execution events', 'openup performance API', 'new openup directives', 'openup directive', 'performance tools']",1305,221,18,1306,220,10,0,1,5
all-optical logic nor gate using two-cascaded semiconductor optical amplifiers the authors present a novel all-optical logic nor gate using two-cascaded semiconductor optical. amplifiers (soas) in a counterpropagating feedback configuration. this configuration accentuates the gain nonlinearity due to the mutual gain modulation of the two soas. the all-optical nor gate feasibility has been demonstrated delivering an extinction ratio higher than 12 db over a wide range of wavelength ,All-optical logic NOR gate using two-cascaded semiconductor optical amplifiers The authors present a novel all-optical logic NOR gate using two-cascaded semiconductor optical. amplifiers (SOAs) in a counterpropagating feedback configuration. This configuration accentuates the gain nonlinearity due to the mutual gain modulation of the two SOAS. The all-optical NOR gate feasibility has been demonstrated delivering an extinction ratio higher than 12 dB over a wide range of wavelength,"['all-optical logic NOR gate', 'two-cascaded semiconductor optical amplifiers', 'SOA', 'counterpropagating feedback configuration', 'gain nonlinearity', 'mutual gain modulation', 'extinction ratio', 'wide wavelength range', 'laser feedback', 'optical logic', 'optical modulation', 'semiconductor optical amplifiers']","['two-cascaded semiconductor', 'optical amplifiers', 'gate', 'NOR', 'logic', 'mutual gain modulation', 'feedback configuration', 'optical', 'two-cascaded', 'semiconductor']",419,68,12,419,67,10,0,0,2
cooperative three- and four-player quantum games a cooperative multi-player quantum game played by 3 and 4 players has been studied. a quantum superposed operator is introduced in this work which solves the non-zero sum difficulty in previous treatments. the role of quantum entanglement of the initial state is discussed in detail ,Cooperative three- and four-player quantum games A cooperative multi-player quantum game played by 3 and 4 players has been studied. A quantum superposed operator is introduced in this work which solves the non-zero sum difficulty in previous treatments. The role of quantum entanglement of the initial state is discussed in detail,"['cooperative three-player quantum games', 'quantum entanglement', 'initial state', 'cooperative four-player quantum games', 'quantum superposed operator', 'nonzero sum difficulty', 'bound states', 'cooperative systems', 'game theory', 'probability', 'quantum computing', 'quantum theory']","['four-player quantum games', 'non-zero sum difficulty', 'quantum entanglement', 'previous treatments', 'multi-layer', 'quantum', 'game', 'four-player', 'cooperative', 'Cooperative']",281,52,12,281,51,10,0,0,1
"optimization of the memory weighting function in stochastic functional self-organized sorting performed by a team of autonomous mobile agents the activity of a team of autonomous mobile agents formed by identical ""robot-like-ant"" individuals capable of performing a random walk through an environment that are able to recognize and move different ""objects"" is modeled. the emergent desired behavior is a distributed sorting and clustering based only on local information and a memory register that records the past objects encountered. an optimum weighting function for the memory registers is theoretically derived. the optimum time-dependent weighting function allows sorting and clustering of the randomly distributed objects in the shortest time. by maximizing the average speed of a texture feature (the contrast) we check the central assumption, the intermediate steady-states hypothesis, of our theoretical result. it is proved that the algorithm optimization based on maximum speed variation of the contrast feature gives relationships similar to the theoretically derived annealing law ","Optimization of the memory weighting function in stochastic functional self-organized sorting performed by a team of autonomous mobile agents The activity of a team of autonomous mobile agents formed by identical “robot-like-ant” individuals capable of performing a random walk through an environment that are able to recognize and move different “objects” is modeled. The emergent desired behavior is a distributed sorting and clustering based only on local information and a memory register that records the past objects encountered. An optimum. weighting function for the memory registers is theoretically derived. The optimum time-dependent weighting function allows sorting and clustering of the randomly distributed objects in the shortest time. By maximizing the average speed of a texture feature (the contrast) we check the central assumption, the intermediate steady-states hypothesis, of our theoretical result. It is proved that the algorithm optimization based on maximum speed variation of the contrast feature gives relationships similar to the theoretically derived annealing law","['autonomous mobile agents', 'random walk', 'memory weighting function', 'sorting', 'clustering', 'algorithm optimization', 'distributed algorithms', 'mobile computing', 'robots', 'self-adjusting systems', 'software agents', 'sorting']","['autonomous mobile agents', 'memory register', 'optimum weighting function', 'memory weighting function', 'different objects', 'past objects', 'function', 'weighting', 'objects', 'memory']",939,157,12,940,156,10,4,3,5
"recruitment and retention of women graduate students in computer science and engineering: results of a workshop organized by the computing research association this document is the report of a workshop that convened a group of experts to discuss the recruitment and retention of women in computer science and engineering (cse) graduate programs. participants included long-time members of the cse academic and research communities, social scientists engaged in relevant research, and directors of successful retention efforts. the report is a compendium of the experience and expertise of workshop participants, rather than the result of a full-scale, scholarly study into the range of issues. its goal is to provide departments with practical advice on recruitment and retention in the form of a set of specific recommendations ","Recruitment and retention of women graduate students in computer science and engineering: results of a workshop organized by the Computing Research Association This document is the report of a workshop that convened a group of experts to discuss the recruitment and retention of women in computer science and engineering (CSE) graduate programs. Participants included long-time members of the CSE academic and research communities, social scientists engaged in relevant research, and directors of successful retention efforts. The report is a compendium of the experience and expertise of workshop participants, rather than the result of a full-scale, scholarly study into the range of issues. Its goal is to provide departments with practical advice on recruitment and retention in the form of a set of specific recommendations.","['recruitment', 'retention', 'women graduate students', 'computer science', 'engineering', 'Computing Research Association', 'social scientists', 'academic communities', 'research communities', 'directors', 'workshop participants', 'computer science', 'employment', 'gender issues', 'human resource management', 'social aspects of automation']","['computer science', 'Computing Research Association', 'successful retention efforts', 'women', 'workshop participants', 'engineering results', 'graduate students', 'graduate', 'retention', 'computer']",705,125,16,706,124,10,0,1,3
"is diversity in computing a moral matter? we have presented an ethical argument that takes into consideration the subtleties of the issue surrounding under-representation in computing. we should emphasize that there is nothing subtle about overt, unfair discrimination. where such injustice occurs, we condemn it. our concern is that discrimination need not be explicit or overt. it need not be individual-to-individual. rather, it can be subtly built into social practices and social institutions. our analysis raises ethical questions about aspects of computing that drive women away, aspects that can be changed in ways that improve the profession and access to the profession. we hope that computing will move towards these improvements ","Is diversity in computing a moral matter? We have presented an ethical argument that takes into consideration the subtleties of the issue surrounding under-representation in computing We should emphasize that there is nothing subtle about overt, unfair discrimination. Where such injustice occurs, we condemn it. Our concern is that discrimination need not be explicit or overt. It need not be. individual-to-individual. Rather, it can be subtly built into social practices and social institutions. Our analysis raises ethical questions about aspects of computing that drive women away, aspects that can be changed in ways that improve the profession and access to the profession. We hope that computing will move towards these improvements","['ethical argument', 'computing under-representation', 'unfair discrimination', 'social practices', 'social institutions', 'women', 'computer science', 'gender issues', 'professional aspects', 'social aspects of automation']","['overt unfair discrimination', 'social institutions', 'ethical questions', 'social practices', 'ethical argument', 'moral matter', 'diversity', 'ethical', 'moral', 'matter']",630,112,10,630,111,10,0,2,1
"nuts and bolts: implementing descriptive standards to enable virtual collections to date, online archival information systems have relied heavily on legacy finding aids for data to encode and provide to end users, despite fairly strong indications in the archival literature that such legacy data is problematic even as a mediated access tool. archivists have only just begun to study the utility of archival descriptive data for end users in unmediated settings such as via the web. the ability of future archival information systems to respond to the expectations and needs of end users is inextricably linked to archivists getting their collective data house in order. the general international standard archival description (isad(g)) offers the profession a place from which to start extricating ourselves from the idiosyncracies of our legacy data and description practices ","Nuts and bolts: implementing descriptive standards to enable virtual collections To date, online archival information systems have relied heavily on legacy finding aids for data to encode and provide to end users, despite fairly strong indications in the archival literature that such legacy data is problematic even as a mediated access tool. Archivists have only just begun to study the utility of archival descriptive data for end users in unmediated settings such as via the Web. The ability of future archival information systems to respond to the expectations and needs of end users is inextricably linked to archivists getting their collective data house in order. The General International Standard Archival Description (ISAD(G)) offers the profession a place from which to start extricating ourselves from the idiosyncracies of our legacy data and description practices","['descriptive standards', 'virtual collections', 'online archival information systems', 'end users', 'archival literature', 'legacy data', 'mediated access tool', 'archivists', 'archival descriptive data', 'archival information systems', 'collective data house', 'General International Standard Archival Description', 'ISAD', 'Online Archive of California', 'OAC', 'information resources', 'information retrieval systems', 'professional aspects', 'records management', 'standards']","['archival descriptive data', 'descriptive standards', 'description practices', 'collective data house', 'virtual collections', 'legacy finding aids', 'archival literature', 'such legacy data', 'descriptive', 'legacy data']",746,134,20,746,133,10,0,0,4
"using molecular equivalence numbers to visually explore structural features that distinguish chemical libraries a molecular equivalence number (meqnum) classifies a molecule with respect to a class of structural features or topological shapes such as its cyclic system or its set of functional groups. meqnums can be used to organize molecular structures into nonoverlapping, yet highly relatable classes. we illustrate the construction of some different types of meqnums and present via examples some methods of comparing diverse chemical libraries based on meqnums. in the examples we compare a library which is a random sample from the mdl drug data report (mddr) with a library which is a random sample from the available chemical directory (acd). in our analyses, we discover some interesting features of the topological shape of a molecule and its set of functional groups that are strongly linked with compounds occurring in the mddr but not in the acd. we also illustrate the utility of molecular equivalence indices in delineating the structural domain over which an sar conclusion is valid ","Using molecular equivalence numbers to visually explore structural features that distinguish chemical libraries A molecular equivalence number (meqnum) classifies a molecule with respect to a class of structural features or topological shapes such as its cyclic system or its set of functional groups. Meqnums can be used to organize molecular structures into nonoverlapping, yet highly relatable classes. We illustrate the construction of some different types of meqnums and present via examples some methods of comparing diverse chemical libraries based on meqnums. In the examples we compare a library which is a random sample from the MDL Drug Data Report (MDDR) with a library which is a random sample from the Available Chemical Directory (ACD) In our analyses, we discover some interesting features of the topological shape of a molecule and its set of functional groups that are strongly linked with compounds occurring in the MDDR but not in the ACD. We also illustrate the utility of molecular equivalence indices in delineating the structural domain over which an SAR conclusion is valid","['molecular equivalence number', 'molecule classification', 'structural features', 'topological shapes', 'cyclic system', 'functional groups', 'nonoverlapping relatable classes', 'chemical libraries', 'MDL Drug Data Report', 'Available Chemical Directory', 'molecular equivalence indices', 'biology computing', 'chemical structure', 'chemistry computing', 'equivalence classes', 'medical computing', 'medical information systems', 'molecular configurations', 'organic compounds', 'pharmaceutical industry', 'scientific information systems', 'special libraries']","['structural features', 'molecular', 'molecular equivalence numbers', 'molecular equivalence indices', 'diverse chemical libraries', 'functional groups menus', 'molecular structures', 'structural domain', 'structures', 'chemical libraries']",929,172,22,928,171,10,0,1,3
"market watch - air conditioning after a boom period in the late nineties, the air conditioning market finds itself in something of a lull at present, but manufacturers aren't panicking ","Market watch - air conditioning After a boom period in the late nineties, the air conditioning market finds itself in something of a lull at present, but manufacturers aren't panicking","['air conditioning', 'market', 'air conditioning']","['air conditioning market', 'late nineties', 'Market watch', 'boom period', 'Market', 'air', 'conditioning', 'boom', 'watch', 'air conditioning']",155,31,3,155,30,10,0,0,0
"a modified fieller interval for the interval estimation of effective doses for a logistic dose-response curve interval estimation of the gamma % effective dose ( mu /sub gamma / say) is often based on the asymptotic variance of the maximum likelihood estimator (delta interval) or fieller's theorem (fieller interval). sitter and wu (1993) compared the delta and fieller intervals for the median effective dose ( mu /sub 50/) assuming a logistic dose-response curve. their results indicated that although fieller intervals are generally superior to delta intervals, they appear to be conservative. here an adjusted form of the fieller interval for mu /sub gamma / termed an adjusted fieller (af) interval is introduced. a comparison of the af interval with the delta and fieller intervals is provided and the properties of these three interval estimation methods are investigated ","A modified Fieller interval for the interval estimation of effective doses for a logistic dose-response curve Interval estimation of the gamma % effective dose ( mu /sub gamma / say) is often based on the asymptotic variance of the maximum likelihood estimator (delta interval) or Fieller's theorem (Fieller interval). Sitter and Wu (1993) compared the delta and Fieller intervals for the median effective dose ( mu /sub 50/) assuming a logistic dose-response curve. Their results indicated that although Fieller intervals are generally superior to delta intervals, they appear to be conservative. Here an adjusted form of the Fieller interval for mu /sub gamma / termed an adjusted Fieller (AF) interval is introduced. A comparison of the AF interval with the delta and Fieller intervals is provided and the properties of these three interval estimation methods are investigated","['modified Fieller interval', 'interval estimation', 'effective doses', 'logistic dose-response curve', 'asymptotic variance', 'maximum likelihood estimator', 'delta interval', ""Fieller's theorem"", 'median effective dose', 'biology computing', 'data analysis', 'maximum likelihood estimation', 'medical computing', 'statistical analysis']","['interval', 'filler', 'delta intervals', 'interval estimation methods', 'filler intervals Sitter', 'adjusted filler', 'safe interval', 'AF interval', 'filler intervals', 'interval estimation']",743,138,14,743,137,10,0,0,6
"hew selects network management software for more than 100 years, hamburgische electricitats-werke ag (hew) has provided a reliable electricity service to the city of hamburg, germany. today, the company supplies electricity to some 1.7 million inhabitants via 285000 connections. during 1999, the year the energy market was started in germany, hew needed to operate and maintain a safe and reliable network cheaply. the development and implementation of a distribution management system (dms) is key to the success of hew. hew's strategy was to obtain efficient new software for network management that also offered a good platform for future applications. following a pilot and prequalification phase, hew invited several companies to process the requirements catalog and to submit a detailed tender. the network information management system, xpower, developed by tekla oyj, successfully passed hew's test program and satisfied all the performance and system capacity requirements. the system met all hew's conditions by presenting the reality of a network with the attributes of the operating resources. xpower platform provides the ability to integrate future applications ","HEW selects network management software For more than 100 years, Hamburgische Electricitats-Werke AG (HEW) has provided a reliable electricity service to the city of Hamburg, Germany. Today, the company supplies electricity to some 1.7 million inhabitants via 285000 connections. During 1999, the year the energy market was started in Germany, HEW needed to operate and maintain a safe and reliable network cheaply. The development and implementation of a distribution management system (DMS) is key to the success of HEW. HEW's strategy was to obtain efficient new software for network management that also offered a good platform for future applications. Following a pilot and prequalification phase, HEW invited several companies to process the requirements catalog and to submit a detailed tender. The network information management system, Xpower, developed by Tekla Oy), successfully passed HEW's test program and satisfied all the performance and system capacity requirements. The system met all HEW's conditions by presenting the reality of a network with the attributes of the operating resources. Xpower platform provides the ability to integrate future applications","['Hamburgische Electricitats-Werke', 'Hamburg', 'Germany', 'distribution management system', 'network management software', 'Xpower', 'Tekla Oyj', 'management', 'power distribution control', 'software packages']","['network', 'distribution management system', 'system capacity requirements', 'reliable electricity service', 'network management software', 'requalification phase HEW', 'reliable network', 'germany HEW', 'HEW', 'network management']",1004,174,10,1004,173,10,1,1,4
"advancements during the past quarter century in on-line monitoring of motor and generator winding insulation electrical insulation plays a critical role in the operation of motor and generator rotor and stator windings. premature failure of the insulation can cost millions of dollars per day. with advancements in electronics, sensors, computers and software, tremendous progress has been made in the past 25 yr which has transformed on-line insulation monitoring from a rarely used and expensive tool, to the point where 50% of large utility generators in north america are now equipped for such monitoring. this review paper outlines the motivation for online monitoring, discusses the transition to today's technology, and describes the variety of methods now in use for rotor winding and stator winding monitoring ","Advancements during the past quarter century in on-line monitoring of motor and generator winding insulation Electrical insulation plays a critical role in the operation of motor and generator rotor and stator windings. Premature failure of the insulation can cost millions of dollars per day. With advancements in electronics, sensors, computers and software, tremendous progress has been made in the past 25 yr which has transformed on-line insulation monitoring from a rarely used and expensive tool, to the point where 50% of large utility generators in North America are now equipped for such monitoring. This review paper outlines the motivation for online monitoring, discusses the transition to today's technology, and describes the variety of methods now in use for rotor winding and stator winding monitoring","['generator winding insulation', 'motor generator winding insulation', 'winding insulation on-line monitoring', 'premature insulation failure', 'electrical insulation', 'electronics', 'sensors', 'computers', 'software', 'rotor windings', 'stator windings', 'temperature monitoring', 'condition monitors', 'tagging compounds', 'ozone monitoring', 'PD monitoring', 'magnetic flux monitoring', 'partial discharge monitoring', 'endwinding vibration monitoring', 'chemical variables measurement', 'computerised monitoring', 'condition monitoring', 'electric generators', 'electric motors', 'insulation testing', 'machine insulation', 'magnetic flux', 'magnetic variables measurement', 'partial discharge measurement', 'rotors', 'stators', 'temperature measurement', 'vibration measurement']","['monitoring', 'on-line insulation monitoring', 'large utility generators', 'Electrical insulation', 'past quarter century', 'on-line monitoring', 'online monitoring', 'such monitoring', 'generator rotor', 'insulation']",695,125,33,695,124,10,0,0,5
"towards an ontology of approximate reason this article introduces structural aspects in an ontology of approximate reason. the basic assumption in this ontology is that approximate reason is a capability of an agent. agents are designed to classify information granules derived from sensors that respond to stimuli in the environment of an agent or received from other agents. classification of information granules is carried out in the context of parameterized approximation spaces and a calculus of granules. judgment in agents is a faculty of thinking about (classifying) the particular relative to decision rules derived from data. judgment in agents is reflective, but not in the classical philosophical sense (e.g., the notion of judgment in kant). in an agent, a reflective judgment itself is an assertion that a particular decision rule derived from data is applicable to an object (input). that is, a reflective judgment by an agent is an assertion that a particular vector of attribute (sensor) values matches to some degree the conditions for a particular rule. in effect, this form of judgment is an assertion that a vector of sensor values reflects a known property of data expressed by a decision rule. since the reasoning underlying a reflective judgment is inductive and surjective (not based on a priori conditions or universals), this form of judgment is reflective, but not in the sense of kant. unlike kant, a reflective judgment is surjective in the sense that it maps experimental attribute values onto the most closely matching descriptors (conditions) in a derived rule. again, unlike kant's notion of judgment, a reflective judgment is not the result of searching for a universal that pertains to a particular set of values of descriptors. rather, a reflective judgment by an agent is a form of recognition that a particular vector of sensor values pertains to a particular rule in some degree. this recognition takes the form of an assertion that a particular descriptor vector is associated with a particular decision rule. these considerations can be repeated for other forms of classifiers besides those defined by decision rules ","Towards an ontology of approximate reason This article introduces structural aspects in an ontology of approximate reason. The basic assumption in this ontology is that approximate reason is a capability of an agent. Agents are designed to classify information granules derived from sensors that respond to stimuli in the environment of an agent or received from other agents. Classification of information granules is carried out in the context of parameterized approximation spaces and a calculus of granules. Judgment in agents is a faculty of thinking about (classifying) the particular relative to decision rules derived from data. Judgment in agents is, reflective, but not in the classical philosophical sense (e.g., the notion of judgment in Kant). In an agent, a reflective judgment itself is an assertion that a particular decision rule derived from data is, applicable to an object (input). That is, a reflective judgment by an agent is an assertion that a particular vector of attribute (sensor) values matches to some degree the conditions for a particular rule. In effect, this form of judgment is an assertion that a vector of sensor values reflects a known property of data expressed by a decision rule Since the reasoning underlying a reflective judgment is inductive and surjective (not based on a priori conditions or universals), this form of judgment is reflective, but not in the sense of Kant. Unlike Kant, a reflective judgment is surjective in the sense that it maps experimental attribute values onto the most closely matching descriptors (conditions) in a derived rule. Again, unlike Kant's notion of judgment, a reflective judgment is not the result of searching for a universal that pertains to a particular set of values of descriptors. Rather, a reflective judgment by an agent is a form of recognition that a particular vector of sensor values pertains to a particular rule in some degree. This recognition takes the form of an assertion that a particular descriptor vector is associated with a particular decision Tule. These considerations can be repeated for other forms of classifiers besides those defined by decision rules","['ontology', 'approximate reason', 'information granules', 'parameterized approximation spaces', 'granules', 'decision rules', 'reflective judgment', 'pattern recognition', 'rough sets', 'inference mechanisms', 'neural nets', 'software agents', 'uncertainty handling']","['reflective judgment', 'approximate reason', 'particular decision rule', 'ontology', 'particular rule', 'parameterized approximation spaces', 'other agents Classification', 'agent Agents', 'reasoning', 'approximation']",1816,345,13,1817,344,10,1,4,2
"linear tense logics of increasing sets we provide an extension of the language of linear tense logic with future and past connectives f and p, respectively, by a modality that quantifies over the points of some set which is assumed to increase in the course of time. in this way we obtain a general framework for modelling growth qualitatively. we develop an appropriate logical system, prove a corresponding completeness and decidability result and discuss the various kinds of flow of time in the new context. we also consider decreasing sets briefly ","Linear tense logics of increasing sets We provide an extension of the language of linear tense logic with future and past connectives F and P, respectively, by a modality that quantifies, ‘over the points of some set which is assumed to increase in the course of time. In this way we obtain a general framework for modelling growth qualitatively. We develop an appropriate logical system, prove a corresponding completeness and decidability result and discuss the various kinds of flow of time in the new context. We also consider decreasing sets briefly","['linear tense logic', 'future and past connectives', 'logical system', 'completeness', 'decidability', 'decreasing sets', 'temporal reasoning', 'decidability', 'equivalence classes', 'temporal logic']","['appropriate logical system', 'Linear tense logics', 'past connectives F', 'sets briefly', 'extension', 'tense', 'Linear', 'sets', 'logics', 'linear tense logic']",462,92,10,464,91,10,4,2,3
"evaluation of existing and new feature recognition algorithms. 1. theory and implementation this is the first of two papers evaluating the performance of general-purpose feature detection techniques for geometric models. in this paper, six different methods are described to identify sets of faces that bound depression and protrusion faces. each algorithm has been implemented and tested on eight components from the national design repository. the algorithms studied include previously published general-purpose feature detection algorithms such as the single-face inner-loop and concavity techniques. others are improvements to existing algorithms such as extensions of the two-dimensional convex hull method to handle curved faces as well as protrusions. lastly, new algorithms based on the three-dimensional convex hull, minimum concave, visible and multiple-face inner-loop face sets are described ","Evaluation of existing and new feature recognition algorithms. 1. Theory and implementation This is the first of two papers evaluating the performance of general-purpose feature detection techniques for geometric models. In this paper, six different methods are described to identify sets of faces that bound depression and protrusion faces. Each algorithm has been implemented and tested on eight components from the National Design Repository. The algorithms studied include previously published general-purpose feature detection algorithms such as the single-face inner-loop and concavity techniques. Others are improvements to existing algorithms such as extensions of the two-dimensional convex hull method to handle curved faces as well as protrusions. Lastly, new algorithms based on the three-dimensional convex hull, minimum concave, visible and multiple-face inner-loop face sets are described","['feature recognition algorithms', 'geometric models', 'general-purpose feature detection techniques', 'sets of faces', 'depression faces', 'protrusion faces', 'National Design Repository', 'single-face inner-loop technique', 'concavity technique', 'two-dimensional convex hull method', 'curved faces', 'three-dimensional convex hull', 'CAD/CAM software', 'geometric reasoning algorithms', 'minimum concave', 'visible inner-loop face sets', 'multiple-face inner-loop face sets', 'CAD/CAM', 'computational geometry', 'feature extraction', 'spatial reasoning']","['feature', 'concavity techniques Others', 'different methods', 'new algorithms', 'curved faces', 'recognition', 'Evaluation', 'papers', 'new', 'algorithms']",780,125,21,780,124,10,0,0,6
"sensing and control of double-sided arc welding process the welding industry is driven to improve productivity without sacrificing quality. for thick material welding, the current practice is to use backing or multiple passes. the laser welding process, capable of achieving deep narrow penetration, can significantly improve welding productivity for such applications by reducing the number of passes. however, its competitiveness in comparison with traditional arc welding is weakened by its high cost, strict fit-up requirement, and difficulty in welding large structures. in this work, a different method, referred to as double-sided arc welding (dsaw) is developed to improve the arc concentration for arc welding. a sensing and control system is developed to achieve deep narrow penetration under variations in welding conditions. experiments verified that the pulsed keyhole dsaw system developed is capable of achieving deep narrow penetration on a 1/2 inch thick square butt joint in a single pass ","Sensing and control of double-sided arc welding process The welding industry is driven to improve productivity without sacrificing quality. For thick material welding, the current practice is to use backing or multiple passes. The laser welding process, capable of achieving deep narrow penetration, can significantly improve welding productivity for such applications by reducing the number of passes. However, its competitiveness in comparison with traditional arc welding is weakened by its high cost, strict fit-up requirement, and difficulty in welding large structures. In this work, a different method, referred to as double-sided arc welding (DSAW) is developed to improve the arc concentration for arc welding. A sensing and control system is, developed to achieve deep narrow penetration under variations in welding conditions. Experiments verified that the pulsed keyhole DSAW system developed is capable of achieving deep narrow penetration on a 1/2 inch thick square butt joint in a single pass","['double-sided arc welding', 'laser welding process', 'control system', 'process control', 'thick material welding', 'energy density', 'controlled pulse keyhole', 'arc welding', 'control systems', 'process control']","['welding', 'deep narrow penetration', 'double-sided arc welding', 'welding conditions Experiments', 'traditional arc welding', 'thick material welding', 'laser welding process', 'welding productivity', 'welding industry', 'arc welding A']",858,150,10,859,149,10,0,1,1
"duality revisited: construction of fractional frequency distributions based on two dual lotka laws fractional frequency distributions of, for example, authors with a certain (fractional) number of papers are very irregular, and therefore not easy to model or to explain. the article gives a first attempt to this by as suming two simple lotka laws (with exponent 2): one for the number of authors with n papers (total count here) and one for the number of papers with n authors, n in n. based on an earlier made convolution model of egghe, interpreted and reworked now for discrete scores, we are able to produce theoretical fractional frequency distributions with only one parameter, which are in very close agreement with the practical ones as found in a large dataset produced earlier by rao (1995). the article also shows that (irregular) fractional frequency distributions are a consequence of lotka's law, and are not examples of breakdowns of this famous historical law ","Duality revisited: construction of fractional frequency distributions based on ‘two dual Lotka laws Fractional frequency distributions of, for example, authors with a certain (fractional) number of papers are very irregular, and therefore not easy to model or to explain. The article gives a first attempt to this by as suming two simple Lotka laws (with exponent 2): one for the number of authors with n papers (total count here) and one for the number of papers with n authors, n in N. Based on an earlier made convolution model of Egghe, interpreted and reworked now for discrete scores, we are able to produce theoretical fractional frequency distributions with only one parameter, which are in very close agreement with the practical ones as found in a large dataset produced earlier by Rao (1995). The article also shows that (irregular) fractional frequency distributions are a consequence of Lotka's law, and are not examples of breakdowns of this famous historical aw","['dual Lotka laws', 'convolution model', 'discrete scores', 'irregular fractional frequency distributions', 'citation analysis']","['certain fractional number', 'simple Lotka laws', 'example authors', 'dual Lotka laws', 'n authors n', 'fractional', 'frequency', 'distributions', 'fractional frequency distributions', 'Fractional frequency distributions']",819,159,5,819,158,10,5,2,2
"new water management system begins operation at us projects the us army corps of engineers has developed a new automated information system to support its water control management mission. the new system provides a variety of decision support tools, enabling water control managers to acquire, transform, verify, store, display, analyse, and disseminate data and information efficiently and around the clock ","New water management system begins operation at US projects The US Army Corps of Engineers has developed a new automated information system to support its water control management mission. The new system provides a variety of decision support tools, enabling water control managers to acquire, transform, verify, store, display, analyse, and disseminate data and information efficiently and around the clock","['watershed modelling', 'water management system', 'US projects', 'US Army Corps of Engineers', 'automated information system', 'water control management mission', 'decision support tools', 'water control managers', 'data dissemination', 'data acquisition', 'data storage', 'data verification', 'data display', 'data analysis', 'data visualization', 'decision support system', 'Corps Water Management System', 'data acquisition', 'data visualisation', 'decision support systems', 'engineering information systems', 'geophysics computing', 'management', 'natural resources']","['management', 'water control managers', 'decision support tools', 'information system', 'US Army Corps', 'US projects', 'new system', 'system', 'US', 'water']",348,61,24,348,60,10,0,0,0
"layer-based machining: recent development and support structure design there is growing interest in additive and subtractive shaping theories that are synthesized to integrate the layered manufacturing process and material removal process. layer-based machining has emerged as a promising method for integrated additive and subtractive shaping theory. in the paper, major layer-based machining systems are reviewed and compared according to characteristics of stock layers, numerical control machining configurations, stacking operations, input format and raw materials. support structure, a major issue in machining-based systems which has seldom been addressed in previous research, is investigated in the paper with considerations of four situations: floating overhang, cantilever, vaulted overhang and ceiling. except for the floating overhang where a support structure should not be overlooked, the necessity for support structures for the other three situations is determined by stress and deflection analysis. this is demonstrated by the machining of a large castle model ","Layer-based machining: recent development and support structure design There is growing interest in additive and subtractive shaping theories that are synthesized to integrate the layered manufacturing process and material removal process. Layer-based machining has emerged as a promising method for integrated additive and subtractive shaping theory. In the Paper, major layer-based machining systems are reviewed and compared according to characteristics of stock layers, numerical control machining configurations, stacking operations, input format and raw materials. Support structure, a major issue in machining-based systems which has seldom been addressed in previous research, is investigated in the paper with considerations of four situations: floating overhang, cantilever, vaulted overhang and ceiling. Except for the floating ‘overhang where a support structure should not be overlooked, the necessity for support structures for the other three situations is determined by stress and deflection analysis. This is demonstrated by the machining of a large castle model","['layer-based machining', 'support structure design', 'additive shaping theories', 'subtractive shaping theories', 'layered manufacturing process', 'material removal process', 'stock layers', 'numerical control machining configurations', 'stacking operations', 'input format', 'raw materials', 'floating overhang', 'cantilever', 'vaulted overhang', 'ceiling', 'stress', 'deflection analysis', 'machining', 'numerical control', 'rapid prototyping (industrial)']","['water-based machining', 'layered manufacturing process', 'subtractive shaping theory', 'support structure design', 'material removal process', 'support structures', 'recent development', 'structure', 'machining', 'water-based']",931,149,20,932,148,10,8,1,11
"application of heuristic methods for conformance test selection in this paper we focus on the test selection problem. it is modeled after a real-life problem that arises in telecommunication when one has to check the reliability of an application. we apply different metaheuristics, namely reactive tabu search (rts), genetic algorithms (ga) and simulated annealing (sa) to solve the problem. we propose some modifications to the conventional schemes including an adaptive neighbourhood sampling in rts, an adaptive variable mutation rate in ga and an adaptive variable neighbourhood structure in sa. the performance of the algorithms is evaluated in different models for existing protocols. computational results show that ga and sa can provide high-quality solutions in acceptable time compared to the results of a commercial software, which makes them applicable in practical test selection ","Application of heuristic methods for conformance test selection In this paper we focus on the test selection problem. It is modeled after a real-life problem that arises in telecommunication when one has to check the reliability of an application. We apply different metaheuristics, namely Reactive Tabu Search (RTS), Genetic Algorithms (GA) and Simulated Annealing (SA) to solve the problem. We propose some modifications to the conventional schemes including an adaptive neighbourhood sampling in RTS, an adaptive variable mutation rate in GA and an adaptive variable neighbourhood structure in SA. The performance of the algorithms is evaluated in different models for existing protocols. Computational results show that GA and SA can provide high-quality solutions in acceptable time compared to the results of a ‘commercial software, which makes them applicable in practical test selection","['telecommunication conformance test selection', 'heuristic methods', 'test selection problem', 'reliability', 'metaheuristics', 'reactive Tabu search', 'genetic algorithms', 'simulated annealing', 'adaptive neighbourhood sampling', 'adaptive variable mutation rate', 'adaptive variable neighbourhood structure', 'ISDN protocol', 'GSM protocol', 'adaptive systems', 'cellular radio', 'conformance testing', 'genetic algorithms', 'heuristic programming', 'ISDN', 'protocols', 'search problems', 'simulated annealing', 'telecommunication computing', 'telecommunication equipment testing', 'telecommunication network reliability']","['Application', 'conformance test selection', 'practical test selection', 'test selection problem', 'real-life problem', 'heuristic methods', 'selection', 'test', 'methods', 'conformance']",762,133,25,763,132,10,9,1,11
"the results of experimental studies of the reflooding of fuel-rod assemblies from above and problems for future investigations problems in studying the reflooding of assemblies from above conducted at foreign and russian experimental installations are considered. the efficiency of cooling and flow reversal under countercurrent flow of steam and water, as well as the scale effect are analyzed. the tasks for future experiments that are necessary for the development of modern correlations for the loss-of-coolant accident (loca) computer codes are stated ","The results of experimental studies of the reflooding of fuel-rod assemblies from above and problems for future investigations Problems in studying the reflooding of assemblies from above conducted at foreign and Russian experimental installations are considered. The efficiency of cooling and flow reversal under countercurrent flow of steam and water, as well as the scale effect are analyzed. The tasks for future experiments that are necessary for the development of modern correlations for the loss-of-coolant accident (LOCA) computer codes are stated","['fuel-rod assemblies reflooding', 'Russian experimental installations', 'cooling efficiency', 'flow reversal', 'countercurrent flow', 'steam', 'water', 'loss-of-coolant accident computer codes', 'LOCA computer codes', 'fission reactor cooling', 'fission reactor fuel', 'fuel element failure', 'nuclear engineering computing']","['flooding', 'Russian experimental installations', 'future investigations Problems', 'above', 'experimental studies', 'fuelwood assemblies', 'future experiments', 'results', 'experimental', 'assemblies']",476,82,13,476,81,10,0,0,4
"five-axis nc milling of ruled surfaces: optimal geometry of a conical tool the side milling of ruled surfaces using a conical milling cutter was studied. this is a field that has largely been ignored by research scientists, but it is much used in industry, especially to machine turbine blades. we first suggest an improved positioning with respect to the directrices of the ruled surface. as compared with the methods already developed for the cylindrical cutter, this positioning enables the error between the cutter and the work-piece to be reduced. an algorithm is then introduced to calculate error so one can determine the cutter dimensions (cone radius and angle) in order to respect the tolerance interval imposed by the design office. this study provides an opportunity to determine cutters with greater dimensions, thus alleviating bending problems during milling ","Five-axis NC milling of ruled surfaces: optimal geometry of a conical tool The side milling of ruled surfaces using a conical milling cutter was studied. This is a field that has largely been ignored by research scientists, but it is much used in industry, especially to machine turbine blades. We first suggest an improved positioning with respect to the directrices of the ruled surface. As compared with the methods already developed for the cylindrical cutter, this positioning enables the error between the cutter and the work-piece to be reduced. An algorithm is then introduced to calculate error so one can determine the cutter dimensions (cone radius and angle) in order to respect the tolerance interval imposed by the design office. This study provides an ‘opportunity to determine cutters with greater dimensions, thus alleviating bending problems during milling","['five-axis NC milling', 'ruled surfaces', 'optimal geometry', 'conical', 'side milling', 'conical milling cutter', 'positioning', 'cutter dimensions', 'tolerance interval', 'CAD/CAM', 'machine tools', 'machining', 'numerical control', 'position control']","['surfaces', 'milling', 'conical milling cutter', 'Five-axis NC milling', 'cylindrical cutter', 'optimal geometry', 'cutter', 'side milling', 'conical tool', 'conical']",737,138,14,738,137,10,10,1,2
explicit solutions for transcendental equations a simple method to formulate an explicit expression for the roots of any analytic transcendental function is presented. the method is based on cauchy's integral theorem and uses only basic concepts of complex integration. a convenient method for numerically evaluating the exact expression is presented. the application of both the formulation and evaluation of the exact expression is illustrated for several classical root finding problems ,Explicit solutions for transcendental equations Asimple method to formulate an explicit expression for the roots of any analytic transcendental function is presented. The method is based on Cauchy's integral theorem and uses only basic concepts of complex integration. A convenient method for numerically evaluating the exact expression is presented. The application of both the formulation and evaluation of the exact expression is illustrated for several classical root finding problems,"['analytic functions', 'transcendental equations', 'Cauchy integral theorem', 'complex integration', 'root finding', 'singularity', 'polynomial', 'Fourier transform', 'approximation theory', 'Fourier transforms', 'integral equations', 'mathematics computing', 'polynomials']","['exact expression', 'analytic transcendental function', 'transcendental equations', 'explicit expression', 'Explicit solutions', 'convenient method', 'simple method', 'method', 'transcendental', 'expression']",420,71,13,420,69,10,207,64,2
"the curious ways of professional cultures and the ""two-body opportunity"" when two professionals are a couple, we sometimes refer to them as having a ""two-body problem."" however, when each partner of a couple exists in the same cultures, they also have an opportunity for deeply shared understanding and empathy, simply because each understands at a deep level the culture in which the other works. i explore this notion. a couple has what we call the ""two-body problem"" when both are professionals who are qualified for a kind of position that is relatively rare and who are very selective about the positions that they accept. for example, there are relatively scant numbers of jobs as a computer science professor-at any level. an individual considering an academic job may only be interested in research universities, or in teaching universities, restricting the choice of open positions substantially. the classic two-body ""problem"" arises when one partner wants to accept a new position that requires geographical relocation. then, the other partner also needs to find a new position. moreover, it can be very difficult to find a suitable position when they are naturally scarce ","The curious ways of professional cultures and the ""two-body opportunity” When two professionals are a couple, we sometimes refer to them as having a ""two-body problem."" However, when each partner of a couple exists in the same cultures, they also have an opportunity for deeply shared understanding and empathy, simply because each understands at a deep level the culture in which the other works. | explore this notion. A couple has what we call the ""two-body problem"" when both are professionals who are qualified for a kind of position that is, relatively rare and who are very selective about the positions that they accept. For example, there are relatively scant numbers of jobs as a computer science professor-at any level. An individual considering an academic job may only be interested in research universities, or in teaching universities, restricting the choice of open positions substantially. The classic two-body ""problem"" arises when one partner wants to accept a new position that requires geographical relocation Then, the other partner also needs to find a new position. Moreover, it can be very difficult to find a suitable position when they are naturally scarce","['professional cultures', 'computer science', 'two-body opportunity', 'academics', 'professional aspects']","['two-body problem', 'professionals', 'new position', 'professional cultures', 'two-body opportunity', 'notion A couple', 'same cultures', 'curious ways', 'culture', 'two-body']",995,190,5,995,189,10,2,4,1
"adaptive stabilization of undamped flexible structures in the paper non-identifier-based adaptive stabilization of undamped flexible structures is considered in the case of collocated input and output operators. the systems have poles and zeros on the imaginary axis. in the case where velocity feedback is available, the adaptive stabilizer is constructed by an adaptive pd-controller (proportional plus derivative controller). in the case where only position feedback is available, the adaptive stabilizer is constructed by an adaptive p-controller for the augmented system which consists of the controlled system and a parallel compensator. numerical examples are given to illustrate the effectiveness of the proposed controllers ","Adaptive stabilization of undamped flexible structures In the paper non-identifier-based adaptive stabilization of undamped flexible structures is considered in the case of collocated input and output operators. The systems have poles and zeros on the imaginary axis. In the case where velocity feedback is available, the adaptive stabilizer is constructed by an adaptive PD-controller (proportional plus derivative controller). In the case where only position feedback is available, the adaptive stabilizer is constructed by an adaptive P-controller for the augmented system which consists of the controlled system and a parallel compensator. Numerical examples are given to illustrate the effectiveness of the proposed controllers","['adaptive stabilization', 'undamped flexible structures', 'poles and zeros', 'imaginary axis', 'velocity feedback', 'adaptive PD-controller', 'proportional plus derivative controller', 'position feedback', 'adaptive P-controller', 'augmented system', 'parallel compensator', 'adaptive control', 'closed loop systems', 'compensation', 'flexible structures', 'poles and zeros', 'position control', 'proportional control', 'stability', 'two-term control', 'velocity control']","['adaptive', 'undamped flexible structures', 'adaptive stabilizer', 'non-identifier-based adaptive stabilization', 'adaptive PD-controller', 'adaptive controller', 'controlled system', 'Adaptive stabilization', 'flexible', 'stabilizer']",631,103,21,631,102,10,0,0,6
"incorporating multi-leaf collimator leaf sequencing into iterative imrt optimization intensity modulated radiation therapy (imrt) treatment planning typically considers beam optimization and beam delivery as separate tasks. following optimization, a multi-leaf collimator (mlc) or other beam delivery device is used to generate fluence patterns for patient treatment delivery. due to limitations and characteristics of the mlc, the deliverable intensity distributions often differ from those produced by the optimizer, leading to differences between the delivered and the optimized doses. objective function parameters are then adjusted empirically, and the plan is reoptimized to achieve a desired deliverable dose distribution. the resulting plan, though usually acceptable, may not be the best achievable. a method has been developed to incorporate the mlc restrictions into the optimization process. our in-house imrt system has been modified to include the calculation of the deliverable intensity into the optimizer. in this process, prior to dose calculation, the mlc leaf sequencer is used to convert intensities to dynamic mlc sequences, from which the deliverable intensities are then determined. all other optimization steps remain the same. to evaluate the effectiveness of deliverable-based optimization, 17 patient cases have been studied. compared with standard optimization plus conversion to deliverable beams, deliverable-based optimization results show improved isodose coverage and a reduced dose to critical structures. deliverable-based optimization results are close to the original nondeliverable optimization results, suggesting that imrt can overcome the mlc limitations by adjusting individual beamlets. the use of deliverable-based optimization may reduce the need for empirical adjustment of objective function parameters and reoptimization of a plan to achieve desired results ","Incorporating multi-leaf collimator leaf sequencing into iterative IMRT optimization Intensity modulated radiation therapy (IMRT) treatment planning typically considers beam optimization and beam delivery as separate tasks. Following optimization, a multi-leaf collimator (MLC) or other beam delivery device is used to generate fluence patterns for patient treatment delivery. Due to limitations and characteristics of the MLC, the deliverable intensity distributions often differ from those produced by the optimizer, leading to differences between the delivered and the optimized doses. Objective function parameters are then adjusted empirically, and the pian is reoptimized to achieve a desired deliverable dose distribution. The resulting plan, though usually acceptable, may not be the best achievable. A method has been developed to incorporate the MLC restrictions into the optimization process. Our in-house IMRT system has been modified to include the calculation of the deliverable intensity into the optimizer. In this process, prior to dose calculation, the MLC leaf sequencer is used to convert intensities to dynamic MLC sequences, from which the deliverable intensities are then determined. All other optimization steps remain the same. To evaluate the effectiveness of deliverable-based optimization, 17 patient cases have been studied. Compared with standard optimization plus conversion to deliverable beams, deliverable-based optimization results show improved isodose coverage and a reduced dose to critical structures. Deliverable-based optimization results are close to the original nondeliverable optimization results, suggesting that IMRT can ‘overcome the MLC limitations by adjusting individual beamlets. The use of deliverable-based optimization may reduce the need for empirical adjustment of objective function parameters and reoptimization of a plan to achieve desired results","['intensity modulated radiation therapy', 'treatment planning', 'iterative optimization', 'multileaf collimator leaf sequencing', 'beam delivery', 'beam optimization', 'fluence patterns', 'objective function parameters', 'deliverable dose distribution', 'empirical adjustment', 'tumor dose', 'optimized intensity', 'gradient-based search algorithm', 'beamlet ray intensities', 'Newton method', 'dose-volume objective values', 'dosimetry', 'gradient methods', 'medical computing', 'Newton method', 'optimisation', 'radiation therapy']","['optimization', 'multi-year collimator mlc', 'other optimization steps', 'standard optimization', 'optimization process', 'MLC leaf sequencer', 'beam optimization', 'deliverable-based optimization', 'deliverable-based optimization results', 'Deliverable-based optimization results']",1648,260,22,1649,259,10,9,2,7
"central hub for design assets: adobe golive 6.0 adobe golive is a strong contender for web authoring and publishing. version 6.0 features a flexible gui environment combined with a comprehensive workgroup and collaboration server, plus tight integration with leading design tools ","Central hub for design assets: Adobe GoLive 6.0 Adobe GoLive is a strong contender for Web authoring and publishing. Version 6.0 features a flexible GUI environment combined with a comprehensive workgroup and collaboration server, plus tight integration with leading design tools","['Adobe GoLive 6.0', 'Flash', 'Real', 'Java', 'application servers', 'Web authoring', 'GUI', 'workgroup server', 'collaboration server', 'LiveMotion 2.0', 'animation and scripting tool', 'Macromedia SWF format', 'workgroup environment', 'Web publishing environment', 'design-centric dynamic content', 'authoring systems', 'graphical user interfaces', 'groupware', 'information resources', 'Internet', 'software reviews']","['Adobe olive', 'flexible GUI environment', 'design assets Adobe', 'strong contender', 'design tools', 'Central hub', 'design', 'Adobe', 'assets', 'olive']",239,42,21,239,41,10,0,0,2
"support communities for women in computing this article highlights the many activities provided by the support communities available for women in computing. thousands of women actively participate in these programs and they receive many benefits including networking and professional support. in addition, the organizations and associations help promote the accomplishments of women computer scientists and disseminate valuable information. this article surveys some of these organizations and concludes with a list of suggestions for how faculty members can incorporate the benefits of these organizations in their own institutions ","Support communities for women in computing This article highlights the many activities provided by the support communities. available for women in computing. Thousands of women actively Participate in these programs and they receive many benefits including networking and professional support. In addition, the organizations and associations help promote the accomplishments of women computer scientists and disseminate valuable information. This article surveys some of these organizations and concludes with a list of suggestions for how faculty members can incorporate the benefits of these organizations in their own institutions","['support communities', 'women', 'computing', 'networking', 'professional support', 'information dissemination', 'faculty members', 'computer science education', 'gender issues', 'professional communication']","['article', 'women computer scientists', 'professional support', 'many activities', 'many benefits', 'many', 'women', 'communities', 'support communities', 'Support communities']",546,88,10,547,87,10,0,1,3
"designing a new urban internet the parallel between designing a web site and the construction of a building is a familiar one, but how often do we think of the internet as having parks and streets? it would be absurd to say that the internet could ever take the place of real, livable communities; however, it is safe to say that the context for using the internet is on a path of change. as the internet evolves beyond a simple linkage of disparate web sites and applications, the challenge for information architects is establishing a process by which to structure, organize, and design networked environments. the principles that guide new urbanism can offer much insight into networked electronic environment design. at the core of every new urbanism principle is the idea of ""wholeness""-of making sure that neighborhoods and communities are knit together in a way that supports civic activities, economic development, efficient ecosystems, aesthetic beauty, and human interaction ","Designing a new urban Internet The parallel between designing a Web site and the construction of a building is a familiar one, but how often do we think of the Internet as having parks and streets? It would be absurd to say that the Internet could ever take the place of real, livable communities; however, it is safe to say that the context for using the Internet is on a path of change. As the Internet evolves beyond a simple linkage of disparate Web sites and applications, the challenge for Information Architects is establishing a process by which to structure, organize, and design networked environments. The principles that guide New Urbanism can offer much insight into networked electronic environment design. At the core of every New Urbanism principle is the idea of ""wholeness""-of making sure that neighborhoods and communities are knit together in a way that supports civic activities, economic development, efficient ecosystems, aesthetic beauty, and human interaction","['Web site', 'Internet', 'information architects', 'private-public sector cooperation', 'global information networks', 'networked environments', 'networked electronic environment design', 'communities', 'electronic publishing', 'hypermedia', 'information resources', 'Internet']","['Designing', 'real liable communities', 'disparate Web sites', 'new urban Internet', 'Web', 'site', 'new', 'urban', 'Web site', 'Internet']",827,159,12,827,158,10,0,0,3
fresh tracks [food processing] bar code labels and wireless terminals linked to a centralized database accurately track meat products from receiving to customers for farmland foods ,Fresh tracks [food processing] Bar code labels and wireless terminals linked to a centralized database accurately track meat products from receiving to customers for Farmland Foods,"['food processing', 'bar code labels', 'wireless terminals', 'Farmland Foods', 'automatic data capture', 'Intermec Technologies', 'bar codes', 'data acquisition', 'food processing industry', 'mobile computing', 'production control']","['centralized database', 'wireless terminals', 'food', 'meat products', 'processing', 'tracks', 'labels', 'Fresh', 'code', 'Bar']",155,27,11,155,26,10,0,0,1
"an identity-based society oriented signature scheme with anonymous signers in this paper, we propose a new society oriented scheme, based on the guillou-quisquater (1989) signature scheme. the scheme is identity-based and the signatures are verified with respect to only one identity. that is, the verifier does not have to know the identity of the co-signers, but just that of the organization they represent ","An identity-based society oriented signature scheme with anonymous signers In this paper, we propose a new society oriented scheme, based on the Guillou-Quisquater (1989) signature scheme. The scheme is identity-based and the signatures are verified with respect to only one identity. That is, the verifier does not have to know the identity of the co-signers, but just that of the organization they represent","['identity-based society oriented signature scheme', 'anonymous signers', 'signature verification', 'cryptography']","['signature scheme', 'identity-based society', 'anonymous signers', 'new society', 'identity', 'society', 'anonymous', 'identity-based', 'scheme', 'signature']",347,64,4,347,63,10,0,0,0
"ten years of strategies to increase participation of women in computing programs. the central queensland university experience: 1999-2001 in the late eighties, the participation rate of women in information technology courses in most australian universities was around 25%. this low level of women's participation in computing courses occurs not only in australia but also overseas. more studies indicate that the participation rates have not improved and in fact may be even further in decline. participation rates in the workforce also appear to be in decline. concerned at the imbalance within australia, the federal government directed all australian universities to increase the number of women in courses leading to a professional computing qualification (i.e., information technology courses) to 40% of students by 1995. this paper details one australian university's approach, over a 10 year period (1991-2001), to redress this imbalance. we provide examples of intervention strategies developed and the outcomes for these strategies. we present the outcomes against a background frame of the australian higher education scene of that decade which was influenced by funding levels to universities in general and to equity programs in particular. we present data related to the participation of women in computing programs along with snapshots of the overall changing student demographics over this period ","Ten years of strategies to increase participation of women in computing programs. The Central Queensland University experience: 1999-2001 In the late eighties, the participation rate of women in information technology courses in most Australian Universities was around 25%. This low level ‘of women’s participation in computing courses occurs not only in Australia but also overseas. More studies indicate that the Participation rates have not improved and in fact may be even further in decline. Participation rates in the workforce also appear to be in decline. Concerned at the imbalance within Australia, the Federal government directed all Australian Universities to increase the number ‘of women in courses leading to a professional computing qualification (Le., information technology courses) to 40% of students by 1995. This Paper details one Australian university's approach, over a 10 year period (1991-2001), to redress this imbalance. We provide examples of intervention strategies developed and the outcomes for these strategies. We present the outcomes against a background frame of the Australian Higher Education scene of that decade which was influenced by funding levels to universities in general and to equity programs in particular. We present data related to the participation of women in ‘computing programs along with snapshots of the overall changing student demographics over this period","['Central Queensland University', 'women', 'computing programs', 'demographics', 'Australian Higher Education', 'computer science education', 'demography', 'gender issues', 'management of change']","['information technology courses', 'Australian university approach', 'most Australian Universities', 'decline Participation rates', 'intervention strategies', 'womens participation', 'year period', 'women', 'participation rate', 'Australian Universities']",1204,210,9,1206,209,10,18,5,2
"an acl for a dynamic system of agents in this article we present the design of an acl for a dynamic system of agents. the acl includes a set of conversation performatives extended with operations to register, create, and terminate agents. the main design goal at the agent-level is to provide only knowledge-level primitives that are well integrated with the dynamic nature of the system. this goal has been achieved by defining an anonymous interaction protocol which enables agents to request and supply knowledge without considering symbol-level issues concerning management of agent names, routing, and agent reachability. this anonymous interaction protocol exploits a distributed facilitator schema which is hidden at the agent-level and provides mechanisms for registering capabilities of agents and delivering requests according to the competence of agents. we present a formal specification of the acl and of the underlying architecture, exploiting an algebra of actors, and illustrate it with the help of a graphical notation. this approach provides the basis for discussing dynamic primitives in acl and for studying properties of dynamic multi agent systems, for example concerning the behavior of agents and the correctness of their conversation policies ","An ACL for a dynamic system of agents In this article we present the design of an ACL for a dynamic system of agents. The ACL includes a set of conversation performatives extended with operations to register, create, and terminate agents. The main design goal at the agent-level is to provide only knowledge-level primitives, that are well integrated with the dynamic nature of the system. This goal has been achieved by defining an anonymous interaction protocol which enables agents to request and supply knowledge without considering symbol-level issues concerning management of agent names, routing, and agent reachability. This anonymous interaction protocol exploits a distributed facilitator schema which is hidden at the agent-level and provides mechanisms for registering capabilities of agents and delivering requests according to the competence of agents. We present a formal specification of the ACL and of the underlying architecture, exploiting an algebra of actors, and illustrate it with the help of a graphical notation. This approach provides the basis for discussing dynamic primitives in ACL and for studying properties of ‘dynamic multi agent systems, for example concerning the behavior of agents and the correctness of their conversation policies.","['ACL', 'dynamic system of agents', 'system of agents', 'agents', 'Agent Communication Languages', 'dynamic system', 'distributed facilitator', 'actors', 'anonymous interaction protocol', 'high level languages', 'multi-agent systems', 'software agents']","['agents', 'ACL', 'dynamic system', 'dynamic', 'dynamic primitives', 'agent reachability', 'main design goal', 'dynamic nature', 'agent names', 'systems']",1077,192,12,1080,191,10,7,3,3
fully automatic algorithm for region of interest location in camera calibration we present an automatic method for region of interest (roi) location in camera calibration used in computer vision inspection. an intelligent roi location algorithm based on the radon transform is developed to automate the calibration process. the algorithm remains robust even if the anchor target has a notable rotation angle in the target plane. this method functions well although the anchor target is not carefully positioned. several improvement methods are studied to avoid the algorithm's huge time/space consumption problem. the algorithm runs about 100 times faster if these improvement methods are applied. using this method fully automatic camera calibration is achieved without human interactive roi specification. experiments show that this algorithm can help to calibrate the intrinsic parameters of the zoom lens and the camera parameters quickly and automatically ,Fully automatic algorithm for region of interest location in camera calibration We present an automatic method for region of interest (ROI) location in camera calibration used in computer vision inspection. An intelligent RO location algorithm based on the Radon transform is developed to automate the calibration process. The algorithm remains robust even if the anchor target has a notable rotation angle in the target plane This method functions well although the anchor target is not carefully positioned. Several improvement methods are studied to avoid the algorithm's huge time/space consumption problem. The algorithm runs about 100 times faster if these improvement methods are applied. Using this method fully automatic camera calibration is achieved without human interactive ROI specification. Experiments show that this algorithm can help to calibrate the intrinsic parameters of the zoom lens and the camera parameters quickly and automatically,"['Fully automatic algorithm', 'interest location', 'camera calibration', 'region of interest location', 'computer vision inspection', 'ROI location algorithm', 'Radon transform', 'calibration process', 'rotation angle', 'time/space consumption problem', 'fully automatic camera calibration', 'human interactive specification', 'intrinsic parameters', 'zoom lens', 'camera parameters', 'automatic optical inspection', 'calibration', 'cameras', 'computer vision', 'image sensors', 'lenses', 'Radon transforms']","['improvement methods', 'automatic camera calibration', 'interest groin location', 'calibration process', 'automatic algorithm', 'interest location', 'camera parameters', 'automatic method', 'algorithm', 'camera calibration']",821,141,22,819,140,10,0,2,6
"integrating building management system and facilities management on the internet recently, it is of great interest to adopt the internet/intranet to develop building management systems (bms) and facilities management systems (fms). this paper addresses two technical issues: the web-based access (including database integration) and the integration of bms and fms. these should be addressed for accessing bms remotely via the internet, integrating control networks using the internet protocols and infrastructures, and using internet/intranet for building facilities management. an experimental internet-enabled system that integrates building and facilities management systems has been developed and tested. this system integrated open control networks with the internet and is developed utilizing the embedded web server, the pc web server and the distributed component object model (dcom) software development technology on the platform of an open control network. three strategies for interconnecting bms local networks via internet/intranet are presented and analyzed ","Integrating building management system and facilities management on the Internet Recently, it is of great interest to adopt the Internevintranet to develop building management systems (BMS) and facilities management systems (FMS). This paper addresses two technical issues: the Web-based access (including database integration) and the integration of BMS and FMS. These should be addressed for accessing BMS remotely via the Internet, integrating control networks using the Internet protocols and infrastructures, and using Internet/intranet for building facilities management. An experimental Internet-enabled system that integrates building and facilities management systems has been developed and tested. This system integrated open control networks with the Internet and is developed utilizing the embedded Web server, the PC Web server and the Distributed Component Object Model (DCOM) software development technology on the platform of an open control network. Three strategies for interconnecting BMS local networks via Internetiintranet are presented and analyzed","['intranet', 'building management systems', 'BMS', 'facilities management systems', 'FMS', 'Web-based access', 'database integration', 'Internet protocols', 'embedded Web server', 'PC Web server', 'Distributed Component Object Model', 'DCOM', 'software development technology', 'open control network', 'local network interconnection', 'building management systems', 'distributed object management', 'integrated software', 'Internet', 'intranets', 'LAN interconnection', 'open systems', 'protocols']","['facilities management systems', 'building management system', 'open control network', 'control networks', 'experimental Internet-enabled system', 'management', 'facilities', 'systems', 'building', 'facilities management']",928,146,23,927,145,10,10,2,13
"algebraic conditions for high-order convergent deferred correction schemes based on runge-kutta-nystrom methods for second order boundary value problems in [t. van hecke, m. van daele, j. comp. appl. math., vol. 132, p. 107-125, (2001)] the investigation of high-order convergence of deferred correction schemes for the numerical solution of second order nonlinear two-point boundary value problems not containing the first derivative, is made. the derivation of the algebraic conditions to raise the increase of order by the deferred correction scheme was based on taylor series expansions. in this paper we describe a more elegant way by means of p-series to obtain this necessary conditions and generalize this idea to equations of the form y"" = f (t, y, y') ","Algebraic conditions for high-order convergent deferred correction schemes based on Runge-Kutta-Nystrom methods for second order boundary value problems In [(T. Van Hecke, M. Van Daele, J. Comp. Appl. Math., vol. 132, p. 107-125, (2001)] the investigation of high-order convergence of deferred correction schemes for the numerical solution of second order nonlinear ‘two-point boundary value problems not containing the first derivative, is made. The derivation of the algebraic conditions to raise the increase of order by the deferred correction scheme was based on Taylor series expansions. In this paper we describe a more elegant way by means of P-series to obtain this necessary conditions and generalize this idea to equations of the form y"" =f (ty. y')","['high-order convergent deferred correction schemes', 'Runge-Kutta-Nystrom methods', 'second order boundary value problems', 'deferred correction schemes', 'second order nonlinear two-point boundary value problems', 'algebraic conditions', 'Taylor series expansions', 'boundary-value problems', 'convergence of numerical methods', 'Runge-Kutta methods', 'series (mathematics)']","['correction schemes', 'second order', 'higher-order convergence', 'necessary conditions', 'conditions', 'convergence', 'schemes', 'correction', 'algebraic conditions', 'Algebraic conditions']",644,119,11,645,116,10,15,5,2
"syndicators turn to the enterprise syndicators have started reshaping offerings, products, and services towards the marketplace that was looking for enterprise-wide content syndication technology and service. syndication companies are turning themselves into infrastructure companies. many syndication companies are now focusing their efforts on enterprise clients instead of the risky dot coms ","Syndicators turn to the enterprise Syndicators have started reshaping offerings, products, and services towards the marketplace that was looking for enterprise-wide content syndication technology and service. Syndication companies are turning themselves into infrastructure companies. Many syndication companies are now focusing their efforts on enterprise clients instead of the risky dot coms","['enterprise-wide content syndication technology', 'business model', 'enterprise clients', 'aggregator', 'business Web sites', 'customer base', 'infrastructure companies', 'electronic commerce', 'information resources']","['service Syndication companies', 'Many syndication companies', 'infrastructure companies', 'enterprise indicators', 'offerings products', 'enterprise clients', 'companies', 'services', 'enterprise', 'indicators']",344,52,9,344,51,10,0,0,0
"unlocking the potential of videoconferencing i propose in this paper to show, through a number of case studies, that videoconferencing is user-friendly, cost-effective, time-effective and life-enhancing for people of all ages and abilities and that it requires only a creative and imaginative approach to unlock its potential. i believe that these benefits need not, and should not, be restricted to the education sector. my examples will range from simple storytelling, through accessing international experts, professional development and distance learning in a variety of forms, to the use of videoconferencing for virtual meetings and planning sessions. in some cases, extracts from the reactions and responses of the participants will be included to illustrate the impact of the medium ","Unlocking the potential of videoconferencing | propose in this paper to show, through a number of case studies, that videoconferencing is user-friendly, cost-effective, time-effective and life-enhancing for people of all ages and abilities and that it requires only a creative and imaginative approach to unlock its potential. | believe that these benefits need not, and should not, be restricted to the education sector. My examples will range from simple storytelling, through accessing international experts, professional development and distance learning in a variety of forms, to the use of videoconferencing for virtual meetings and planning sessions. In some cases, extracts from the reactions and responses of the participants will be included to illustrate the impact of the medium","['videoconferencing', 'benefits', 'case studies', 'education', 'education', 'teleconferencing']","['videoconferencing |', 'potential', 'imaginative approach', 'virtual meetings', 'education sector', 'cases extracts', 'case studies', 'paper', 'cases', 'videoconferencing']",674,118,6,674,117,10,2,2,1
switching controller design via convex polyhedral lyapunov functions we propose a systematic switching control design method for a class of nonlinear discrete time hybrid systems. the novelty of the adopted approach is in the fact that unlike conventional control the control burden is shifted to a logical level thus creating the need for the development of new analysis/design methods ,Switching controller design via convex polyhedral Lyapunov functions We propose a systematic switching control design method for a class of nonlinear discrete time hybrid systems. The novelty of the adopted approach is in the fact that unlike conventional control the control burden is shifted to a logical level thus creating the need for the development of new analysis/design methods,"['switching controller design', 'convex polyhedral Lyapunov functions', 'nonlinear discrete time hybrid systems', 'systematic design method', 'asymptotic stability', 'control system analysis', 'control system synthesis', 'discrete time systems', 'Lyapunov methods', 'nonlinear control systems', 'optimal control']","['new analysis/design methods', 'convex polyhedrin Lyapunov', 'control', 'conventional control', 'controller design', 'control burden', 'logical level', 'design', 'Lyapunov', 'polyhedrin']",328,60,11,328,59,10,0,0,1
"are we there yet?: facing the never-ending speed and change of technology in midlife this essay is a personal reflection on entering librarianship in middle age at a time when the profession, like society in general, is experiencing rapidly accelerating change. much of this change is due to the increased use of computers and information technologies in the library setting. these aids in the production, collection, storage, retrieval, and dissemination of the collective information, knowledge, and sometimes wisdom of the past and the contemporary world can exhilarate or burden depending on one's worldview, the organization, and the flexibility of the workplace. this writer finds herself working in a library where everyone is expected continually to explore and use new ways of working and providing library service to a campus and a wider community. no time is spent in reflecting on what was, but all efforts are to anticipate and prepare for what will be ","Are we there yet?: facing the never-ending speed and change of technology in midlife This essay is a personal reflection on entering librarianship in middle age at a time when the profession, like society in general, is experiencing rapidly accelerating change. Much of this change is due to the increased use of computers and information technologies in the library setting. These aids in the production, collection, storage, retrieval, and dissemination of the collective information, knowledge, and sometimes wisdom of the past and the contemporary world can exhilarate ‘or burden depending on one's worldview, the organization, and the flexibility of the workplace. This writer finds herself working in a library where everyone is expected continually to explore and use new ways of working and providing library service to a campus and a wider ‘community. No time is spent in reflecting on what was, but all efforts are to anticipate and prepare for what will be","['librarianship', 'middle age', 'changing technology', 'computers', 'information technologies', 'dissemination', 'retrieval', 'storage', 'collection', 'library automation', 'technology transfer']","['change', 'collective information knowledge', 'information technologies', 'personal reflection', 'never-ending speed', 'library setting', 'middle age', 'speed', 'technologies', 'never-ending']",812,155,11,814,154,10,11,2,6
"design of high-performance wavelets for image coding using a perceptual time domain criterion this paper presents a new biorthogonal linear-phase wavelet design for image compression. instead of calculating the prototype filters as spectral factors of a half-band filter, the design is based on the direct optimization of the low pass analysis filter using an objective function directly related to a perceptual criterion for image compression. this function is defined as the product of the theoretical coding gain and an index called the peak-to-peak ratio, which was shown to have high correlation with perceptual quality. a distinctive feature of the proposed technique is a procedure by which, given a ""good"" starting filter, ""good"" filters of longer lengths are generated. the results are excellent, showing a clear improvement in perceptual image quality. also, we devised a criterion for constraining the coefficients of the filters in order to design wavelets with minimum ringing ","Design of high-performance wavelets for image coding using a perceptual time domain criterion This paper presents a new biorthogonal linear-phase wavelet design for image ‘compression. Instead of calculating the prototype filters as spectral factors of a half-band filter, the design is based on the direct ‘optimization of the low pass analysis filter using an objective function directly related to a perceptual criterion for image ‘compression. This function is defined as the product of the theoretical coding gain and an index called the peak-to-peak ratio, which was shown to have high correlation with perceptual quality. A distinctive feature of the proposed technique is a procedure by which, given a ""good"" starting filter, ""good"" fiters of longer lengths are generated. The results are excellent, showing a clear improvement in perceptual image quality. Also, we devised a criterion for constraining the coefficients of the filters in order to design wavelets with minimum ringing","['high-performance wavelets', 'image coding', 'perceptual time domain criterion', 'biorthogonal linear-phase wavelet design', 'image compression', 'prototype filters', 'half-band filter', 'low pass filter', 'analysis filter', 'objective function', 'coding gain', 'peak-to-peak ratio', 'perceptual image quality', 'filter banks', 'channel bank filters', 'data compression', 'image coding', 'low-pass filters', 'optimisation', 'time-domain analysis', 'transform coding', 'visual perception', 'wavelet transforms']","['image compression', 'high-performance wavelets', 'perceptual image quality', 'perceptual criterion', 'perceptual quality', 'prototype filters', 'half-back filter', 'good filters', 'filters', 'image']",840,151,23,842,150,10,38,4,4
"adaptive filtering for noise reduction in hue saturation intensity color space even though the hue saturation intensity (hsi) color model has been widely used in color image processing and analysis, the conversion formulas from the rgb color model to hsi are nonlinear and complicated in comparison with the conversion formulas of other color models. when an rgb image is degraded by random gaussian noise, this nonlinearity leads to a nonuniform noise distribution in hsi, making accurate image analysis more difficult. we have analyzed the noise characteristics of the hsi color model and developed an adaptive spatial filtering method to reduce the magnitude of noise and the nonuniformity of noise variance in the hsi color space. with this adaptive filtering method, the filter kernel for each pixel is dynamically adjusted, depending on the values of intensity and saturation. in our experiments we have filtered the saturation and hue components and generated edge maps from color gradients. we have found that by using the adaptive filtering method, the minimum error rate in edge detection improves by approximately 15% ","Adaptive filtering for noise reduction in hue saturation intensity color space Even though the hue saturation intensity (HS!) color model has been widely used in color image processing and analysis, the conversion formulas from the RGB color model to HSI are nonlinear and complicated in comparison with the conversion formulas of other color models. When an RGB image is degraded by random Gaussian noise, this nonlinearity leads to a nonuniform noise distribution in HSI, making accurate image analysis more difficult. We have analyzed the noise characteristics of the HSI color model and developed an adaptive spatial filtering method to reduce the magnitude of noise and the nonuniformity of noise variance in the HSI color space. With this adaptive filtering method, the filter kernel for each pixel is dynamically adjusted, depending on the values of intensity and saturation. In our experiments we have filtered the saturation and hue components and generated edge maps from color gradients. We have found that by using the adaptive filtering method, the minimum error rate in edge detection improves by approximately 15%","['adaptive filtering', 'noise reduction', 'hue saturation intensity color space', 'color image processing', 'color image analysis', 'RGB color model', 'random Gaussian noise', 'nonuniform noise distribution', 'accurate image analysis', 'adaptive spatial filtering method', 'nonuniformity', 'noise variance', 'HSI color space', 'filter kernel', 'pixel', 'saturation', 'intensity', 'generated edge maps', 'color gradients', 'edge detection', 'minimum error rate', 'adaptive optics', 'colorimetry', 'edge detection', 'image colour analysis', 'optical noise', 'optical saturation', 'spatial filters']","['hue saturation intensity', 'color space', 'non-uniform noise distribution', 'color image processing', 'random Gaussian noise', 'other color models', 'noise reduction', 'color gradients', 'RGB color model', 'color model']",953,177,28,953,176,10,1,1,9
"supporting unified interface to wrapper generator in integrated information retrieval given the ever-increasing scale and diversity of information and applications on the internet, improving the technology of information retrieval is an urgent research objective. retrieved information is either semi-structured or unstructured in format and its sources are extremely heterogeneous. in consequence, the task of efficiently gathering and extracting information from documents can be both difficult and tedious. given this variety of sources and formats, many choose to use mediator/wrapper architecture, but its use demands a fast means of generating efficient wrappers. in this paper, we present a design for an automatic extensible markup language (xml)-based framework with which to generate wrappers rapidly. wrappers created with this framework support a unified interface for a meta-search information retrieval system based on the internet search service using the common object request broker architecture (corba) standard. greatly advantaged by the compatibility of corba and xml, a user can quickly and easily develop information-gathering applications, such as a meta-search engine or any other information source retrieval method. the two main things our design provides are a method of wrapper generation that is fast, simple, and efficient, and a wrapper generator that is corba and xml-compliant and that supports a unified interface ","Supporting unified interface to wrapper generator in integrated information retrieval Given the ever-increasing scale and diversity of information and applications ‘on the Internet, improving the technology of information retrieval is. an urgent research objective. Retrieved information is either semi-structured or unstructured in format and its sources are extremely heterogeneous. In consequence, the task of efficiently gathering and extracting information from documents can be both difficult and tedious. Given this variety of sources and formats, many choose to use mediator/wrapper architecture, but its use demands a fast means of generating efficient wrappers. In this paper, we present a design for an automatic eXtensible Markup Language (XML)-based framework with which to generate wrappers rapidly. Wrappers created with this framework support a unified interface for a meta-search information retrieval system based on the Internet Search Service using the Common Object Request Broker Architecture (CORBA) standard. Greatly advantaged by the compatibility of CORBA and XML, a user can quickly and easily develop information-gathering applications, such as a meta-search engine or any other information source retrieval method. The two main things our design provides are a method of wrapper generation that is fast, simple, and efficient, and a wrapper generator that is CORBA and XML-compliant and that supports a unified interface","['unified interface', 'wrapper generator', 'integrated information retrieval', 'Internet', 'automatic eXtensible Markup Language', 'CORBA', 'meta-search engine', 'distributed object management', 'hypermedia markup languages', 'information retrieval', 'pattern matching']","['information retrieval', 'unified interface', 'wrapper generator', 'urgent research objective', 'efficient wrappers', 'wrappers', 'generator', 'retrieval', 'interface', 'information']",1242,207,11,1244,206,10,2,2,4
"analysis of the surface roughness and dimensional accuracy capability of fused deposition modelling processes building up materials in layers poses significant challenges from the viewpoint of material science, heat transfer and applied mechanics. however, numerous aspects of the use of these technologies have yet to be studied. one of these aspects is the characterization of the surface roughness and dimensional precision obtainable in layered manufacturing processes. in this paper, a study of roughness parameters obtained through the use of these manufacturing processes was made. prototype parts were manufactured using fdm techniques and an experimental analysis of the resulting roughness average (r/sub a/) and rms roughness (r/sub q/) obtained through the use of these manufacturing processes was carried out. dimensional parameters were also studied in order to determine the capability of the fused deposition modelling process for manufacturing parts ","Analysis of the surface roughness and dimensional accuracy capability of fused deposition modelling processes Building up materials in layers poses significant challenges from the viewpoint of material science, heat transfer and applied mechanics. However, numerous aspects of the use of these technologies have yet to be studied. One of these aspects is the characterization of the surface roughness and dimensional precision obtainable in layered manufacturing processes. In this paper, a study of roughness parameters obtained through the use of these manufacturing processes was made. Prototype parts were manufactured using FDM techniques and an experimental analysis of the resulting roughness average (R/Sub a/) and rms roughness (R/Sub q/) obtained through the use of these manufacturing processes was carried out. Dimensional parameters were also studied in order to determine the capability of the Fused Deposition Modelling process for manufacturing parts","['fused deposition modelling processes', 'surface roughness', 'dimensional accuracy capability', 'dimensional precision', 'layered manufacturing processes', 'prototype parts', 'roughness average', 'rms roughness', 'rapid prototyping', 'three-dimensional solid objects', 'CAD model', 'CNC-controlled robot', 'extrusion head', 'computerised numerical control', 'extrusion', 'industrial robots', 'rapid prototyping (industrial)', 'surface topography measurement']","['surface roughness', 'roughness', 'layered manufacturing processes', 'dimensional accuracy capability', 'deposition modelling processes', 'dimensional precision', 'roughness parameters', 'process', 'roughness average', 'rms roughness']",829,139,18,829,138,10,0,0,1
"a synergic analysis for web-based enterprise resources planning systems as the central nervous system for managing an organization's mission and critical business data, enterprise resource planning (erp) system has evolved to become the backbone of e-business implementation. since an erp system is multimodule application software that helps a company manage its important business functions, it should be versatile enough to automate every aspect of business processes, including e-business ","‘Assynergic analysis for Web-based enterprise resources planning systems As the central nervous system for managing an organization's mission and critical business data, Enterprise Resource Planning (ERP) system has evolved to become the backbone of e-business implementation. Since an ERP system is multimodule application software that helps a company manage its important business functions, it should be versatile enough to automate every aspect of business processes, including e-business","['Enterprise Resource Planning', 'e-business', 'ERP', 'customer relationship management', 'synergic analysis', 'Web-based enterprise resources planning', 'business data processing']","['business', 'pen-based enterprise resources', 'business implementation', 'central nervous system', 'Assynergic analysis', 'important business', 'business processes', 'ERP system', 'system', 'enterprise']",425,69,7,427,67,10,284,67,1
"spectral characteristics of the linear systems over a bounded time interval consideration was given to the spectral characteristics of the linear dynamic systems over a bounded time interval. singular characteristics of standard dynamic blocks, transcendental characteristic equations, and partial spectra of the singular functions were studied. relationship between the spectra under study and the classical frequency characteristic was demonstrated ","Spectral characteristics of the linear systems over a bounded time interval Consideration was given to the spectral characteristics of the linear dynamic systems over a bounded time interval. Singular characteristics of standard dynamic blocks, transcendental characteristic equations, and partial spectra of the singular functions were studied. Relationship between the spectra under study and the classical frequency characteristic was demonstrated","['spectral characteristics', 'bounded time interval', 'linear dynamic systems', 'singular characteristics', 'standard dynamic blocks', 'transcendental characteristic equations', 'partial spectra', 'singular functions', 'frequency characteristic', 'linear systems', 'spectral analysis']","['transcendental characteristic equations', 'classical frequency characteristic', 'Singular characteristics', 'linear dynamic systems', 'bounded time intervals', 'linear systems', 'characteristics', 'bounded time', 'spectral characteristics', 'Spectral characteristics']",392,60,11,392,59,10,0,0,1
"optimization of planning an advertising campaign of goods and services a generalization of the mathematical model and operations research problems formulated on its basis, which were presented by belenky (2001) in the framework of an approach to planning an advertising campaign of goods and services, is considered, and corresponding nonlinear programming problems with linear constraints are formulated ","Optimization of planning an advertising campaign of goods and services A generalization of the mathematical model and operations research problems formulated on its basis, which were presented by Belenky (2001) in the framework of an approach to planning an advertising campaign of goods and services, is considered, and corresponding nonlinear programming problems with linear constraints are formulated","['optimization', 'advertising campaign planning', 'operations research', 'OR', 'nonlinear programming', 'advertising', 'nonlinear programming']","['advertising campaign', 'services', 'operations research problems', 'goods', 'mathematical model', 'generalization', 'Optimization', 'problems', 'campaign', 'advertising']",348,58,7,348,57,10,0,0,3
"the contiguity in r/m an r.e. degree c is contiguous if deg/sub wtt/(a)=deg/sub wtt/(b) for any r.e. sets a,b in c. in this paper, we generalize the notation of contiguity to the structure r/m, the upper semilattice of the r.e. degree set r modulo the cappable r.e. degree set m. an element [c] in r/m is contiguous if [deg/sub wtt/(a)]=[deg/sub wtt/(b)] for any r.e. sets a, b such that deg/sub t/(a),deg/sub t/(b) in [c]. it is proved in this paper that every nonzero element in r/m is not contiguous, i.e., for every element [c] in r/m, if [c] not=[o] then there exist at least two r.e. sets a, b such that deg/sub t/(a), deg/sub t/(b) in [c] and [deg/sub wtt/(a)] not=[deg/sub wtt/(b)] ","The contiguity in R/M Ane. degree cis contiguous if deg/sub wtt/(A)=deg/sub wtt/(B) for any re sets A.B in c. In this paper, we generalize the notation of contiguity to the structure R/M, the upper semilattice of the re. degree set R modulo the cappable r.e. degree set M. An element [c] in R/M is contiguous if [deg/sub wtt/(A)]=[deg/sub wtt/(B)] for any re. sets A, B such that deg/sub T/(A),deg/sub T/(B) in [c]. It is proved in this paper that every nonzero element in R/M is not contiguous, ie., for every element [c] in R/M, if [c] not=[o] then there exist at least two re. sets A, B such that deg/sub T/(A), deg/sub T/(B) in [c] and [deg/sub wtt(A)] not=[deg/sub wtt/(B)]","['contiguity', 'Turing degree', 'recursively enumerable set', 'upper semilattice', 'nonzero element', 'recursion theory', 'recursive functions', 'Turing machines']","['deg/sub T', 'deg/sub', 'contiguity', 'red sets', 'degree set M', 'degree cis', 'B', 'wtt', 'deg/sub wtt', '= deg/sub wtt']",568,123,8,559,120,10,316,114,2
on the design of gain-scheduled trajectory tracking controllers [auv application] a new methodology is proposed for the design of trajectory tracking controllers for autonomous vehicles. the design technique builds on gain scheduling control theory. an application is made to the design of a trajectory tracking controller for a prototype autonomous underwater vehicle (auv). the effectiveness and advantages of the new control laws derived are illustrated in simulation using a full set of non-linear equations of motion of the vehicle ,On the design of gain-scheduled trajectory tracking controllers [AUV application] Anew methodology is proposed for the design of trajectory tracking controllers. for autonomous vehicles. The design technique builds on gain scheduling control theory. An application is made to the design of a trajectory tracking controller for a prototype autonomous underwater vehicle (AUV). The effectiveness and advantages of the new control laws derived are illustrated in simulation using a full set of non-linear equations of motion of the vehicle,"['gain-scheduled trajectory tracking controller design', 'autonomous vehicles', 'gain scheduling control theory', 'autonomous underwater vehicle', 'control laws', 'nonlinear equations of motion', 'control system synthesis', 'nonlinear control systems', 'position control', 'remotely operated vehicles', 'tracking', 'underwater vehicles']","['trajectory tracking controller', 'application', 'autonomous underwater vehicle', 'autonomous vehicles', 'new control laws', 'design technique', 'control', 'design', 'tracking', 'trajectory']",458,80,12,459,78,10,220,68,0
"wavelet collocation methods for a first kind boundary integral equation in acoustic scattering in this paper we consider a wavelet algorithm for the piecewise constant collocation method applied to the boundary element solution of a first kind integral equation arising in acoustic scattering. the conventional stiffness matrix is transformed into the corresponding matrix with respect to wavelet bases, and it is approximated by a compressed matrix. finally, the stiffness matrix is multiplied by diagonal preconditioners such that the resulting matrix of the system of linear equations is well conditioned and sparse. using this matrix, the boundary integral equation can be solved effectively ","Wavelet collocation methods for a first kind boundary integral equation in acoustic scattering In this paper we consider a wavelet algorithm for the piecewise constant collocation method applied to the boundary element solution of a first kind integral equation arising in acoustic scattering. The conventional stiffness matrix is transformed into the corresponding matrix with respect to wavelet bases, and it is approximated by a compressed matrix. Finally, the stiffness matrix is multiplied by diagonal preconditioners such that the resulting matrix of the system of linear equations is well conditioned and sparse. Using this matrix, the boundary integral equation can be solved effectively","['first kind integral operators', 'piecewise constant collocation', 'wavelet algorithm', 'boundary element solution', 'boundary integral equation', 'wavelet transform', 'computational complexity', 'acoustic scattering', 'stiffness matrix', 'linear equations', 'acoustics', 'boundary-value problems', 'communication complexity', 'integral equations', 'wavelet transforms']","['integral equation', 'conventional stiffness matrix', 'wavelets collocation methods', 'boundary element solution', 'corresponding matrix', 'first kind boundary', 'wavelets algorithm', 'wavelets bases', 'first kind', 'stiffness matrix']",594,103,15,594,102,10,0,0,2
"optimization of element-by-element fem in hpf 1.1 in this study, poisson's equation is numerically evaluated by the element-by-element (ebe) finite-element method in a parallel environment using hpf 1.1 (high-performance fortran). in order to achieve high parallel efficiency, the data structures have been altered to node-based data instead of mixtures of node- and element-based data, representing a node-based ebe finite-element scheme (nebe). the parallel machine used in this study was the nec sx-4, and experiments were performed on a single node having 32 processors sharing common memory. the hpf compiler used in the experiments is hpf/sx rev 2.0 released in 1997 (unofficial), which supports hpf 1.1. models containing approximately 200 000 and 1,500,000 degrees of freedom were analyzed in order to evaluate the method. the calculation time, parallel efficiency, and memory used were compared. the performance of hpf in the conjugate gradient solver for the large model, using the nec sx-4 compiler option-noshrunk, was about 85% that of the message passing interface ","Optimization of element-by-element FEM in HPF 1.1 In this study, Poisson's equation is numerically evaluated by the element-by-element (EBE) finite-element method in a parallel environment using HPF 1.1 (High-Performance Fortran). In order to achieve high parallel efficiency, the data structures have been altered to node-based data instead of mixtures of node- and element-based data, representing a node-based EBE finite-element scheme (nEBE). The parallel machine used in this study was the NEC SX-4, and experiments were performed on a single node having 32 processors sharing common memory. The HPF compiler used in the experiments is HPF/SX Rev 2.0 released in 1997 (unofficial), which supports HPF 1.1. Models containing approximately 200 000 and 1,500,000 degrees of freedom were analyzed in order to evaluate the method. The calculation time, parallel efficiency, and memory used were compared. The performance of HPF in the conjugate gradient solver for the large model, using the NEC SX-4 compiler option-noshrunk, was about 85% that of the message passing interface","['finite element method', 'parallel programs', 'Poisson equation', 'HPF compiler', 'conjugate gradient solver', 'message passing', 'element-by-element', 'HPF', 'finite element analysis', 'FORTRAN', 'parallel programming', 'program compilers']","['high parallel efficiency', 'study poisons equation', 'element-by-element FEM', 'parallel environment', 'parallel machine', 'home-based data', 'HPF compiler', 'HPF', 'element-by-element', 'parallel efficiency']",918,162,12,918,161,10,0,0,3
estimation of trifocal tensor using gmm a novel estimation of a trifocal tensor based on the gaussian mixture model (gmm) is presented. the mixture model is built assuming that the residuals of inliers and outliers belong to different gaussian distributions. the bayesian rule is then employed to detect the inliers for re-estimation. experiments show that the presented method is more precise and relatively unaffected by outliers ,Estimation of trifocal tensor using GMM A novel estimation of a trifocal tensor based on the Gaussian mixture model (GMM) is presented. The mixture model is built assuming that the residuals of inliers and outliers belong to different Gaussian distributions. The Bayesian rule is then employed to detect the inliers for re-estimation. Experiments show that the presented method is more precise and relatively unaffected by outliers,"['trifocal tensor estimation', 'GMM', 'Gaussian mixture model', 'Gaussian distributions', 'Bayesian rule', 'inliers', 'outliers', 'motion analysis', 'image data', 'image analysis', 'Bayes methods', 'covariance matrices', 'Gaussian distribution', 'image motion analysis', 'tensors']","['bifocal tensor', 'different Gaussian distributions', 'Gaussian mixture model', 'novel estimation', 'mm A', 'mm', 'Estimation', 'mixture model', 'tensor', 'bifocal']",366,67,15,366,66,10,0,0,4
"labscape: a smart environment for the cell biology laboratory labscape is a smart environment that we designed to improve the experience of people who work in a cell biology laboratory. our goal in creating it was to simplify, laboratory work by making information available where it is needed and by collecting and organizing data where and when it is created into a formal representation that others can understand and process. by helping biologists produce a more complete record of their work with less effort, labscape is designed to foster improved collaboration in conjunction with increased individual efficiency and satisfaction. a user-driven system, although technologically conservative, embraces a central goal of ubiquitous computing: to enhance the ability to perform domain tasks through fluid interaction with computational resources. smart environments could soon replace the pen and paper commonly used in the laboratory setting ","Labscape: a smart environment for the cell biology laboratory Labscape is a smart environment that we designed to improve the experience of people who work in a cell biology laboratory. Our goal in creating it was to simplify, laboratory work by making information available where it is needed and by collecting and organizing data where and when itis, created into a formal representation that others can understand and process. By helping biologists produce a more complete record of their work with less effort, Labscape is designed to foster improved collaboration in conjunction with increased individual efficiency and satisfaction. A user-driven system, although technologically conservative, embraces a central goal of ubiquitous computing: to enhance the ability to perform domain tasks through fluid interaction with computational resources. Smart environments could soon replace the pen and paper commonly used in the laboratory setting","['cell biology', 'Labscape', 'laboratory work', 'ubiquitous computing', 'smart environment', 'experimental technologies', 'biochemical procedure', 'biology computing', 'laboratory techniques']","['cell biology laboratory', 'laboratory', 'less effort landscape', 'laboratory setting', 'Smart environments', 'laboratory work', 'smart', 'environment', 'cell', 'smart environment']",807,142,9,808,140,10,313,83,2
"a review of methodologies used in research on cadastral development world-wide, much attention has been given to cadastral development. as a consequence of experiences made during recent decades, several authors have stated the need for research in the domain of cadastre and proposed methodologies to be used. the paper contributes to the acceptance of research methodologies needed for cadastral development, and thereby enhances theory in the cadastral domain. the paper reviews nine publications on cadastre and identifies the methodologies used. the review focuses on the institutional, social, political and economic aspects of cadastral development, rather than on the technical aspects. the main conclusion is that the methodologies used are largely those of the social sciences. that agrees with the notion that cadastre relates as much to people and institutions, as it relates to land, and that cadastral systems are shaped by social, political and economic conditions, as well as technology. since the geodetic survey profession has been the keeper of the cadastre, geodetic surveyors will have to deal ever more with social science matters, a fact that universities will have to consider ","A review of methodologies used in research on cadastral development World-wide, much attention has been given to cadastral development. As a ‘consequence of experiences made during recent decades, several authors have stated the need for research in the domain of cadastre and proposed methodologies to be used. The paper contributes to the acceptance of research methodologies needed for cadastral development, and thereby enhances theory in the cadastral domain. The paper reviews nine publications on cadastre and identifies the methodologies used. The review focuses on the institutional, social, political and economic aspects of cadastral development, rather than on the technical aspects. The main conclusion is that the methodologies used are largely those of the social sciences. That agrees with the notion that cadastre relates as much to people and institutions, as it relates to land, and that cadastral systems are shaped by social, political and economic conditions, as well as technology. Since the geodetic survey profession has been the keeper of the cadastre, geodetic surveyors will have to deal ever more with social science matters, a fact that universities will have to consider","['cadastral development methodologies', 'cadastre', 'research methodologies', 'political aspects', 'economic aspects', 'social sciences', 'economic conditions', 'geodetic survey profession', 'geodetic surveyors', 'land registration', 'case study', 'cartography', 'geodesy', 'government data processing', 'politics', 'socio-economic effects', 'town and country planning']","['cadastral development', 'cadastre', 'review', 'social science', 'research methodologies', 'cadastral systems', 'cadastral domain', 'research', 'methodologies', 'development']",1019,183,17,1020,182,10,11,1,4
an inverse problem for a model of a hierarchical structure we consider the inverse problem for the identification of the coefficient in a parabolic equation. the model is applied to describe the functioning of a hierarchical structure; it is also relevant for heat-conduction theory. unique solvability of the inverse problem is proved ,An inverse problem for a model of a hierarchical structure We consider the inverse problem for the identification of the coefficient in a parabolic equation. The model is applied to describe the functioning of a hierarchical structure; itis also relevant for heat-conduction theory. Unique solvability of the inverse problem is proved,"['inverse problem', 'hierarchical structure', 'parabolic equation', 'heat-conduction theory', 'unique solvability', 'heat conduction', 'inverse problems', 'partial differential equations']","['inverse problem', 'hierarchical structure', 'model', 'heat-conduction theory', 'parabolic equation', 'Unique sociability', 'structure', 'hierarchical', 'problem', 'inverse']",284,53,8,284,51,10,49,14,0
"fast accurate meg source localization using a multilayer perceptron trained with real brain noise iterative gradient methods such as levenberg-marquardt (lm) are in widespread use for source localization from electroencephalographic (eeg) and magnetoencephalographic (meg) signals. unfortunately, lm depends sensitively on the initial guess, necessitating repeated runs. this, combined with lm's high per-step cost, makes its computational burden quite high. to reduce this burden, we trained a multilayer perceptron (mlp) as a realtime localizer. we used an analytical model of quasistatic electromagnetic propagation through a spherical head to map randomly chosen dipoles to sensor activities according to the sensor geometry of a 4d neuroimaging neuromag-122 meg system, and trained a mlp to invert this mapping in the absence of noise or in the presence of various sorts of noise such as white gaussian noise, correlated noise, or real brain noise. a mlp structure was chosen to trade off computation and accuracy. this mlp was trained four times, with each type of noise. we measured the effects of initial guesses on lm performance, which motivated a hybrid mlp-start-lm method, in which the trained mlp initializes lm. we also compared the localization performance of lm, mlps, and hybrid mlp-start-lms for realistic brain signals. trained mlps are much faster than other methods, while the hybrid mlp-start-lms are faster and more accurate than fixed-4-start-lm. in particular, the hybrid mlp-start-lm initialized by a mlp trained with the real brain noise dataset is 60 times faster and is comparable in accuracy to random-20-start-lm, and this hybrid system (localization error: 0.28 cm, computation time: 36 ms) shows almost as good performance as optimal-1-start-lm (localization error: 0.23 cm, computation time: 22 ms), which initializes lm with the correct dipole location. mlps trained with noise perform better than the mlp trained without noise, and the mlp trained with real brain noise is almost as good an initial guesser for lm as the correct dipole location ","Fast accurate MEG source localization using a multilayer perceptron trained with real brain noise Iterative gradient methods such as Levenberg-Marquardt (LM) are in widespread use for source localization from electroencephalographic (EEG) and magnetoencephalographic (MEG) signals. Unfortunately, LM depends sensitively on the initial guess, necessitating repeated runs. This, ‘combined with LM's high per-step cost, makes its computational burden quite high. To reduce this burden, we trained a multilayer perceptron (MLP) as a realtime localizer. We used an analytical model of quasistatic electromagnetic propagation through a spherical head to map randomly chosen dipoles to sensor activities according to the sensor geometry of a 4D Neuroimaging Neuromag-122 MEG system, and trained a MLP to invert this mapping in the absence of noise or in the presence of various sorts of noise such as white Gaussian noise, correlated noise, or real brain noise. A MLP structure was chosen to trade off ‘computation and accuracy. This MLP was trained four times, with each type of noise. We measured the effects of initial guesses on LM performance, which motivated a hybrid MLP-start-LM method, in which the trained MLP initializes LM. We also compared the localization performance of LM, MLPs, and hybrid MLP-start-LMs for realistic brain signals. Trained MLPs are much faster than other methods, while the hybrid MLP-start-LMs are faster and more accurate than fixed-4-start-LM. In particular, the hybrid MLP-start-LM initialized by a MLP trained with the real brain noise dataset is 60 times faster and is comparable in accuracy to random-20-start-LM, and this hybrid system (localization error: 0.28 cm, computation time: 36 ms) shows almost as good performance as optimal-1-start-LM (localization error: 0.23 cm, computation time: 22 ms), which initializes LM with the correct dipole location. MLPs trained with noise perform better than the MLP trained without noise, and the MLP trained with real brain noise is almost as good an initial guesser for LM as the correct dipole location","['MEG source localization', 'fast accurate localization', 'multilayer perceptron', 'real brain noise', 'real-time localizer', 'analytical model', 'quasistatic electromagnetic propagation', 'spherical head', 'white Gaussian noise', 'correlated noise', 'computation accuracy', 'Levenberg-Marquardt method', 'iterative gradient methods', 'forward model', 'Gaussian noise', 'gradient methods', 'inverse problems', 'learning (artificial intelligence)', 'magnetoencephalography', 'medical signal processing', 'multilayer perceptrons', 'white noise']","['real brain noise', 'hybrid MLP-start-LMs', 'source localization', 'localization', 'noise', 'optimal-1-start-LM localization error', 'hybrid MLP-start-LM method', 'localization performance', 'Neuromag-122 MEG system', 'white Gaussian noise']",1767,316,22,1769,315,10,19,2,6
"the chemical brotherhood it has always been more difficult for chemistry to keep up in the internet age but a new language could herald a new era for the discipline. the paper discusses cml, or chemical mark-up language. the extensible mark-up language provides a universal format for structured documents and data on the web and so offers a way for scientists and others to carry a wide range of information types across the net in a transparent way. all that is needed is an xml browser ","The chemical brotherhood Ithas always been more difficult for chemistry to keep up in the Internet age but a new language could herald a new era for the discipline. The paper discusses CML, or chemical mark-up language. The eXtensible Mark-up Language provides a universal format for structured documents and data ‘on the Web and so offers a way for scientists and others to carry a wide range of information types across the net in a transparent way. All that is needed is an XML browser","['chemistry', 'Internet', 'CML', 'chemical mark-up language', 'eXtensible Mark-up Language', 'structured document format', 'World Wide Web', 'XML browser', 'chemistry computing', 'hypermedia markup languages', 'information resources', 'Internet']","['eXtensible Mark-up Language', 'chemical mark-up language', 'chemical brotherhood', 'new language', 'Internet age', 'chemistry', 'language', 'new era', 'new', 'chemical']",403,87,12,404,85,10,237,82,2
"on the relationship between parametric variation and state feedback in chaos control in this letter, we study the popular parametric variation chaos control and state-feedback methodologies in chaos control, and point out for the first time that they are actually equivalent in the sense that there exist diffeomorphisms that can convert one to the other for most smooth chaotic systems. detailed conversions are worked out for typical discrete chaotic maps (logistic, henon) and continuous flows (rossler, lorenz) for illustration. this unifies the two seemingly different approaches from the physics and the engineering communities on chaos control. this new perspective reveals some new potential applications such as chaos synchronization and normal form analysis from a unified mathematical point of view ","On the relationship between parametric variation and state feedback in chaos control In this Letter, we study the popular parametric variation chaos control and state-feedback methodologies in chaos control, and point out for the first time that they are actually equivalent in the sense that there exist diffeomorphisms that can convert one to the other for most smooth chaotic systems. Detailed conversions are worked out for typical discrete chaotic maps (logistic, Henon) and continuous flows (Rossier, Lorenz) for illustration. This unifies the two seemingly different approaches from the physics and the engineering communities on chaos control. This new perspective reveals some new potential applications such as chaos synchronization and normal form analysis from a unified mathematical point of view","['parametric variation', 'chaos control', 'state-feedback', 'logistic', 'Henon map', 'continuous flows', 'Rossler system', 'Lorenz system', 'diffeomorphisms', 'chaos', 'Henon mapping', 'nonlinear control systems', 'state feedback']","['chaos control', 'parametric variation', 'new potential applications', 'chaos synchronization', 'state feedback', 'relationship', 'chaos', 'variation', 'parametric', 'control']",691,120,13,691,119,10,1,1,4
"new kit on the block [it upgrades] as time passes, new hardware and software replace the old. the hows are straightforward: it resellers and consultants can help with upgrade practicalities. will dalrymple examines the business issues and costs involved in it upgrades ","New kit on the block [IT upgrades] As time passes, new hardware and software replace the old. The hows are straightforward: IT resellers and consultants can help with upgrade practicalities. Will Dalrymple examines the business issues and costs involved in IT upgrades","['IT upgrades', 'business issues', 'costs', 'IT resellers', 'consultants', 'Microsoft', 'contracts', 'DP management']","['upgrades', 'upgrade practicalities', 'business issues', 'new hardware', 'New kit', 'block', 'time', 'new', 'New', 'kit']",227,43,8,227,42,10,0,0,3
"flexibility analysis of complex technical systems under uncertainty an important problem in designing technical systems under partial uncertainty of the initial physical, chemical, and technological data is the determination of a design in which the technical system is flexible, i.e., its control system is capable of guaranteeing that the constraints hold even under changes in external and internal factors and application of fuzzy mathematical models in its design. three flexibility problems, viz., the flexibility of a technical system of given structure, structural flexibility of a technical system, and the optimal design guaranteeing the flexibility of a technical system, are studied. two approaches to these problems are elaborated. results of a computation experiment are given ","Flexibility analysis of complex technical systems under uncertainty An important problem in designing technical systems under partial uncertainty of the initial physical, chemical, and technological data is the determination of a design in which the technical system is flexible, e., its control system is capable of guaranteeing that the constraints hold even under changes in external and internal factors and application of fuzzy mathematical models in its design. Three flexibility problems, viz., the flexibility of a technical system of given structure, structural flexibility of a technical system, and the optimal design guaranteeing the flexibility of a technical system, are studied. Two approaches to these problems are elaborated. Results of a computation experiment are given","['flexibility analysis', 'complex technical systems', 'partial uncertainty', 'control system', 'fuzzy mathematical models', 'structural flexibility', 'optimal design', 'chemical technology', 'fuzzy set theory', 'large-scale systems', 'mathematical programming', 'uncertain systems']","['system', 'complex technical systems', 'structural flexibility', 'flexibility problems', 'Flexibility analysis', 'control system', 'flexible e.', 'technical', 'flexible', 'technical system']",677,115,12,675,114,10,2,1,3
"the best circulant preconditioners for hermitian toeplitz systems.ii. the multiple-zero case for pt.i. see siam j. numer. anal., vol. 38, p. 876-896. circulant-type preconditioners have been proposed previously for ill-conditioned hermitian toeplitz systems that are generated by nonnegative continuous functions with a zero of even order. the proposed circulant preconditioners can be constructed without requiring explicit knowledge of the generating functions. it was shown that the spectra of the preconditioned matrices are uniformly bounded except for a fixed number of outliers and that all eigenvalues are uniformly bounded away from zero. therefore the conjugate gradient method converges linearly when applied to solving the circulant preconditioned systems. previously it was claimed that this result can be extended to the case where the generating functions have multiple zeros. the main aim of this paper is to give a complete convergence proof of the method for this class of generating functions ","The best circulant preconditioners for Hermitian Toeplitz systems.ll. The multiple-zero case For pt.l. see SIAM J. Numer. Anal., vol. 38, p. 876-896. Circulant-type preconditioners have been proposed previously for il-conditioned Hermitian Toeplitz systems that are generated by nonnegative continuous functions with a zero of even order. The proposed circulant preconditioners can be constructed without requiring explicit knowledge of the generating functions. It was shown that the spectra of the preconditioned matrices are uniformly bounded except for a fixed number of outliers and that all eigenvalues are uniformly bounded away from zero. Therefore the conjugate gradient method converges linearly when applied to solving the circulant preconditioned systems. Previously it was claimed that this result can be extended to the case where the generating functions have multiple zeros. The main aim of this paper is to give a complete convergence proof of the method for this class of generating functions","['circulant preconditioners', 'Hermitian Toeplitz systems', 'multiple-zero case', 'nonnegative continuous functions', 'generating functions', 'preconditioned matrices', 'eigenvalues', 'conjugate gradient method', 'computational complexity', 'conjugate gradient methods', 'eigenvalues and eigenfunctions', 'Hermitian matrices', 'Toeplitz matrices']","['non-negative continuous functions', 'circular reconditioned systems', 'Hermitian Toeplitz systems.ll', 'Circulant-type preconditions', 'best circular preconditions', 'multiple-zero case', 'SIAM J. number', 'preconditions', 'circular', 'circular preconditions']",864,149,13,863,148,10,15,3,4
"on the relationship between omega -automata and temporal logic normal forms we consider the relationship between omega -automata and a specific logical formulation based on a normal form for temporal logic formulae. while this normal form was developed for use with execution and clausal resolution in temporal logics, we show how it can represent, syntactically, omega -automata in a high-level way. technical proofs of the correctness of this representation are given ","On the relationship between omega -automata and temporal logic normal forms We consider the relationship between omega -automata and a specific logical formulation based on a normal form for temporal logic formulae. While this normal form was developed for use with execution and clausal resolution in temporal logics, we show how it can represent, syntactically, omega -automata in a high-level way. Technical proofs of the correctness of this representation are given","['omega -automata', 'temporal logic normal forms', 'logical formulation', 'clausal resolution', 'program correctness', 'automata theory', 'temporal logic', 'theorem proving']","['omega automata', 'specific logical formulation', 'temporal logic formulae', 'temporal logics', 'logic', 'normal forms', 'temporal', 'normal', 'omega', 'automata']",399,72,8,399,71,10,0,0,1
"closed-loop model set validation under a stochastic framework deals with probabilistic model set validation. it is assumed that the dynamics of a multi-input multi-output (mimo) plant is described by a model set with unstructured uncertainties, and identification experiments are performed in closed loop. a necessary and sufficient condition has been derived for the consistency of the model set with both the stabilizing controller and closed-loop frequency domain experimental data (fded). in this condition, only the euclidean norm of a complex vector is involved, and this complex vector depends linearly on both the disturbances and the measurement errors. based on this condition, an analytic formula has been derived for the sample unfalsified probability (sup) of the model set. some of the asymptotic statistical properties of the sup have also been briefly discussed. a numerical example is included to illustrate the efficiency of the suggested method in model set quality evaluation ","Closed-loop model set validation under a stochastic framework Deals with probabilistic model set validation. It is assumed that the dynamics of a multi-input multi-output (MIMO) plant is described by a model set with unstructured uncertainties, and identification experiments are performed in closed loop. A necessary and sufficient condition has been derived for the consistency of the model set with both the stabilizing controller and closed-loop frequency domain experimental data (FDED) In this condition, only the Euclidean norm of a complex vector is involved, and this complex vector depends linearly on both the disturbances and the measurement errors. Based on this condition, an analytic formula has been derived for the sample unfalsified probability (SUP) of the model set. Some of the asymptotic statistical properties of the SUP have also been briefly discussed. A numerical ‘example is included to illustrate the efficiency of the suggested method in model set quality evaluation","['closed-loop model set validation', 'stochastic framework', 'probabilistic model set validation', 'multi-input multi-output plant', 'MIMO plant', 'unstructured uncertainties', 'necessary and sufficient condition', 'stabilizing controller', 'closed-loop frequency domain experimental data', 'Euclidean norm', 'complex vector', 'asymptotic statistical properties', 'robust control', 'unstructured uncertainty', 'closed loop systems', 'identification', 'MIMO systems', 'probability', 'robust control', 'stochastic processes', 'uncertain systems', 'vectors']","['validation', 'asymptotic statistical properties', 'closed-loop frequency domain', 'stochastic framework Deals', 'model', 'sufficient condition', 'probabilistic model', 'Closed-loop model', 'Closed-loop', 'stochastic']",847,150,22,847,149,10,7,2,6
"copyright management in the digital age listening to and buying music online is becoming increasingly popular with consumers. so much so that merrill lynch forecasts the value of the online music market will explode from $8 million in 2001 to $1,409 million in 2005. but online delivery is not without problems; the issue of copyright management in particular has become a serious thorn in the side for digital content creators. martin brass, ex- music producer and senior industry consultant at syntegra, explains ","Copyright management in the digital age Listening to and buying music online is becoming increasingly popular with ‘consumers. So much so that Merrill Lynch forecasts the value of the online music market will explode from $8 million in 2001 to $1,409 million in 2005. But online delivery is not without problems; the issue of copyright management in particular has become a serious thorn in the side for digital content creators. Martin Brass, ex- music producer and senior industry consultant at Syntegra, explains","['digital age', 'online music delivery', 'music industry', 'Internet', 'Napster', 'digital content creators', 'copyright', 'Internet', 'music']","['online music market', 'online delivery', 'music producer', 'music online', 'digital age', 'online', 'music', 'management', 'copyright management', 'Copyright management']",433,83,9,434,82,10,10,1,0
"the culture of usability now that most of us agree that usability testing is an integral investment in site development, it's time to recognize that the standard approach falls short. it is possible to do less work and get better results while spending less money. by bringing usability testing in-house and breaking tests into more manageable sessions, you can vastly improve your online offering without affecting your profit margin ","The culture of usability Now that most of us agree that usability testing is an integral investment in site development, it's time to recognize that the standard approach falls short. It is possible to do less work and get better results while spending less money. By bringing usability testing in-house and breaking tests into more manageable sessions, you can vastly improve your online offering without affecting your profit margin","['usability testing program', 'Web site', 'Internet', 'program testing', 'user interfaces']","['usability testing in-house', 'more manageable sessions', 'integral investment', 'site development', 'less money', 'less work', 'culture', 'tests', 'usability', 'usability testing']",366,70,5,366,69,10,0,0,0
"direct aperture optimization: a turnkey solution for step-and-shoot imrt imrt treatment plans for step-and-shoot delivery have traditionally been produced through the optimization of intensity distributions (or maps) for each beam angle. the optimization step is followed by the application of a leaf-sequencing algorithm that translates each intensity map into a set of deliverable aperture shapes. in this article, we introduce an automated planning system in which we bypass the traditional intensity optimization, and instead directly optimize the shapes and the weights of the apertures. we call this approach ""direct aperture optimization."" this technique allows the user to specify the maximum number of apertures per beam direction, and hence provides significant control over the complexity of the treatment delivery. this is possible because the machine dependent delivery constraints imposed by the mlc are enforced within the aperture optimization algorithm rather than in a separate leaf-sequencing step. the leaf settings and the aperture intensities are optimized simultaneously using a simulated annealing algorithm. we have tested direct aperture optimization on a variety of patient cases using the egs4/beam monte carlo package for our dose calculation engine. the results demonstrate that direct aperture optimization can produce highly conformal step-and-shoot treatment plans using only three to five apertures per beam direction. as compared with traditional optimization strategies, our studies demonstrate that direct aperture optimization can result in a significant reduction in both the number of beam segments and the number of monitor units. direct aperture optimization therefore produces highly efficient treatment deliveries that maintain the full dosimetric benefits of imrt ","Direct aperture optimization: A turnkey solution for step-and-shoot IMRT IMRT treatment plans for step-and-shoot delivery have traditionally been produced through the optimization of intensity distributions (or maps) for each beam angle. The optimization step is followed by the application of a leaf-sequencing algorithm that translates each intensity map into a set of deliverable aperture shapes. In this, article, we introduce an automated planning system in which we bypass the traditional intensity optimization, and instead directly optimize the shapes and the weights of the apertures. We call this approach “direct aperture optimization."" This technique allows the user to specify the maximum number of apertures per beam direction, and hence provides significant control over the complexity of the treatment delivery. This is possible because the machine dependent delivery constraints imposed by the MLC are enforced within the aperture optimization algorithm rather than in a separate leaf-sequencing step. The leaf settings and the aperture intensities are optimized simultaneously using a simulated annealing algorithm. We have tested direct aperture optimization on a variety of patient cases using the EGS4/BEAM Monte Carlo package for our dose calculation engine. The results demonstrate that direct aperture optimization can produce highly conformal step-and-shoot treatment plans using only three to five apertures per beam direction. As compared with traditional optimization strategies, our studies demonstrate that direct aperture ‘optimization can result in a significant reduction in both the number of beam segments and the number of monitor units. Direct aperture optimization therefore produces highly efficient treatment deliveries that maintain the full dosimetric benefits of IMRT","['direct aperture optimization', 'turnkey solution', 'step-and-shoot IMRT', 'IMRT treatment plans', 'intensity distributions', 'maps', 'beam angle', 'optimization step', 'leaf-sequencing algorithm', 'intensity map', 'deliverable aperture shapes', 'automated planning system', 'aperture weights', 'aperture shapes', 'treatment delivery complexity', 'machine dependent delivery constraints', 'MLC', 'aperture optimization algorithm', 'leaf settings', 'aperture intensities', 'simulated annealing algorithm', 'patient cases', 'EGS4/BEAM Monte Carlo package', 'dose calculation engine', 'highly conformal step-and-shoot treatment plans', 'beam segments', 'monitor units', 'highly efficient treatment deliveries', 'full dosimetric benefits', 'dosimetry', 'intensity modulation', 'inverse problems', 'medical computing', 'Monte Carlo methods', 'optimisation', 'planning', 'radiation therapy', 'simulated annealing']","['optimization', 'apertures', 'traditional optimization strategies', 'traditional intensity optimization', 'aperture optimization algorithm', 'deliverable aperture shapes', 'aperture intensities', 'optimization step', 'direct aperture optimization', 'Direct aperture optimization']",1554,256,38,1556,255,10,13,3,19
"narx-based technique for the modelling of magneto-rheological damping devices this paper presents a methodology for identifying variable-structure nonlinear models of magneto-rheological dampers (mrd) and similar devices. its peculiarity with respect to the mainstream literature is to be especially conceived for obtaining models that are structurally simple, easy to estimate and well suited for model-based control. this goal is pursued by adopting linear-in-the-parameters narx models, for which an identification method is developed based on the minimization of the simulation error. this method is capable of selecting the model structure together with the parameters, thus it does not require a priori structural information. a set of validation tests is reported, with the aim of demonstrating the technique's efficiency by comparing it to a widely accepted mrd modelling approach ","NARX-based technique for the modelling of magneto-rheological damping devices This paper presents a methodology for identifying variable-structure nonlinear models of magneto-rheological dampers (MRD) and similar devices. Its peculiarity with respect to the mainstream literature is to be especially conceived for obtaining models that are structurally simple, easy to estimate and well suited for model-based control. This goal is, pursued by adopting linear-in-the-parameters NARX models, for which an identification method is developed based on the minimization of the simulation error. This method is capable of selecting the model structure together with the parameters, thus it does not require a priori structural information. A set of validation tests is reported, with the aim of demonstrating the technique's efficiency by comparing it to a widely accepted MRD modelling approach","['modelling', 'magnetorheological damping', 'model-based control', 'NARX models', 'identification', 'minimization', 'simulation error', 'validation', 'MRD modelling', 'control system analysis', 'damping', 'identification', 'intelligent actuators', 'magnetorheology', 'minimisation', 'nonlinear control systems']","['linear-in-the-parameters marx models', 'magneto-rheological dampers mrds', 'MRD modelling approach', 'techniques efficiency', 'car-based technique', 'nonlinear models', 'similar devices', 'model structure', 'modelling', 'magneto-rheological']",763,127,16,764,126,10,0,1,7
"a server-side program for delivering experiments with animations a server-side program for animation experiments is presented. the program is capable of delivering an experiment composed of discrete animation sequences in various file formats, collecting a discrete or continuous response from the observer, evaluating the appropriateness of the response, and ensuring that the user is not proceeding at an unreasonable rate. most parameters of the program are controllable by experimenter-edited text files or simple switches in the program code, thereby minimizing the need for programming to create new experiments. a simple demonstration experiment is discussed and is freely available ","Aserver-side program for delivering experiments with animations A server-side program for animation experiments is presented. The program is capable of delivering an experiment composed of discrete animation sequences in various file formats, collecting a discrete or continuous response from the observer, evaluating the appropriateness of the response, and ensuring that the user is not proceeding at an unreasonable rate. Most parameters of the program are controllable by experimenter-edited text files or simple switches in the program code, thereby minimizing the need for programming to create new experiments. Asimple demonstration experiment is discussed and is freely available","['server-side program', 'animation experiment delivery', 'discrete animation sequences', 'file formats', 'Web based psychological experiments', 'Internet', 'experimenter-edited text files', 'computer animation', 'information resources', 'Internet', 'psychology']","['program', 'simple demonstration experiment', 'discrete animation sequences', 'animation experiments', 'Aserver-side program', 'severnside program', 'new experiments', 'program code', 'animations', 'experiments']",592,99,11,592,96,10,310,96,2
"optical two-step modified signed-digit addition based on binary logic gates a new modified signed-digit (msd) addition algorithm based on binary logic gates is proposed for parallel computing. it is shown that by encoding each of the input msd digits and flag digits into a pair of binary bits, the number of addition steps can be reduced to two. the flag digit is introduced to characterize the next low order pair (nlop) of the input digits in order to suppress carry propagation. the rules for two-step addition of binary coded msd (bcmsd) numbers are formulated that can be implemented using optical shadow-casting logic system ","Optical two-step modified signed-digit addition based on binary logic gates Anew modified signed-digit (MSD) addition algorithm based on binary logic gates is proposed for parallel computing. It is shown that by encoding each of the input MSD digits and flag digits into a pair of binary bits, the number of addition steps can be reduced to two. The flag digit is introduced to characterize the next low order pair (NLOP) of the input digits in order to suppress carry propagation. The rules for two-step addition of binary coded MSD (BCMSD) numbers are formulated that can be implemented using optical shadow-casting logic system","['optical two-step modified signed-digit addition', 'binary logic gates', 'modified signed-digit addition algorithm', 'parallel computing', 'input MSD digits', 'flag digits', 'binary bits', 'addition steps', 'low order pair', 'carry propagation suppression', 'two-step addition', 'binary coded MSD', 'optical shadow-casting logic system', 'adders', 'digital arithmetic', 'encoding', 'optical logic', 'parallel architectures']","['binary logic gates', 'flag digit', 'addition', 'signed-digit addition', 'two-step addition', 'input MSD digits', 'addition steps', 'input digits', 'binary bits', 'binary']",529,104,18,529,102,10,285,92,3
"information access for all: meeting the needs of deaf and hard of hearing people discusses the nature of deafness and hearing impairments, with particular reference to the impact which the onset of hearing loss presents at various ages. the author goes on to present practical tips for interacting with deaf and hard of hearing clients in various communication contexts, including sightreading, tty communications, and asl interpreters. an annotated list of suggested readings is appended ","Information access for all: meeting the needs of deaf and hard of hearing people Discusses the nature of deafness and hearing impairments, with particular reference to the impact which the onset of hearing loss presents at various ages. The author goes on to present practical tips for interacting with deaf and hard of hearing clients in various ‘communication contexts, including sightreading, TTY communications, and ASL interpreters. An annotated list of suggested readings is appended","['information access', 'deaf clients', 'hard of hearing clients', 'deafness', 'hearing impairments', 'communication contexts', 'sightreading', 'TTY communications', 'ASL interpreters', 'handicapped aids', 'information retrieval']","['various communication contexts', 'present practical tips', 'hearing impairments', 'Information access', 'hard', 'various ages', 'hearing loss', 'deafness', 'needs', 'Information']",415,75,11,416,74,10,12,1,3
"numerical validation of solutions of complementarity problems: the nonlinear case this paper proposes a validation method for solutions of nonlinear complementarity problems. the validation procedure performs a computational test. if the result of the test is positive, then it is guaranteed that a given multi-dimensional interval either includes a solution or excludes all solutions of the nonlinear complementarity problem ","Numerical validation of solutions of complementarity problems: the nonlinear case This paper proposes a validation method for solutions of nonlinear complementarity problems. The validation procedure performs a computational test. If the result of the test is positive, then itis guaranteed that a given multi-dimensional interval either includes a solution or excludes all solutions of the nonlinear complementarity problem","['numerical validation', 'computational test', 'nonlinear complementarity problem', 'optimization', 'matrix algebra', 'optimisation']","['nonlinear complementarity problem', 'solutions', 'complementarity problems', 'validation procedure', 'Numerical validation', 'validation method', 'nonlinear case', 'validation', 'nonlinear', 'complementarity']",367,60,6,367,58,10,61,20,1
"mathematical models of functioning of an insurance company with allowance for the rate of return models of the functioning of insurance companies are suggested, when the free capital increases from interest at a certain rate. the basic characteristics of the capital of a company are studied in the stationary regime ","Mathematical models of functioning of an insurance company with allowance for the rate of return Models of the functioning of insurance companies are suggested, when the free capital increases from interest at a certain rate. The basic characteristics of the capital of a company are studied in the stationary regime","['mathematical models', 'insurance company functioning', 'return rate allowance', 'free capital increase', 'interest', 'stationary regime', 'insurance']","['free capital increases', 'basic characteristics', 'insurance companies', 'Mathematical models', 'return Models', 'certain rate', 'rate', 'capital', 'companies', 'insurance']",267,51,7,267,50,10,0,0,2
"enterprise content integration iii: agari mediaware's media star since we introduced the term enterprise content integration (eci) in january, the concept has gained momentum in the market. in addition to context media's interchange platform and savantech's photon commerce, agari mediaware's media star is in the fray. it is a middleware platform that allows large media companies to integrate their digital systems with great flexibility ","Enterprise content integration III: Agari Mediaware's Media Star Since we introduced the term Enterprise Content Integration (ECl) in January, the concept has gained momentum in the market. In addition to Context Media's Interchange Platform and Savantech's Photon Commerce, Agari Mediaware's Media Star is in the fray. It is a middleware platform that allows large media companies to integrate their digital systems with great flexibility","['Agari Mediaware Media Star', 'enterprise content integration', 'middleware', 'database management systems', 'document handling', 'electronic publishing', 'workflow management software']","['Enterprise content integration', 'atari Mediaware', 'Media Star', 'Media', 'large media companies', 'Star', 'atari', 'Mediaware', 'Enterprise', 'integration']",376,65,7,376,64,10,1,1,1
"an active functionality service for e-business applications service based architectures are a powerful approach to meet the fast evolution of business rules and the corresponding software. an active functionality service that detects events and involves the appropriate business rules is a critical component of such a service-based middleware architecture. in this paper we present an active functionality service that is capable of detecting events in heterogeneous environments, it uses an integral ontology-based approach for the semantic interpretation of heterogeneous events and data, and provides notifications through a publish/subscribe notification mechanism. the power of this approach is illustrated with the help of an auction application and through the personalization of car and driver portals in internet-enabled vehicles ","An active functionality service for e-business applications Service based architectures are a powerful approach to meet the fast evolution of business rules and the corresponding software. An active functionality service that detects events and involves the appropriate business rules is a critical component of such a service-based middleware architecture. In this paper we present an active functionality service that is capable of detecting events in heterogeneous environments, it uses an integral ontology-based approach for the semantic interpretation of heterogeneous events and data, and provides notifications through a publish/subscribe notification mechanism. The power of this approach is illustrated with the help of an auction application and through the personalization of car and driver portals in Internet-enabled vehicles","['active functionality service', 'e-business applications', 'business rules', 'software', 'event detection', 'service-based middleware architecture', 'heterogeneous environments', 'ontology based approach', 'semantic interpretation', 'publish/subscribe notification mechanism', 'auction application', 'personalized car portals', 'personalized driver portals', 'Internet-enabled vehicles', 'client-server systems', 'driver information systems', 'electronic commerce', 'information dissemination', 'Internet', 'knowledge engineering', 'online front-ends']","['active functionality service', 'service-based middleware architecture', 'business applications Service', 'appropriate business rules', 'service', 'heterogeneous events', 'powerful approach', 'business rules', 'active', 'functionality']",724,117,21,724,116,10,0,0,6
"spatial solutions [office furniture] take the stress out of the office by considering the design of furniture and staff needs, before major buying decisions ","Spatial solutions [office furniture] Take the stress out of the office by considering the design of furniture and staff needs, before major buying decisions","['office furniture', 'staff needs', 'buying decisions', 'furniture']","['furniture', 'office', 'major buying decisions', 'staff needs', 'solutions', 'Spatial', 'stress', 'design', 'staff', 'needs']",133,25,4,133,24,10,0,0,0
"presentation media, information complexity, and learning outcomes multimedia computing provides a variety of information presentation modality combinations. educators have observed that visuals enhance learning which suggests that multimedia presentations should be superior to text-only and text with static pictures in facilitating optimal human information processing and, therefore, comprehension. the article reports the findings from a 3 (text-only, overhead slides, and multimedia presentation)*2 (high and low information complexity) factorial experiment. subjects read a text script, viewed an acetate overhead slide presentation, or viewed a multimedia presentation depicting the greenhouse effect (low complexity) or photocopier operation (high complexity). multimedia was superior to text-only and overhead slides for comprehension. information complexity diminished comprehension and perceived presentation quality. multimedia was able to reduce the negative impact of information complexity on comprehension and increase the extent of sustained attention to the presentation. these findings suggest that multimedia presentations invoke the use of both the verbal and visual working memory channels resulting in a reduction of the cognitive load imposed by increased information complexity. moreover, multimedia superiority in facilitating comprehension goes beyond its ability to increase sustained attention; the quality and effectiveness of information processing attained (i.e., use of verbal and visual working memory) is also significant ","Presentation media, information complexity, and learning outcomes Multimedia computing provides a variety of information presentation modality combinations. Educators have observed that visuals enhance learning which suggests that multimedia presentations should be superior to text-only and text with static pictures in facilitating optimal human information processing and, therefore, comprehension. The article reports the findings from a 3 (text-only, overhead slides, and multimedia presentation)*2 (high and low information complexity) factorial experiment. Subjects read a text script, viewed an acetate ‘overhead slide presentation, or viewed a multimedia presentation depicting the greenhouse effect (low complexity) or photocopier operation (high complexity). Multimedia was superior to text-only and ‘overhead slides for comprehension. Information complexity diminished ‘comprehension and perceived presentation quality. Multimedia was able to reduce the negative impact of information complexity on ‘comprehension and increase the extent of sustained attention to the presentation. These findings suggest that multimedia presentations invoke the use of both the verbal and visual working memory channels resulting in a reduction of the cognitive load imposed by increased information complexity. Moreover, multimedia superiority in facilitating comprehension goes beyond its ability to increase sustained attention; the quality and effectiveness of information processing attained (i.e., use of verbal and visual working memory) is also significant","['presentation media', 'information complexity', 'learning outcomes', 'cognitive processing limitations', 'human working memory', 'verbal working memory channel', 'visual working memory channel', 'multimedia computing', 'information presentation modality combinations', 'educators', 'multimedia presentations', 'static pictures', 'optimal human information processing', 'overhead slides', 'text script', 'acetate overhead slide presentation', 'multimedia presentation', 'greenhouse effect', 'photocopier operation', 'cognitive load', 'multimedia superiority', 'sustained attention', 'educational computing', 'ergonomics', 'human factors', 'multimedia computing', 'user interfaces']","['information', 'complexity', 'multimedia presentations', 'information processing', 'information complexity', 'comprehension Information complexity', 'low information complexity', 'high complexity Multimedia', 'presentation quality', 'low complexity']",1356,202,27,1360,201,10,42,4,10
"strong completeness of lattice-valued logic this paper shows strong completeness of the system l for lattice valued logic given by s. titani (1999), in which she formulates a lattice-valued set theory by introducing the logical implication which represents the order relation on the lattice. syntax and semantics concerned are described and strong completeness is proved ","Strong completeness of lattice-valued logic This paper shows strong completeness of the system L for lattice valued logic given by S. Titani (1999), in which she formulates a lattice-valued set theory by introducing the logical implication which represents the order relation on the lattice. Syntax and semantics concerned are described and strong completeness is proved","['strong completeness', 'lattice-valued set theory', 'order relation', 'semantics', 'syntax', 'lattice-valued logic', 'formal logic']","['lattice-valued set theory', 'lattice-valued logic', 'logical implication', 'lattice Syntax', 'system L', 'logic', 'lattice-valued', 'completeness', 'strong completeness', 'Strong completeness']",316,56,7,316,55,10,0,0,2
"state-of-the-art in orthopaedic surgical navigation with a focus on medical image modalities this paper presents a review of surgical navigation systems in orthopaedics and categorizes these systems according to the image modalities that are used for the visualization of surgical action. medical images used to be an essential part of surgical education and documentation as well as diagnosis and operation planning over many years. with the recent introduction of navigation techniques in orthopaedic surgery, a new field of application has been opened. today surgical navigation systems - also known as image-guided surgery systems - are available for various applications in orthopaedic surgery. they visualize the position and orientation of surgical instruments as graphical overlays onto a medical image of the operated anatomy on a computer monitor. preoperative image data such as computed tomography scans or intra operatively generated images (for example, ultrasonic, endoscopic or fluoroscopic images) are suitable for this purpose. a new category of medical images termed 'surgeon-defined anatomy' has been developed that exclusively relies upon the usage of navigation technology. points on the anatomy are digitized interactively by the surgeon and are used to build up an abstract geometrical model of the bony structures to be operated on. this technique may be used when no other image data is available or appropriate for a given application ","State-of-the-art in orthopaedic surgical navigation with a focus on medical image modalities This paper presents a review of surgical navigation systems in orthopaedics and categorizes these systems according to the image modalities that are used for the visualization of surgical action. Medical images used to be an essential part of surgical education and documentation as well as, diagnosis and operation planning over many years. With the recent introduction of navigation techniques in orthopaedic surgery, a new field of application has been opened. Today surgical navigation systems - also known as image-guided surgery systems - are available for various applications in orthopaedic surgery. They visualize the position and orientation of surgical instruments as graphical overlays ‘onto a medical image of the operated anatomy on a computer monitor. Preoperative image data such as computed tomography scans or intra operatively generated images (for example, ultrasonic, endoscopic or fluoroscopic images) are suitable for this purpose. A new category of medical images termed 'surgeon-defined anatomy’ has been developed that exclusively relies upon the usage of navigation technology. Points on the anatomy are digitized interactively by the surgeon and are used to build up an abstract geometrical model of the bony structures to be operated on. This technique may be used when no other image data is available or appropriate for a given application","['orthopaedic surgical navigation', 'medical image modalities', 'surgical action visualization', 'medical image processing', 'surgical education', 'image-guided surgery systems', 'surgical instruments', 'graphical overlays', 'computer monitor', 'computed tomography scans', 'intra operatively generated images', 'surgeon-defined anatomy', 'abstract geometrical model', 'bony structures', 'image registration', 'computerised tomography', 'data visualisation', 'image registration', 'medical image processing', 'orthopaedics', 'surgery']","['surgical navigation systems', 'orthopaedic surgery', 'Medical images', 'orthopaedic surgical navigation', 'medical image modalities', 'Preoperative image data', 'other image data', 'surgical action', 'medical image', 'image modalities']",1244,219,21,1246,218,10,5,3,7
blind identification of non-stationary ma systems a new adaptive algorithm for blind identification of time-varying ma channels is derived. this algorithm proposes the use of a novel system of equations derived by combining the third- and fourth-order statistics of the output signals of ma models. this overdetermined system of equations has the important property that it can be solved adaptively because of their symmetries via an overdetermined recursive instrumental variable-type algorithm. this algorithm shows good behaviour in arbitrary noisy environments and good performance in tracking time-varying systems ,Blind identification of non-stationary MA systems ‘Anew adaptive algorithm for blind identification of time-varying MA channels is derived. This algorithm proposes the use of a novel system of equations derived by combining the third- and fourth-order statistics of the output signals of MA models. This overdetermined system of equations has the important property that it can be solved adaptively because of their symmetries via an overdetermined recursive instrumental variable-type algorithm. This algorithm shows good behaviour in arbitrary noisy environments and good performance in tracking time-varying systems,"['blind identification', 'time-varying channels', 'nonstationary systems', 'adaptive algorithm', 'fourth-order statistics', 'third-order statistics', 'MA models', 'overdetermined recursive algorithm', 'recursive instrumental variable algorithm', 'arbitrary noisy environments', 'tracking', 'iterative algorithms', 'additive Gaussian noise', 'higher-order statistics', 'adaptive estimation', 'adaptive signal processing', 'blind equalisers', 'Gaussian noise', 'higher order statistics', 'moving average processes', 'parameter estimation', 'time-varying channels']","['non-stationary MA systems', 'time-varying MA channels', 'overdetermined system', 'time-varying systems', 'adaptive algorithm', 'novel system', 'MA models', 'MA', 'blind identification', 'Blind identification']",532,88,22,533,86,10,289,80,3
"a comparison of the discounted utility model and hyperbolic discounting models in the case of social and private intertemporal preferences for health whilst there is substantial evidence that hyperbolic discounting models describe intertemporal preferences for monetary outcomes better than the discounted utility (du) model, there is only very limited evidence in the context of health outcomes. this study elicits private and social intertemporal preferences for non-fatal changes in health. specific functional forms of the du model and three hyperbolic models are fitted. the results show that the stationarity axiom is violated, and that the hyperbolic models fit the data better than the du model. intertemporal preferences for private and social decisions are found to be very similar ","‘A.comparison of the discounted utility model and hyperbolic discounting models in the case of social and private intertemporal preferences for health Whilst there is substantial evidence that hyperbolic discounting models describe intertemporal preferences for monetary outcomes better than the discounted utility (DU) model, there is only very limited evidence in the context of health outcomes. This study elicits private and social intertemporal preferences for non-fatal changes in health. Specific functional forms of the DU model and three hyperbolic models are fitted. The results show that the stationarity axiom is violated, and that the hyperbolic models fit the data better than the DU model Intertemporal preferences for private and social decisions are found to be very similar","['discounted utility model', 'hyperbolic discounting models', 'intertemporal preferences', 'health outcomes', 'private decisions', 'social decisions', 'decision theory', 'health care', 'social sciences']","['hyperbolic discounting models', 'hyperbolic models', 'DU model', 'private intertemporal preferences', 'social intertemporal preferences', 'intertemporal preferences', 'discounted utility model', 'due model', 'models', 'discounted utility']",675,118,9,676,116,10,435,116,1
"frontier between separability and quantum entanglement in a many spin system we discuss the critical point x/sub c/ separating the quantum entangled and separable states in two series of n spins s in the simple mixed state characterized by the matrix operator rho = x| phi >< phi |+1-x/d/sup n/i/sub d/n, where x in [0, 1], d = 2s + 1, i/sub d/n is the d/sup n/ * d/sup n/ unity matrix and | phi > is a special entangled state. the cases x = 0 and x = 1 correspond respectively to fully random spins and to a fully entangled state. in the first of these series we consider special states | phi > invariant under charge conjugation, that generalizes the n = 2 spin s = 1/2 einstein-podolsky-rosen state, and in the second one we consider generalizations of the werner (1989) density matrices. the evaluation of the critical point x/sub c/ was done through bounds coming from the partial transposition method of peres (1996) and the conditional nonextensive entropy criterion. our results suggest the conjecture that whenever the bounds coming from both methods coincide the result of x/sub c/ is the exact one. the results we present are relevant for the discussion of quantum computing, teleportation and cryptography ","Frontier between separability and quantum entanglement in a many spin system We discuss the critical point x/sub c/ separating the quantum entangled and separable states in two series of N spins S in the simple mixed state characterized by the matrix operator rho = x| phi >< phi |+1-x/D/sup N/l/sub D/N, where x in [0, 1], D = 2S + 1, Vsub DIN is the D/sup N/* D/sup N/ unity matrix and | phi > is a special entangled state. The cases x = 0 and x = 1 correspond respectively to fully random spins and to a fully entangled state. In the first of these series we consider special states | phi > invariant under charge conjugation, that generalizes the N = 2 spin S = 1/2 Einstein-Podolsky-Rosen state, and in the second one we consider generalizations of the Werner (1989) density matrices. The evaluation of the critical point x/sub c/ was done through bounds coming from the partial transposition method of Peres (1996) and the conditional nonextensive entropy criterion. Our results suggest the conjecture that whenever the bounds coming from both methods coincide the result of x/sub c/ is the exact one. The results we present are relevant for the discussion of quantum computing, teleportation and cryptography","['separability', 'quantum entanglement', 'many spin system', 'separable states', 'matrix operator', 'unity matrix', 'entangled state', 'random spin', 'charge conjugation', 'Einstein-Podolsky-Rosen state', 'Werner density matrices', 'critical point', 'partial transposition method', 'nonextensive entropy criterion', 'quantum computing', 'teleportation', 'cryptography', 'bound states', 'eigenvalues and eigenfunctions', 'entropy', 'EPR paradox', 'many-body problems', 'probability', 'quantum communication', 'random processes']","['sub c', 'quantum computing teleportation', 'special entangled state', 'quantum entanglement', 'simple mixed state', 'separable states', 'many spin system', 'special states', 'entangled', 'entangled state']",1008,211,25,1007,209,10,439,146,9
"verification of timed automata based on similarity the paper presents a modification of the standard partitioning technique to generate abstract state spaces preserving similarity for timed automata. since this relation is weaker than bisimilarity, most of the obtained models (state spaces) are smaller than bisimilar ones, but still preserve the universal fragments of branching time temporal logics. the theoretical results are exemplified for strong, delay, and observational simulation relations ","Verification of timed automata based on similarity The paper presents a modification of the standard partitioning technique to generate abstract state spaces preserving similarity for Timed Automata. Since this relation is weaker than bisimilarity, most of the obtained models (state spaces) are smaller than bisimilar ones, but stil preserve the universal fragments of branching time temporal logics. The theoretical results are exemplified for strong, delay, and ‘observational simulation relations","['timed automata verification', 'partitioning technique', 'abstract state spaces', 'bisimilarity', 'universal fragments', 'branching time temporal logics', 'observational simulation relations', 'automata theory', 'bisimulation equivalence', 'minimisation', 'temporal logic']","['automata', 'observational simulation relations', 'standard partitioning technique', 'abstract state spaces', 'similar ones', 'modification', 'Verification', 'paper', 'spaces', 'similar']",432,70,11,432,69,10,13,2,4
"design and implementation of a flexible manufacturing control system using neural network design and implementation of a sequential controller based on the concept of artificial neural networks for a flexible manufacturing system are presented. the recurrent neural network (rnn) type is used for such a purpose. contrary to the programmable controller, an rnn-based sequential controller is based on a definite mathematical model rather than depending on the experience and trial and error techniques. the proposed controller is also more flexible because it is not limited by the restrictions of the finite state automata theory. adequate guidelines of how to construct an rnn-based sequential controller are presented. these guidelines are applied to different case studies. the proposed controller is tested by simulations and real-time experiments. these tests prove the successfulness of the proposed controller performances. theoretical as well as experimental results are presented and discussed indicating that the proposed design procedure using elman's rnn can be effective in designing a sequential controller for event-based type manufacturing systems. in addition, the simulation results assure the effectiveness of the proposed controller to overcome the effect of noisy inputs ","Design and implementation of a flexible manufacturing control system using neural network Design and implementation of a sequential controller based on the concept of artificial neural networks for a flexible manufacturing system are presented. The recurrent neural network (RNN) type is used for such a purpose. Contrary to the programmable controller, an RNN-based sequential controller is based on a definite mathematical model rather than depending on the experience and trial and error techniques. The proposed controller is also more flexible because it is not limited by the restrictions of the finite state automata theory. Adequate guidelines of how to construct an RNN-based sequential controller are presented. These guidelines are applied to different case studies. The proposed controller is tested by simulations and real-time experiments. These tests prove the successfulness of the proposed controller performances. Theoretical as well as experimental results are presented and discussed indicating that the proposed design procedure using Elman's RNN can be effective in designing a sequential controller for event-based type manufacturing systems. In addition, the simulation results assure the effectiveness of the proposed controller to overcome the effect of noisy inputs","['Elman network', 'finite state automata', 'flexible manufacturing systems', 'ladder language', 'noisy inputs', 'pneumatic system', 'programmable controller', 'recurrent neural network', 'sequential control', 'learning', 'FMS', 'finite automata', 'flexible manufacturing systems', 'learning (artificial intelligence)', 'neurocontrollers', 'programmable controllers', 'recurrent neural nets']","['controller', 'pen-based sequential controller', 'proposed controller performances', 'flexible manufacturing system', 'artificial neural networks', 'recurrent neural network', 'programmable controller', 'neural network Design', 'proposed controller', 'sequential controller']",1108,186,17,1108,185,10,0,0,2
"multiresolution markov models for signal and image processing reviews a significant component of the rich field of statistical multiresolution (mr) modeling and processing. these mr methods have found application and permeated the literature of a widely scattered set of disciplines, and one of our principal objectives is to present a single, coherent picture of this framework. a second goal is to describe how this topic fits into the even larger field of mr methods and concepts-in particular, making ties to topics such as wavelets and multigrid methods. a third goal is to provide several alternate viewpoints for this body of work, as the methods and concepts we describe intersect with a number of other fields. the principle focus of our presentation is the class of mr markov processes defined on pyramidally organized trees. the attractiveness of these models stems from both the very efficient algorithms they admit and their expressive power and broad applicability. we show how a variety of methods and models relate to this framework including models for self-similar and 1/f processes. we also illustrate how these methods have been used in practice ","Multiresolution Markov models for signal and image processing Reviews a significant component of the rich field of statistical multiresolution (MR) modeling and processing. These MR methods have found application and permeated the literature of a widely scattered set of disciplines, and one of our principal objectives is to present a single, coherent picture of this framework. A second goal is to describe how this topic fits into the even larger field of MR methods and concepts-in particular, making ties to topics such as wavelets and multigrid methods. A third goal is to provide several alternate viewpoints for this body of work, as the methods and concepts we describe intersect with a number of other fields. The principle focus of our presentation is the class of MR Markov processes defined on pyramidally organized trees. The attractiveness of these models stems, from both the very efficient algorithms they admit and their expressive power and broad applicability. We show how a variety of methods and models relate to this framework including models for seff-similar and 1/f processes. We also illustrate how these methods have been used in practice","['multiresolution Markov models', 'statistical multiresolution modeling', 'wavelets', 'multigrid methods', 'pyramidally organized trees', 'self-similar processes', '1/f processes', 'image processing', 'Markov processes', 'signal processing', 'statistical analysis', 'trees (mathematics)', 'wavelet transforms']","['MR methods', 'processes', 'Multiresolution Markov models', 'methods', 'multigrid methods', 'image processing', 'rich field', 'MR Markov', 'Markov', 'models']",981,186,13,982,185,10,1,2,2
"minimizing the number of successor states in the stubborn set method combinatorial explosion which occurs in parallel compositions of ltss can be alleviated by letting the stubborn set method construct on-the-fly a reduced lts that is cffd- or csp-equivalent to the actual parallel composition. this article considers the problem of minimizing the number of successor states of a given state in the reduced lts. the problem can be solved by constructing an and/or-graph with weighted vertices and by finding a set of vertices that satisfies a certain constraint such that no set of vertices satisfying the constraint has a smaller sum of weights. without weights, the and/or-graph can be constructed in low-degree polynomial time w.r.t. the length of the input of the problem. however, since actions can be nondeterministic and transitions can share target states, it is not known whether the weights are generally computable in polynomial time. consequently, it is an open problem whether minimizing the number of successor states is as ""easy"" as minimizing the number of successor transitions ","Minimizing the number of successor states in the stubborn set method Combinatorial explosion which occurs in parallel compositions of LTSs can be alleviated by letting the stubborn set method construct on-the-fly a reduced LTS that is CFFD- or CSP-equivalent to the actual parallel composition. This article considers the problem of minimizing the number of successor states of a given state in the reduced LTS. The problem can be solved by constructing an and/or-graph with weighted vertices and by finding a set of vertices that satisfies a certain constraint such that no set of vertices satisfying the constraint has a smaller sum of weights. Without weights, the and/or-graph can be constructed in low-degree polynomial time w.r-t. the length of the input of the problem. However, since actions can be nondeterministic and transitions can share target states, it is not known whether the weights are generally computable in polynomial time. Consequently, it is an open problem whether minimizing the number of successor states is as ""easy"" as minimizing the number of successor transitions,","['stubborn set method', 'combinatorial explosion', 'weighted vertices', 'low-degree polynomial time', 'CSP-equivalence', 'combinatorial mathematics', 'communicating sequential processes']","['successor states', 'number', 'stubborn set method', 'actual parallel composition', 'successor transitions', 'parallel compositions', 'weighted vertices', 'target states', 'state', 'successor']",924,172,7,925,171,10,1,2,2
"data storage: re-format. closely tracking a fast-moving sector in the past few years the data center market has changed dramatically, forcing many companies into consolidation or bankruptcy. gone are the days when companies raised millions of dollars to acquire large industrial buildings and transform them into glittering, high-tech palaces filled with the latest telecommunication and data technology. whereas manufacturers of communication technology deliver the racked equipment in these, often mission-critical, facilities, abb focuses mainly on the building infrastructure. besides the very important redundant power supply, abb also provides the redundant air conditioning and the security system ","Data storage: reformat. Closely tracking a fast-moving sector In the past few years the data center market has changed dramatically, forcing many companies into consolidation or bankruptcy. Gone are the days when ‘companies raised millions of dollars to acquire large industrial buildings and transform them into glittering, high-tech palaces filled with the latest telecommunication and data technology. Whereas, manufacturers of communication technology deliver the racked equipment in these, often mission-critical, facilities, ABB focuses mainly on the building infrastructure. Besides the very important redundant power supply, ABB also provides the redundant air conditioning and the security system","['building management', 'data centers', 'building infrastructure', 'mission-critical facilities', 'ABB', 'engineering management', 'project management', 'installation', 'commissioning', 'redundant power supply', 'redundant air conditioning', 'security system', 'air conditioning', 'building management systems', 'installation', 'power supplies to apparatus', 'project management', 'security']","['fast-moving sector', 'data center market', 'data technology', 'past few years', 'Data storage', 'Data', 'past', 'sector', 'storage', 'fast-moving']",609,97,18,610,96,10,16,3,7
"adaptive digital watermarking using fuzzy logic techniques digital watermarking has been proposed for copyright protection in our digital society. we propose an adaptive digital watermarking scheme based on the human visual system model and a fuzzy logic technique. the fuzzy logic approach is employed to obtain the different strengths and lengths of a watermark by the local characteristics of the image in our proposed scheme. in our experiments, this scheme provides a more robust and imperceptible watermark ","Adaptive digital watermarking using fuzzy logic techniques Digital watermarking has been proposed for copyright protection in our digital society. We propose an adaptive digital watermarking scheme based on the human visual system model and a fuzzy logic technique. The fuzzy logic approach is employed to obtain the different strengths and lengths of a watermark by the local characteristics of the image in our proposed scheme. In our experiments, this scheme provides a more robust and imperceptible watermark","['adaptive digital watermarking', 'fuzzy logic techniques', 'copyright protection', 'digital society', 'human visual system model', 'local characteristics', 'imperceptible watermark', 'robust watermark', 'image processing', 'copy protection', 'data compression', 'discrete cosine transforms', 'fuzzy logic', 'image coding', 'inference mechanisms']","['Adaptive digital watermarking', 'fuzzy logic techniques', 'scheme', 'imperceptible watermark', 'fuzzy logic approach', 'digital society', 'watermark', 'digital', 'logic', 'fuzzy']",436,78,15,436,77,10,0,0,3
a generalized pert/cpm implementation in a spreadsheet this paper describes the implementation of the traditional pert/cpm algorithm for finding the critical path in a project network in a spreadsheet. the problem is of importance due to the recent shift of attention to using the spreadsheet environment as a vehicle for delivering management science/operations research (ms/or) techniques to end-users ,"A generalized PERT/CPM implementation in a spreadsheet This paper describes the implementation of the traditional PERT/CPM algorithm, for finding the critical path in a project network in a spreadsheet. The problem is of importance due to the recent shift of attention to using the spreadsheet environment as a vehicle for delivering management science/operations research (MS/OR) techniques to end-users","['generalized PERT/CPM implementation', 'spreadsheet', 'critical path', 'MS/OR techniques', 'PERT', 'spreadsheet programs']","['management science/operations research', 'traditional PERT/CPM algorithm', 'spreadsheet environment', 'PERT/CPM implementation', 'project network', 'critical path', 'paper', 'PERT/CPM', 'spreadsheet', 'implementation']",346,59,6,347,58,10,0,1,1
mathematical model of functioning of an insurance company with allowance for advertising expenses a mathematical model of the functioning of an insurance company with allowance for advertising expenses is suggested. the basic characteristics of the capital of the company and the advertising efficiency are examined in the case in which the advertising expenses are proportional to the capital ,Mathematical model of functioning of an insurance company with allowance for advertising expenses ‘A mathematical model of the functioning of an insurance company with allowance for advertising expenses is suggested. The basic characteristics of the capital of the company and the advertising efficiency are examined in the case in which the advertising expenses are proportional to the capital,"['insurance company functioning', 'advertising expenses allowance', 'capital', 'mathematical model', 'advertising', 'insurance']","['advertising expenses', 'insurance company', 'advertising efficiency', 'advertising', 'model', 'company', 'insurance', 'mathematical model', 'Mathematical model', 'expenses']",336,59,6,337,58,10,1,1,1
when reference works are not books-the new edition of the guide to reference books the author considers the history of the guide to reference books (grb) and its importance in librarianship. he discusses the ways in which the new edition is taking advantage of changing times. grb has become a cornerstone of the literature of u.s. librarianship. the biggest change grb will undergo to become grs (guide to reference sources) will be designing it primarily as a web product ,When reference works are not books-the new edition of the Guide to Reference Books The author considers the history of the Guide to Reference Books (GRB) and its importance in librarianship. He discusses the ways in which the new edition is taking advantage of changing times. GRB has become a cornerstone of the literature of U.S. librarianship. The biggest change GRB will undergo to become GRS (Guide to Reference Sources) will be designing it primarily as a Web product,"['reference works', 'Guide to Reference Books', 'history', 'librarianship', 'GRB', 'GRS', 'Guide to Reference Sources', 'Web product', 'Internet', 'information resources', 'information science', 'Internet', 'library automation']","['Reference', 'books-the new edition', 'Reference Books grb2', 'biggest change grab', 'Reference sources', 'reference works', 'new', 'edition', 'new edition', 'Reference Books']",395,80,13,395,79,10,0,0,5
"a transactional asynchronous replication scheme for mobile database systems in mobile database systems, mobility of users has a significant impact on data replication. as a result, the various replica control protocols that exist today in traditional distributed and multidatabase environments are no longer suitable. to solve this problem, a new mobile database replication scheme, the transaction-level result-set propagation (tlrsp) model, is put forward in this paper. the conflict detection and resolution strategy based on tlrsp is discussed in detail, and the implementation algorithm is proposed. in order to compare the performance of the tlrsp model with that of other mobile replication schemes, we have developed a detailed simulation model. experimental results show that the tlrsp model provides an efficient support for replicated mobile database systems by reducing reprocessing overhead and maintaining database consistency ","A transactional asynchronous replication scheme for mobile database systems In mobile database systems, mobility of users has a significant impact on data replication. As a result, the various replica control protocols that exist today in traditional distributed and multidatabase environments are no longer suitable. To solve this problem, a new mobile database replication scheme, the Transaction-Level Result-Set Propagation (TLRSP) model, is put forward in this paper. The conflict detection and resolution strategy based on TLRSP is discussed in detail, and the implementation algorithm is proposed. In order to compare the performance of the TLRSP model with that of other mobile replication schemes, we have developed a detailed simulation model. Experimental results show that the TLRSP model provides an efficient support for replicated mobile database systems by reducing reprocessing overhead and maintaining database consistency","['mobile database', 'data replication', 'distributed database', 'multidatabase', 'mobile database replication', 'Transaction-Level Result-Set Propagation', 'mobile computing', 'conflict reconciliation', 'transaction', 'concurrency control', 'mobile computing', 'replicated databases', 'transaction processing']","['mobile database systems', 'scheme', 'trip model', 'detailed simulation model', 'database consistency', 'data replication', 'mobility', 'database', 'replication', 'systems']",808,134,13,808,133,10,0,0,3
"rational systems exhibit moderate risk aversion with respect to ""gambles"" on variable-resolution compression in an embedded wavelet scheme for progressive transmission, a tree structure naturally defines the spatial relationship on the hierarchical pyramid. transform coefficients over each tree correspond to a unique local spatial region of the original image, and they can be coded bit-plane by bit-plane through successive-approximation quantization. after receiving the approximate value of some coefficients, the decoder can obtain a reconstructed image. we show a rational system for progressive transmission that, in absence of a priori knowledge about regions of interest, chooses at any truncation time among alternative trees for further transmission in such a way as to avoid certain forms of behavioral inconsistency. we prove that some rational transmission systems might exhibit aversion to risk involving ""gambles"" on tree-dependent quality of encoding while others favor taking such risks. based on an acceptable predictor for visual distinctness from digital imagery, we demonstrate that, without any outside knowledge, risk-prone systems as well as those with strong risk aversion appear in capable of attaining the quality of reconstructions that can be achieved with moderate risk-averse behavior ","Rational systems exhibit moderate risk aversion with respect to ""gambles"" on variable-resolution compression In an embedded wavelet scheme for progressive transmission, a tree structure naturally defines the spatial relationship on the hierarchical pyramid Transform coefficients over each tree correspond to a unique local spatial region of the original image, and they can be coded bit-plane by bit-plane through successive-approximation quantization. After receiving the approximate value of some coefficients, the decoder can obtain a reconstructed image. We show a rational system for progressive transmission that, in absence of a priori knowledge about regions of interest, chooses at any truncation time among alternative trees for further transmission in such a way as to avoid certain forms of behavioral inconsistency. We prove that some rational transmission systems might exhibit aversion to risk involving ""gambles"" on tree-dependent quality of encoding while others favor taking such risks. Based on an acceptable predictor for visual distinctness from digital imagery, we demonstrate that, without any outside knowledge, risk-prone systems as well as those with strong risk aversion appear in capable of attaining the quality of reconstructions that can be achieved with moderate risk-averse behavior","['variable-resolution compression', 'progressive transmission', 'rational system', 'moderate risk aversion', 'embedded wavelet scheme', 'tree structure', 'hierarchical pyramid spatial relationship', 'transform coefficients', 'local spatial region', 'successive-approximation quantization', 'reconstructed image', 'truncation time', 'behavioral inconsistency avoidance', 'gambles', 'image encoding', 'acceptable predictor', 'visual distinctness', 'digital imagery', 'embedded coding', 'rate control optimization', 'decision problem', 'progressive transmission utility functions', 'information theoretic measure', 'decision theory', 'decision trees', 'decoding', 'image coding', 'image reconstruction', 'quantisation (signal)', 'wavelet transforms']","['progressive transmission', 'rational transmission systems', 'moderate risk-averse behavior', 'moderate risk aversion', 'strong risk aversion', 'risk-prone systems', 'Rational systems', 'such risks', 'systems', 'rational system']",1131,188,30,1130,187,10,0,1,12
"adaptive neural/fuzzy control for interpolated nonlinear systems adaptive control for nonlinear time-varying systems is of both theoretical and practical importance. we propose an adaptive control methodology for a class of nonlinear systems with a time-varying structure. this class of systems is composed of interpolations of nonlinear subsystems which are input-output feedback linearizable. both indirect and direct adaptive control methods are developed, where the spatially localized models (in the form of takagi-sugeno fuzzy systems or radial basis function neural networks) are used as online approximators to learn the unknown dynamics of the system. without assumptions on rate of change of system dynamics, the proposed adaptive control methods guarantee that all internal signals of the system are bounded and the tracking error is asymptotically stable. the performance of the adaptive controller is demonstrated using a jet engine control problem ","Adaptive neural/fuzzy control for interpolated nonlinear systems Adaptive control for nonlinear time-varying systems is of both theoretical and practical importance. We propose an adaptive control methodology for a class of nonlinear systems with a time-varying structure. This class of systems is composed of interpolations of nonlinear subsystems which are input-output feedback linearizable. Both indirect and direct adaptive control methods are developed, where the spatially localized models (in the form of Takagi-Sugeno fuzzy systems or radial basis function neural networks) are used as online approximators to learn the unknown ‘dynamics of the system. Without assumptions on rate of change of system ‘dynamics, the proposed adaptive control methods guarantee that all internal signals of the system are bounded and the tracking error is asymptotically stable. The performance of the adaptive controller is demonstrated using a jet engine control problem","['adaptive neural/fuzzy control', 'interpolated nonlinear systems', 'time-varying systems', 'input-output feedback linearizable systems', 'indirect control', 'direct control', 'spatially localized models', 'Takagi-Sugeno fuzzy systems', 'radial basis function neural networks', 'online approximators', 'unknown dynamics', 'tracking error', 'jet engine control', 'stability analysis', 'adaptive control', 'aerospace engines', 'asymptotic stability', 'fuzzy control', 'neurocontrollers', 'nonlinear control systems', 'radial basis function networks', 'time-varying systems']","['adaptive control methods', 'nonlinear systems', 'Adaptive control', 'systems', 'nonlinear time-varying systems', 'Adaptive neural/fuzzy control', 'adaptive control methodology', 'adaptive controller', 'system dynamics', 'fuzzy systems']",825,138,22,827,137,10,17,2,4
"approximation and complexity. ii. iterated integration for pt. i. see ibid., no. 1, p. 289-95 (2001). we introduce two classes of real analytic functions w contained in/implied by u on an interval. starting with rational functions to construct functions in w we allow the application of three types of operations: addition, integration, and multiplication by a polynomial with rational coefficients. in a similar way, to construct functions in u we allow integration, addition, and multiplication of functions already constructed in u and multiplication by rational numbers. thus, u is a subring of the ring of pfaffian functions. two lower bounds on the l/sub infinity /-norm are proved on a function f from w (or from u, respectively) in terms of the complexity of constructing f ","Approximation and complexity. Il. Iterated integration For pt. I. see ibid., no. 1, p. 289-95 (2001). We introduce two classes of real analytic functions W contained in/implied by U on an interval. Starting with rational functions to construct functions in W we allow the application of three types of operations: addition, integration, and multiplication by a polynomial with rational coefficients. In a similar way, to construct functions in U we allow integration, addition, and multiplication of functions already constructed in U and multiplication by rational numbers. Thus, U is a subring of the ring of Pfaffian functions. Two lower bounds on the L/sub infinity /-norm are proved on a function f from W (or from U, respectively) in terms of the complexity of constructing f","['real analytic functions', 'rational functions', 'addition', 'integration', 'multiplication', 'polynomial', 'Pfaffian functions', 'lower bounds', 'L/sub infinity /-norm', 'computational complexity', 'polynomials', 'rational functions']","['complexity', 'functions', 'operations addition integration', 'ill Iterated integration', 'rational coefficients', 'integration addition', 'rational functions', 'Pfaffian functions', 'Approximation', 'integration']",657,126,12,657,125,10,1,1,4
"the influence of tollbooths on highway traffic we study the effects of tollbooths on the traffic flow. the highway traffic is simulated by the nagel-schreckenberg model. various types of toll collection are examined, which can be characterized either by a waiting time or a reduced speed. a first-order phase transition is observed. the phase separation results a saturated flow, which is observed as a plateau region in the fundamental diagram. the effects of lane expansion near the tollbooth are examined. the full capacity of a highway can be restored. the emergence of vehicle queuing is studied. besides the numerical results, we also obtain analytical expressions for various quantities. the numerical simulations can be well described by the analytical formulas. we also discuss the influence on the travel time and its variance. the tollbooth increases the travel time but decreases its variance. the differences between long- and short-distance travelers are also discussed ","The influence of tollbooths on highway traffic We study the effects of tollbooths on the traffic flow. The highway traffic is simulated by the Nagel-Schreckenberg model. Various types of toll collection are examined, which can be characterized either by a waiting time or a reduced speed. A first-order phase transition is observed The phase separation results a saturated flow, which is observed as a plateau region in the fundamental diagram. The effects of lane expansion near the tollbooth are examined. The full capacity of a highway can be restored. The emergence of vehicle queuing is studied. Besides the numerical results, we also obtain analytical expressions for various quantities. The numerical simulations can be well described by the analytical formulas. We also discuss the influence on the travel time and its variance. The tollbooth increases the travel time but decreases its variance. The differences between long- and short-distance travelers are also discussed","['highway traffic', 'tollbooths', 'Nagel-Schreckenberg model', 'toll collection', 'waiting time', 'reduced speed', 'first-order phase transition', 'saturated flow', 'lane expansion', 'vehicle queuing', 'numerical simulations', 'road traffic']","['tolbooth', 'highway traffic', 'travel time', 'influence', 'first-order phase transition', 'numerical simulations', 'saturated flow', 'traffic flow', 'traffic', 'highway']",833,152,12,832,151,10,0,1,6
"on the discretization of double-bracket flows this paper extends the method of magnus series to lie-algebraic equations originating in double-bracket flows. we show that the solution of the isospectral flow y' = [[y,n],y], y(o) = y/sub 0/ in sym(n), can be represented in the form y(t) = e/sup omega (t)/y/sub 0/e/sup - omega (1)/, where the taylor expansion of omega can be constructed explicitly, term-by-term, identifying individual expansion terms with certain rooted trees with bicolor leaves. this approach is extended to other lie-algebraic equations that can be appropriately expressed in terms of a finite ""alphabet"" ","On the discretization of double-bracket flows This paper extends the method of Magnus series to Lie-algebraic equations originating in double-bracket flows. We show that the solution of the isospectral flow Y"" = [[Y.N].Y], Y(O) = Yisub O/ in Sym(n), can be represented in the form Y(t) = e/sup Omega (t)/Y/sub O/e/sup - Omega (1)/, where the Taylor expansion of Omega can be constructed explicitly, term-by-term, identifying individual expansion terms with certain rooted trees with bicolor leaves. This approach is extended to other Lie-algebraic equations that can be appropriately expressed in terms of a finite ""alphabet""","['double-bracket flows discretization', 'Magnus series', 'Lie-algebraic equations', 'isospectral flow', 'Taylor expansion', 'bicolor leaves', 'fluid dynamics', 'Lie algebras', 'matrix algebra', 'trees (mathematics)']","['double-bracket', 'other Lie-algebraic equations', 'individual expansion terms', 'isospectral flow y', 'discretization', 'Magnus series', 'method', '= sub', 'Lie-algebraic', 'Lie-algebraic equations']",531,96,10,531,95,10,6,5,1
"pervasive computing goes to work: interfacing to the enterprise the paperless office is an idea whose time has come, and come, and come again. to see how pervasive computing applications might bring some substance to this dream, the author spoke recently with key managers and technologists at mckesson corporation (san francisco), a healthcare supplier, service, and technology company with us$50 billion in sales last year, and also at avantgo (hayward, calif.), a provider of mobile infrastructure software and services. for the past several years, mckesson has used mobility middleware developed by avantgo to deploy major supply chain applications with thousands of pervasive clients and multiple servers that replace existing paper-based tracking systems. according to mckesson's managers, their system greatly reduced errors and associated costs caused by redelivery or loss of valuable products, giving mckesson a solid return on its investment ","Pervasive computing goes to work: interfacing to the enterprise The paperless office is an idea whose time has come, and come, and come again To see how pervasive computing applications might bring some substance to this dream, the author spoke recently with key managers and technologists at McKesson Corporation (San Francisco), a healthcare supplier, service, and technology company with USS5O billion in sales last year, and also at AvantGo (Hayward, Calif.), a provider of mobile infrastructure software and services. For the past several years, McKesson has used mobility middleware developed by AvantGo to deploy major supply chain applications with thousands of pervasive clients and multiple servers that replace existing paper-based tracking systems. According to McKesson's managers, their system greatly reduced errors and associated costs caused by redelivery or loss of valuable products, giving McKesson a solid return on its investment","['paperless office', 'pervasive clients', 'multiple servers', 'mobile workers', 'enterprise resource planning', 'data warehousing', 'business data processing', 'client-server systems', 'mobile computing', 'user interfaces']","['pervasive computing applications', 'pervasive clients', 'paperless office', 'enterprise', 'pervasive', 'Pervasive', 'computing', 'office', 'paperless', 'Pervasive computing']",813,141,10,812,140,10,2,2,1
"life after bankruptcy [telecom carriers] how comeback telecom carriers are changing industry economics, and why others may have no choice but to follow their lead ","Life after bankruptcy [telecom carriers] How comeback telecom carriers are changing industry economics, and why others may have no choice but to follow their lead","['telecom carriers', 'industry economics', 'restructured companies', 'debt levels', 'bankruptcy', 'telecommunication']","['bankruptcy telecom carriers', 'comeback telecom carriers', 'industry economics', 'Life', 'telecom', 'carriers', 'industry', 'comeback', 'economics', 'bankruptcy']",138,26,6,138,25,10,0,0,0
"anatomy of the coupling query in a web warehouse to populate a data warehouse specifically designed for web data, i.e. web warehouse, it is imperative to harness relevant documents from the web. in this paper, we describe a query mechanism called coupling query to glean relevant web data in the context of our web warehousing system called warehouse of web data (whoweda). a coupling query may be used for querying both html and xml documents. important features of our query mechanism are the ability to query metadata, content, internal and external (hyperlink) structure of web documents based on partial knowledge, ability to express constraints on tag attributes and tagless segment of data, ability to express conjunctive as well as disjunctive query conditions compactly, ability to control execution of a web query and preservation of the topological structure of hyperlinked documents in the query results. we also discuss how to formulate a query graphically and in textual form using a coupling graph and coupling text, respectively ","Anatomy of the coupling query in a Web warehouse To populate a data warehouse specifically designed for Web data, Ie. Web warehouse, it is imperative to harness relevant documents from the Web. In this paper, we describe a query mechanism called coupling query to glean relevant Web data in the context of our Web warehousing system called Warehouse Of Web Data (WHOWEDA). A coupling query may be used for querying both HTML and XML documents. Important features of our query mechanism are the ability to query metadata, content, internal and external (hyperlink) structure of Web documents based on partial knowledge, ability to express constraints on tag attributes and tagless segment of data, ability to express conjunctive as well as disjunctive query conditions compactly, ability to control execution of a Web query and preservation of the topological structure of hyperlinked documents in the query results. We also discuss how to formulate a query graphically and in textual form using a coupling graph and coupling text, respectively","['coupling query', 'Web warehouse', 'data warehouse', 'Warehouse Of Web Data', 'HTML documents', 'XML documents', 'metadata', 'content', 'internal structure', 'external structure', 'Web documents', 'partial knowledge', 'tag attributes', 'tagless segment', 'disjunctive query conditions', 'conjunctive query conditions', 'execution control', 'topological structure', 'hyperlinked documents', 'graphical query formulation', 'textual query formulation', 'coupling text', 'data warehouses', 'information resources', 'query formulation']","['coupling query', 'query mechanism', 'Web warehouse', 'disjunctive query conditions', 'Web warehousing system', 'relevant Web data', 'Web documents', 'Web data Ie', 'Web query', 'Web Data']",880,166,25,879,165,10,2,1,6
"controller performance analysis with lqg benchmark obtained under closed loop conditions this paper proposes a new method for obtaining a linear quadratic gaussian (lqg) benchmark in terms of the variances of process input and output from closed-loop data, for assessing the controller performance. lqg benchmark has been proposed in the literature to assess controller performance since the lqg tradeoff curve represents the limit of performance in terms of input and output variances. however, an explicit parametric model is required to calculate the lqg benchmark. in this work, we propose a data driven subspace approach to calculate the lqg benchmark under closed-loop conditions with certain external excitations. the optimal lqg-benchmark variances are obtained directly from the subspace matrices corresponding to the deterministic inputs and the stochastic inputs, which are identified using closed-loop data with setpoint excitation. these variances are used for assessing the controller performance. the method proposed in this paper is applicable to both univariate and multivariate systems. profit analysis for the implementation of feedforward control to the existing feedback-only control system is also analyzed under the optimal lqg performance framework ","Controller performance analysis with LQG benchmark obtained under closed loop conditions This paper proposes a new method for obtaining a linear quadratic Gaussian (LQG) benchmark in terms of the variances of process input and output from closed-loop data, for assessing the controller performance. LAG. benchmark has been proposed in the literature to assess controller performance since the LQG tradeoff curve represents the limit of performance in terms of input and output variances. However, an explicit parametric model is required to calculate the LQG benchmark In this work, we propose a data driven subspace approach to calculate the LQG benchmark under closed-loop conditions with certain external excitations. The optimal LQG-benchmark variances are obtained directly from the subspace matrices corresponding to the deterministic inputs and the stochastic inputs, which are identified using closed-loop data with setpoint excitation. These variances are used for assessing the controller performance. The method proposed in this paper is applicable to both univariate and multivariate systems. Profit analysis for the implementation of feedforward control to the existing feedback-only control system is also analyzed under the optimal LQG performance framework","['controller performance analysis', 'LQG benchmark', 'linear quadratic Gaussian benchmark', 'closed-loop data', 'subspace matrices', 'deterministic inputs', 'stochastic inputs', 'univariate systems', 'multivariate systems', 'profit analysis', 'feedforward control', 'state space model', 'closed loop systems', 'control system analysis', 'feedforward', 'linear quadratic Gaussian control', 'matrix algebra', 'nonparametric statistics', 'state-space methods']","['leg benchmark', 'optimal LQG-benchmark variances', 'Controller performance analysis', 'feedback-only control system', 'feedforward control', 'leg tradeoff curve', 'benchmark', 'performance', 'leg', 'controller performance']",1092,182,19,1092,181,10,1,2,5
"descriptological foundations of programming descriptological foundations of programming are constructed. an explication of the concept of a descriptive process is given. the operations of introduction and elimination of abstraction at the level of processes are refined. an intensional concept of a bipolar function is introduced. an explication of the concept of introduction and extraction of abstraction at the bipole level is given. on this basis, a complete set of descriptological operations is constructed ","Descriptological foundations of programming Descriptological foundations of programming are constructed. An explication of the concept of a descriptive process is given. The operations of introduction and elimination of abstraction at the level of processes are refined. An intensional concept of a bipolar function is introduced. An explication of the concept of introduction and extraction of abstraction at the bipole level is given. On this basis, a complete set of descriptological operations is constructed","['descriptological foundations', 'programming', 'descriptive process', 'intensional concept', 'bipolar function', 'bipole level', 'programming']","['Descriptological foundations', 'programming', 'explication', 'intensional concept', 'descriptive process', 'bipolar function', 'dipole level', 'concept', 'foundations', 'Descriptological']",440,74,7,440,73,10,0,0,0
"the p-p rearrangement and failure-tolerance of double p-ary multirings and generalized hypercubes it is shown that an arbitrary grouped p-element permutation can be implemented in a conflict-free way through the commutation of channels on the double p-ary multiring or the double p-ary hypercube. it is revealed that in arbitrary single-element permutations, these commutators display the property of the (p-1)-nodal failure-tolerance and the generalized hypercube displays in addition the property of the (p-1)-channel failure-tolerance ","The p-p rearrangement and fallure-tolerance of double p-ary multirings and generalized hypercubes Itis shown that an arbitrary grouped p-element permutation can be implemented in a confict-free way through the commutation of channels on the double p-ary multiring or the double p-ary hypercube. It is revealed that in arbitrary single-element permutations, these commutators display the property of the (p-1)-nodal failure-tolerance and the generalized hypercube displays in addition the property of the (p-1)-channel failure-tolerance","['p-p rearrangement', 'failure-tolerance', 'double p-ary multirings', 'generalized hypercubes', 'p-element permutation', 'conflict-free implementation', 'single-element permutations', 'commutators', 'computational complexity', 'fault tolerant computing', 'hypercube networks']","['arbitrary single-element permutations', 'double part mutterings', 'double part hypercube', 'hypercube displays', 'p-p rearrangement', 'hypercubes', 'double part', 'double', 'part', 'element permutation']",465,74,11,464,72,10,216,61,2
"identification of evolving fuzzy rule-based models an approach to identification of evolving fuzzy rule-based (er) models is proposed. er models implement a method for the noniterative update of both the rule-base structure and parameters by incremental unsupervised learning. the rule-base evolves by adding more informative rules than those that previously formed the model. in addition, existing rules can be replaced with new rules based on ranking using the informative potential of the data. in this way, the rule-base structure is inherited and updated when new informative data become available, rather than being completely retrained. the adaptive nature of these evolving rule-based models, in combination with the highly transparent and compact form of fuzzy rules, makes them a promising candidate for modeling and control of complex processes, competitive to neural networks. the approach has been tested on a benchmark problem and on an air-conditioning component modeling application using data from an installation serving a real building. the results illustrate the viability and efficiency of the approach ","Identification of evolving fuzzy rule-based models An approach to identification of evolving fuzzy rule-based (eR) models is proposed. eR models implement a method for the noniterative update of both the rule-base structure and parameters by incremental unsupervised learning. The rule-base evolves by adding more informative rules than those that previously formed the model. In addition, existing rules can be replaced with new rules based on ranking using the informative potential of the data. In this way, the rule-base structure is inherited and updated when new informative data become available, rather than being completely retrained. The adaptive nature of these evolving rule-based models, in combination with the highly transparent and compact form of fuzzy rules, makes them a promising candidate for modeling and control of complex processes, competitive to neural networks. The approach has been tested on a benchmark problem and on an air-conditioning component modeling application using data from an installation serving a real building. The results illustrate the viability and efficiency of the approach","['evolving fuzzy rule-based models', 'identification', 'noniterative update', 'rule-base structure', 'incremental unsupervised learning', 'ranking', 'informative potential', 'fuzzy rules', 'complex processes', 'air-conditioning component modeling', 'adaptive nonlinear control', 'fault detection', 'fault diagnostics', 'performance analysis', 'forecasting', 'knowledge extraction', 'robotics', 'behavior modeling', 'fuzzy logic', 'fuzzy set theory', 'identification', 'modelling', 'unsupervised learning']","['rule-based structure', 'rule-based', 'eR models', 'fuzzy rule-based models', 'more informative rules', 'new informative data', 'fuzzy rules', 'new rules', 'models', 'rule-based models']",959,166,23,959,165,10,0,0,8
"antipersistent markov behavior in foreign exchange markets a quantitative check of efficiency in us dollar/deutsche mark exchange rates is developed using high-frequency (tick by tick) data. the antipersistent markov behavior of log-price fluctuations of given size implies, in principle, the possibility of a statistical forecast. we introduce and measure the available information of the quote sequence, and we show how it can be profitable following a particular trading rule ","Antipersistent Markov behavior in foreign exchange markets A quantitative check of efficiency in US dollar/Deutsche mark exchange rates is developed using high-frequency (tick by tick) data. The antipersistent Markov behavior of log-price fluctuations of given size implies, in principle, the possibility of a statistical forecast. We introduce and measure the available information of the quote sequence, and we show how it can be profitable following a particular trading rule","['antipersistent Markov behavior', 'foreign exchange markets', 'efficiency', 'US dollar', 'Deutsche mark', 'exchange rates', 'high-frequency data', 'log-price fluctuations', 'statistical forecast', 'quote sequence', 'trading rule', 'Shannon entropy', 'forecasting', 'entropy', 'fluctuations', 'forecasting theory', 'foreign exchange trading', 'Markov processes', 'nonlinear dynamical systems', 'probability', 'time series']","['foreign exchange markets', 'low-price fluctuations', 'quantitative check', 'tick data', 'Markov', 'exchange', 'behavior', 'Antipersistent', 'antipersistent Markov behavior', 'Antipersistent Markov behavior']",410,70,21,410,69,10,0,0,6
"phase control of higher-order squeezing of a quantum field in a recent experiment [phys. rev. lett. 88 (2002) 023601], phase-dependent photon statistics in a c.w. system has been observed in the mixing of a coherent field with a two-photon source. their system has the advantage over other atomic transition-based fluorescent systems. in this paper, we examine further the squeezing properties of higher-order quantum fluctuations in one of the quadrature components of the combined field in this system. we demonstrate that efficient and lasting higher-order squeezing effects could be observed with proper choice of the relative phase between the pump and coherent fields. this nonclassical feature is attributed to a constructive two-photon interference. relationship between the second- and higher-order squeezing of the field is discussed ","Phase control of higher-order squeezing of a quantum field Ina recent experiment [Phys. Rev. Lett. 88 (2002) 023601], phase-dependent photon statistics in a c.w. system has been observed in the mixing of a coherent field with a two-photon source. Their system has the advantage over other atomic transition-based fluorescent systems. In this paper, we examine further the squeezing properties of higher-order quantum fluctuations in one of the quadrature components of the combined field in this system. We demonstrate that efficient and lasting higher-order squeezing effects could be observed with proper choice of the relative phase between the pump and coherent fields. This nonclassical feature is attributed to a constructive two-photon interference. Relationship between the second- and higher-order squeezing of the field is discussed","['phase control', 'higher-order squeezing', 'quantum field', 'phase-dependent photon statistics', 'coherent field mixing', 'atomic transition-based fluorescent systems', 'quantum fluctuations', 'two-photon interference', 'bound states', 'fluctuations', 'optical squeezing', 'phase control', 'quantum theory', 'two-photon processes']","['coherent field', 'higher-order quantum fluctuations', 'field', 'quantum field Ina', 'combined field', 'Phase control', 'c.w. system', 'quantum', 'system', 'higher-order']",720,125,14,720,123,10,415,114,3
"the existence condition of gamma -acyclic database schemes with mvds constraints it is very important to use database technology for a large-scale system such as erp and mis. a good database design may improve the performance of the system. some research shows that a gamma -acyclic database scheme has many good properties, e.g., each connected join expression is monotonous, which helps to improve query performance of the database system. thus what conditions are needed to generate a gamma -acyclic database scheme for a given relational scheme? in this paper, the sufficient and necessary condition of the existence of gamma -acyclic, join-lossless and dependencies-preserved database schemes meeting 4nf is given ","The existence condition of gamma -acyclic database schemes with MVDs constraints It is very important to use database technology for a large-scale system such as ERP and MIS. A good database design may improve the performance of the system. Some research shows that a gamma -acyclic database scheme has many good properties, e.g., each connected join expression is monotonous, which helps to improve query performance of the database system. Thus what conditions are needed to generate a gamma -acyclic database scheme for a given relational scheme? In this paper, the sufficient and necessary condition of the existence of gamma -acyclic, Join-lossless and dependencies-preserved database schemes meeting 4NF is given","['existence condition', 'database technology', 'large-scale system', 'connected join expression', 'query performance', 'sufficient and necessary condition', 'gamma -acyclic database schemes', 'MVDs constraints', 'database theory', 'multivalued logic']","['database', 'acyclic', 'gamma', 'dependencies-preserved database schemes', 'good database design', 'necessary condition', 'existence condition', 'database technology', 'relational scheme', 'database system']",610,110,10,610,109,10,0,0,3
"cutting the cord [wireless health care] more and more healthcare executives are electing to cut the cord to their existing computer systems by implementing mobile technology. the allure of information anywhere, anytime is intoxicating, demonstrated by the cell phones and personal digital assistants (pdas) that adorn today's professionals. the utility and convenience of these devices is undeniable. but what is the best strategy for implementing a mobile solution within a healthcare enterprise, be it large or small-and under what circumstances? what types of healthcare workers benefit most from mobile technology? and how state-of-the-art is security for wireless applications and devices? these are the questions that healthcare executives are asking-and should be asking-as they evaluate mobile solutions ","Cutting the cord [wireless health care] More and more healthcare executives are electing to cut the cord to their existing computer systems by implementing mobile technology. The allure of information anywhere, anytime is intoxicating, demonstrated by the cell phones and personal digital assistants (PDAs) that adorn today's professionals. The utility and convenience of these devices is undeniable. But what is the best strategy for implementing a mobile solution within a healthcare enterprise, be it large or small-and under what circumstances? What types of healthcare workers benefit most from mobile technology? And how state-of-the-art is security for wireless applications and devices? These are the questions that healthcare executives are asking-and should be asking-as they evaluate mobile solutions","['healthcare', 'mobile computing', 'wireless computing', 'security', 'health care', 'mobile computing', 'security of data']","['healthcare executives', 'mobile technology', 'healthcare', 'wireless applications', 'healthcare enterprise', 'wireless health care', 'healthcare workers', 'mobile solutions', 'cord', 'mobile']",696,117,7,696,116,10,0,0,1
"assignment of periods and priorities of messages and tasks in distributed control systems presents a task and message-based scheduling method to guarantee the given end-to-end constraints including precedence constraints, time constraints, and period and priority of task and message. the method is an integrated one considering both tasks executed in each node and messages transmitted via the network and is designed to apply to a general distributed control system that has multiple loops and a single loop has sensor nodes with multiple sensors, actuator nodes with multiple actuators, controller nodes with multiple tasks, and several types of constraints. the assigning method of the optimal period and priority of task and message is proposed, using the presented task and message-based scheduling method ","Assignment of periods and priorities of messages and tasks in distributed control systems Presents a task and message-based scheduling method to guarantee the given end-to-end constraints including precedence constraints, time constraints, and period and priority of task and message. The method is an integrated one considering both tasks executed in each node and messages transmitted via the network and is designed to apply to a general distributed control system that has multiple loops and a single loop has sensor nodes with multiple sensors, actuator nodes with multiple actuators, controller nodes with multiple tasks, and several types of constraints. The assigning method of the optimal period and priority of task and message is proposed, using the presented task and message-based scheduling method","['periods assignment', 'priorities assignment', 'message-based scheduling method', 'distributed control systems', 'task-based scheduling method', 'end-to-end constraints', 'precedence constraints', 'time constraints', 'controller area networks', 'distributed control', 'field buses', 'message passing', 'scheduling']","['message-based scheduling method', 'control system', 'messages', 'end-to-end constraints', 'assigning method', 'optimal period', 'multiple tasks', 'sensor nodes', 'periods', 'tasks']",691,122,13,691,121,10,0,0,0
"nuvox shows staying power with new cash, new market who says you can't raise cash in today's telecom market? nuvox communications positions itself for the long run with $78.5 million in funding and a new credit facility ","NuVox shows staying power with new cash, new market ‘Who says you can't raise cash in today's telecom market? NuVox Communications Positions itself for the long run with $78.5 million in funding and a new credit facility","['telecom', 'competitive carrier market', 'NuVox Communications', 'investors', 'telecommunication']","['NuVox', 'new credit facility', 'new market', 'new cash', 'long run', 'todays', 'power', 'new', 'cash', 'market']",183,38,5,184,37,10,3,1,1
"methods for outlier detection in prediction if a prediction sample is different from the calibration samples, it can be considered as an outlier in prediction. in this work, two techniques, the use of uncertainty estimation and the convex hull method are studied to detect such prediction outliers. classical techniques (mahalanobis distance and x-residuals), potential functions and robust techniques are used for comparison. it is concluded that the combination of the convex hull method and uncertainty estimation offers a practical way for detecting outliers in prediction. by adding the potential function method, inliers can also be detected ","Methods for outlier detection in prediction Ifa prediction sample is different from the calibration samples, it can be considered as an outlier in prediction. In this work, two techniques, the use of uncertainty estimation and the convex hull method are studied to detect such prediction outliers. Classical techniques (Mahalanobis distance and X-residuals), potential functions and robust techniques are used for comparison. It is concluded that the combination of the convex hull method and uncertainty estimation offers a practical way for detecting outliers in prediction. By adding the potential function method, inliers can also be detected","['outlier detection', 'prediction sample', 'calibration samples', 'uncertainty estimation', 'convex hull method', 'Mahalanobis distance', 'X-residuals', 'potential functions', 'robust techniques', 'inliers', 'calibration', 'chemistry computing', 'uncertainty handling']","['uncertainty estimation', 'convex hull method', 'predictions', 'such prediction outliers', 'Classical techniques', 'potential functions', 'calibration samples', 'robust techniques', 'outlier detection', 'outlier']",552,97,13,552,95,10,307,89,3
"fractional differentiation in passive vibration control from a single-degree-of-freedom model used to illustrate the concept of vibration isolation, a method to transform the design for a suspension into a design for a robust controller is presented. fractional differentiation is used to model the viscoelastic behaviour of the suspension. the use of fractional differentiation not only permits optimisation of just four suspension parameters, showing the 'compactness' of the fractional derivative operator, but also leads to robustness of the suspension's performance to uncertainty of the sprung mass. as an example, an engine suspension is studied ","Fractional differentiation in passive vibration control From a single-degree-of-freedom model used to illustrate the concept of vibration isolation, a method to transform the design for a suspension into a design for a robust controller is presented. Fractional differentiation is used to model the viscoelastic behaviour of the suspension. The use of fractional differentiation not only permits optimisation of just four suspension parameters, showing the ‘compactness’ of the fractional derivative operator, but also leads to robustness of the suspension's performance to uncertainty of the sprung mass. As an example, an engine suspension is studied","['fractional differentiation', 'passive vibration control', 'vibration isolation', 'suspension', 'robust controller', 'viscoelastic behaviour', 'sprung mass', 'engine suspension', 'damping', 'differentiation', 'engines', 'vibration isolation', 'viscoelasticity']","['suspension', 'passive vibration control', 'suspensions performance', 'suspension parameters', 'vibration isolation', 'robust controller', 'engine suspension', 'differentiation', 'fractional differentiation', 'Fractional differentiation']",560,94,13,560,93,10,2,1,2
"how much should publishers spend on technology? a study confirms that spending on publishing-specific information technology (it) resources is growing much faster than it spending for general business activities, at least among leading publishers in the scientific, technical and medical (stm) market. the survey asked about information technology funding and staffing levels-past, present and future-and also inquired about activities in content management, web delivery, computer support and customer relationship management. the results provide a starting point for measuring information technology growth and budget allocations in this publishing segment ","How much should publishers spend on technology? A study confirms that spending on publishing-specific information technology (IT) resources is growing much faster than IT spending for general business activities, at least among leading publishers in the scientific, technical and medical (STM) market. The survey asked about information technology funding and staffing levels-past, present and future-and also inquired about activities in content management, Web delivery, computer support and customer relationship management. The results provide a starting point for measuring information technology growth and budget allocations in this publishing segment","['IT spending', 'content management', 'Web delivery', 'publishing', 'budget', 'computer support', 'customer relationship management', 'DP management', 'publishing']","['technology', 'publishers', 'publishing-specific information technology', 'information technology funding', 'information technology growth', 'general business activities', 'technology A study', 'publishing segment', 'much', 'information']",571,89,9,571,88,10,0,0,6
"comments on some recent methods for the simultaneous determination of polynomial zeros in this note we give some comments on the recent results concerning a simultaneous method of the fourth-order for finding complex zeros in circular interval arithmetic. the main discussion is directed to a rediscovered iterative formula and its modification, presented recently in sun and kosmol, (2001). the presented comments include some critical parts of the papers petkovic, trickovic, herceg, (1998) and sun and kosmol, (2001) which treat the same subject ","Comments on some recent methods for the simultaneous determination of polynomial zeros In this note we give some comments on the recent results concerning a simultaneous method of the fourth-order for finding complex zeros in circular interval arithmetic. The main discussion is directed to a rediscovered iterative formula and its modification, presented recently in Sun and Kosmol, (2001). The presented comments include some critical parts of the papers Petkovic, Trickovic, Herceg, (1998) and Sun and Kosmol, (2001) which treat the same subject","['polynomial', 'zeros', 'complex zeros', 'circular interval arithmetic', 'iterative formula', 'iterative methods', 'poles and zeros', 'polynomials']","['circular interval arithmetic', 'simultaneous determination', 'simultaneous method', 'polynomial zeros', 'recent results', 'recent methods', 'complex zeros', 'recent', 'simultaneous', 'method']",467,83,8,467,82,10,0,0,1
"integrated support based on task models for the design, evaluation, and documentation of interactive safety-critical systems: a case study in the air-traffic control domain this paper presents an approach to using task models in both the design and the evaluation phases of interactive safety-critical applications. we explain how it is possible to use information contained in task models to support the design and development of effective user interfaces. moreover, we show how task models can also support a systematic inspection-based usability assessment by examining possible deviations that can occur while users interact with the system, an important issue especially when coping with the peculiar requirements of safety-critical applications. such evaluation provides useful technical documentation to help users achieve an in-depth understanding of the system and its design rationale. lastly, a description of the application of our approach to a real case study in the air-traffic control domain will illustrate the main features of the proposed method. in particular, we discuss examples taken from an application for air-traffic controllers in an aerodrome supported by graphical user interfaces for data-link communications with pilots ","Integrated support based on task models for the design, evaluation, and documentation of interactive safety-critical systems: a case study in the air-traffic control domain This paper presents an approach to using task models in both the design and the evaluation phases of interactive safety-critical applications. We explain how it is possible to use information contained in task models to support the design and development of effective user interfaces. Moreover, we show how task models can also support a systematic inspection-based usability assessment by examining possible deviations that can occur while users interact with the system, an important issue especially when coping with the peculiar requirements of safety-critical applications. Such evaluation provides useful technical documentation to help users achieve an in-depth understanding of the system and its design rationale. Lastly, a description of the application of our approach to a real case study in the air-traffic control domain will ilustrate the main features of the proposed method. In particular, we discuss examples taken from an application for air-traffic controllers in an aerodrome supported by graphical user interfaces for data-link communications with pilots","['integrated support', 'user interfaces', 'inspection-based usability assessment', 'technical documentation', 'graphical user interfaces', 'data-link communications', 'task models', 'interactive safety-critical systems', 'air-traffic control domain', 'air traffic control', 'graphical user interfaces', 'safety-critical software', 'system documentation']","['task models', 'air-traffic control domain', 'interactive safety-critical applications', 'interactive safety-critical systems', 'effective user interfaces', 'Integrated support', 'design evaluation', 'safety-critical applications', 'task', 'models']",1070,182,13,1069,181,10,7,1,3
"numerical behaviour of stable and unstable solitary waves in this paper we analyse the behaviour in time of the numerical approximations to solitary wave solutions of the generalized benjamin-bona-mahony equation. this equation possesses an important property: the stability of these solutions depends on their velocity. we identify the error propagation mechanisms in both the stable and unstable case. in particular, we show that in the stable case, numerical methods that preserve some conserved quantities of the problem are more appropriate for the simulation of this kind of solutions ","Numerical behaviour of stable and unstable solitary waves In this paper we analyse the behaviour in time of the numerical approximations. to solitary wave solutions of the generalized Benjamin-Bona-Mahony equation. This equation possesses an important property: the stability of these solutions depends on their velocity. We identify the error propagation mechanisms in both the stable and unstable case. In Particular, we show that in the stable case, numerical methods that preserve some conserved quantities of the problem are more appropriate for the simulation of this kind of solutions","['numerical behaviour', 'unstable solitary waves', 'numerical approximations', 'generalized Benjamin-Bona-Mahony equation', 'error propagation mechanisms', 'numerical methods', 'stable solitary waves', 'error analysis', 'numerical analysis', 'solitons']","['stable', 'numerical approximations', 'unstable solitary waves', 'solitary wave solutions', 'Numerical behaviour', 'numerical methods', 'unstable case', 'unstable', 'behaviour', 'stable case']",503,89,10,504,88,10,0,1,2
"the theory of information reversal the end of the industrial age coincides with the advent of the information society as the next model of social and economic organization, which brings about significant changes in the way modern man conceives work and the social environment. the functional basis of the new model is pivoted upon the effort to formulate the theory on the violent reversal of the basic relationship between man and information, and isolate it as one of the components for the creation of the new electronic reality. the objective of the theory of reversal is to effectively contribute to the formulation of a new definition consideration in regards to the concept of the emerging information society. in order to empirically apply the theory of reversal, we examine a case study based on the example of the digital library ","The theory of information reversal The end of the industrial age coincides with the advent of the information society as the next model of social and economic organization, which brings about significant changes in the way modern man conceives work and the social environment. The functional basis of the new model is pivoted upon the effort to formulate the theory on the violent reversal of the basic relationship between man and information, and isolate it as one of the components for the creation of the new electronic reality. The objective of the theory of reversal is to effectively contribute to the formulation of a new definition consideration in regards to the concept of the emerging information society. In order to empirically apply the theory of reversal, we examine a case study based ‘on the example of the digital library","['information reversal theory', 'information society', 'industrial age', 'social organization', 'economic organization', 'case study', 'digital library', 'information systems', 'digital libraries', 'information systems', 'social aspects of automation']","['theory', 'information society', 'new definition consideration', 'new electronic reality', 'information reversal', 'violent reversal', 'next model', 'new model', 'reversal', 'information']",701,140,11,702,139,10,2,1,4
"leveraging an alternative source of computer scientists: reentry programs much has been written about the leaky pipeline of women in computer science (cs), with the percentage of women decreasing as one moves from lower levels, such as college, to higher levels, culminating in full professorship. while significant attention focused on keeping women from leaving the pipeline, there is also an opportunity to bring women into the pipeline through non-traditional programs, instead of requiring that everyone enter at the undergraduate level. both mills college, a small liberal arts institution for women, and uc berkeley, a large research university, established programs in the 80's to increase the number of women in computer science by tapping non-traditional students. both programs share the core value of accommodating older students lacking technical backgrounds. the two programs have produced similar results: graduate degrees earned in computer science by students who would not have qualified without these programs, professional employment in the computer science field by women and minorities, and a recognition that this population represents a rich source of talent for our nation ","Leveraging an alternative source of computer scientists: reentry programs Much has been written about the leaky pipeline of women in computer science (CS), with the percentage of women decreasing as one moves from lower levels, such as college, to higher levels, culminating in full professorship. While significant attention focused on keeping women from leaving the pipeline, there is also an opportunity to bring women into the pipeline through non-traditional programs, instead of requiring that everyone enter at the undergraduate level. Both Mills College, a small liberal arts institution for women, and UC Berkeley, a large research university, established programs in the 80's to increase the number of women in computer science by tapping non-traditional students. Both programs share the core value of accommodating older students lacking technical backgrounds. The two programs have produced similar results: graduate degrees earned in computer science by students who would not have qualified without these programs, professional employment in the computer science field by women and minorities, and a recognition that this population represents a rich source of talent for our nation","['reentry programs', 'computer science', 'women', 'Mills College', 'UC Berkeley', 'graduate degrees', 'students', 'professional employment', 'minorities', 'computer science education', 'educational courses', 'gender issues']","['women', 'programs', 'non-traditional programs', 'computer science field', 'established programs', 'computer scientists', 'alternative source', 're-entry programs', 'computer', 'computer science']",1021,178,12,1021,177,10,0,0,6
"distributed servers approach for large-scale secure multicast in order to offer backward and forward secrecy for multicast applications (i.e., a new member cannot decrypt the multicast data sent before its joining and a former member cannot decrypt the data sent after its leaving), the data encryption key has to be changed whenever a user joins or leaves the system. such a change has to be made known to all the current users. the bandwidth used for such re-key messaging can be high when the user pool is large. we propose a distributed servers approach to minimize the overall system bandwidth (and complexity) by splitting the user pool into multiple groups each served by a (logical) server. after presenting an analytic model for the system based on a hierarchical key tree, we show that there is an optimal number of servers to achieve minimum system bandwidth. as the underlying user traffic fluctuates, we propose a simple dynamic scheme with low overhead where a physical server adaptively splits and merges its traffic into multiple groups each served by a logical server so as to minimize its total bandwidth. our results show that a distributed servers approach is able to substantially reduce the total bandwidth required as compared with the traditional single-server approach, especially for those applications with a large user pool, short holding time, and relatively low bandwidth of a data stream, as in the internet stock quote applications ","Distributed servers approach for large-scale secure multicast In order to offer backward and forward secrecy for multicast applications, (Le., anew member cannot decrypt the multicast data sent before its joining and a former member cannot decrypt the data sent after its leaving), the data encryption key has to be changed whenever a user Joins or leaves the system. Such a change has to be made known to all the current users. The bandwidth used for such re-key messaging can be high when the user pool is large. We propose a distributed servers approach to minimize the overall system bandwidth (and complexity) by splitting the user pool into multiple groups each served by a (logical) server. After presenting an analytic model for the system based on a hierarchical key tree, we show that there is an optimal number of servers to achieve minimum system bandwidth. As the underlying user traffic fluctuates, we propose a simple dynamic scheme with low ‘overhead where a physical server adaptively splits and merges its traffic into multiple groups each served by a logical server so as to minimize its total bandwidth. Our results show that a distributed servers approach is able to substantially reduce the total bandwidth required as compared with the traditional single-server approach, especially for those applications with a large user pool, short holding time, and relatively low bandwidth of a data stream, as in the Internet stock quote applications","['distributed servers', 'large-scale secure multicast', 'backward secrecy', 'forward secrecy', 'multicast applications', 'data encryption key', 're-key messaging', 'system bandwidth', 'system complexity', 'hierarchical key tree', 'user traffic', 'traffic merging', 'short holding time', 'Internet stock quote applications', 'dynamic split-and-merge scheme', 'key management', 'cryptography', 'distributed processing', 'Internet', 'large-scale systems', 'multicast communication', 'stock markets', 'telecommunication traffic']","['servers approach', 'logical server', 'overall system bandwidth', 'minimum system bandwidth', 'multicast applications', 'former member cannon', 'physical server', 'large user pool', 'multicast data', 'servers']",1227,238,23,1228,236,10,695,219,5
"it security issues: the need for end user oriented research considerable attention has been given to the technical and policy issues involved with it security issues in recent years. the growth of e-commerce and the internet, as well as widely publicized hacker attacks, have brought it security into prominent focus and routine corporate attention. yet, much more research is needed from the end user (eu) perspective. this position paper is a call for such research and outlines some possible directions of interest ","IT security issues: the need for end user oriented research Considerable attention has been given to the technical and policy issues involved with IT security issues in recent years. The growth of e-commerce and the Internet, as well as widely publicized hacker attacks, have brought IT security into prominent focus and routine corporate attention. Yet, much more research is needed from the end user (EU) perspective. This position paper is a call for such research and outlines some possible directions of interest","['IT security', 'end user oriented research', 'e-commerce', 'Internet', 'hacker attacks', 'information technology research', 'end user computing', 'information technology', 'personal computing', 'security of data']","['security issues', 'end user', 'routine corporate attention', 'Considerable attention', 'such research', 'policy issues', 'recent years', 'issues', 'research', 'security']",436,83,10,436,82,10,0,0,3
"second term [international telecommunication union] later this month yoshio utsumi is expected to be re-elected for a second four year term as secretary general of the international telecommunication union. here he talks to matthew may about getting involved in internet addressing, the prospects for 3g, the need for further reform of his organisation... and the translating telephone ","Second term [International Telecommunication Union] Later this month Yoshio Utsumi is expected to be re-elected for a second four year term as secretary general of the International Telecommunication Union. Here he talks to Matthew May about getting involved in internet addressing, the prospects for 3g, the need for further reform of his organisation... and the translating telephone","['International Telecommunication Union', 'internet addressing', 'translating telephone', '3G', 'telecommunication']","['month toshio Utsumi', 'secretary general', 'Second term', 'year term', 'term', 'union', 'Second', 'Telecommunication', 'international Telecommunication union', 'International Telecommunication union']",329,58,5,329,57,10,0,0,3
"full-screen ultrafast video modes over-clocked by simple vesa routines and registers reprogramming under ms-dos fast full-screen presentation of stimuli is necessary in psychological research. although spitczok von brisinski (1994) introduced a method that achieved ultrafast display by reprogramming the registers, he could not produce an acceptable full-screen display. in this report, the author introduces a new method combining vesa routine calling with register reprogramming that can yield a display at 640 * 480 resolution, with a refresh rate of about 150 hz ","Full-screen ultrafast video modes over-clocked by simple VESA routines and registers reprogramming under MS-DOS Fast full-screen presentation of stimuli is necessary in psychological research. Although Spitczok von Brisinski (1994) introduced a method that achieved ultrafast display by reprogramming the registers, he could not produce an acceptable full-screen display. In this report, the author introduces a new method combining VESA routine calling with register reprogramming that can yield a display at 640 * 480 resolution, with a refresh rate of about 150 Hz","['full-screen ultrafast video modes', 'fast full-screen stimuli presentation', 'psychological research', 'VESA routine calling', 'MS-DOS', 'register reprogramming', 'computer displays', 'psychology', 'video signal processing']","['acceptable full-screen display', 'Fast full-screen presentation', 'register re-programming', 'Spitczok von Brisinski', 'simple VESA routines', 'ultrafax display', 'new method', 'Full-screen', 'display', 'ultrafax']",486,83,9,486,82,10,0,0,2
"in search of strategic operations research/management science we define strategic or/ms as ""or/ms work that leads to a sustainable competitive advantage."" we found evidence of strategic or/ms in the literature of strategic information systems (sis) and or/ms. we examined 30 early examples of sis, many of which contained or/ms work. many of the most successful had high or/ms content, while the least successful contained none. the inclusion of or/ms work may be a key to sustaining an advantage from information technology. we also examined the edelman prize finalist articles published between 1990 and 1999. we found that 13 of the 42 private sector applications meet our definition of strategic or/ms ","In search of strategic operations research/management science We define strategic OR/MS as ""OR/MS work that leads to a sustainable competitive advantage.” We found evidence of strategic OR/MS in the literature of strategic information systems (SIS) and OR/MS. We ‘examined 30 early examples of SIS, many of which contained OR/MS work. Many of the most successful had high OR/MS content, while the least successful contained none. The inclusion of OR/MS work may be a key to sustaining an advantage from information technology. We also examined the Edelman Prize finalist articles published between 1990 and 1999. We found that 13 of the 42 private sector applications meet our definition of strategic ORIMS","['operations research', 'management science', 'strategic OR/MS', 'strategic information systems', 'SIS', 'information systems', 'management science', 'operations research']","['strategic forms', 'strategic', 'sustainable competitive advantage', 'research/management science', 'forms', 'strategic operations', 'high forms content', 'strategic rims', 'search', 'research/management']",596,111,8,597,110,10,10,3,2
solution of a euclidean combinatorial optimization problem by the dynamic-programming method a class of euclidean combinatorial optimization problems is selected that can be solved by the dynamic programming method. the problem of allocation of servicing enterprises is solved as an example ,Solution of a Euclidean combinatorial optimization problem by the ‘dynamic-programming method A.class of Euclidean combinatorial optimization problems is selected that can be solved by the dynamic programming method. The problem of allocation of servicing enterprises is solved as an example,"['Euclidean combinatorial optimization problem', 'dynamic programming method', 'computational geometry', 'dynamic programming', 'optimisation']","['problem', 'combinatorial', 'optimization', 'Euclidean', 'dynamic-programming method class', 'dynamic programming method', 'servicing enterprises', 'Solution', 'method', 'dynamic-programming']",250,42,5,252,40,10,118,30,0
"efficient simplicial reconstructions of manifolds from their samples an algorithm for manifold learning is presented. given only samples of a finite-dimensional differentiable manifold and no a priori knowledge of the manifold's geometry or topology except for its dimension, the goal is to find a description of the manifold. the learned manifold must approximate the true manifold well, both geometrically and topologically, when the sampling density is sufficiently high. the proposed algorithm constructs a simplicial complex based on approximations to the tangent bundle of the manifold. an important property of the algorithm is that its complexity depends on the dimension of the manifold, rather than that of the embedding space. successful examples are presented in the cases of learning curves in the plane, curves in space, and surfaces in space; in addition, a case when the algorithm fails is analyzed ","Efficient simplicial reconstructions of manifolds from their samples An algorithm for manifold learning is presented. Given only samples of a finite-dimensional differentiable manifold and no a priori knowledge of the manifold's geometry or topology except for its dimension, the goal is to find a description of the manifold. The learned manifold must approximate the true manifold well, both geometrically and topologically, when the sampling density is sufficiently high. The proposed algorithm constructs a simplicial complex based on approximations to the tangent bundle of the manifold. An important property of the algorithm is that its complexity depends on the dimension of the manifold, rather than that of the embedding space. Successful examples are presented in the cases of learning curves in the plane, curves in space, and surfaces in space; in addition, a case when the algorithm fails is analyzed","['simplicial reconstructions', 'manifold learning', 'finite-dimensional differentiable manifold', 'learned manifold', 'true manifold', 'simplicial complex', 'sampling density', 'computational complexity', 'computational geometry', 'computer vision', 'Hilbert spaces', 'learning (artificial intelligence)', 'topology']","['manifold', 'finite-dimensional differentiable manifold', 'Efficient simplicity reconstructions', 'algorithm constructs', 'simplicity complex', 'manifolds geometry', 'manifold learning', 'sampling density', 'sampling', 'algorithm']",776,140,13,776,139,10,0,0,1
"a spatial rainfall simulator for crop production modeling in southern africa this paper describes a methodology for simulating rainfall in dekads across a set of spatial units in areas where long-term meteorological records are available for a small number of sites only. the work forms part of a larger simulation model of the food system in a district of zimbabwe, which includes a crop production component for yields of maize, small grains and groundnuts. only a limited number of meteorological stations are available within or surrounding the district that have long time series of rainfall records. preliminary analysis of rainfall data for these stations suggested that intra-seasonal temporal correlation was negligible, but that rainfall at any given station was correlated with rainfall at neighbouring stations. this spatial correlation structure can be modeled using a multivariate normal distribution consisting of 30 related variables, representing dekadly rainfall in each of the 30 wards. for each ward, log-transformed rainfall for each of the 36 dekads in the year was characterized by a mean and standard deviation, which were interpolated from surrounding meteorological stations. a covariance matrix derived from a distance measure was then used to represent the spatial correlation between wards. sets of random numbers were then drawn from this distribution to simulate rainfall across the wards in any given dekad. cross-validation of estimated rainfall parameters against observed parameters for the one meteorological station within the district suggests that the interpolation process works well. the methodology developed is useful in situations where long-term climatic records are scarce and where rainfall shows pronounced spatial correlation, but negligible temporal correlation ","A spatial rainfall simulator for crop production modeling in Southern Africa This paper describes a methodology for simulating rainfall in dekads across a set of spatial units in areas where long-term meteorological records are avallable for a small number of sites only. The work forms part of a larger simulation model of the food system in a district of Zimbabwe, which includes a crop production component for yields of maize, small grains and groundnuts. Only a limited number of meteorological stations are available within or surrounding the district that have long time series of rainfall records. Preliminary analysis of rainfall data for these stations suggested that intra-seasonal temporal correlation was negligible, but that rainfall at any given station was correlated with rainfall at neighbouring stations. This spatial correlation structure ‘can be modeled using a multivariate normal distribution consisting of 30 related variables, representing dekadly rainfall in each of the 30 wards. For each ward, log-transformed rainfall for each of the 36 dekads in the year was characterized by a mean and standard deviation, which were interpolated from surrounding meteorological stations. A covariance matrix derived from a distance measure was then used to represent the spatial correlation between wards. Sets of random numbers were then drawn from this distribution to simulate rainfall across the wards in any given dekad. Cross-validation of estimated rainfall parameters against observed parameters for the one meteorological station within the district suggests that the interpolation process works well. The methodology developed is useful in situations where long-term climatic records are scarce and where rainfall shows pronounced spatial correlation, but negligible temporal correlation","['simulating rainfall', 'crop production modeling', 'Zimbabwe', 'covariance matrix', 'rainfall records', 'rainfall data', 'spatial correlation', 'multivariate normal distribution', 'parameter estimation', 'Southern Africa', 'agriculture', 'modelling', 'parameter estimation', 'rain', 'weather forecasting']","['rainfall', 'pronounced spatial correlation', 'spatial correlation structure', 'spatial rainfall simulator', 'log-transformed rainfall', 'rainfall parameters', 'rainfall records', 'deadly rainfall', 'rainfall data', 'spatial correlation']",1547,266,15,1548,265,10,4,2,6
"reconstructing surfaces by volumetric regularization using radial basis functions we present a new method of surface reconstruction that generates smooth and seamless models from sparse, noisy, nonuniform, and low resolution range data. data acquisition techniques from computer vision, such as stereo range images and space carving, produce 3d point sets that are imprecise and nonuniform when compared to laser or optical range scanners. traditional reconstruction algorithms designed for dense and precise data do not produce smooth reconstructions when applied to vision-based data sets. our method constructs a 3d implicit surface, formulated as a sum of weighted radial basis functions. we achieve three primary advantages over existing algorithms: (1) the implicit functions we construct estimate the surface well in regions where there is little data, (2) the reconstructed surface is insensitive to noise in data acquisition because we can allow the surface to approximate, rather than exactly interpolate, the data, and (3) the reconstructed surface is locally detailed, yet globally smooth, because we use radial basis functions that achieve multiple orders of smoothness ","Reconstructing surfaces by volumetric regularization using radial basis functions. We present a new method of surface reconstruction that generates smooth and seamless models from sparse, noisy, nonuniform, and low resolution range data. Data acquisition techniques from computer vision, such as stereo range images and space carving, produce 3D point sets that are imprecise and nonuniform when compared to laser or optical range scanners. Traditional reconstruction algorithms designed for dense and precise data do not produce smooth reconstructions when applied to vision-based data sets. Our method constructs a 3D implicit surface, formulated as a sum of weighted radial basis functions. We achieve three primary advantages over existing algorithms: (1) the implicit functions we construct estimate the surface well in regions where there is little data, (2) the reconstructed surface is insensitive to noise in data acquisition because we can allow the surface to approximate, rather than exactly interpolate, the data, and (3) the reconstructed surface is locally detailed, yet globally smooth, because we use radial basis functions that achieve multiple orders of smoothness","['surfaces reconstruction', 'volumetric regularization', 'radial basis functions', 'sparse range data', 'noisy data', 'nonuniform data', 'low resolution range data', 'data acquisition techniques', 'computer vision', 'stereo range images', 'space carving', '3D point sets', 'vision-based data sets', '3D implicit surface', 'weighted radial basis functions', 'computer vision', 'image reconstruction', 'image texture', 'interpolation', 'surface fitting']","['radial basis functions', 'reconstructed surface', 'data acquisition', 'vision-based data sets', 'surface reconstruction', 'implicit functions', 'implicit surface', 'precise data', 'little data', 'surfaces']",1011,173,20,1012,172,10,0,1,5
exact frequency-domain reconstruction for thermoacoustic tomography. i. planar geometry we report an exact and fast fourier-domain reconstruction algorithm for thermoacoustic tomography in a planar configuration assuming thermal confinement and constant acoustic speed. the effects of the finite size of the detector and the finite length of the excitation pulse are explicitly included in the reconstruction algorithm. the algorithm is numerically and experimentally verified. we also demonstrate that the blurring caused by the finite size of the detector surface is the primary limiting factor on the resolution and that it can be compensated for by deconvolution ,Exact frequency-domain reconstruction for thermoacoustic tomography. |. Planar geometry We report an exact and fast Fourier-domain reconstruction algorithm for thermoacoustic tomography in a planar configuration assuming thermal confinement and constant acoustic speed. The effects of the finite size of the detector and the finite length of the excitation pulse are explicitly included in the reconstruction algorithm. The algorithm is numerically and experimentally verified. We also demonstrate that the blurring caused by the finite size of the detector surface is the primary limiting factor on the resolution and that it can be ‘compensated for by deconvolution,"['medical diagnostic imaging', 'exact frequency-domain reconstruction', 'planar configuration', 'thermal confinement', 'constant acoustic speed', 'blurring', 'finite detector surface size', 'primary limiting factor', 'deconvolution', 'resolution limitation', 'excitation pulse', 'reconstruction algorithm', 'thermoacoustic tomography', 'planar geometry', 'acoustic tomography', 'biomedical ultrasonics', 'image reconstruction', 'medical image processing', 'thermoacoustics']","['thermoacoustic tomography', 'reconstruction algorithm', 'finite size', 'Exact frequency-domain reconstruction', 'finite length', 'reconstruction', 'Exact', 'algorithm', 'tomography', 'thermoacoustic']",572,96,19,573,95,10,12,2,8
"understanding internet traffic streams: dragonflies and tortoises we present the concept of network traffic streams and the ways they aggregate into flows through internet links. we describe a method of measuring the size and lifetime of internet streams, and use this method to characterize traffic distributions at two different sites. we find that although most streams (about 45 percent of them) are dragonflies, lasting less than 2 seconds, a significant number of streams have lifetimes of hours to days, and can carry a high proportion (50-60 percent) of the total bytes on a given link. we define tortoises as streams that last longer than 15 minutes. we point out that streams can be classified not only by lifetime (dragonflies and tortoises) but also by size (mice and elephants), and note that stream size and lifetime are independent dimensions. we submit that isps need to be aware of the distribution of internet stream sizes, and the impact of the difference in behavior between short and long streams. in particular, any forwarding cache mechanisms in internet routers must be able to cope with a high volume of short streams. in addition isps should realize that long-running streams can contribute a significant fraction of their packet and byte volumes-something they may not have allowed for when using traditional ""flat rate user bandwidth consumption"" approaches to provisioning and engineering ","Understanding Internet traffic streams: dragonflies and tortoises We present the concept of network traffic streams and the ways they aggregate into flows through Internet links. We describe a method of measuring the size and lifetime of Internet streams, and use this method to characterize traffic distributions at two different sites. We find that although most streams (about 45 percent of them) are dragonflies, lasting less than 2 seconds, a significant number of streams have lifetimes of hours to days, and can carry a high proportion (50-60 percent) of the total bytes on a given link. We define tortoises as streams that last longer than 15 minutes. We point out that streams can be classified not only by lifetime (dragonflies and tortoises) but also by size (mice and elephants), and note that stream size and lifetime are independent dimensions. We submit that ISPs need to be aware of the distribution of Internet stream sizes, and the impact of the difference in behavior between short and long streams. In particular, any forwarding cache mechanisms in Internet routers must be able to cope with a high volume of short streams. In addition ISPs should realize that long-running streams can contribute a significant fraction of their packet and byte volumes-something they may not have allowed for when using traditional ""flat rate user bandwidth consumption” approaches to provisioning and engineering","['Internet traffic streams', 'dragonflies', 'tortoises', 'network traffic streams', 'Internet stream size measurement', 'Internet stream lifetime measurement', 'traffic distributions', 'mice', 'elephants', 'ISP', 'forwarding cache mechanisms', 'Internet routers', 'long-running streams', 'packet volume', 'byte volume', 'traffic provisioning', 'traffic engineering', 'Internet', 'packet switching', 'performance evaluation', 'telecommunication network routing', 'telecommunication traffic']","['streams', 'network traffic streams', 'Internet stream sizes', 'long-running streams', 'Internet streams', 'Internet links', 'short streams', 'most streams', 'long streams', 'stream size']",1193,226,22,1193,225,10,1,1,7
"where have all the pc makers gone? pc makers are dwindling. if you are planning to make a pc purchase soon, here are a few things to look out for before you buy ","Where have all the PC makers gone? PC makers are dwindling. If you are planning to make a PC purchase soon, here are a few things to look out for before you buy","['PC purchase', 'PC makers', 'DP industry', 'microcomputers']","['PC makers', 'PC purchase', 'few things', 'PC', 'few', 'things', 'purchase', 'makers']",128,34,4,128,33,8,0,0,0
"creating the right mail model if you know your post room is not as efficiently organised as it might be, but you are not sure how best to go about making improvements, then consider this advice from john edgar of consultant mcs ","Creating the right mail model If you know your post room is not as efficiently organised as it might be, but you are not sure how best to go about making improvements, then consider this advice from John Edgar of consultant MCS","['mail', 'post room', 'MCS', 'consultant', 'mailing systems']","['right mail model', 'John Edgar', 'post room', 'sure', 'best', 'room', 'post', 'mail', 'right', 'model']",186,43,5,186,42,10,0,0,2
"computing 2002: democracy, education, and the future computer scientists, computer engineers, information technologists, and their collective products have grown and changed in quantity, quality, and nature. in the first decade of this new century, it should become apparent to everyone that the computing and information fields, broadly defined, will have a profound impact on every element of every person's life. the author considers how women and girls of the world have been neither educated for computing nor served by computing. globally, women's participation in computer science grew for a while, then dropped precipitously. computing, science, engineering, and society will suffer if this decline continues, because women have different perspectives on technology, what it is important for, how it should be built, which projects should be funded, and so on. to create a positive future, to assure that women equally influence the future, computing education must change ","Computing 2002: democracy, education, and the future Computer scientists, computer engineers, information technologists, and their collective products have grown and changed in quantity, quality, and nature. In the first decade of this new century, it should become apparent to everyone that the computing and information fields, broadly defined, will have a profound impact on every element of every person's life. The author considers how women and girls of the world have been neither educated for computing nor served by computing. Globally, women's participation in computer science grew for a while, then dropped precipitously. Computing, science, engineering, and society will suffer if this decline continues, because women have different perspectives on technology, what it is important for, how it should be built, which projects should be funded, and so on. To create a positive future, to assure that women equally influence the future, computing education must change","['computer science education', 'gender issues', 'future', 'women', 'girls', 'society', 'democracy', 'computer science education', 'gender issues', 'social aspects of automation']","['future computing education', 'womens participation', 'science engineering', 'democracy education', 'computer science', 'positive future', 'future', 'women', 'Computer', 'computing']",835,147,10,835,146,10,0,0,2
"survey says! [online world of polls and surveys] many content managers miss the fundamental interactivity of the web by not using polls and surveys. using interactive features-like a poll or quiz-offers your readers an opportunity to become more engaged in your content. using a survey to gather feedback about your content provides cost-effective data to help make modifications or plot the appropriate course of action. the web has allowed us to take traditional market research and turn it on its ear. surveys and polls can be conducted faster and cheaper than with telephone and mail. but if you are running a web site, should you care about polls and surveys? do you know the difference between the two in web-speak? ","Survey says! [online world of polls and surveys] Many content managers miss the fundamental interactivity of the Web by not using polls and surveys. Using interactive features-like a poll or quiz-offers your readers an opportunity to become more engaged in your content. Using a survey to gather feedback about your content provides cost-effective data to help make modifications or plot the appropriate course of action. The Web has allowed us to take traditional market research and turn it on its ear. Surveys and polls can be conducted faster and cheaper than with telephone and mail. But if you are running a Web site, should you care about polls and surveys? Do you know the difference between the two in Web-speak?","['site owners', 'polls', 'surveys', 'content managers', 'World Wide Web', 'site feedback', 'information resources']","['polls', 'traditional market research', 'fundamental interactivity', 'Many content managers', 'online world', 'ears Surveys', 'Web site', 'surveys', 'Surveys', 'content']",602,121,7,602,120,10,0,0,0
"all-optical xor gate using semiconductor optical amplifiers without additional input beam the novel design of an all-optical xor gate by using cross-gain modulation of semiconductor optical amplifiers has been suggested and demonstrated successfully at 10 gb/s. boolean ab and ab of the two input signals a and b have been obtained and combined to achieve the all-optical xor gate. no additional input beam such as a clock signal or continuous wave light is used in this new design, which is required in other all-optical xor gates ","All-optical XOR gate using semiconductor optical amplifiers without additional input beam The novel design of an all-optical XOR gate by using cross-gain modulation of semiconductor optical amplifiers has been suggested and demonstrated successfully at 10 Gb/s. Boolean AB and AB of the two input signals A and B have been obtained and combined to achieve the all-optical XOR gate. No additional input beam such as a clock signal or continuous wave light is used in this new design, which is required in other all-optical XOR gates","['semiconductor optical amplifiers', 'all-optical-XOR gate', 'design', 'cross-gain modulation', 'Boolean logic', '10 Gbit/s', 'optical design techniques', 'optical logic', 'optical modulation', 'semiconductor optical amplifiers']","['additional input beam', 'optical amplifiers', 'semiconductor', 'b.s. Boolean AB', 'input signals', 'novel design', 'new design', 'gates', 'input', 'optical']",446,87,10,446,86,10,0,0,1
a parareal in time procedure for the control of partial differential equations we have proposed in a previous note a time discretization for partial differential evolution equation that allows for parallel implementations. this scheme is here reinterpreted as a preconditioning procedure on an algebraic setting of the time discretization. this allows for extending the parallel methodology to the problem of optimal control for partial differential equations. we report a first numerical implementation that reveals a large interest ,"A parareal in time procedure for the control of partial differential equations We have proposed in a previous note a time discretization for partial differential evolution equation that allows for parallel implementations. This scheme is here reinterpreted as a preconditioning procedure on an algebraic setting of the time discretization. This, allows for extending the parallel methodology to the problem of optimal control for partial differential equations. We report a first numerical implementation that reveals a large interest","['time procedure', 'partial differential equation control', 'evolution equation', 'preconditioning procedure', 'Hilbert space', 'algebraic setting', 'time discretization', 'optimal control', 'Hilbert spaces', 'optimal control', 'partial differential equations']","['partial differential equations', 'time discretization', 'first numerical implementation', 'parallel implementations', 'optimal control', 'time procedure', 'time', 'partial', 'equations', 'differential']",457,78,11,458,77,10,0,1,2
"female computer science doctorates: what does the survey of earned doctorates reveal? based on the national center for education statistics (2000), in the 1997-1998 academic year 26.7% of earned bachelors' degrees, 29.0% of earned masters' degrees and 16.3% of earned doctorates' degrees in computer science were awarded to women. as these percentages suggest, women are underrepresented at all academic levels in computer science (camp, 1997). the most severe shortage occurs at the top level-the doctorate in computer science. we know very little about the women who persist to the top level of academic achievement in computer science. this paper examines a subset of data collected through the survey of earned doctorates (sed). the specific focus of this paper is to identify trends that have emerged from the sed with respect to females completing doctorates in computer science between the academic years 1990-1991 and 1999-2000. although computer science doctorates include doctorates in information science, prior research (camp, 1997) suggests that the percentage of women completing doctorates in information science as compared to computer science is low. the specific research questions are: 1. how does the percentage of women who complete doctorates in computer science compare to those that complete doctorates in other fields? 2. how does the length of time in school and the sources of funding differ for females as compared to males who complete doctorates in computer science? 3. where do women go after completing doctorates in computer science and what positions do they acquire? how do these experiences differ from their male peers? ","Female computer science doctorates: what does the survey of earned doctorates reveal? Based on the National Center for Education Statistics (2000), in the 1997-1998 academic year 26.7% of earned bachelors’ degrees, 29.0% of earned masters’ degrees and 16.3% of earned doctorates’ degrees in computer science were awarded to women. As these percentages suggest, women are underrepresented at all academic levels in computer science (Camp, 1997). The most severe shortage occurs at the top level-the doctorate in computer science. We know very little about the women who persist to the top level of academic achievement in computer science. This paper ‘examines a subset of data collected through the Survey of Earned Doctorates (SED). The specific focus of this paper is to identity trends that have emerged from the SED with respect to females completing doctorates in computer science between the academic years 1990-1991 and 1999-2000. Although computer science doctorates include doctorates in information science, prior research (Camp, 1997) suggests that the percentage of women completing doctorates in information science as compared to computer science is low. The specific research questions are: 1. How does the percentage of women who complete doctorates in computer science compare to those that complete doctorates in other fields? 2. How does the length of time in school and the sources of funding differ for females as compared to males who ‘complete doctorates in computer science? 3. Where do women go after ‘completing doctorates in computer science and what positions do they acquire? How do these experiences differ from their male peers?","['female computer science doctorates', 'Survey of Earned Doctorates', 'information science', 'computer science education', 'gender issues', 'information science']","['computer science', 'science', 'computer', 'complete doctorates', 'computer science doctorates', 'information science', 'top level-the doctorate', 'Female computer science', 'computer science camp', 'doctorates degrees']",1402,256,6,1405,255,10,30,7,0
"information architecture: looking ahead it may be a bit strange to consider where the field of information architecture (ia) is headed. after all, many would argue that it's too new to be considered as a field at all, or that it is mislabeled, and by no means is there a widely accepted definition of what information architecture actually is. practicing information architects probably number in the thousands, and this vibrant group is already building various forms of communal infrastructure, ranging from an ia journal and a self-organizing ""library"" of resources to a passel of local professional groups and degree-granting academic programs. so the profession has achieved a beachhead that will enable it to stabilize and perhaps even grow during these difficult times ","Information architecture: looking ahead It may be a bit strange to consider where the field of information architecture (IA) is headed. A¥ter all, many would argue that it's too new to be considered as a field at all, or that it is mislabeled, and by no means is there a widely accepted definition of what information architecture actually is. Practicing information architects probably number in the. thousands, and this vibrant group is already building various forms of ‘communal infrastructure, ranging from an IA journal and a self-organizing ""library"" of resources to a passel of local professional groups and degree-granting academic programs. So the profession has achieved a beachhead that will enable it to stabilize and perhaps even grow during these difficult times","['information architecture', 'information architects', 'communal infrastructure', 'local professional groups', 'degree-granting academic programs', 'electronic publishing', 'hypermedia', 'information resources']","['information architecture ian', 'field', 'information architects', 'strange', 'information', 'Information', 'bit', 'architecture', 'information architecture', 'Information architecture']",654,123,8,656,122,10,8,3,3
"input-output based pole-placement controller for a class of time-delay systems a controller structure valid for siso plants involving both internal and external point delays is presented. the control signal is based only on the input and output plant signals. the controller allows finite or infinite spectrum assignment. the most important feature of the proposed controller is that it only involves the use of a class of point-delayed signals. thus the controller synthesis involves less computational cost than former methods. since the plant control input is generated by filtering the input and output plant signals, this controller structure is potentially applicable to the adaptive case of unknown plant parameters ","Input-output based pole-placement controller for a class of time-delay systems, A controller structure valid for SISO plants involving both internal and external point delays is presented. The control signal is based only on the input and output plant signals. The controller allows finite or infinite spectrum assignment. The most important feature of the proposed controller is that it only involves the use of a class of Point-delayed signals. Thus the controller synthesis involves less computational cost than former methods. Since the plant control input is generated by filtering the input and output plant signals, this, controller structure is potentially applicable to the adaptive case of unknown plant parameters","['I/O-based pole-placement controller', 'input-output based pole-placement controller', 'time-delay systems', 'SISO plants', 'internal point delays', 'and external point delays', 'finite spectrum assignment', 'infinite spectrum assignment', 'point-delayed signals', 'controller synthesis', 'computational cost', 'filtering', 'adaptive control', 'computational complexity', 'control system synthesis', 'delay systems', 'filtering theory', 'pole assignment', 'uncertain systems']","['controller', 'output plant signals', 'controller structure', 'pole-placement controller', 'unknown plant parameters', 'controller synthesis', 'proposed controller', 'plant control input', 'control signal', 'sis plants']",615,109,19,617,108,10,0,2,7
"shortchanging the future of information technology: the untapped resource building on ideas from a virtual workshop and additional input from the scientific community, the cise directorate at the national science foundation established the information technology workforce program (itwf) in march 2000 to support a broad set of scientific research studies focused on the under-representation of women and minorities in the information technology workforce. in this paper, we explore various approaches that the funded researchers are taking to address the problem of women in information technology. we begin with a brief history of the itwf, and then focus on some of the research projects in terms of their goals, approaches, and expected outcomes ","Shortchanging the future of information technology: the untapped resource Building on ideas from a virtual workshop and additional input from the scientific community, the CISE Directorate at the National Science Foundation established the Information Technology Workforce Program (TWF) in March 2000 to support a broad set of scientific research studies focused on the under-representation of women and minorities in the information technology workforce. In this paper, we explore various approaches that the funded researchers are taking to address the problem of women in information technology. We begin with a brief history of the ITWF, and then focus on some of the research projects in terms of their goals, approaches, and expected outcomes","['information technology future', 'untapped resources', 'virtual workshop', 'CISE Directorate', 'National Science Foundation', 'Information Technology Workforce Program', 'ITWF', 'scientific research studies', 'women under-representation', 'history', 'computer science', 'gender issues', 'history', 'information technology', 'personnel']","['information technology workforce', 'scientific research studies', 'untapped resource Building', 'scientific community', 'funded researchers', 'research projects', 'virtual workshop', 'technology', 'information', 'information technology']",638,113,15,637,112,10,4,1,5
"a pretopological approach for structural analysis the aim of this paper is to present a methodological approach for problems encountered in structural analysis. this approach is based upon the pretopological concepts of pseudoclosure and minimal closed subsets. the advantage of this approach is that it provides a framework which is general enough to model and formulate different types of connections that exist between the elements of a population. in addition, it has enabled us to develop a new structural analysis algorithm. an explanation of the definitions and properties of the pretopological concepts applied in this work is first shown and illustrated in sample settings. the structural analysis algorithm is then described and the results obtained in an economic study of the impact of geographic proximity on scientific collaborations are presented ","A pretopological approach for structural analysis The aim of this paper is to present a methodological approach for problems encountered in structural analysis. This approach is based upon the pretopological concepts of pseudoclosure and minimal closed subsets. The advantage of this approach is that it provides a framework which is general enough to model and formulate different types of connections that exist between the elements of a population. In addition, it has enabled us to develop a new structural analysis algorithm. An explanation of the definitions and properties of the pretopological concepts applied in this work is first shown and illustrated in sample settings. The structural analysis algorithm is then described and the results obtained in an economic study of the impact of geographic proximity on scientific collaborations are presented","['pretopological approach', 'structural analysis', 'minimal closed subsets', 'pseudoclosure', 'connections', 'economic study', 'geographic proximity', 'scientific collaborations', 'set theory', 'topology']","['structural analysis algorithm', 'pretopological concepts', 'pretopological approach', 'methodological approach', 'minimal closed subsets', 'approach', 'pretopological', 'analysis', 'structural', 'structural analysis']",732,131,10,732,130,10,0,0,5
"a characterization of generalized pareto distributions by progressive censoring schemes and goodness-of-fit tests in this paper we generalize a characterization property of generalized pareto distributions, which is known for ordinary order statistics, to arbitrary schemes of progressive type-ii censored order statistics. various goodness-of-fit tests for generalized pareto distributions based on progressively censored data statistics are discussed ","A characterization of generalized Pareto distributions by progressive censoring schemes and goodness-of-fit tests In this paper we generalize a characterization property of generalized Pareto distributions, which is known for ordinary order statistics, to arbitrary schemes of progressive type-II censored order statistics. Various goodness-of-fit tests for generalized Pareto distributions based on progressively censored data statistics are discussed","['generalized Pareto distributions', 'progressive censoring schemes', 'goodness-of-fit tests', 'progressive type-II censored order statistics', 'ordinary order statistics', 'Pareto distribution']","['Pareto distributions', 'Various goodness-of-fit tests', 'ordinary order statistics', 'characterization property', 'progressive type-1', 'data statistics', 'order statistics', 'goodness-of-fit tests', 'Pareto', 'distributions']",397,57,6,397,56,10,0,0,0
knowledge flow management for distributed team software development cognitive cooperation is often neglected in current team software development processes. this issue becomes more important than ever when team members are globally distributed. this paper presents a notion of knowledge flow and the related management mechanism for realizing an ordered knowledge sharing and cognitive cooperation in a geographically distributed team software development process. the knowledge flow can carry and accumulate knowledge when it goes through from one team member to another. the coordination between the knowledge flow process and the workflow process of a development team provides a new way to improve traditional team software development processes. a knowledge grid platform has been implemented to support the knowledge flow management across the internet ,Knowledge flow management for distributed team software development Cognitive cooperation is often neglected in current team software development processes. This issue becomes more important than ever when team members are globally distributed. This paper presents a notion of knowledge flow and the related management mechanism for realizing an ordered knowledge sharing and cognitive cooperation in a geographically distributed team software development process. The knowledge flow can carry and accumulate knowledge when it goes through from one team member to another. The coordination between the knowledge flow process and the workflow process of a development team provides a new way to improve traditional team software development processes. A knowledge grid platform has been implemented to support the knowledge flow Management across the Internet,"['knowledge flow management', 'distributed team software development', 'cognitive cooperation', 'knowledge flow representation', 'ordered knowledge sharing', 'workflow process', 'knowledge grid platform', 'Internet', 'software development management', 'cooperative work', 'groupware', 'Internet', 'knowledge representation', 'software development management', 'workflow management software']","['team software development', 'knowledge flow', 'team', 'knowledge grid platform', 'knowledge flow process', 'knowledge sharing', 'development team', 'team members', 'knowledge flow Management', 'Knowledge flow management']",737,123,15,737,122,10,0,0,3
"designing and delivering a university course - a process (or operations) management perspective with over 30 years of academic experience in both engineering and management faculties, involving trial and error experimentation in teaching as well as reading relevant literature and observing other instructors in action, the author has accumulated a number of ideas, regarding the preparation and delivery of a university course, that should be of interest to other instructors. this should be particularly the case for those individuals who have had little or no teaching experience (e.g. those whose graduate education was recently completed at research-oriented institutions providing little guidance with respect to teaching). a particular perspective is used to convey the ideas, namely one of viewing the preparation and delivery of a course as two major processes that should provide outputs or outcomes that are of value to a number of customers, in particular, students ","Designing and delivering a university course - a process (or operations) management perspective With over 30 years of academic experience in both engineering and management faculties, involving trial and error experimentation in teaching as well as reading relevant literature and observing other instructors in action, the author has accumulated a number of ideas, regarding the preparation and delivery of a university course, that should be of interest to other instructors. This should be particularly the case for those individuals who have had little or no teaching experience (e.g those whose graduate education was recently completed at research-oriented institutions providing little guidance with respect to teaching). A particular perspective is used to convey the ideas, namely one of viewing the preparation and delivery of a course as two major processes that should provide outputs or outcomes that are of value to a number of customers, in particular, students","['university course delivery', 'management perspective', 'academic experience', 'management faculties', 'engineering faculties', 'search-oriented institutions', 'educational courses']","['university course', 'other instructors', 'operations management perspective', 'particular perspective', 'management faculties', 'teaching experience', 'major processes', 'process', 'course', 'university']",831,148,7,830,147,10,0,1,1
"on m/d/1 queue with deterministic server vacations we study a single server vacation queue with poisson arrivals, deterministic service of constant duration b (> 0) and deterministic vacations of constant duration d (> 0) and designate this model as m/d/d/1. after completion of each service, the server may take a vacation with probability p or may continue working in the system with probability 1 - p. we obtain time-dependent as well as steady state probability generation functions for the number in the system. for the steady state we obtain explicitly the mean number and the mean waiting time for the system and for the queue. all known results of the m/d/1 queue are derived as a special case. finally, a numerical illustration is discussed ","On M/D/1 queue with deterministic server vacations We study a single server vacation queue with Poisson arrivals, deterministic service of constant duration b (> 0) and deterministic vacations of constant duration d (> 0) and designate this model as M/D/D/1. After completion of each service, the server may take a vacation with probability p or may continue working in the system with probability 1 - p. We obtain time-dependent as well as steady state probability generation functions for the number in the system. For the steady state we obtain explicitly the mean number and the mean waiting time for the system and for the queue. All known results of the M/D/1 queue are derived as a special case. Finally, a numerical illustration is discussed","['M/D/1 queue', 'deterministic server vacations', 'Poisson arrivals', 'deterministic service', 'deterministic vacations', 'M/D/D/1 model', 'time-dependent probability generation functions', 'steady state probability generation functions', 'mean number', 'mean waiting time', 'probability', 'queueing theory', 'stochastic processes']","['M/D/1 queue', 'deterministic server vacations', 'deterministic vacations', 'deterministic service', 'constant duration d', 'constant duration b', 'deterministic', 'vacation', 'server', 'queue']",626,125,13,626,124,10,0,0,5
"noninvasive myocardial activation time imaging: a novel inverse algorithm applied to clinical ecg mapping data linear approaches like the minimum-norm least-square algorithm show insufficient performance when it comes to estimating the activation time map on the surface of the heart from electrocardiographic (ecg) mapping data. additional regularization has to be considered leading to a nonlinear problem formulation. the gauss-newton approach is one of the standard mathematical tools capable of solving this kind of problem. to our experience, this algorithm has specific drawbacks which are caused by the applied regularization procedure. in particular, under clinical conditions the amount of regularization cannot be determined clearly. for this reason, we have developed an iterative algorithm solving this nonlinear problem by a sequence of regularized linear problems. at each step of iteration, an individual l-curve is computed. subsequent iteration steps are performed with the individual optimal regularization parameter. this novel approach is compared with the standard gauss-newton approach. both methods are applied to simulated ecg mapping data as well as to single beat sinus rhythm data from two patients recorded in the catheter laboratory. the proposed approach shows excellent numerical and computational performance, even under clinical conditions at which the gauss-newton approach begins to break down ","Noninvasive myocardial activation time imaging: a novel inverse algorithm applied to clinical ECG mapping data Linear approaches like the minimum-norm least-square algorithm show insufficient performance when it comes to estimating the activation time map on the surface of the heart from electrocardiographic (ECG) mapping data. Additional regularization has to be considered leading to a nonlinear problem formulation. The Gauss-Newton approach is one of the standard mathematical tools capable of solving this kind of problem. To our experience, this algorithm has specific drawbacks which are caused by the applied regularization procedure. In particular, under clinical conditions the amount of regularization cannot be determined clearly. For this reason, we have developed an iterative algorithm solving this nonlinear problem by a sequence of regularized linear problems. At each step of iteration, an individual L-curve is ‘computed. Subsequent iteration steps are performed with the individual optimal regularization parameter. This novel approach is compared with the standard Gauss-Newton approach. Both methods are applied to simulated ECG mapping data as well as to single beat sinus rhythm data from two patients recorded in the catheter laboratory. The proposed approach shows excellent numerical and computational performance, even under clinical conditions at which the Gauss-Newton approach begins to break down’","['noninvasive myocardial activation time imaging', 'electrodiagnostics', 'activation time imaging', 'L-curve method', 'noninvasive electrocardiography', 'tikhonov regularization', 'Gauss-Newton approach', 'individual optimal regularization parameter', 'catheter laboratory', 'clinical conditions', 'iteration steps', 'heart surface', 'regularization procedure', 'inverse algorithm', 'clinical ECG mapping data', 'electrocardiography', 'inverse problems', 'iterative methods', 'medical image processing', 'muscle']","['minimum-norm least-squares algorithm', 'standard Gauss-Newton approach', 'novel inverse algorithm', 'iterative algorithm', 'activation time map', 'Linear approaches', 'novel approach', 'time', 'activation', 'Gauss-Newton approach']",1228,203,20,1230,202,10,9,2,6
"single and multi-interval legendre tau -methods in time for parabolic equations in this paper, we take the parabolic equation with periodic boundary conditions as a model to present a spectral method with the fourier approximation in spatial and single/multi-interval legendre petrov-galerkin methods in time. for the single interval spectral method in time, we obtain the optimal error estimate in l/sup 2/-norm. for the multi-interval spectral method in time, the l/sup 2/-optimal error estimate is valid in spatial. numerical results show the efficiency of the methods ","Single and mult:-interval Legendre tau -methods in time for parabolic equations In this paper, we take the parabolic equation with periodic boundary conditions as a model to present a spectral method with the Fourier approximation in spatial and single/mult:-interval Legendre Petrov-Galerkin methods in time. For the single interval spectral method in time, we obtain the optimal error estimate in L/sup 2/-norm. For the multi-interval spectral method in time, the L/sup 2/-optimal error estimate is valid in spatial. Numerical results show the efficiency of the methods","['interval decomposition', 'parabolic equation', 'interval spectral method', 'error analysis', 'optimal error estimation', 'periodic boundary conditions', 'Fourier approximation', 'Legendre Petrov-Galerkin method', 'partial differential equations', 'approximation theory', 'boundary-value problems', 'convergence of numerical methods', 'error analysis', 'optimisation', 'parabolic equations', 'partial differential equations']","['optimal error estimate', 'methods', 'parabolic equation', 'interval Legendre', 'time', 'multi-interval spectral method', 'interval', 'single interval', 'spectral method', 'Legendre']",487,86,16,487,85,10,2,2,2
"subseven's honey pot program a serious security threat today are malicious executables, especially new, unseen malicious executables often arriving as email attachments. these new malicious executables are created at the rate of thousands every year and pose a serious threat. current anti-virus systems attempt to detect these new malicious programs with heuristics generated by hand. this approach is costly and often ineffective. we introduce the trojan horse subseven, its capabilities and influence over intrusion detection systems. a honey pot program is implemented, simulating the subseven server. the honey pot program provides feedback and stores data to and from the subseven's client ","SubSeven's Honey Pot program A serious security threat today are malicious executables, especially new, unseen malicious executables often arriving as email attachments. These new malicious executables are created at the rate of thousands every year and pose a serious threat. Current anti-virus systems attempt to detect these new malicious programs with heuristics generated by hand. This approach is costly and often ineffective. We introduce the Trojan Horse SubSeven, its capabilities and influence over intrusion detection systems. A Honey Pot program is implemented, simulating the SubSeven Server. The Honey Pot Program provides feedback and stores data to and from the SubSeven's client","['Honey Pot program', 'security threat', 'malicious executables', 'email attachments', 'anti-virus systems', 'Trojan Horse', 'SubSeven', 'intrusion detection systems', 'computer viruses', 'electronic mail']","['intrusion detection systems', 'new malicious executables', 'new malicious programs', 'Trojan Horse SubSeven', 'serious threat', 'malicious', 'SubSeven', 'malicious executables', 'Honey Pot program', 'Honey Pot Program']",595,102,10,595,101,10,0,0,1
"is open source more or less secure? networks dominate today's computing landscape and commercial technical protection is lagging behind attack technology. as a result, protection programme success depends more on prudent management decisions than on the selection of technical safeguards. the paper takes a management view of protection and seeks to reconcile the need for security with the limitations of technology ","Is open source more or less secure? Networks dominate today's computing landscape and commercial technical protection is lagging behind attack technology. As a result, protection programme success depends more on prudent management decisions than on the selection of technical safeguards. The paper takes a management view of protection and seeks to reconcile the need for security with the limitations of technology","['open source software security', 'computer networks', 'commercial technical protection', 'attack technology', 'management', 'data security', 'computer networks', 'public domain software', 'security of data']","['commercial technical protection', 'prudent management decisions', 'technical safeguards', 'attack technology', 'secure Networks', 'management view', 'open source', 'security', 'Networks', 'protection']",356,62,9,356,61,10,0,0,0
"option pricing from path integral for non-gaussian fluctuations. natural martingale and application to truncated levy distributions within a path integral formalism for non-gaussian price fluctuations, we set up a simple stochastic calculus and derive a natural martingale for option pricing from the wealth balance of options, stocks, and bonds. the resulting formula is evaluated for truncated levy distributions ","Option pricing from path integral for non-Gaussian fluctuations. Natural martingale and application to truncated Levy distributions Within a path integral formalism for non-Gaussian price fluctuations, we set up a simple stochastic calculus and derive a natural martingale for option pricing from the wealth balance of options, stocks, and bonds. The resulting formula is evaluated for truncated Levy distributions","['option pricing', 'path integrals', 'stochastic calculus', 'stocks', 'bonds', 'nonGaussian fluctuations', 'natural martingale', 'truncated Levy distributions', 'fluctuations', 'integration', 'statistical mechanics', 'stochastic processes', 'stock markets']","['Levy distributions', 'non-russian price fluctuations', 'non-russian fluctuations', 'integral formalism', 'pricing', 'integral', 'option pricing', 'Option pricing', 'natural martindale', 'Natural martindale']",357,59,13,357,58,10,0,0,3
"a decision support model for selecting product/service benefit positionings the art (and science) of successful product/service positioning generally hinges on the firm's ability to select a set of attractively priced consumer benefits that are: valued by the buyer, distinctive in one or more respects, believable, deliverable, and sustainable (under actual or potential competitive abilities to imitate, neutralize, or overcome) in the target markets that the firm selects. for many years, the ubiquitous quadrant chart has been used to provide a simple graph of product/service benefits (usually called product/service attributes) described in terms of consumers' perceptions of the importance of attributes (to brand/supplier choice) and the performance of competing firms on these attributes. this paper describes a model that extends the quadrant chart concept to a decision support system that optimizes a firm's market share for a specified product/service. in particular, we describe a decision support model that utilizes relatively simple marketing research data on consumers' judged benefit importances, and supplier performances on these benefits to develop message components for specified target buyers. a case study is used to illustrate the model. the study deals with developing advertising message components for a relatively new entrant in the us air shipping market. we also discuss, more briefly, management reactions to application of the model to date, and areas for further research and model extension ","A decision support model for selecting product/service benefit positionings The art (and science) of successful product/service positioning generally hinges on the firm's ability to select a set of attractively priced ‘consumer benefits that are: valued by the buyer, distinctive in one or more respects, believable, deliverable, and sustainable (under actual or potential competitive abilities to imitate, neutralize, or overcome) in the target markets that the firm selects. For many years, the ubiquitous quadrant chart has been used to provide a simple graph of product/service benefits (usually called product/service attributes) described in terms of consumers’ perceptions of the importance of attributes (to brand/supplier choice) and the performance of competing firms on these attributes. This paper describes a model that extends the quadrant chart concept to a decision support system that optimizes a firm's market share for a specified product/service. In particular, we describe a decision support model that utilizes relatively simple marketing research data on consumers’ judged benefit importances, and supplier performances on these benefits to develop message components for specified target buyers. A case study is used to illustrate the model. The study deals with developing advertising message components for a relatively new entrant in the US air shipping market. We also discuss, more briefly, management reactions to application of the model to date, and areas for further research and model extension","['product/service benefit positionings', 'decision support model', 'attractively priced consumer benefits', 'quadrant chart', 'simple graph', 'product/service attributes', 'brand/supplier choice', 'market share optimization', 'marketing research data', 'consumer judged benefit importances', 'message components', 'advertising message components', 'US air shipping market', 'management reactions', 'advertising', 'greedy heuristic', 'optimal message design', 'advertising', 'decision support systems', 'marketing', 'sensitivity analysis']","['product/service', 'decision support model', 'successful product/service', 'product/service attributes', 'product/service benefits', 'decision support system', 'benefit importances', 'firms market share', 'consumer benefits', 'firms ability']",1306,223,21,1307,222,10,10,3,10
"simple...but complex flexpro 5.0, from weisang and co., is one of those products which aim to serve an often ignored range of data users: those who, in flexpro's words, are interested in documenting, analysing and archiving data in the simplest way possible. the online help system is clearly designed to promote the product in this market segment, with a very clear introduction from first principles and a hands-on tutorial, and the live project to which it was applied was selected with this in mind ","Simple...But complex FlexPro 5.0, from Weisang and Co., is one of those products which aim to serve an often ignored range of data users: those who, in FlexPro's words, are interested in documenting, analysing and archiving data in the simplest way possible. The online help system is clearly designed to promote the product in this market segment, with a very clear introduction from first principles and a hands-on tutorial, and the live project to which it was applied was selected with this in mind","['FlexPro 5.0', 'data archiving', 'data analysis', 'data documentation', 'online help system', 'hands-on tutorial', 'data analysis', 'software reviews', 'statistical databases']","['product', 'Simple', 'online help system', 'complex flexure', 'archiving data', 'data users', 'weismann', 'co.', 'flexure', 'complex']",419,85,9,419,84,10,0,0,2
optimization of the characteristics of computational processes in scalable resources the scalableness of resources is taken to mean the possibility of the prior change in the obtained dynamic characteristics of computational processes for a certain basic set of processors and the communication medium in an effort to optimize the dynamics of software applications. a method is put forward for the generation of optimal strategies-a set of the versions of the fulfillment of programs on the basis of a vector criterion. the method is urgent for the effective use of resources of computational clusters and metacomputational media and also for dynamic control of processes in real time on the basis of the static scaling ,Optimization of the characteristics of computational processes in scalable resources The scalableness of resources is taken to mean the possibility of the prior change in the obtained dynamic characteristics of computational processes for a certain basic set of processors and the communication medium in an effort to optimize the dynamics of software applications. ‘A method is put forward for the generation of optimal strategies-a set of the versions of the fulfilment of programs on the basis of a vector criterion. The method is urgent for the effective use of resources of computational clusters and metacomputational media and also for dynamic control of processes in real time on the basis of the static scaling,"['computational processes', 'scalable resources', 'dynamic characteristics', 'communication medium', 'software applications', 'optimal strategies', 'vector criterion', 'computational clusters', 'metacomputational media', 'dynamic control', 'static scaling', 'distributed processing', 'optimisation', 'resource allocation']","['computational processes', 'dynamic characteristics', 'computational clusters', 'scalable resources', 'certain basic set', 'dynamic control', 'computational', 'resources', 'characteristics', 'processes']",607,114,14,607,113,10,5,2,6
"application of hybrid models for prediction and optimization of enzyme fermentation process. a comparative study the paper presents a comparison of the biotechnological process prediction and optimization results obtained by using different structure hybrid mathematical models for modeling of the same bioprocess. the hybrid models under investigation consist of the product mass balance equation in which different means - an artificial neural network, fuzzy-neural network and cell age distribution based calculation scheme - are incorporated for modeling the specific biosynthesis rate of a desired product. experimental data from alpha -amylase laboratory and industrial fermentation processes are used for model parameter identification and the process prediction tests ","Application of hybrid models for prediction and optimization of enzyme fermentation process. A comparative study The paper presents a comparison of the biotechnological process prediction and optimization results obtained by using different structure hybrid mathematical models for modeling of the same bioprocess. The hybrid models under investigation consist of the product mass balance equation in which different means - an artificial neural network, fuzzy-neural network and cell age distribution based calculation scheme - are incorporated for modeling the specific biosynthesis rate of a desired product. Experimental data from alpha -amylase laboratory and industrial fermentation processes are used for model parameter identification and the process prediction tests","['mathematical models', 'bioprocess', 'hybrid models', 'product mass balance equation', 'fuzzy-neural network', 'cell age distribution', 'biosynthesis rate', 'enzyme fermentation', 'industrial processes', 'optimization', 'identification', 'biocontrol', 'fermentation', 'fuzzy neural nets', 'identification', 'optimisation', 'process control', 'proteins']","['hybrid models', 'process', 'biotechnological process prediction', 'industrial fermentation processes', 'model parameter identification', 'different structure hybrid', 'process prediction tests', 'mathematical models', 'models', 'hybrid']",670,107,18,670,106,10,0,0,6
"weighted energy linear quadratic regulator vibration control of piezoelectric composite plates in this paper on finite element linear quadratic regulator (lqr) vibration control of smart piezoelectric composite plates, we propose the use of the total weighted energy method to select the weighting matrices. by constructing the optimal performance function as a relative measure of the total kinetic energy, strain energy and input energy of the system, only three design variables need to be considered to achieve a balance between the desired higher damping effect and lower input cost. modal control analysis is used to interpret the effects of three energy weight factors on the damping ratios and modal voltages and it is shown that the modal damping effect will increase with the kinetic energy weight factor, approaching square root (2/2) as the strain energy weight factor increases and decrease with the input energy weight factor. numerical results agree well with those from the modal control analysis. since the control problem is simplified to three design variables only, the computational cost will be greatly reduced and a more accurate structural control analysis becomes more attractive for large systems ","Weighted energy linear quadratic regulator vibration control of piezoelectric composite plates In this paper on finite element linear quadratic regulator (LR) vibration control of smart piezoelectric composite plates, we propose the use of the total weighted energy method to select the weighting matrices. By constructing the optimal performance function as a relative measure of the total kinetic energy, strain energy and input energy of the system, only three design variables need to be considered to achieve a balance between the desired higher damping effect and lower input cost. Modal control analysis is used to interpret the effects of three energy weight factors on the damping ratios and modal voltages and it is shown that the modal damping effect will increase with the kinetic energy weight factor, approaching square root (2/2) as the strain energy weight factor increases and decrease with the input energy weight factor. Numerical results agree well with those from the modal control analysis. Since the control problem is simplified to three design variables only, the computational cost will be greatly reduced and a more accurate structural control analysis becomes more attractive for large systems","['finite element linear quadratic regulator', 'vibration control', 'smart piezoelectric composite plates', 'total weighted energy', 'weighting matrices', 'optimal performance function', 'total kinetic energy', 'strain energy', 'damping effect', 'modal control analysis', 'damping ratios', 'strain energy weight factor', 'numerical results', 'computational cost', 'structural control analysis', 'composite materials', 'control system analysis computing', 'feedback', 'finite element analysis', 'intelligent actuators', 'intelligent control', 'intelligent structures', 'optimal control', 'piezoelectric actuators', 'structural engineering computing', 'vibration control']","['energy weight factors', 'piezoelectric composite plates', 'energy', 'Weighted energy', 'input energy', 'control', 'weighting matrices', 'control problem', 'modal control analysis', 'Modal control analysis']",1036,188,26,1035,187,10,2,1,6
"an eight-year study of internet-based remote medical counselling we carried out a prospective study of an internet-based remote counselling service. a total of 15,456 internet users visited the web site over eight years. from these, 1500 users were randomly selected for analysis. medical counselling had been granted to 901 of the people requesting it (60%). one hundred and sixty-four physicians formed project groups to process the requests and responded using email. the distribution of patients using the service was similar to the availability of the internet: 78% were from the european union, north america and australia. sixty-seven per cent of the patients lived in urban areas and the remainder were residents of remote rural areas with limited local medical coverage. sixty-five per cent of the requests were about problems of internal medicine and 30% of the requests concerned surgical issues. the remaining 5% of the patients sought information about recent developments, such as molecular medicine or aviation medicine. during the project, our portal became inaccessible five times, and counselling was not possible on 44 days. there was no hacking of the web site. internet-based medical counselling is a helpful addition to conventional practice ","An eight-year study of Internet-based remote medical counselling We carried out a prospective study of an Internet-based remote counselling service. A total of 15,456 Internet users visited the Web site over eight years. From these, 1500 users were randomly selected for analysis. Medical counselling had been granted to 901 of the people requesting it (60%). One hundred and sixty-four physicians formed project groups to process the requests and responded using email. The distribution of patients using the service was similar to the availability of the Internet: 78% were from the European Union, North ‘America and Australia. Sixty-seven per cent of the patients lived in urban areas and the remainder were residents of remote rural areas with limited local medical coverage. Sixty-five per cent of the requests were about problems of internal medicine and 30% of the requests concerned surgical issues. The remaining 5% of the patients sought information about recent developments, such as molecular medicine or aviation medicine. During the project, our portal became inaccessible five times, and counselling was not possible on 44 days. There was no hacking of the Web site. Internet-based medical counselling is a helpful addition to conventional practice","['Internet-based remote medical counselling', 'Internet users', 'Web site', 'email', 'urban areas', 'remote rural areas', 'surgical issues', 'telemedicine', 'medical education', 'portal', 'biomedical education', 'educational computing', 'electronic mail', 'information resources', 'Internet', 'telemedicine']","['Web site', 'Internet-based medical counselling', 'remote rural areas', 'prospective study', 'eight-year study', 'counselling', 'study', 'remote', 'Internet-based', 'Medical counselling']",1071,194,16,1072,193,10,7,1,5
"modeling discourse in collaborative work support systems: a knowledge representation and configuration perspective collaborative work processes usually raise a lot of intricate debates and negotiations among participants, whereas conflicts of interest are inevitable and support for achieving consensus and compromise is required. individual contributions, brought up by parties with different backgrounds and interests, need to be appropriately structured and maintained. this paper presents a model of discourse acts that participants use to communicate their attitudes to each other, or affect the attitudes of others, in such environments. the first part deals with the knowledge representation and communication aspects of the problem, while the second one, in the context of an already implemented system, namely hermes, with issues related to the configuration of the contributions asserted at each discourse instance. the overall work focuses on the machinery needed in a computer-assisted collaborative work environment, the aim being to further enhance the human-computer interaction ","Modeling discourse in collaborative work support systems: a knowledge representation and configuration perspective Collaborative work processes usually raise a lot of intricate debates and negotiations among participants, whereas conflicts of interest are inevitable and support for achieving consensus and compromise is required. Individual contributions, brought up by parties with different backgrounds and interests, need to be appropriately structured and maintained. This paper presents a model of discourse acts that participants use to communicate their attitudes to each other, or affect the attitudes of others, in such environments. The first part deals with the knowledge representation and communication aspects of the problem, while the second one, in the context of an already implemented system, namely HERMES, with issues related to the configuration of the contributions asserted at each discourse instance. The overall work focuses on the machinery needed in a computer-assisted collaborative work environment, the aim being to further enhance the human-computer interaction","['discourse modeling', 'collaborative work support systems', 'knowledge representation', 'conflicts of interest', 'consensus', 'compromise', 'knowledge communication', 'HERMES', 'human-computer interaction', 'business data processing', 'groupware', 'knowledge representation', 'user modelling']","['knowledge representation', 'Collaborative work processes', 'configuration perspective', 'Individual contributions', 'modelling discourse', 'discourse instance', 'overall work', 'work', 'discourse', 'modelling']",942,153,13,942,152,10,0,0,5
"priming the pipeline [women in computer science careers] in 1997 the backyard project, a pilot program of the garnett foundation, was instituted to encourage high school girls to explore careers in the computer industry. at that time, the garnett foundation commissioned the global strategy group to execute a survey of 652 college-bound high school students (grades 9 through 12), to help discover directions that the backyard project might take to try to move toward the mission of the pilot program. it conducted the study by telephone between march 25 and april 8, 1997 in the silicon valley, boston, and austin metropolitan areas. it conducted all interviews using a random digit dialing methodology, derived from a file of american households with high incidences of adolescent children. the top six answers from girls to the survey question ""why are girls less likely to pursue computer science careers?"" in order of perceived importance by the girls were: not enough role models; women have other interests; didn't know about the industry; limited opportunity; negative media; and too nerdy. these responses are discussed ","Priming the pipeline [women in computer science careers] In 1997 The Backyard Project, a pilot program of the Garnett Foundation, was instituted to encourage high school girls to explore careers in the ‘computer industry. At that time, the Garnett Foundation commissioned the Global Strategy Group to execute a survey of 652 college-bound high school students (grades 9 through 12), to help discover directions that The Backyard Project might take to try to move toward the mission of the pilot program. It conducted the study by telephone between March 25 and April 8, 1997 in the Silicon Valley, Boston, and Austin metropolitan areas. It conducted all interviews using a random digit dialing methodology, derived from a file of American households with high incidences of adolescent children. The top six answers from girls to the survey question ""why are girls less likely to pursue computer science careers?"" in order of perceived importance by the girls were not enough role models; women have other interests; didn't know about the industry; limited opportunity; negative media; and too nerdy. These responses are discussed","['The Backyard Project', 'high school girls', 'computer industry careers', 'college-bound high school students', 'computer science education', 'DP industry', 'employment', 'gender issues']","['computer science careers', 'Backyard project', 'pilot program', 'high school girls', 'computer industry', 'pipeline women', 'computer', 'girls', 'women', 'careers']",952,179,8,952,178,10,8,2,0
"applying bgl to computational geometry the author applies boost graph library to the domain of computational geometry. first, he formulates a concrete problem in graph terms. second, he develops a way to transform the output of an existing algorithm into an appropriate boost graph library data structure. finally, he implements two new algorithms for my boost graph library graph. the first algorithm gets the job done, but could have been written in any programming language. the second algorithm, however, shows the power of boost graph library's generic programming approach.graphs, graphics, and generic programming combine in this novel use of the boost graph library ","Applying BGL to computational geometry The author applies Boost Graph Library to the domain of computational geometry. First, he formulates a concrete problem in graph terms. Second, he develops a way to transform the output of an existing algorithm into an appropriate Boost Graph Library data structure. Finally, he implements ‘two new algorithms for my Boost Graph Library graph. The first algorithm gets the job done, but could have been written in any programming language. The second algorithm, however, shows the power of Boost Graph Library's generic programming approach.Graphs, graphics, and generic programming combine in this novel use of the Boost Graph Library","['Boost libraries', 'C++', 'threads', 'smart pointers', 'Boost Graph Library', 'graph-theoretic concepts', 'directed graph', 'file dependencies', 'computational geometry', 'BGL graph', 'generic programming approach', 'C++ language', 'computational geometry', 'directed graphs', 'software libraries']","['computational geometry', 'Graph', 'generic programming approach.Graphs', 'second algorithm', 'first algorithm', 'new algorithms', 'Boost', 'Library', 'Boost Graph library', 'Boost Graph Library']",571,104,15,572,103,10,3,1,0
"a modal logic for indiscernibility and complementarity in information systems in this paper, we study indiscernibility relations and complementarity relations in information systems, the first-order characterization of indiscernibility and complementarity is obtained through a duality result between information systems and certain structures of relational type characterized by first-order conditions. the modal analysis of indiscernibility and complementarity is performed through a modal logic which modalities correspond to indiscernibility relations and complementarity relations in information systems ","‘A modal logic for indiscernibility and complementarity in information systems In this paper, we study indiscernibility relations and complementarity relations in information systems, The first-order characterization of indiscernibility and complementarity is obtained through a duality result between information systems and certain structures of relational type characterized by first-order conditions. The modal analysis of indiscernibility and complementarity is performed through a modal logic which modalities correspond to indiscernibility relations and complementarity relations in information systems","['modal logic', 'indiscernibility', 'complementarity', 'information systems', 'first-order characterization', 'duality result', 'relational type', 'first-order conditions', 'formal logic', 'information systems', 'knowledge representation']","['information systems', 'indiscernibility relations', 'complementarity relations', 'modal logic', 'relational type', 'modal analysis', 'modalities', 'relations', 'complementarity', 'indiscernibility']",535,75,11,536,74,10,1,1,3
"a comparison of high-power converter topologies for the implementation of facts controllers this paper compares four power converter topologies for the implementation of flexible ac transmission system (facts) controllers: three multilevel topologies (multipoint clamped (mpc), chain, and nested cell) and the well-established multipulse topology. in keeping with the need to implement very-high-power inverters, switching frequency is restricted to line frequency. the study addresses device count, dc filter ratings, restrictions on voltage control, active power transfer through the dc link, and balancing of dc-link voltages. emphasis is placed on capacitor sizing because of its impact on the cost and size of the facts controller. a method for the dimensioning the dc capacitor filter is presented. it is found that the chain converter is attractive for the implementation of a static compensator or a static synchronous series compensator. the mpc converter is attractive for the implementation of a unified power flow controller or an interline power flow controller, but a special arrangement is required to overcome the limitations on voltage control ","‘A comparison of high-power converter topologies for the implementation of FACTS controllers This paper compares four power converter topologies for the implementation of flexible AC transmission system (FACTS) controllers: three multilevel topologies (multipoint camped (MPC), chain, and nested cell) and the well-established multipulse topology. In keeping with the need to implement very-high-power inverters, switching frequency is restricted to line frequency. The study addresses device count, DC filter ratings, restrictions on voltage control, active power transfer through the DC link, and balancing of DC-link voltages. Emphasis is placed on capacitor sizing because of its impact on the cost and size of the, FACTS controller. A method for the dimensioning the DC capacitor filter, is presented. It is found that the chain converter is attractive for the implementation of a static compensator or a static synchronous series compensator. The MPC converter is attractive for the implementation of a unified power flow controller or an interline power flow controller, but a special arrangement is required to overcome the limitations on voltage control","['FACTS controllers', 'high-power converter topologies comparison', 'multilevel topologies', 'multipulse topology', 'inverters', 'switching frequency', 'device count', 'DC filter ratings', 'multipoint clamped topology', 'unified power flow controller', 'static compensator', 'static synchronous series compensator', 'STATCOM', 'UPFC', 'DC-AC power convertors', 'flexible AC transmission systems', 'invertors', 'load flow control', 'power capacitors', 'power convertors', 'power filters', 'power transmission control', 'static VAr compensators', 'switching circuits']","['FACTS controllers', 'voltage control', 'converter', 'well-established multiple topology', 'high-power converter topologies', 'multilevel topologies', 'chain converter', 'MPC converter', 'topologies', 'power converter topologies']",992,170,24,994,169,10,6,4,6
"using duality to implicitize and find cusps and inflection points of bezier curves a planar algebraic curve c has an implicit equation and a tangential equation. the tangential equation defines a dual curve to c. starting with a parametrization of c, we find a parametrization of the dual curve, and the tangential equation and implicit equation of c in a novel way. we also find equations whose roots are the parameter values of the cusps and inflection points of c. methods include polar reciprocation and the theory of envelopes ","Using duality to implicitize and find cusps and inflection points of Bezier curves planar algebraic curve C has an implicit equation and a tangential equation. The tangential equation defines a dual curve to C. Starting with a parametrization of C, we find a parametrization of the dual curve, and the tangential equation and implicit equation of C in a novel way. We also find equations whose roots are the parameter values of the cusps and inflection points of C. Methods include polar reciprocation and the theory of envelopes","['planar algebraic curve', 'implicit equation', 'tangential equation', 'dual curve', 'parametrization', 'cusps', 'inflection points', 'polar reciprocation', 'envelope theory', 'Bezier curves', 'computational geometry']","['tangential equation', 'inflection points', 'implicit equation', 'dual curve', 'cusps', 'Bezier curves', 'equation', 'curves', 'points', 'inflection']",443,90,11,442,88,10,216,75,2
"an operations research approach to the problem of the sugar cane selection selection for superior clones is the most important aspect of sugar cane improvement programs, and is a long and expensive process. while studies have investigated different components of selection independently, there has not been a whole system approach to improve the process. this study observes the problem as an integrated system, where if one parameter changes the state of the whole system changes. a computer based stochastic simulation model that accurately represents the selection was developed. the paper describes the simulation model, showing its accuracy as well as how a combination of dynamic programming and branch and bound can be applied to the model to optimise the selection system, giving a new application of these techniques. the model can be directly applied to any region targeted by sugar cane breeding programs or to other clonally propagated crops ","‘An operations research approach to the problem of the sugar cane selection Selection for superior clones is the most important aspect of sugar cane improvement programs, and is a long and expensive process. While studies have investigated different components of selection independently, there has not been a whole system approach to improve the process. This study observes the problem as an integrated system, where if one parameter changes the state of the whole system changes. A computer based stochastic simulation model that accurately represents the selection was developed. The paper describes the simulation model, showing its accuracy as well as how a combination of dynamic programming and branch and bound can be applied to the model to ‘optimise the selection system, giving a new application of these techniques. The model can be directly applied to any region targeted by sugar cane breeding programs or to other clonally propagated crops","['operations research approach', 'sugar cane selection', 'superior clones', 'improvement programs', 'computer based stochastic simulation model', 'dynamic programming', 'branch and bound', 'breeding programs', 'clonally propagated crops', 'agriculture', 'agriculture', 'covariance matrices', 'dynamic programming', 'genetics', 'operations research', 'simulation', 'tree searching']","['sugar cane', 'operations research approach', 'stochastic simulation model', 'whole system approach', 'whole system changes', 'integrated system', 'selection system', 'selection', 'simulation model', 'sugar']",805,150,17,807,149,10,10,2,7
"harmless delays in cohen-grossberg neural networks without assuming monotonicity and differentiability of the activation functions and any symmetry of interconnections, we establish some sufficient conditions for the globally asymptotic stability of a unique equilibrium for the cohen-grossberg (1983) neural network with multiple delays. lyapunov functionals and functions combined with the razumikhin technique are employed. the criteria are all independent of the magnitudes of the delays, and thus the delays under these conditions are harmless ","Harmless delays in Cohen-Grossberg neural networks Without assuming monotonicity and differentiability of the activation functions, and any symmetry of interconnections, we establish some sufficient conditions for the globally asymptotic stability of a unique equilibrium for the Cohen-Grossberg (1983) neural network with multiple delays. Lyapunov functionals and functions combined with the Razumikhin technique are employed. The criteria are all independent of the magnitudes of the delays, and thus the delays under these conditions are harmless","['harmless delays', 'Cohen-Grossberg neural networks', 'monotonicity', 'differentiability', 'activation functions', 'interconnections', 'globally asymptotic stability', 'multiple delays', 'Lyapunov functionals', 'Razumikhin technique', 'asymptotic stability', 'delay-differential systems', 'differential equations', 'interconnections', 'Lyapunov methods', 'neural nets', 'nonlinear dynamical systems']","['Cohen-Grossberg', 'multiple delays Lyapunov', 'sufficient conditions', 'activation functions', 'unique equilibrium', 'neural networks', 'Harmless delays', 'delays', 'neural', 'network']",475,75,17,476,74,10,0,1,7
"a pid standard: what, why, how? the paper is written for all who develop and use p&ids. it will aid in solving the long existing and continuing problem of confusing information on p&ids. the acronym p&id is widely understood to mean the principal document used to define the details of how a process works and how it is controlled. the isa dictionary definition for p&id tells what they do, ""show the interconnection of process equipment and the instrumentation used to control the process. in the process industry a standard set of symbols is used to prepare drawings of processes. the instrument symbols used in these drawings are generally based on isa-s5.1."" in the paper the isa standard is referred to as isa-5.1. the article develops the concept of the ""standard"" and poses some of the questions that the ""standard"" can answer ","APID standard: What, why, how? The paper is written for all who develop and use P&IDs. It will aid in solving the long existing and continuing problem of confusing information on PalDs. The acronym P&ID is widely understood to mean the principal document used to define the details of how a process works and how it is controlled. The ISA Dictionary definition for P&lD tells what they do, ""show the interconnection of process equipment and the instrumentation used to control the process. In the process industry a standard set of symbols is used to prepare drawings of processes. The instrument symbols used in these drawings are generally based on ISA-S5.1."" In the paper the ISA standard is referred to as ISA-5.1. The article develops the concept of the ""standard"" and poses some of the questions that the ""standard"" can answer","['P&ID standard', 'principal document', 'process controlled', 'ISA-5.1', 'ISA standard', 'diagrams', 'instrumentation', 'process control', 'safety', 'standardisation', 'standards']","['paper', 'ISA Dictionary definition', 'standard', 'process', 'instrument symbols', 'process equipment', 'process industry', 'standard set', 'ISA standard', 'P&ID s']",693,142,11,693,140,10,400,140,3
"h-matrix approximation for the operator exponential with applications we previously developed a data-sparse and accurate approximation to parabolic solution operators in the case of a rather general elliptic part given by a strongly p-positive operator . also a class of matrices (h-matrices) has been analysed which are data-sparse and allow an approximate matrix arithmetic with almost linear complexity. in particular, the matrix-vector/matrix-matrix product with such matrices as well as the computation of the inverse have linear-logarithmic cost. in this paper, we apply the h-matrix techniques to approximate the exponent of an elliptic operator. starting with the dunford-cauchy representation for the operator exponent, we then discretise the integral by the exponentially convergent quadrature rule involving a short sum of resolvents. the latter are approximated by the h-matrices. our algorithm inherits a two-level parallelism with respect to both the computation of resolvents and the treatment of different time values. in the case of smooth data (coefficients, boundaries), we prove the linear-logarithmic complexity of the method ","H-matrix approximation for the operator exponential with applications We previously developed a data-sparse and accurate approximation to parabolic solution operators in the case of a rather general elliptic part given by a strongly P-positive operator . Also a class of matrices (H-matrices) has been analysed which are data-sparse and allow an approximate matrix arithmetic with almost linear complexity. In particular, the matrix-vector/matrix-matrix product with such matrices as well as the computation of the inverse have linear-logarithmic cost. In this paper, we apply the H-matrix techniques to approximate the exponent of an elliptic operator. Starting with the Dunford-Cauchy representation for the operator exponent, we then discretise the integral by the exponentially convergent quadrature rule involving a short sum of resolvents. The latter are approximated by the H-matrices. Our algorithm inherits a two-level parallelism with respect to both the ‘computation of resolvents and the treatment of different time values. In the case of smooth data (coefficients, boundaries), we prove the linear-logarithmic complexity of the method","['H-matrix approximation', 'operator exponential', 'data-sparse approximation', 'almost linear complexity', 'parabolic solution operators', 'Dunford-Cauchy representation', 'exponentially convergent quadrature rule', 'strongly P-positive operator', 'approximation theory', 'computational complexity', 'differential equations', 'matrix algebra']","['operator exponent', 'operator', 'approximate matrix arithmetic', 'parabolic solution operators', 'accurate approximation', 'matrix approximation', 'positive operator', 'matrix techniques', 'elliptic operator', 'approximate']",984,164,12,985,163,10,11,1,3
application of traditional system design techniques to web site design after several decades of computer program construction there emerged a set of principles that provided guidance to produce more manageable programs. with the emergence of the plethora of internet web sites one wonders if similar guidelines are followed in their construction. since this is a new technology no apparent universally accepted methods have emerged to guide the designer in web site construction. this paper reviews the traditional principles of structured programming and the preferred characteristics of web sites. finally a mapping of how the traditional guidelines may be applied to web site construction is presented. the application of the traditional principles of structured programming to the design of a web site can provide a more usable site for the visitors to the site. the additional benefit of using these time-honored techniques is the creation of a web site that will be easier to maintain by the development staff ,Application of traditional system design techniques to Web site design After several decades of computer program construction there emerged a set of principles that provided guidance to produce more manageable programs. With the emergence of the plethora of Internet web sites one wonders if similar guidelines are followed in their construction. Since this is a new technology no apparent universally accepted methods have emerged to guide the designer in Web site construction. This paper reviews the traditional principles of structured programming and the preferred characteristics of Web sites. Finally a mapping of how the traditional guidelines may be applied to Web site construction is presented. The application of the traditional principles of structured programming to the design of a Web site can provide a more usable site for the visitors to the site. The additional benefit of using these time-honored techniques is the creation of a Web site that will be. easier to maintain by the development staff,"['Internet Web site design', 'system design techniques', 'structured programming', 'distributed programming', 'information resources', 'Internet', 'structured programming', 'systems analysis']","['traditional principles', 'structured programming', 'Web site construction', 'site', 'Web site', 'computer program construction', 'traditional guidelines', 'Web site design', 'usable site', 'Web']",858,159,8,859,158,10,0,1,1
"four-point wavelets and their applications multiresolution analysis (mra) and wavelets provide useful and efficient tools for representing functions at multiple levels of details. wavelet representations have been used in a broad range of applications, including image compression, physical simulation and numerical analysis. in this paper, the authors construct a new class of wavelets, called four-point wavelets, based on an interpolatory four-point subdivision scheme. they are of local support, symmetric and stable. the analysis and synthesis algorithms have linear time complexity. depending on different weight parameters w, the scaling functions and wavelets generated by the four-point subdivision scheme are of different degrees of smoothness. therefore the user can select better wavelets relevant to the practice among the classes of wavelets. the authors apply the four-point wavelets in signal compression. the results show that the four-point wavelets behave much better than b-spline wavelets in many situations ","Four-point wavelets and their applications Multiresolution analysis (MRA) and wavelets provide useful and efficient tools for representing functions at multiple levels of details. Wavelet representations have been used in a broad range of applications, including image compression, physical simulation and numerical analysis. In this paper, the authors construct a new class of wavelets, called four-point wavelets, based on an interpolatory four-point subdivision scheme. They are of local support, symmetric and stable. The analysis and synthesis algorithms have linear time complexity. Depending on different weight parameters w, the scaling functions and wavelets generated by the four-point subdivision scheme are of different degrees of smoothness. Therefore the user can select better wavelets relevant to the practice among the classes of wavelets. The authors apply the four-point wavelets in signal compression. The results show that the four-point wavelets behave much better than B-spline wavelets in many situations","['four-point wavelets', 'multiresolution analysis', 'wavelet representations', 'image compression', 'physical simulation', 'numerical analysis', 'interpolatory four-point subdivision scheme', 'linear time complexity', 'weight parameters', 'scaling functions', 'B-spline wavelets', 'computational complexity', 'data compression', 'image coding', 'interpolation', 'splines (mathematics)', 'wavelet transforms']","['wavelets', 'four-point subdivision scheme', 'details wavelets representations', 'Multiresolution analysis imran', 'numerical analysis', 'baseline wavelets', 'better wavelets', 'four-point', 'four-point wavelets', 'Four-point wavelets']",885,145,17,885,144,10,0,0,5
"favorable noise uniformity properties of fourier-based interpolation and reconstruction approaches in single-slice helical computed tomography volumes reconstructed by standard methods from single-slice helical computed tomography (ct) data have been shown to have noise levels that are highly nonuniform relative to those in conventional ct. these noise nonuniformities can affect low-contrast object detectability and have also been identified as the cause of the zebra artifacts that plague maximum intensity projection (mip) images of such volumes. while these spatially variant noise levels have their root in the peculiarities of the helical scan geometry, there is also a strong dependence on the interpolation and reconstruction algorithms employed. in this paper, we seek to develop image reconstruction strategies that eliminate or reduce, at its source, the nonuniformity of noise levels in helical ct relative to that in conventional ct. we pursue two approaches, independently and in concert. we argue, and verify, that fourier-based longitudinal interpolation approaches lead to more uniform noise ratios than do the standard 360li and 180li approaches. we also demonstrate that a fourier-based fan-to-parallel rebinning algorithm, used as an alternative to fanbeam filtered backprojection for slice reconstruction, also leads to more uniform noise ratios, even when making use of the 180li and 360li interpolation approaches ","Favorable noise uniformity properties of Fourier-based interpolation and reconstruction approaches in single-slice helical computed tomography Volumes reconstructed by standard methods from single-slice helical computed tomography (CT) data have been shown to have noise levels that are highly nonuniform relative to those in conventional CT. These noise nonuniformities can affect low-contrast object detectability and have also been identified as the cause of the zebra artifacts that plague maximum intensity projection (MIP) images of such volumes. While these spatially variant noise levels have their root in the peculiarities of the helical scan geometry, there is also a strong dependence on the interpolation and reconstruction algorithms employed. In this paper, we seek to develop image reconstruction strategies that eliminate or reduce, at its source, the nonuniformity of noise levels in helical CT relative to that in conventional CT. We pursue two approaches, independently and in concert. We argue, and verity, that Fourier-based longitudinal interpolation approaches lead to more uniform noise ratios than do the standard 360LI and 180LI approaches. We also demonstrate that a Fourier-based fan-to-parallel rebinning algorithm, used as an alternative to fanbeam filtered backprojection for slice reconstruction, also leads to more uniform noise ratios, even when making use of the 180LI and 360LI interpolation approaches","['Fourier-based interpolation', 'single-slice helical computed tomography', 'reconstruction approaches', 'noise uniformity properties', 'medical diagnostic imaging', 'conventional CT', 'Fourier-based fan-to-parallel rebinning algorithm', 'more uniform noise ratios', 'low-contrast object detectability', 'zebra artifacts', 'maximum intensity projection images', 'helical span geometry', 'computerised tomography', 'image reconstruction', 'interpolation', 'medical image processing', 'noise']","['uniform noise ratios', 'noise', 'image reconstruction strategies', '360LI interpolation approaches', 'Fourier-based interpolation', 'reconstruction approaches', 'noise nonuniformities', 'variant noise levels', 'helical CT', 'noise levels']",1236,205,17,1236,204,10,1,1,2
"modelling of complete robot dynamics based on a multi-dimensional, rbf-like neural architecture a neural network based identification approach of manipulator dynamics is presented. for a structured modelling, rbf-like static neural networks are used in order to represent and adapt all model parameters with their non-linear dependences on the joint positions. the neural architecture is hierarchically organised to reach optimal adjustment to structural a priori-knowledge about the identification problem. the model structure is substantially simplified by general system analysis independent of robot type. but also a lot of specific features of the utilised experimental robot are taken into account. a fixed, grid based neuron placement together with application of b-spline polynomial basis functions is utilised favourably for a very effective recursive implementation of the neural architecture. thus, an online identification of a dynamic model is submitted for a complete 6 joint industrial robot ","Modelling of complete robot dynamics based on a multi-dimensional, RBF-lke neural architecture Anneural network based identification approach of manipulator dynamics is presented. For a structured modelling, RBF-like static neural networks are used in order to represent and adapt all model parameters with their non-linear dependences on the joint positions. The neural architecture is hierarchically organised to reach optimal adjustment to structural a priori-knowledge about the identification problem. The model structure is substantially simplified by general system analysis independent of robot type. But also a lot of specific features of the utilised experimental robot are taken into account. A fixed, grid based neuron placement together with application of B-spline polynomial basis functions is utilised favourably for a very effective recursive implementation of the neural architecture. Thus, an online identification of a dynamic model is submitted for a complete 6 joint industrial robot","['complete robot dynamics', 'multi-dimensional RBF-like neural architecture', 'manipulator dynamics', 'static neural networks', 'neural architecture', 'general system analysis', 'fixed grid based neuron placement', 'B-spline polynomial basis functions', 'recursive implementation', 'online identification', 'dynamic model', 'complete 6 joint industrial robot', 'online learning', 'identification', 'learning (artificial intelligence)', 'manipulator dynamics', 'neural net architecture', 'radial basis function networks', 'splines (mathematics)']","['neural architecture', 'utilised experimental robot', 'complete robot dynamics', 'joint industrial robot', 'structured modelling', 'manipulator dynamics', 'model parameters', 'model structure', 'dynamic model', 'robot type']",865,143,19,865,141,10,460,130,5
"on the beth properties of some intuitionistic modal logics let l be one of the intuitionistic modal logics. as in the classical modal case, we define two different forms of the beth property for l, which are denoted by b1 and b2; in this paper we study the relation among b1, b2 and the interpolation properties c1 and c2. it turns out that c1 implies b1, but contrary to the boolean case, is not equivalent to b1. it is shown that b2 and c2 are independent, and moreover it comes out that, in contrast to classical case, there exists an extension of the intuitionistic modal logic of s/sub 4/-type, that has not the property b2. finally we give two algebraic properties, that characterize respectively b1 and b2 ","On the Beth properties of some intuitionistic modal logics Let L be one of the intuitionistic modal logics. As in the classical modal case, we define two different forms of the Beth property for L, which are denoted by B1 and B2; in this paper we study the relation among B1, 52 and the interpolation properties C1 and C2. It turns out that C1 implies B1, but contrary to the boolean case, is not equivalent to B1 Itis shown that B2 and C2 are independent, and moreover it comes out that, in contrast to classical case, there exists an extension of the intuitionistic modal logic of S/sub 4/-type, that has not the property B2. Finally we give two algebraic properties, that characterize respectively B1 and B2","['Beth properties', 'interpolation properties', 'intuitionistic modal logics', 'formal logic']","['intuitionists', 'modal logic', 'interpolation properties C1', 'classical modal case', 'algebraic properties', 'classical case', 'Beth property', 'property b.', 'property', 'modal']",586,128,4,585,126,10,140,51,0
"fresh voices, big ideas [ibm internship program] ibm is matching up computer-science and mba students with its business managers in an 11-week summer internship program and challenging them to develop innovative technology ideas ","Fresh voices, big ideas [IBM internship program] IBM is matching up computer-science and MBA students with its business managers in an 11-week summer internship program and challenging them to develop innovative technology ideas","['internship program', 'IBM business managers', 'MBA college students', 'computer-science students', 'patents', 'DP industry', 'IBM computers', 'research initiatives']","['innovative technology ideas', 'internship program IBM', 'business managers', 'MBA students', 'Fresh voices', 'big ideas', 'ideas', 'Fresh', 'program', 'internship']",196,34,8,196,33,10,0,0,0
"a scanline-based algorithm for the 2d free-form bin packing problem this paper describes a heuristic algorithm for the 2d free-form bin packing (2d-fbp) problem. given a set of 2d free-form bins and a set of 2d free-form items, the 2d-fbp problem is to lay out items inside one or more bins in such a way that the number of bins used is minimized, and for each bin, the yield is maximized. the proposed algorithm handles the problem as a variant of the 1d problem; i.e., items and bins are approximated as sets of scanlines, and scanlines are packed. the details of the algorithm are given, and its application to a nesting problem in a shipbuilding company is reported. the proposed algorithm consists of the basic and the group placement algorithms. the basic placement algorithm is a variant of the first-fit decreasing algorithm which is simply extended from the 1d case to the 2d case by a novel scanline approximation. a numerical study with real instances shows that the basic placement algorithm has sufficient performance for most of the instances, however, the group placement algorithm is required when items must be aligned in columns. the qualities of the resulting layouts are good enough for practical use, and the processing times are good ","A scanline-based algorithm for the 2D free-form bin packing problem This paper describes a heuristic algorithm for the 2D free-form bin packing (2D-FBP) problem. Given a set of 2D free-form bins and a set of 2D free-form items, the 2D-FBP problem is to lay out items inside one or more bins in such a way that the number of bins used is minimized, and for each bin, the yield is maximized. The proposed algorithm handles the problem as a variant of the 1D problem; i., items and bins are approximated as sets of scanlines, and scanlines are packed. The details of the algorithm are given, and its application to a nesting problem in a shipbuilding company is reported. The proposed algorithm consists of the basic and the group placement algorithms. The basic placement algorithm is a variant of the first-fit decreasing algorithm. which is simply extended from the 1D case to the 2D case by a novel scanline approximation. A numerical study with real instances shows that the basic placement algorithm has sufficient performance for most, of the instances, however, the group placement algorithm is required when items must be aligned in columns. The qualities of the resulting layouts are good enough for practical use, and the processing times are good","['scanline-based algorithm', '2D free-form bin packing problem', 'heuristic algorithm', '2D-FBP problem', 'irregular cutting', 'irregular packing', 'nesting problem', 'minimization', 'yield maximization', 'shipbuilding company', 'group placement algorithm', 'first-fit decreasing algorithm', 'bin packing', 'heuristic programming', 'minimisation']","['algorithm', 'group placement algorithm', 'basic placement algorithm', 'free-form bin packing', 'd case', 'scanline-based algorithm', 'heuristic algorithm', 'free-form items', 'free-form bins', 'd']",1045,212,15,1045,211,10,1,3,3
"choice from a three-element set: some lessons of the 2000 presidential campaign in the united states we consider the behavior of four choice rules - plurality voting, approval voting, borda count, and self-consistent choice - when applied to choose the best option from a three-element set. it is assumed that the two main options are preferred by a large majority of the voters, while the third option gets a very small number of votes and influences the election outcome only when the two main options receive a close number of votes. when used to rate the main options, borda count and self-consistent choice contain terms that allow both for the ""strength of preferences"" of the voters and the rating of the main candidates by voters who vote for the third option. in this way, it becomes possible to determine more reliably the winner when plurality voting or approval voting produce close results ","Choice from a three-element set: some lessons of the 2000 presidential campaign in the United States We consider the behavior of four choice rules - plurality voting, approval voting, Borda count, and seff-consistent choice - when applied to choose the best option from a three-element set. It is assumed that the two main options are preferred by a large majority of the voters, while the third option gets a very small number of votes and influences the election outcome only when the two main options receive a close number of votes. When used to rate the main options, Borda count and self-consistent choice contain terms that allow both for the ""strength of preferences"" of the voters and the rating of the main candidates by voters who vote for the third option. In this way, it becomes possible to determine more reliably the winner when plurality voting or approval voting produce close results","['three-element set', '2000 presidential campaign', 'plurality voting', 'approval voting', 'Borda count', 'self-consistent choice', 'mathematical programming']","['main options', 'self-consistent choice', 'three-element set', 'plurality voting', 'approval voting', 'third option', 'presidential campaign', 'best option', 'voting', 'options']",751,153,7,751,152,10,1,1,1
"cooperative mutation based evolutionary programming for continuous function optimization an evolutionary programming (ep) algorithm adapting a new mutation operator is presented. unlike most previous eps, in which each individual is mutated on its own, each individual in the proposed algorithm is mutated in cooperation with the other individuals. this not only enhances convergence speed but also gives more chance to escape from local minima ","Cooperative mutation based evolutionary programming for continuous function optimization An evolutionary programming (EP) algorithm adapting a new mutation operator is. presented. Unlike most previous EPs, in which each individual is, mutated on its own, each individual in the proposed algorithm is mutated in cooperation with the other individuals. This not only enhances convergence speed but also gives more chance to escape from local minima","['cooperative mutation based evolutionary programming', 'continuous function optimization', 'convergence speed', 'local minima', 'convergence', 'evolutionary computation', 'functions', 'optimisation']","['evolutionary programming', 'continuous function optimization', 'new mutation operator', 'Cooperative mutation', 'other individuals', 'most previous eps', 'mutation', 'individual', 'programming', 'evolutionary']",381,65,8,383,64,10,0,2,3
"quasi stage order conditions for sdirk methods the stage order condition is a simplifying assumption that reduces the number of order conditions to be fulfilled when designing a runge-kutta (rk) method. because a dirk (diagonally implicit rk) method cannot have stage order greater than 1, we introduce quasi stage order conditions and derive some of their properties for dirks. we use these conditions to derive a low-order dirk method with embedded error estimator. numerical tests with stiff odes and daes of index 1 and 2 indicate that the method is competitive with other rk methods for low accuracy tolerances ","Quasi stage order conditions for SDIRK methods The stage order condition is a simplifying assumption that reduces the number of order conditions to be fulfilled when designing a Runge-Kutta (RK) method. Because a DIRK (diagonally implicit RK) method cannot have stage order greater than 1, we introduce quasi stage order conditions and derive some of their properties for DIRKs. We use these conditions to derive a low-order DIRK method with embedded error estimator. Numerical tests with stiff ODEs and DAEs of index 1 and 2 indicate that the method is competitive with other RK methods for low accuracy tolerances","['quasi stage order conditions', 'diagonally implicit Runge-Kutta method', 'embedded error estimator', 'numerical tests', 'differential-algebraic systems', 'SDIRK methods', 'differential equations', 'Runge-Kutta methods']","['stage order condition', 'order conditions', 'Runge-Kutta ark method', 'low-order DIRK method', 'order', 'other RK methods', 'dirk methods', 'condition', 'stage', 'stage order']",517,100,8,517,99,10,0,0,2
"effectiveness of user testing and heuristic evaluation as a function of performance classification for different levels of user performance, different types of information are processed and users will make different types of errors. based on the error's immediate cause and the information being processed, usability problems can be classified into three categories. they are usability problems associated with skill-based, rule-based and knowledge-based levels of performance. in this paper, a user interface for a web-based software program was evaluated with two usability evaluation methods, user testing and heuristic evaluation. the experiment discovered that the heuristic evaluation with human factor experts is more effective than user testing in identifying usability problems associated with skill-based and rule-based levels of performance. user testing is more effective than heuristic evaluation in finding usability problems associated with the knowledge-based level of performance. the practical application of this research is also discussed in the paper ","Effectiveness of user testing and heuristic evaluation as a function of performance classification For different levels of user performance, different types of information are processed and users will make different types of errors. Based on the error's immediate cause and the information being processed, usability problems can be classified into three categories. They are usability problems associated with skill-based, rule-based and knowledge-based levels of performance. In this paper, a user interface for a Web-based software program was evaluated with two usability evaluation methods, user testing and heuristic evaluation. The experiment discovered that the heuristic evaluation with human factor experts is more effective than user testing in identifying usability problems associated with skill-based and rule-based levels of performance. User testing is more effective than heuristic evaluation in finding usability problems associated with the knowledge-based level of performance. The practical application of this research is also discussed in the paper","['user testing', 'heuristic evaluation', 'performance classification', 'user performance', 'usability', 'knowledge-based performance levels', 'skill-based performance levels', 'user interface', 'Web-based software', 'experiment', 'human factors', 'rule-based performance levels', 'human factors', 'information resources', 'user interfaces']","['heuristic evaluation', 'usability problems', 'different types', 'performance classification', 'performance User testing', 'user performance', 'different levels', 'user interface', 'users', 'user testing']",924,149,15,924,148,10,0,0,2
"senate to powell: regulate more [fcc] fcc chairman michael powell pitched a six-step market-based recovery plan to the senate last week, but two members of the commerce committee told him telecom's revival requires more reliance on regulation ","Senate to Powell: regulate more [FCC] FCC Chairman Michael Powell pitched a six-step market-based recovery plan to the Senate last week, but two members of the Commerce Committee told him telecom's revival requires more reliance on regulation","['FCC', 'recovery plan', 'US Senate Commerce Committee', 'telecom industry', 'telecommunication']","['Senate', 'market-based recovery plan', 'Commerce Committee', 'telecoms revival', 'more reliance', 'last week', 'Chairman', 'more', 'fcc', 'FCC']",206,38,5,206,37,10,0,0,0
"a novel approach for the detection of pathlines in x-ray angiograms: the wavefront propagation algorithm presents a new pathline approach, based on the wavefront propagation principle, and developed in order to reduce the variability in the outcomes of the quantitative coronary artery analysis. this novel approach, called wavepath, reduces the influence of the user-defined start- and endpoints of the vessel segment and is therefore more robust and improves the reproducibility of the lesion quantification substantially. the validation study shows that the wavepath method is totally constant in the middle part of the pathline, even when using the method for constructing a bifurcation or sidebranch pathline. furthermore, the number of corrections needed to guide the wavepath through the correct vessel is decreased from an average of 0.44 corrections per pathline to an average of 0.12 per pathline. therefore, it can be concluded that the wavepath algorithm improves the overall analysis substantially ","Anovel approach for the detection of pathlines in X-ray angiograms: the wavefront propagation algorithm Presents a new pathline approach, based on the wavefront propagation principle, and developed in order to reduce the variability in the outcomes of the quantitative coronary artery analysis. This novel approach, called wavepath, reduces the influence of the user-defined start- and endpoints of the vessel segment and is therefore more robust and improves the reproducibility of the lesion quantification substantially. The validation study shows that the wavepath method is totally constant in the middle part of the pathline, even when using the method for constructing a bifurcation or sidebranch pathline. Furthermore, the number of corrections needed to guide the wavepath through the correct vessel is decreased from an average of 0.44 corrections per pathline to an average of 0.12 per pathline. Therefore, it can be concluded that the wavepath algorithm improves the overall analysis substantially","['wavefront propagation principle', 'quantitative coronary artery analysis', 'user-defined startpoints', 'user-defined endpoints', 'vessel segment', 'lesion quantification', 'wavepath method', 'bifurcation', 'sidebranch pathline', 'corrections', 'correct vessel', 'wavefront propagation algorithm', 'X-ray angiograms', 'algorithm theory', 'bifurcation', 'blood vessels', 'diagnostic radiography', 'medical image processing']","['novel approach', 'wavefront propagation principle', 'wavefront propagation algorithm', 'new pauline approach', 'sidebranch pathline', 'warpath algorithm', 'warpath method', 'approach', 'pathline', 'novel']",861,151,18,861,149,10,502,149,8
"comparison of push and pull systems with transporters: a metamodelling approach analyses push and pull systems with transportation consideration. a multiproduct, multiline, multistage production system was used to compare the two systems. the effects of four factors (processing time variation, demand variation, transporters, batch size) on throughput rate, average waiting time in the system and machine utilization were studied. the study uses metamodels to compare the two systems. they serve a dual purpose of expressing system performance measures in the form of a simple equation and reducing computational time when comparing the two systems. research shows that the number of transporters used and the batch size have a significant effect on the performance measures of both systems ","Comparison of push and pull systems with transporters: a metamodelling approach Analyses push and pull systems with transportation consideration. A multiproduct, muttiiine, multistage production system was used to compare the two systems. The effects of four factors (processing time variation, demand variation, transporters, batch size) on throughput rate, average waiting time in the system and machine utilization were studied. The study uses metamodels to compare the two systems. They serve a dual purpose of expressing system performance measures in the form of a simple equation and reducing computational time when ‘comparing the two systems. Research shows that the number of transporters used and the batch size have a significant effect on the performance measures of both systems","['transporters', 'push systems', 'pull systems', 'metamodelling approach', 'multiproduct multiline multistage production system', 'processing time variation', 'demand variation', 'batch size', 'throughput rate', 'average waiting time', 'machine utilization', 'performance measures', 'batch processing (industrial)', 'materials handling', 'production control', 'statistical analysis']","['system', 'transporters', 'batch size', 'metamodelling approach Analyses', 'multi-stage production system', 'system performance measures', 'computational time', 'systems Research', 'Comparison', 'performance measures']",675,118,16,676,117,10,11,2,3
"restoration of archival documents using a wavelet technique this paper addresses a problem of restoring handwritten archival documents by recovering their contents from the interfering handwriting on the reverse side caused by the seeping of ink. we present a novel method that works by first matching both sides of a document such that the interfering strokes are mapped with the corresponding strokes originating from the reverse side. this facilitates the identification of the foreground and interfering strokes. a wavelet reconstruction process then iteratively enhances the foreground strokes and smears the interfering strokes so as to strengthen the discriminating capability of an improved canny edge detector against the interfering strokes. the method has been shown to restore the documents effectively with average precision and recall rates for foreground text extraction at 84 percent and 96 percent, respectively ","Restoration of archival documents using a wavelet technique This paper addresses a problem of restoring handwritten archival documents by recovering their contents from the interfering handwriting on the reverse side caused by the seeping of ink. We present a novel method that works by first matching both sides of a document such that the interfering strokes are mapped with the corresponding strokes originating from the reverse side. This facilitates the identification of the foreground and interfering strokes. A wavelet reconstruction process then iteratively enhances the foreground strokes and smears the interfering strokes so as to strengthen the discriminating capability of an improved Canny edge detector against the interfering strokes. The method has been shown to restore the documents effectively with average precision and recall rates for foreground text extraction at 84 percent and 96 percent, respectively","['archival documents restoration', 'wavelet technique', 'handwritten archival documents', 'ink seepage', 'wavelet reconstruction process', 'iterative stroke enhancement', 'Canny edge detector', 'document image processing', 'edge detection', 'image restoration', 'wavelet transforms']","['reverse side', 'strokes', 'wavelets reconstruction process', 'handwritten archival documents', 'corresponding strokes', 'wavelets technique', 'foreground strokes', 'documents', 'archival', 'archival documents']",793,137,11,793,136,10,0,0,1
"mobile computing ""killer app"" competition design competitions offer students an excellent way to gain hands-on experience in engineering and computer science courses. the university of florida, in partnership with motorola, has held two mobile computing design competitions. in spring and fall 2001, students in abdelsalam helal's mobile computing class designed killer apps for a motorola smart phone ","Mobile computing ""Killer app"" competition Design competitions offer students an excellent way to gain hands-on experience in engineering and computer science courses. The University of Florida, in partnership with Motorola, has held two mobile computing design competitions. In Spring and Fall 2001, students in Abdelsalam Helal's Mobile Computing class designed killer apps for a Motorola smart phone","['mobile computing', 'smart phone', 'Motorola', 'design competitions', 'computer science education', 'mobile computing']","['competitions', 'students', 'computer science courses', 'Mobile Computing class', 'hands-on experience', 'excellent way', 'killer apps', 'apps', 'killer', 'Mobile']",345,58,6,345,57,10,0,0,2
"development of an integrated and open-architecture precision motion control system in this paper, the development of an integrated and open-architecture precision motion control system is presented. the control system is generally applicable, but it is developed with a particular focus on direct drive servo systems based on linear motors. the overall control system is comprehensive, comprising of various selected control and instrumentation components, integrated within a configuration of hardware architecture centred around a dspace ds1004 dsp processor board. these components include a precision composite controller (comprising of feedforward and feedback control), a disturbance observer, an adaptive notch filter, and a geometrical error compensator. the hardware architecture, software development platform, user interface, and all constituent control components are described ","Development of an integrated and open-architecture precision motion control system In this paper, the development of an integrated and open-architecture precision motion control system is presented. The control system is generally applicable, but it is developed with a particular focus on direct drive servo systems based on linear motors. The overall control system is comprehensive, comprising of various selected control and instrumentation components, integrated within a configuration of hardware architecture centred around a SPACE DS1004 DSP processor board. These components include a precision composite controller (comprising of feedforward and feedback control), a disturbance observer, an adaptive notch filter, and a geometrical error compensator. The hardware architecture, software development platform, user interface, and all constituent control components are described","['motion control', 'direct drive servo systems', 'linear motors', 'dSPACE DS1004 processor', 'composite controller', 'feedforward', 'feedback', 'adaptive notch filter', 'open-architecture', 'precision', 'geometrical error compensation', 'electric drives', 'error compensation', 'feedback', 'feedforward', 'linear motors', 'motion control', 'open systems']","['control', 'precision', 'open-architecture', 'motion', 'constituent control components', 'instrumentation components', 'overall control system', 'composite controller', 'system', 'control system']",772,119,18,771,118,10,5,1,8
pontryagin maximum principle of optimal control governed by fluid dynamic systems with two point boundary state constraint we study the optimal control problem subject to the semilinear equation with a state constraint. we prove certain theorems and give examples of state constraints so that the maximum principle holds. the main difficulty of the problem is to make the sensitivity analysis of the state with respect to the control caused by the unboundedness and nonlinearity of an operator ,Pontryagin maximum principle of optimal control governed by fluid dynamic systems with two point boundary state constraint We study the optimal control problem subject to the semilinear equation with a state constraint. We prove certain theorems and give examples of state constraints so that the maximum principle holds. The main difficulty of the problem is to make the sensitivity analysis of the state with respect to the control caused by the unboundedness and nonlinearity of an operator,"['Pontryagin maximum principle', 'optimal control', 'fluid dynamics', 'semilinear equation', 'state constraints', 'fluid dynamics', 'maximum principle', 'optimal control']","['state constraints', 'optimal control', 'Pontryagin maximum principle', 'maximum', 'principle', 'maximum principle', 'state', 'control', 'optimal', 'constraints']",417,78,8,417,77,10,0,0,1
bounded model checking for the universal fragment of ctl bounded model checking (bmc) has been recently introduced as an efficient verification method for reactive systems. bmc based on sat methods consists in searching for a counterexample of a particular length and generating a propositional formula that is satisfiable iff such a counterexample-exists. this new technique has been introduced by e. clarke et al. for model checking of linear time temporal logic (ltl). our paper shows how the concept of bounded model checking can be extended to actl (the universal fragment of ctl). the implementation of the algorithm for elementary net systems is described together with the experimental results ,Bounded model checking for the universal fragment of CTL Bounded Model Checking (BMC) has been recently introduced as an efficient verification method for reactive systems. BMC based on SAT methods consists in searching for a counterexample of a particular length and generating a propositional formula that is satisfiable iff such a counterexample-exists. This new technique has been introduced by E. Clarke et al. for model checking of linear time temporal logic (LTL). Our paper shows how the concept of bounded model checking can be ‘extended to ACTL (the universal fragment of CTL). The implementation of the algorithm for Elementary Net Systems is described together with the experimental results,"['bounded model checking', 'universal fragment', 'verification method', 'reactive systems', 'SAT methods', 'propositional formula', 'model checking', 'linear time temporal logic', 'elementary net systems', 'bounded semantics', 'computability', 'semantic networks', 'temporal logic']","['universal fragment', 'efficient verification method', 'reactive systems BMC', 'Model Checking bmc', 'SAT methods', 'model', 'checking', 'model checking', 'fragment', 'universal']",594,109,13,595,108,10,8,1,3
"computing grid unlocks research under the uk government's spending review in 2000 the office of science and technology was allocated pounds 98m to establish a three year e-science research and development programme. the programme has a bold vision: to change the dynamic of the way science is undertaken. the term 'e-science' was introduced by john taylor, director general of research councils in the office of science and technology. he saw many areas of science becoming increasingly reliant on new ways of collaborative, multidisciplinary, interorganisation working. e-science is intended to capture these new modes of working. there are two major components to the programme: the science, and the infrastructure to support that science. the infrastructure is generally referred to as the grid. the choice of name resonates with the idea of a future in which computing resources and storage, as well as expensive scientific facilities and software, can be accessed on demand, like electricity. open source prototypes of the middleware are available and under development as part of the e-science programme and other international efforts ","Computing grid unlocks research Under the UK government's spending review in 2000 the Office of Science and Technology was allocated Pounds 98m to establish a three year e-science research and development programme. The programme has a bold vision: to change the dynamic of the way science is undertaken. The term ‘e-science' was introduced by John Taylor, director general of research councils in the Office of Science and Technology. He saw many areas of science becoming increasingly reliant on new ways of collaborative, multidisciplinary, interorganisation working. E-science is intended to capture these new modes of working. There are two major components to the programme: the science, and the infrastructure to support that science. The infrastructure is generally referred to as the Grid. The choice of name resonates with the idea of a future in which computing resources and storage, as well as expensive scientific facilities and software, can be accessed on demand, like electricity. Open source prototypes of the middleware are available and under development as part of the e-science programme and other international efforts","['UK programme', 'grid computing', 'collaboration', 'computing resources', 'e-science', 'middleware', 'software', 'open source prototypes', 'scientific research', 'government policies', 'groupware', 'natural sciences computing', 'wide area networks']","['science', 'year science research', 'development programme', 'science programme', 'research councils', 'UK governments', 'Computing grid', 'way science', 'research', 'grid']",968,175,13,968,174,10,1,1,4
the mutual effects of grid and wind turbine voltage stability control this note considers the results of wind turbine modelling and power system stability investigations. voltage stability of the power grid with grid-connected wind turbines will be improved by using blade angle control for a temporary reduction of the wind turbine power during and shortly after a short circuit fault in the grid ,The mutual effects of grid and wind turbine voltage stability control This note considers the results of wind turbine modelling and power system stability investigations. Voltage stability of the power grid with grid-connected wind turbines will be improved by using blade angle control for a temporary reduction of the wind turbine power during and shortly after a short circuit fault in the grid,"['grid voltage stability control', 'wind turbine voltage stability control', 'wind turbine modelling', 'power system stability', 'power grid', 'grid-connected wind turbines', 'blade angle control', 'wind turbine power reduction', 'short circuit fault', 'offshore wind turbines', 'power system dynamic stability', 'short-circuit currents', 'wind turbines']","['stability', 'grid-connected wind turbines', 'wind turbine modelling', 'blade angle control', 'wind turbine power', 'mutual effects', 'wind', 'power grid', 'grid', 'turbines']",335,64,13,335,63,10,0,0,1
"student consulting projects benefit faculty and industry student consulting projects require students to apply or/ms tools to obtain insight into the activities of firms in the community. these projects benefit faculty by providing clear feedback on the real capabilities of students, a broad connection to local industry, and material for case studies and research. they benefit companies by stimulating new thinking regarding their activities and delivering results they can use. projects provide insights into the end-user modeling mode of or/ms practice. projects support continuous improvement as the lessons gained from a crop of projects enable better teaching during the next course offering, which in turn leads to better projects and further insights into teaching ","Student consulting projects benefit faculty and industry Student consulting projects require students to apply OR/MS tools to obtain insight into the activities of firms in the community. These projects benefit faculty by providing clear feedback on the real capabilities of students, a broad connection to local industry, and material for case studies and research. They benefit companies by stimulating new thinking regarding their activities and delivering results they can use. Projects provide insights into the end-user modeling mode of OR/MS practice. Projects support continuous improvement as the lessons gained from a crop of projects enable better teaching during the next course offering, which in turn leads to better projects and further insights into teaching","['student consulting projects', 'OR/MS tools', 'student placements', 'student capability feedback', 'case study material', 'feedback', 'management education', 'management science', 'operations research']","['projects', 'Student consulting projects', 'faculty', 'forms practice Projects', 'industry Student', 'further insights', 'better projects', 'local industry', 'forms tools', 'Student']",661,115,9,661,114,10,0,0,1
"3d reconstruction from uncalibrated-camera optical flow and its reliability evaluation we present a scheme for reconstructing a 3d structure from optical flow observed by a camera with an unknown focal length in a statistically optimal way as well as evaluating the reliability of the computed shape. first, the flow fundamental matrices are optimally computed from the observed flow. they are then decomposed into the focal length, its rate of change, and the motion parameters. next, the flow is optimally corrected so that it satisfies the epipolar equation exactly. finally, the 3d positions are computed, and their covariance matrices are evaluated. by simulations and real-image experiments, we test the performance of our system and observe how the normalization (gauge) for removing indeterminacy affects the description of uncertainty ","3D reconstruction from uncalibrated-camera optical flow and its reliability evaluation We present a scheme for reconstructing a 3D structure from optical flow observed by a camera with an unknown focal length in a statistically optimal way as well as evaluating the reliability of the computed shape. First, the flow fundamental matrices are optimally computed from the observed flow. They are then decomposed into the focal length, its rate of change, and the motion parameters. Next, the flow is optimally corrected so that it satisfies the epipolar equation exactly. Finally, the 3D positions are computed, and their covariance matrices are evaluated. By simulations and real-image experiments, we test the performance of our system and observe how the normalization (gauge) for removing indeterminacy affects the description of uncertainty","['3D reconstruction', 'uncalibrated-camera optical flow', 'reliability evaluation', 'flow fundamental matrices', 'motion parameters', 'epipolar equation', 'covariance matrices', 'real-image experiments', 'normalization', 'covariance matrices', 'image reconstruction', 'image sequences', 'renormalisation']","['uncalibrated-camera optical flow', 'reliability evaluation', 'unknown focal length', 'd reconstruction', 'd structure', 'd positions', 'd', 'flow', 'optical flow', 'focal length']",718,127,13,718,126,10,0,0,6
synchronizing experiments with linear interval systems concerns generalized control problems without exact information. <p>a method of constructing a minimal synchronizing sequence for a linear interval system over the field of real numbers is developed. this problem is reduced to a system of linear inequalities ,Synchronizing experiments with linear interval systems Concerns generalized control problems without exact information. <P>A method of constructing a minimal synchronizing sequence for a linear interval system over the field of real numbers is developed. This problem is reduced to a system of linear inequalities,"['synchronizing experiments', 'linear interval systems', 'minimal synchronizing sequence construction', 'real numbers', 'linear inequalities', 'generalized control problems', 'controllability', 'control system analysis', 'controllability', 'discrete systems', 'linear systems', 'minimisation', 'synchronisation', 'uncertain systems']","['linear interval system', 'minimal synchronising sequence', 'linear inequalities', 'control problems', 'real numbers', 'experiments', 'systems', 'linear', 'problem', 'interval']",270,45,14,270,44,10,0,0,0
"control of a thrust-vectored flying wing: a receding horizon - lpv approach this paper deals with the application of receding horizon methods to hover and forward flight models of an experimental tethered flying wing developed at caltech. the dynamics of the system are representative of a vertical landing and take off aircraft, such as a harrier around hover, or a thrust-vectored aircraft such as f18-harv or x-31 in forward flight. the adopted control methodology is a hybrid of receding horizon techniques and control lyapunov function (clf)-based ideas. first, a clf is generated using quasi-lpv methods and then, by using the clf as the terminal cost in the receding horizon optimization, stability is guaranteed. the main advantage of this approach is that stability can be guaranteed without imposing constraints in the on-line optimization, allowing the problem to be solved in a more efficient manner. models of the experimental set-up are obtained for the hover and forward flight modes. numerical simulations for different time horizons are presented to illustrate the effectiveness of the discussed methods. specifically, it is shown that a mere upper bound on the cost-to-go is not an appropriate choice for a terminal cost, when the horizon length is short. simulation results are presented using experimentally verified model parameters ","Control of a thrust-vectored flying wing: a receding horizon - LPV approach This paper deals with the application of receding horizon methods to hover and forward flight models of an experimental tethered flying wing developed at Caltech. The dynamics of the system are representative of a vertical landing and take off aircraft, such as a Harrier around hover, or a thrust-vectored aircraft such as F18-HARV or X-31 in forward flight. The adopted control methodology is a hybrid of receding horizon techniques and control Lyapunov function (CLF)-based ideas. First, a CLF is generated using quasi-LPV methods and then, by using the CLF as the terminal cost in the receding horizon optimization, stability is guaranteed. The main advantage of this approach is that stability can be guaranteed without imposing constraints in the on-line optimization, allowing the problem to be solved in a more efficient manner. Models of the experimental set-up are obtained for the hover and forward flight modes. Numerical simulations for different time horizons are presented to illustrate the effectiveness of the discussed methods. Specifically, it is shown that a mere upper bound on the cost-to-go is not an appropriate choice for a terminal cost, when the horizon length is short. Simulation results are presented using experimentally verified model parameters","['thrust-vectored flying wing control', 'receding horizon-LPV approach', 'hover flight models', 'forward flight models', 'tethered flying wing', 'Caltech', 'vertical landing take off aircraft', 'Harrier around hover', 'thrust-vectored aircraft', 'F18-HARV', 'X-31', 'receding horizon techniques', 'control Lyapunov function-based ideas', 'quasi-LPV methods', 'receding horizon optimization', 'stability guarantee', 'numerical simulations', 'terminal cost', 'nonlinear system', 'aircraft control', 'control system synthesis', 'Lyapunov methods', 'nonlinear control systems', 'optimal control']","['horizon', 'horizon optimization stability', 'thrust-vectored aircraft', 'different time horizons', 'horizon techniques', 'horizon methods', 'horizon length', 'flight models', 'lp approach', 'thrust-vectored']",1145,210,24,1145,209,10,0,0,7
"application-layer multicasting with delaunay triangulation overlays application-layer multicast supports group applications without the need for a network-layer multicast protocol. here, applications arrange themselves in a logical overlay network and transfer data within the overlay. we present an application-layer multicast solution that uses a delaunay triangulation as an overlay network topology. an advantage of using a delaunay triangulation is that it allows each application to locally derive next-hop routing information without requiring a routing protocol in the overlay. a disadvantage of using a delaunay triangulation is that the mapping of the overlay to the network topology at the network and data link layer may be suboptimal. we present a protocol, called delaunay triangulation (dt protocol), which constructs delaunay triangulation overlay networks. we present measurement experiments of the dt protocol for overlay networks with up to 10 000 members, that are running on a local pc cluster with 100 linux pcs. the results show that the protocol stabilizes quickly, e.g., an overlay network with 10 000 nodes can be built in just over 30 s. the traffic measurements indicate that the average overhead of a node is only a few kilobits per second if the overlay network is in a steady state. results of throughput experiments of multicast transmissions (using tcp unicast connections between neighbors in the overlay network) show an achievable throughput of approximately 15 mb/s in an overlay with 100 nodes and 2 mb/s in an overlay with 1000 nodes ","Application-layer multicasting with Delaunay triangulation overiays Application-layer multicast supports group applications without the need for a network-layer multicast protocol. Here, applications arrange themselves in a logical overlay network and transfer data within the overlay. We present an application-layer multicast solution that uses a Delaunay triangulation as an overlay network topology. An advantage of using a Delaunay triangulation is that it allows each application to locally derive next-hop routing information without requiring a routing protocol in the overlay. A disadvantage of using a Delaunay triangulation is that the mapping of the overlay to the network topology at the network and data link layer may be suboptimal. We present a protocol, called Delaunay triangulation (DT protocol), which constructs Delaunay triangulation overlay networks. We present measurement experiments of the DT protocol for overay networks with up to 10 000 members, that are running on a local PC cluster with 100 Linux PCs. The results show that the protocol stabilizes quickly, €.9., an overlay network with 10 000 nodes can be built in just over 30 s. The traffic measurements indicate that the average overhead of a node is only a few kilobits per second if the overlay network is ina steady state. Results of throughput experiments of multicast transmissions (using TCP unicast connections between neighbors in the overlay network) show an achievable throughput of approximately 15 Mb/s in an ‘overlay with 100 nodes and 2 Mb/s in an overlay with 1000 nodes","['application-layer multicasting', 'Delaunay triangulation overlays', 'group applications', 'network-layer multicast protocol', 'logical overlay network', 'data transfer', 'Delaunay triangulation protocol', 'measurement experiments', 'overlay networks', 'local PC cluster', 'Linux PC', 'overlay network topology', 'next-hop routing information', 'data link layer', 'DT protocol', 'network nodes', 'traffic measurements', 'average overhead', 'throughput experiments', 'multicast transmissions', 'TCP unicast connections', '15 Mbit/s', '2 Mbit/s', 'mesh generation', 'microcomputer applications', 'multicast communication', 'network topology', 'performance evaluation', 'telecommunication network routing', 'telecommunication traffic', 'transport protocols', 'workstation clusters']","['Delaunay triangulation', 'overlay', 'Application-layer multicast', 'overlay network', 'network-layer multicast protocol', 'overlay network topology', 'logical overlay network', 'overlay A disadvantage', 'network topology', 'Delaunay']",1333,241,32,1333,239,10,144,46,10
"do you see what i see? [visual technology in law firms] think of how well-done computer presentations can aid in the learning experience. they are, however, less common in client meetings, settlement conferences and the courtroom. and you have to wonder why, when the same benefits of attention focus and visual learning apply in those legal communication settings. the software and hardware components are easy to use, and they're increasingly affordable to boot. the next time you need to convey a point to an audience (be it one person or many), think of how you might benefit from the visual impact available through presentation software like powerpoint. anyone will understand you more easily when assisted by visual input, and it may make all the difference in reaching visual-focused learners ","Do you see what | see? [visual technology in law firms] Think of how well-done computer presentations can aid in the learning experience. They are, however, less common in client meetings, settlement conferences and the courtroom. And you have to wonder why, when the same benefits of attention focus and visual learning apply in those legal communication settings. The software and hardware ‘components are easy to use, and they're increasingly affordable to boot. The next time you need to convey a point to an audience (be it ‘one person or many), think of how you might benefit from the visual impact available through presentation software like PowerPoint. Anyone will understand you more easily when assisted by visual input, and it may make all the difference in reaching visual-focused learners","['computer presentations', 'visual technology', 'law firms', 'PowerPoint', 'business graphics', 'law administration']","['visual', 'well-honed computer presentations', 'legal communication settings', 'presentation software', 'visual technology', 'visual learning', 'visual impact', 'visual input', 'law firms', 'presentation']",672,130,6,674,129,10,14,3,1
"the quadratic 0-1 knapsack problem with series-parallel support we consider various special cases of the quadratic 0-1 knapsack problem (qkp) for which the underlying graph structure is fairly simple. for the variant with edge series-parallel graphs, we give a dynamic programming algorithm with pseudo-polynomial time complexity, and a fully polynomial time approximation scheme. in strong contrast to this, the variant with vertex series-parallel graphs is shown to be strongly np-complete ","The quadratic 0-1 knapsack problem with series-parallel support We consider various special cases of the quadratic 0-1 knapsack problem (QKP) for which the underlying graph structure is fairly simple. For the variant with edge series-parallel graphs, we give a dynamic programming algorithm with pseudo-polynomial time complexity, and a fully polynomial time approximation scheme. In strong contrast to this, the, variant with vertex series-parallel graphs is shown to be strongly NP-complete","['quadratic 0-1 knapsack problem', 'series-parallel support', 'underlying graph structure', 'dynamic programming algorithm', 'pseudo-polynomial time complexity', 'fully polynomial time approximation scheme', 'NP-complete problem', 'computational complexity', 'dynamic programming', 'graph theory', 'knapsack problems', 'polynomial approximation']","['quadratic', 'knapsack', 'problem', 'edge series-parallel graphs', 'series-parallel support', 'various special cases', 'graph structure', 'r-1', 'series-parallel', 'series-parallel graphs']",422,71,12,423,70,10,0,1,4
"use of neural networks in the analysis of particle size distribution by laser diffraction: tests with different particle systems the application of forward light scattering methods for estimating the particle size distribution (psd) is usually limited by the occurrence of multiple scattering, which affects the angular distribution of light in highly concentrated suspensions, thus resulting in false calculations by the conventionally adopted algorithms. in this paper, a previously proposed neural network-based method is tested with different particle systems, in order to evaluate its applicability. in the first step of the study, experiments were carried out with solid-liquid suspensions having different characteristics of particle shape and size distribution, under varying solid concentrations. the experimental results, consisting of the angular distribution of light intensity, particle shape and suspension concentration, were used as input data in the fitting of neural network models (nn) that replaced the optical model to provide the psd. the reference values of particle shape and psd for the nn fitting were based on image analysis. comparisons between the psd values computed by the nn model and the reference values indicate that the method can be used in monitoring the psd of particles with different shapes in highly concentrated suspensions, thus extending the range of application of forward laser diffraction to a number of systems with industrial interest ","Use of neural networks in the analysis of particle size distribution by laser diffraction: tests with different particle systems The application of forward light scattering methods for estimating the particle size distribution (PSD) is usually limited by the occurrence of multiple scattering, which affects the angular distribution of light in highly concentrated suspensions, thus resulting in false calculations by the conventionally adopted algorithms. In this paper, a previously proposed neural network-based method is tested with different particle systems, in order to evaluate its applicability. In the first step of the study, experiments were carried out with solid-liquid suspensions having different characteristics of particle shape and size distribution, under varying solid concentrations. The experimental results, consisting of the angular distribution of light intensity, particle shape and suspension concentration, were used as input data in the fitting of neural network models (NN) that replaced the optical model to provide the PSD. The reference values of particle shape and PSD for the NN fitting were based on image analysis. Comparisons between the PSD values computed by the NN model and the reference values indicate that the method can be used in monitoring the PSD of particles with different shapes in highly concentrated suspensions, thus extending the range of application of forward laser diffraction to a number of systems with industrial interest","['particle size distribution', 'laser diffraction', 'neural network modeling', 'forward light scattering', 'multiple scattering', 'angular distribution of light', 'solid-liquid suspensions', 'particle shape distribution', 'image analysis', 'pattern recognition', 'powdered materials', 'backpropagation algorithm', 'Fraunhofer optical model', 'fluidized catalytic cracking', 'backpropagation', 'Fraunhofer diffraction', 'light scattering', 'measurement by laser beam', 'neural nets', 'particle size measurement', 'pattern recognition', 'physics computing', 'suspensions']","['particle shape', 'particle size distribution', 'different particle systems', 'concentrated suspensions', 'particle', 'neural network-based method', 'neural network models', 'different shapes', 'neural networks', 'size distribution']",1267,219,23,1267,218,10,0,0,5
record makers [uk health records] plans for a massive cradle-to-grave electronic records project have been revealed by the government. is the scheme really viable? ,Record makers [UK health records] Plans for a massive cradle-to-grave electronic records project have been revealed by the government. Is the scheme really viable?,"['UK health records', 'electronic records project', 'integrated care records services', 'health care', 'social care', 'health care']","['health records Plans', 'cradle-to-grave', 'Record makers', 'electronic', 'massive', 'Record', 'Plans', 'makers', 'health', 'records']",140,25,6,140,24,10,0,0,0
on lag windows connected with jacobi polynomials lag windows whose corresponding spectral windows are jacobi polynomials or sums of jacobi polynomials are introduced. the bias and variance of their spectral density estimators are investigated and their window bandwidth and characteristic exponent are determined ,On lag windows connected with Jacobi polynomials Lag windows whose corresponding spectral windows are Jacobi polynomials or sums of Jacobi polynomials are introduced. The bias and variance of their spectral density estimators are investigated and their window bandwidth and characteristic exponent are determined,"['lag windows', 'Jacobi polynomials', 'spectral windows', 'spectral density estimators', 'window bandwidth', 'characteristic exponent', 'Jacobian matrices', 'polynomials']","['Jacobi polynomials', 'corresponding spectral windows', 'spectral density estimators', 'window bandwidth', 'windows', 'spectral', 'lag windows', 'Lag windows', 'Jacobi', 'polynomials']",270,44,8,270,43,10,0,0,1
"effects of the transition to a client-centred team organization in administrative surveying work a new work organization was introduced in administrative surveying work in sweden during 1998. the new work organization implied a transition to a client-centred team-based organization and required a change in competence from specialist to generalist knowledge as well as a transition to a new information technology, implying a greater integration within the company. the aim of this study was to follow the surveyors for two years from the start of the transition and investigate how perceived consequences of the transition, job, organizational factors, well-being and effectiveness measures changed between 1998 and 2000. the teamwork profile and qps nordic questionnaire were used. the 205 surveyors who participated in all three study phases constituted the study group. the result showed that surveyors who perceived that they were working as generalists rated the improvements in job and organizational factors significantly higher than those who perceived that they were not yet generalists. improvements were noted in 2000 in quality of service to clients, time available to handle a case and effectiveness of teamwork in a transfer to a team-based work organization group, cohesion and continuous improvement practices-for example, learning by doing, mentoring and guided delegation-were important to improve the social effectiveness of group work ","Effects of the transition to a client-centred team organization in administrative surveying work ‘Anew work organization was introduced in administrative surveying work in Sweden during 1998. The new work organization implied a transition to a client-centred team-based organization and required a change in ‘competence from specialist to generalist knowledge as well as a transition to a new information technology, implying a greater integration within the company. The aim of this study was to follow the surveyors for two years from the start of the transition and investigate how perceived consequences of the transition, job, organizational factors, well-being and effectiveness measures changed between 1998 and 2000. The Teamwork Profile and QPS Nordic questionnaire were used. The 205 surveyors who participated in all three study phases constituted the study group. The result showed that surveyors who perceived that they were working as generalists rated the improvements in job and organizational factors significantly higher than those who perceived that they were not yet generalists. Improvements were noted in 2000 in quality of service to clients, time available to handle a case and effectiveness of teamwork in a transfer to a team-based work organization group, cohesion and continuous improvement practices-for example, learning by doing, mentoring and guided delegation-were important to improve the social effectiveness of group work","['client-centred team organization', 'administrative surveying work', 'information technology', 'company', 'job', 'organizational factors', 'effectiveness measures', 'Teamwork Profile', 'QPS Nordic questionnaire', 'social effectiveness', 'public administrative sector', 'human factors', 'human resource management', 'information technology', 'personnel', 'public administration', 'social aspects of automation']","['organization', 'new work organization', 'work', 'client-centred team-based organization', 'client-centred team organization', 'new information technology', 'transition job', 'group work', 'client-centred', 'transition']",1243,215,17,1245,213,10,671,200,6
"a 3-stage pipelined architecture for multi-view images decoder in this paper, we proposed the architecture of the decoder which implements the multi-view images decoding algorithm. the study of the hardware structure of the multi-view image processing has not been accomplished. the proposed multi-view images decoder operates in a three stage pipelined manner and extracts the depth of the pixels of the decoded image every clock. the multi-view images decoder consists of three modules, node selector which transfers the value of the nodes repeatedly and depth extractor which extracts the depth of each pixel from the four values of the nodes and affine transformer which generates the projecting position on the image plane from the values of the pixels and the specified viewpoint. the proposed architecture is designed and simulated by the max+plusii design tool and the operating frequency is 30 mhz. the image can be constructed in a real time by the decoder with the proposed architecture ","A3-stage pipelined architecture for multi-view images decoder In this paper, we proposed the architecture of the decoder which implements the multi-view images decoding algorithm. The study of the hardware structure of the multi-view image processing has not been accomplished The proposed multi-view images decoder operates in a three stage pipelined manner and extracts the depth of the pixels of the decoded image every clock. The multi-view images decoder consists of three modules, Node selector which transfers the value of the nodes repeatedly and Depth Extractor which extracts the depth of each pixel from the four values of the nodes and Affine Transformer which generates the projecting position on the image plane from the values of the pixels and the specified viewpoint. The proposed architecture is designed and simulated by the Max+Plusll design tool and the operating frequency is 30 MHz. The image can be constructed in a real time by the decoder with the proposed architecture","['three-stage pipelined architecture', 'multi-view images decoder', 'hardware structure', 'pixel depth', 'node selector', 'depth extractor', 'affine transformer', 'viewpoint', 'Max+PlusII design tool', 'operating frequency', '30 MHz', 'decoding', 'image coding', 'parallel architectures', 'pipeline processing']","['multiview images decoder', 'on-stage pipeline architecture', 'multiview image processing', 'stage pipeline manner', 'image plane', 'multiview', 'architecture', 'images', 'decoder', 'multiview images']",841,158,15,840,156,10,534,156,9
"broadcasts keep staff in picture [intranets] mark hawkins, chief operating officer at uk-based streaming media specialist twofourtv, explains how firms can benefit by linking their corporate intranets to broadcasting technology ","Broadcasts keep staff in picture [intranets] Mark Hawkins, chief operating officer at UK-based streaming media specialist ‘Twofourtv, explains how firms can benefit by linking their corporate intranets to broadcasting technology","['corporate intranets', 'Twofourtv', 'streaming media', 'broadcasting technology', 'broadcasting', 'intranets']","['Mark hawkins', 'Broadcasts', 'intranets', 'UK-based', 'picture', 'officer', 'staff', 'chief', 'Mark', 'hawkins']",198,31,6,199,30,10,10,1,4
"improved detection of lung nodules by using a temporal subtraction technique the authors evaluated the effect of a temporal subtraction technique for digital chest radiography with regard to the accuracy of detection of lung nodules. twenty solitary lung nodules smaller than 30 mm in diameter, including 10 lung cancers and 10 benign nodules, were used. the nodules were grouped subjectively according to their subtlety. for nonnodular cases, 20 nodules without perceptible interval changes were selected. all chest radiographs were obtained by using a computed radiographic system, and temporal subtraction images were produced by using a program developed at the university of chicago. the effect of the temporal subtraction image was evaluated by using an observer performance study, with use of receiver operating characteristic analysis. observer performance with temporal subtraction images was substantially improved (a/sub z/ = 0.980 and 0.958), as compared with that without temporal subtraction images (a/sub z/ = 0.920 and 0.825) for the certified radiologists and radiology residents, respectively. the temporal subtraction technique clearly improved diagnostic accuracy for detecting lung nodules, especially subtle cases. in conclusion, the temporal subtraction technique is useful for improving detection accuracy for peripheral lung nodules on digital chest radiographs ","Improved detection of lung nodules by using a temporal subtraction technique The authors evaluated the effect of a temporal subtraction technique for digital chest radiography with regard to the accuracy of detection of lung nodules. Twenty solitary lung nodules smaller than 30 mm in diameter, including 10 lung cancers and 10 benign nodules, were used. The nodules were grouped subjectively according to their subtlety. For nonnodular cases, 20 nodules without perceptible interval changes were selected. All chest radiographs were obtained by using a computed radiographic system, and temporal subtraction images were produced by using a program developed at the University of Chicago. The effect of the temporal subtraction image was evaluated by using an observer performance study, with use of receiver operating characteristic analysis. Observer performance with temporal subtraction images was substantially improved (A/sub z/ = 0.980 and 0.958), as compared with that without temporal subtraction images (A/sub z/ = 0.920 and 0.825) for the certified radiologists and radiology residents, respectively. The temporal subtraction technique clearly improved diagnostic accuracy for detecting lung nodules, especially subtle cases. In conclusion, the temporal subtraction technique is useful for improving detection accuracy for peripheral lung nodules on digital chest radiographs","['improved lung nodules detection', 'temporal subtraction technique', 'digital chest radiography', 'perceptible interval changes', 'observer performance', 'peripheral lung nodules', 'radiology residents', 'certified radiologists', 'subtle cases', 'University of Chicago', 'computed radiographic system', 'medical diagnostic imaging', '30 mm', 'cancer', 'diagnostic radiography', 'lung', 'medical image processing']","['temporal subtraction technique', 'temporal subtraction images', 'nodules', 'lung nodules', 'lung', 'peripheral lung nodules', 'solitary lung nodules', 'detection accuracy', 'benign nodules', 'lung cancers']",1191,197,17,1191,196,10,0,0,10
"an interlingua-based chinese-english mt system chinese-english machine translation is a significant and challenging problem in information processing. the paper presents an interlingua-based chinese-english natural language translation system (icent). it introduces the realization mechanism of chinese language analysis, which contains syntactic parsing and semantic analyzing and gives the design of interlingua in details. experimental results and system evaluation are given. the result is satisfying ","An interlingua-based Chinese-English MT system Chinese-English machine translation is a significant and challenging problem in information processing. The paper presents an interlingua-based Chinese-English natural language translation system (ICENT). It introduces the realization mechanism of Chinese language analysis, which contains syntactic parsing and semantic analyzing and gives the design of interlingua in details. Experimental results and system evaluation are given. The result is satistying","['interlingua-based Chinese-English machine translation system', 'information processing', 'natural language translation system', 'syntactic parsing', 'semantic analyzing', 'grammars', 'language translation', 'natural language interfaces']","['interlingua-based', 'Chinese-English machine translation', 'Chinese language analysis', 'information processing', 'challenging problem', 'system evaluation', 'system', 'language', 'translation', 'Chinese-English']",442,64,8,442,63,10,1,1,2
"induced-shear piezoelectric actuators for rotor blade trailing edge flaps much of the current rotorcraft research is focused on improving performance by reducing unwanted helicopter noise and vibration. one of the most promising active rotorcraft vibration control systems is an active trailing edge flap. in this paper, an induced-shear piezoelectric tube actuator is used in conjunction with a simple lever-cusp hinge amplification device to generate a useful combination of trailing edge flap deflections and hinge moments. a finite-element model of the actuator tube and trailing edge flap (including aerodynamic and inertial loading) was used to guide the design of the actuator-flap system. a full-scale induced shear tube actuator flap system was fabricated and bench top testing was conducted to validate the analysis. hinge moments corresponding to various rotor speeds were applied to the actuator using mechanical springs. the testing demonstrated that for an applied electric field of 3 kv cm/sup -1/ the tube actuator deflected a representative full-scale 12 inch flap +or-2.8 degrees at 0 rpm and +or-1.4 degrees for a hinge moment simulating a 400 rpm condition. the per cent error between the predicted and experimental full-scale flap deflections ranged from 4% (low rpm) to 12.5% (large rpm). increasing the electric field to 4 kv cm/sup -1/ results in +or-2.5 degrees flap deflection at a rotation speed of 400 rpm, according to the design analysis. a trade study was conducted to compare the performance of the piezoelectric tube actuator to the state of the art in trailing edge flap actuators and indicated that the induced-shear tube actuator shows promise as a trailing edge flap actuator ","Induced-shear piezoelectric actuators for rotor blade trailing edge flaps Much of the current rotorcraft research is focused on improving performance by reducing unwanted helicopter noise and vibration. One of the most promising active rotorcraft vibration control systems is an active trailing edge flap. In this paper, an induced-shear piezoelectric tube actuator is used in conjunction with a simple lever-cusp hinge amplification device to generate a useful combination of trailing edge flap deflections and hinge moments. A finite-element model of the actuator tube and trailing edge flap (including aerodynamic and inertial loading) was used to guide the design of the actuator-flap system. A full-scale induced shear tube actuator flap system was, fabricated and bench top testing was conducted to validate the analysis. Hinge moments corresponding to various rotor speeds were applied to the actuator using mechanical springs. The testing demonstrated that for an applied electric field of 3 kV cm/sup -1/ the tube actuator deflected a representative full-scale 12 inch flap +01-2.8 degrees at 0 rpm and +or-1.4 degrees for a hinge moment simulating a 400 rpm condition. The per cent error between the predicted and experimental full-scale flap deflections ranged from 4% (low rpm) to 12.5% (large rpm). Increasing the electric field to 4 kV cm/sup -1/ results in +or-2.5 degrees flap deflection at a rotation speed of 400 rpm, according to the design analysis. A trade study was conducted to compare the performance of the piezoelectric tube actuator to the state of the art in trailing edge flap actuators and indicated that the induced-shear tube actuator shows promise as a trailing edge flap actuator","['rotorcraft', 'helicopter noise', 'helicopter vibration', 'vibration control', 'active trailing edge flap', 'lever-cusp hinge amplification device', 'finite-element model', 'aerodynamic loading', '12 inch flap', 'inertial loading', 'design', 'shear tube actuator flap', 'bench top testing', 'piezoelectric tube actuator', 'induced-shear tube actuator', '12 inch', 'aircraft control', 'finite element analysis', 'helicopters', 'noise abatement', 'piezoelectric actuators', 'rotors', 'vibration control']","['actuators', 'piezoelectric tube actuators', 'edge flap actuators', 'Induced-shear piezoelectric actuators', 'induced-shear tube actuators', 'edge flap deflections', 'tube actuators', 'actuators tube', 'edge flaps', 'inch flap']",1448,266,23,1449,265,10,2,2,9
"outlier resistant adaptive matched filtering robust adaptive matched filtering (amf) whereby outlier data vectors are censored from the covariance matrix estimate is considered in a maximum likelihood estimation (mle) setting. it is known that outlier data vectors whose steering vector is highly correlated with the desired steering vector, can significantly degrade the performance of amf algorithms such as sample matrix inversion (smi) or fast maximum likelihood (fml). four new algorithms that censor outliers are presented which are derived via approximation to the mle solution. two algorithms each are related to using the smi or the fml to estimate the unknown underlying covariance matrix. results are presented using computer simulations which demonstrate the relative effectiveness of the four algorithms versus each other and also versus the smi and fml algorithms in the presence of outliers and no outliers. it is shown that one of the censoring algorithms, called the reiterative censored fast maximum likelihood (cfml) technique is significantly superior to the other three censoring methods in stressful outlier scenarios ","Outlier resistant adaptive matched filtering Robust adaptive matched filtering (AMF) whereby outlier data vectors are censored from the covariance matrix estimate is considered in a maximum likelihood estimation (MLE) setting. It is known that outlier data vectors whose steering vector is highly correlated with the desired steering vector, can significantly degrade the performance of AMF algorithms such as sample matrix inversion (SMI) or fast maximum likelihood (FML). Four new algorithms that censor outliers are presented which are derived via approximation to the MLE solution. Two algorithms each are related to using the SMI or the FML to estimate the unknown underlying covariance matrix. Results are presented using ‘computer simulations which demonstrate the relative effectiveness of the four algorithms versus each other and also versus the SMI and FML algorithms in the presence of outliers and no outliers. It is shown that one of the censoring algorithms, called the reiterative censored fast maximum likelihood (CFML) technique is significantly superior to the other three censoring methods in stressful outlier scenarios","['outlier resistant adaptive matched filtering', 'covariance matrix estimate', 'maximum likelihood estimation setting', 'steering vector', 'sample matrix inversion', 'fast maximum likelihood', 'censoring algorithms', 'reiterative censored fast maximum likelihood', 'adaptive filters', 'covariance matrices', 'filtering theory', 'matched filters', 'maximum likelihood estimation']","['fast maximum likelihood', 'outliers', 'outlier data vectors', 'steering vector', 'adaptive', 'stressful outlier scenarios', 'covariance matrix estimate', 'covariance matrix Results', 'new algorithms', 'ml algorithms']",972,169,13,973,168,10,8,1,0
"research into telehealth applications in speech-language pathology a literature review was conducted to investigate the extent to which telehealth has been researched within the domain of speech-language pathology and the outcomes of this research. a total of 13 studies were identified. three early studies demonstrated that telehealth was feasible, although there was no discussion of the cost-effectiveness of this process in terms of patient outcomes. the majority of the subsequent studies indicated positive or encouraging outcomes resulting from telehealth. however, there were a number of shortcomings in the research, including a lack of cost-benefit information, failure to evaluate the technology itself, an absence of studies of the educational and informational aspects of telehealth in relation to speech-language pathology, and the use of telehealth in a limited range of communication disorders. future research into the application of telehealth to speech-language pathology services must adopt a scientific approach, and have a well defined development and evaluation framework that addresses the effectiveness of the technique, patient outcomes and satisfaction, and the cost-benefit relationship ","Research into telehealth applications in speech-language pathology A literature review was conducted to investigate the extent to which telehealth has been researched within the domain of speech-language pathology and the outcomes of this research. A total of 13 studies were identified Three early studies demonstrated that telehealth was feasible, although there was no discussion of the cost-effectiveness of this process in terms of patient outcomes. The majority of the subsequent studies indicated positive or encouraging outcomes resulting from telehealth. However, there were a number of shortcomings in the research, including a lack of cost-benefit information, failure to evaluate the technology itself, an absence of studies of the educational and informational aspects of telehealth in relation to speech-language pathology, and the use of telehealth in a limited range of communication disorders. Future research into the application of telehealth to speech-language pathology services must adopt a scientific approach, and have a well defined development and evaluation framework that addresses the effectiveness of the technique, patient outcomes and satisfaction, and the cost-benefit relationship","['telehealth applications', 'speech-language pathology', 'literature review', 'telemedicine', 'cost-effectiveness', 'patient outcomes', 'cost-benefit analysis', 'communication disorders', 'patient satisfaction', 'cost-benefit analysis', 'medical computing', 'reviews', 'speech', 'telemedicine']","['speech-language pathology services', 'techniques patient outcomes', 'telehealth applications', 'encouraging outcomes', 'early studies', 'pathology', 'speech-language', 'telehealth', 'patient outcomes', 'speech-language pathology']",1046,171,14,1045,170,10,0,1,4
"numerical studies of 2d free surface waves with fixed bottom the motion of surface waves under the effect of bottom is a very interesting and challenging phenomenon in the nature. we use boundary integral method to compute and analyze this problem. in the linear analysis, the linearized equations have bounded error increase under some compatible conditions. this contributes to the cancellation of instable kelvin-helmholtz terms. under the effect of bottom, the existence of equations is hard to determine, but given some limitations it proves true. these limitations are that the swing of interfaces should be small enough, and the distance between surface and bottom should be large enough. in order to maintain the stability of computation, some compatible relationship must be satisfied. in the numerical examples, the simulation of standing waves and breaking waves are calculated. and in the case of shallow bottom, we found that the behavior of waves are rather singular ","Numerical studies of 2D free surface waves with fixed bottom The motion of surface waves under the effect of bottom is a very interesting and challenging phenomenon in the nature. we use boundary integral method to compute and analyze this problem. In the linear analysis, the linearized equations have bounded error increase under some compatible conditions. This contributes to the cancellation of instable Kelvin-Helmholtz terms. Under the effect of bottom, the existence of equations is hard to determine, but given some limitations it proves true. These limitations are that the swing of interfaces should be small enough, and the distance between surface and bottom should be large enough. In order to maintain the stability of computation, some compatible relationship must be satisfied. In the numerical examples, the simulation of standing waves and breaking waves are calculated. And in the case of shallow bottom, we found that the behavior of waves are rather singular","['numerical studies', '2D free surface waves', 'boundary integral method', 'linear analysis', 'linearized equations', 'instable Kelvin-Helmholtz terms', 'boundary integral equations', 'extrapolation']","['free surface waves', 'Numerical studies', 'shallow bottom', 'waves', 'surface', 'free', 'bottom', 'studies', 'Numerical', 'surface waves']",828,154,8,828,153,10,0,0,4
"on generalized gaussian quadratures for exponentials and their applications we introduce new families of gaussian-type quadratures for weighted integrals of exponential functions and consider their applications to integration and interpolation of bandlimited functions. we use a generalization of a representation theorem due to caratheodory to derive these quadratures. for each positive measure, the quadratures are parameterized by eigenvalues of the toeplitz matrix constructed from the trigonometric moments of the measure. for a given accuracy epsilon , selecting an eigenvalue close to epsilon yields an approximate quadrature with that accuracy. to compute its weights and nodes, we present a new fast algorithm. these new quadratures can be used to approximate and integrate bandlimited functions, such as prolate spheroidal wave functions, and essentially bandlimited functions, such as bessel functions. we also develop, for a given precision, an interpolating basis for bandlimited functions on an interval ","On generalized Gaussian quadratures for exponentials and their applications, We introduce new families of Gaussian-type quadratures for weighted integrals of exponential functions and consider their applications to integration and interpolation of bandlimited functions. We use a generalization of a representation theorem due to Caratheodory to derive these quadratures. For each positive measure, the quadratures are parameterized by eigenvalues of the Toeplitz matrix constructed from the trigonometric moments of the measure. For a given accuracy epsilon . selecting an eigenvalue close to epsilon yields an approximate ‘quadrature with that accuracy. To compute its weights and nodes, we present a new fast algorithm. These new quadratures can be used to approximate and integrate bandlimited functions, such as prolate spheroidal wave functions, and essentially bandlimited functions, such as Bessel functions. We also develop, for a given precision, an interpolating basis for bandlimited functions on an interval","['generalized Gaussian quadratures', 'weighted integrals', 'exponential functions', 'integration', 'interpolation', 'bandlimited functions', 'Caratheodory representation theorem', 'eigenvalues', 'Toeplitz matrix', 'trigonometric moments', 'approximation', 'prolate spheroidal wave functions', 'Bessel functions', 'approximation theory', 'Bessel functions', 'eigenvalues and eigenfunctions', 'integration', 'interpolation', 'Toeplitz matrices', 'wave functions']","['bandlimited functions', 'functions', 'quadratures', 'Gaussian-type quadratures', 'spherical wave functions', 'exponential functions', 'Gaussian quadratures', 'new fast algorithm', 'new quadratures', 'new families']",876,144,20,878,143,10,11,3,8
"when a better interface and easy navigation aren't enough: examining the information architecture in a law enforcement agency an information architecture that allows users to easily navigate through a system and quickly recover from mistakes is often defined as a highly usable system. but usability in systems design goes beyond a good interface and efficient navigation. in this article we describe two database systems in a law enforcement agency. one system is a legacy, text-based system with cumbersome navigation (rms); the newer system is a graphical user interface with simplified navigation (copnet). it is hypothesized that law enforcement users will evaluate copnet higher than rms, but experts of the older system will evaluate it higher than others will. we conducted two user studies. one study examined what users thought of rms and copnet, and compared rms experts' evaluations with nonexperts. we found that all users evaluated copnet as more effective, easier to use, and easier to navigate than rms, and this was especially noticeable for users who were not experts with the older system. the second, follow-up study examined use behavior after copnet was deployed some time later. the findings revealed that evaluations of copnet were not associated with its use. if the newer system had a better interface and was easier to navigate than the older, legacy system, why were law enforcement personnel reluctant to switch? we discuss reasons why switching to a new system is difficult, especially for those who are most adept at using the older system. implications for system design and usability are also discussed ","When a better interface and easy navigation aren't enough: examining the information architecture in a law enforcement agency An information architecture that allows users to easily navigate through a system and quickly recover from mistakes is often defined as a highly usable system. But usability in systems design goes beyond a good interface and efficient navigation. In this article we describe two database systems in a law enforcement agency. One system is a legacy, text-based system with cumbersome navigation (RMS); the newer system is a graphical user interface with simplified navigation (CopNet). It is hypothesized that law enforcement users will evaluate CopNet higher than RMS, but experts of the older system will evaluate it higher than others will. We conducted two user studies. One study examined what users thought of RMS and CopNet, and compared RMS experts’ evaluations with nonexperts. We found that all users evaluated CopNet as more effective, easier to use, and easier to navigate than RMS, and this was. especially noticeable for users who were not experts with the older system. The second, follow-up study examined use behavior after CopNet was deployed some time later. The findings revealed that evaluations of CopNet were not associated with its use. If the newer system had a better interface and was easier to navigate than the older, legacy system, why were law enforcement personnel reluctant to switch? We discuss reasons why switching to a new system is difficult, especially for those who are most adept at using the older system. Implications for system design and usability are also discussed","['information architecture', 'law enforcement agency', 'legacy text-based system', 'RMS', 'graphical user interface', 'simplified navigation', 'CopNet', 'law enforcement users', 'graphical user interfaces', 'information systems', 'police data processing']","['newer system', 'older system Implications', 'law enforcement users', 'older legacy system', 'text-based system', 'database systems', 'systems design', 'usable system', 'new system', 'older system']",1377,260,11,1378,259,10,1,2,5
"on the convergence of the bermudez-moreno algorithm with constant parameters a. bermudez and c. moreno (1981) presented a duality numerical algorithm for solving variational inequalities of the second kind. the performance of this algorithm strongly depends on the choice of two constant parameters. assuming a further hypothesis of the inf-sup type, we present here a convergence theorem that improves on the one presented by a. bermudez and c. moreno. we prove that the convergence is linear, and we give the expression of the asymptotic error constant and the explicit form of the optimal parameters, as a function of some constants related to the variational inequality. finally, we present some numerical examples that confirm the theoretical results ","On the convergence of the Bermudez-Moreno algorithm with constant parameters A Bermudez and C. Moreno (1981) presented a duality numerical algorithm for solving variational inequalities of the second kind. The performance of this algorithm strongly depends on the choice of two constant parameters. Assuming a further hypothesis of the inf-sup type, we present here a convergence theorem that improves on the one presented by A. Bermudez and C. Moreno. We prove that the convergence is linear, and we give the expression of the asymptotic error constant and the explicit form of the optimal parameters, as a function of some constants related to the variational inequality. Finally, we present some numerical examples that confirm the theoretical results","['Bermudez-Moreno algorithm', 'duality numerical algorithm', 'variational inequalities', 'convergence theorem', 'asymptotic error constant', 'optimal parameters', 'constant parameters', 'convergence of numerical methods', 'variational techniques']","['constant parameters', 'Bermudez-Moreno algorithm', 'variation inequalities', 'numerical algorithm', 'convergence theorem', 'optimal parameters', 'A. Bermudez', 'constants', 'algorithm', 'convergence']",640,117,9,639,116,10,0,1,0
"approximating martingales for variance reduction in markov process simulation ""knowledge of either analytical or numerical approximations should enable more efficient simulation estimators to be constructed."" this principle seems intuitively plausible and certainly attractive, yet no completely satisfactory general methodology has been developed to exploit it. the authors present a new approach for obtaining variance reduction in markov process simulation that is applicable to a vast array of different performance measures. the approach relies on the construction of a martingale that is then used as an internal control variate ","Approximating martingales for variance reduction in Markov process simulation ""Knowledge of either analytical or numerical approximations should enable more efficient simulation estimators to be constructed."" This principle seems intuitively plausible and certainly attractive, yet no completely satisfactory general methodology has been developed to exploit it. The, authors present a new approach for obtaining variance reduction in Markov process simulation that is applicable to a vast array of different performance measures. The approach relies on the construction of a martingale that is then used as an internal control variate","['Markov process simulation', 'variance reduction', 'approximating martingale-process method', 'martingales', 'performance measures', 'internal control variate', 'complex stochastic processes', 'single-server queue', 'approximation theory', 'Markov processes', 'queueing theory', 'state-space methods']","['Markov process simulation', 'variance reduction', 'martindale', 'efficient simulation estimators', 'different performance measures', 'new approach', 'simulation', 'Markov', 'variance', 'reduction']",547,89,12,548,88,10,0,1,2
"estimation of thermal coefficients of magneto-optical media previously we described a method for estimating the thermal conductivity of magneto-optic recording media. the method relies on identifying the laser power that brings the maximum temperature of the tbfeco layer to as high as the curie temperature. we extensively use a similar method to estimate the heat capacity of a dielectric layer, a tbfeco layer, and an aluminum alloy layer of magneto-optic recording media. measurements are conducted on static disks with a beam of light focused on a tbfeco layer. the method has the advantage of thermal diffusion depending on a multilayer structure and irradiation time ","Estimation of thermal coefficients of magneto-optical media Previously we described a method for estimating the thermal conductivity of magneto-optic recording media. The method relies on identifying the laser power that brings the maximum temperature of the TbFeCo layer to as high as the Curie temperature. We extensively use a similar method to estimate the heat capacity of a dielectric layer, a ToFeCo layer, and an aluminum alloy layer of magneto-optic recording media Measurements are conducted on static disks with a beam of light focused ‘ona TbFeCo layer. The method has the advantage of thermal diffusion depending on a multilayer structure and irradiation time","['thermal coefficients', 'magneto-optical media', 'thermal conductivity', 'laser power', 'maximum temperature', 'TbFeCo layer', 'Curie temperature', 'dielectric layer', 'heat capacity', 'aluminum alloy layer', 'magneto-optic recording media', 'static disks', 'light focusing', 'thermal diffusion', 'multilayer structure', 'irradiation time', 'TbFeCo', 'cobalt alloys', 'iron alloys', 'magnetic thin films', 'magneto-optical recording', 'optical disc storage', 'optical multilayers', 'terbium alloys', 'thermal conductivity measurement']","['magneto-optical recording media', 'TbFeCo layer', 'magneto-optical media', 'thermal conductivity', 'thermal coefficients', 'aluminum alloy layer', 'thermal diffusion', 'dielectric layer', 'similar method', 'toledo layer']",570,105,25,570,103,10,64,21,7
"women of color in computing it is well known that there is a need to increase the number of women in the area of computing, that is in computer science and computer engineering. if we consider women of color, that is women of under-represented ethnicities, we find the numbers are very dismal. the goal of this article is to bring to light the unique issues of women of color based upon the personal experience of one african-american woman who has been in the field of computing for over 20 years (including the years of higher education) ","‘Women of color in computing It is well known that there is a need to increase the number of women in the area of computing, that is in computer science and computer engineering. If we consider women of color, that is women of under-represented ethnicities, we find the numbers are very dismal. The goal of this article is to bring to light the unique issues of women of color based upon the personal experience of one African-American woman who has been in the field of computing for over 20 years (including the years of higher education)","['women of color', 'computer science', 'computer engineering', 'ethnic minority', 'higher education', 'society', 'gender issues', 'computer science', 'computer science education', 'gender issues', 'prejudicial factors']","['women', 'color', 'number', 'under-represented ethnicities', 'African-American woman', 'computer engineering', 'personal experience', 'computer science', 'unique issues', 'computer']",444,97,11,445,96,10,5,1,1
"the dynamics of a railway freight wagon wheelset with dry friction damping we investigate the dynamics of a simple model of a wheelset that supports one end of a railway freight wagon by springs with linear characteristics and dry friction dampers. the wagon runs on an ideal, straight and level track with constant speed. the lateral dynamics in dependence on the speed is examined. we have included stick-slip and hysteresis in our model of the dry friction and assume that coulomb's law holds during the slip phase. it is found that the action of dry friction completely changes the bifurcation diagram, and that the longitudinal component of the dry friction damping forces destabilizes the wagon ","The dynamics of a railway freight wagon wheelset with dry friction damping We investigate the dynamics of a simple model of a wheelset that supports one end of a railway freight wagon by springs with linear characteristics and dry friction dampers. The wagon runs on an ideal, straight and level track with constant speed. The lateral dynamics in dependence on the speed is examined. We have included stick-slip and hysteresis in ‘our model of the dry friction and assume that Coulomb's law holds during the slip phase. It is found that the action of dry friction completely changes the bifurcation diagram, and that the longitudinal ‘component of the dry friction damping forces destabilizes the wagon","['dynamics', 'railway freight wagon wheelset', 'dry friction damping', 'linear characteristics', 'lateral dynamics', 'stick-slip', 'hysteresis', 'Coulomb law', 'bifurcation diagram', 'longitudinal component', 'bifurcation', 'damping', 'dynamics', 'friction', 'railways']","['railway freight wagon', 'dry friction dampers', 'lateral dynamics', 'constant speed', 'simple model', 'dry', 'friction', 'dynamics', 'dry friction', 'wagon']",586,116,15,588,115,10,12,2,7
"accurate modeling of lossy nonuniform transmission lines by using differential quadrature methods this paper discusses an efficient numerical approximation technique, called the differential quadrature method (dqm), which has been adapted to model lossy uniform and nonuniform transmission lines. the dqm can quickly compute the derivative of a function at any point within its bounded domain by estimating a weighted linear sum of values of the function at a small set of points belonging to the domain. using the dqm, the frequency-domain telegrapher's partial differential equations for transmission lines can be discretized into a set of easily solvable algebraic equations. dqm reduces interconnects into multiport models whose port voltages and currents are related by rational formulas in the frequency domain. although the rationalization process in dqm is comparable with the pade approximation of asymptotic waveform evaluation (awe) applied to transmission lines, the derivation mechanisms in these two disparate methods are significantly different. unlike awe, which employs a complex moment-matching process to obtain rational approximation, the dqm requires no approximation of transcendental functions, thereby avoiding the process of moment generation and moment matching. due to global sampling of points in the dqm approximation, it requires far fewer grid points in order to build accurate discrete models than other numerical methods do. the dqm-based time-domain model can be readily integrated in a circuit simulator like spice ","Accurate modeling of lossy nonuniform transmission lines by using differential quadrature methods This paper discusses an efficient numerical approximation technique, called the differential quadrature method (DQM), which has been adapted to model lossy uniform and nonuniform transmission lines. The DQM can quickly ‘compute the derivative of a function at any point within its bounded domain by estimating a weighted linear sum of values of the function at a small set of points belonging to the domain. Using the DQM, the frequency-domain Telegrapher's partial differential equations for transmission lines can be discretized into a set of easily solvable algebraic equations. DQM reduces interconnects into multiport models whose port voltages and currents are related by rational formulas in the frequency domain. Although the rationalization process in DOM is ‘comparable with the Pade approximation of asymptotic waveform evaluation (AWE) applied to transmission lines, the derivation mechanisms in these two disparate methods are significantly different. Unlike AWE, which employs a complex moment-matching process to obtain rational approximation, the DQM requires no approximation of transcendental functions, thereby avoiding the process of moment generation and moment matching. Due to global sampling of points in the DQM approximation, it requires far fewer grid points in order to build accurate discrete models than other numerical methods do. The DQM-based time-domain model can be readily integrated in a circuit simulator like SPICE","['lossy nonuniform transmission lines', 'differential quadrature method', 'numerical approximation technique', 'frequency-domain Telegrapher PDE', 'partial differential equations', 'algebraic equations', 'interconnects', 'multiport models', 'multiconductor transmission lines', 'rationalization process', 'time-domain model', 'circuit simulation', 'integrated circuit interconnections', 'matrix algebra', 'time-domain analysis', 'transient analysis', 'transmission line theory']","['non-uniform transmission lines', 'differential quadrature method', 'dos-based time-domain model', 'accurate discrete models', 'other numerical methods', 'model loss uniform', 'Accurate modelling', 'multiparty models', 'models', 'transmission lines']",1327,224,17,1329,223,10,18,3,3
"new age computing [autonomic computing] autonomic computing (ac), sometimes called self-managed computing, is the name chosen by ibm to describe the company's new initiative aimed at making computing more reliable and problem-free. it is a response to a growing realization that the problem today with computers is not that they need more speed or have too little memory, but that they crash all too often. this article reviews current initiatives being carried out in the ac field by the it industry, followed by key challenges which require to be addressed in its development and implementation ","New age computing [autonomic computing] Autonomic computing (AC), sometimes called self-managed computing, is the name chosen by IBM to describe the company's new initiative aimed at making ‘computing more reliable and problem-free. It is a response to a growing realization that the problem today with computers is not that they need more speed or have too little memory, but that they crash all too often. This article reviews current initiatives being carried out in the AC field by the IT industry, followed by key challenges which require to be addressed in its development and implementation","['autonomic computing', 'new age computing', 'AC', 'self-managed computing', 'IBM initiative', 'computing reliability', 'problem-free computing', 'computer speed', 'computer memory', 'computer crash', 'IT industry initiatives', 'AC requirements', 'AC development', 'AC implementation', 'open standards', 'self-healing computing', 'adaptive algorithms', 'computer network management', 'computer network reliability', 'error correction', 'fault tolerant computing', 'open systems', 'reviews', 'self-adjusting systems', 'software reliability', 'software standards', 'technological forecasting']","['computing', 'self-managed computing', 'New age computing', 'new initiative', 'more speed', 'AC field', 'New', 'autonomic', 'autonomic computing', 'Autonomic computing']",502,96,27,503,95,10,9,1,1
"a novel genetic algorithm for the design of a signed power-of-two coefficient quadrature mirror filter lattice filter bank a novel genetic algorithm (ga) for the design of a canonical signed power-of-two (spt) coefficient lattice structure quadrature mirror filter bank is presented. genetic operations may render the spt representation of a value noncanonical. a new encoding scheme is introduced to encode the spt values. in this new scheme, the canonical property of the spt values is preserved under genetic operations. additionally, two new features that drastically improve the performance of our ga are introduced. (1) an additional level of natural selection is introduced to simulate the effect of natural selection when sperm cells compete to fertilize an ovule; this dramatically improves the offspring survival rate. a conventional ga is analogous to intracytoplasmic sperm injection and has an extremely low offspring survival rate, resulting in very slow convergence. (2) the probability of mutation for each codon of a chromosome is weighted by the reciprocal of its effect. because of these new features, the performance of our new ga outperforms conventional gas ","Anovel genetic algorithm for the design of a signed power-of-two coefficient quadrature mirror filter lattice filter bank Anovel genetic algorithm (GA) for the design of a canonical signed power-of-two (SPT) coefficient lattice structure quadrature mirror filter bank is presented. Genetic operations may render the SPT representation of a value noncanonical. A new encoding scheme is introduced to encode the SPT values. In this new scheme, the canonical property of the SPT values is preserved under genetic operations. Additionally, two new features that drastically improve the performance of our GA are introduced. (1) An additional level of natural selection is introduced to simulate the effect of natural selection when sperm cells compete to fertilize an ovule; this dramatically improves the offspring survival rate. A conventional GA is analogous to intracytoplasmic sperm injection and has an extremely low offspring survival rate, resulting in very slow convergence. (2) The probability ‘of mutation for each codon of a chromosome is weighted by the reciprocal of its effect. Because of these new features, the performance of our new GA outperforms conventional GAS","['genetic algorithm', 'signed power-of-two coefficient lattice structure', 'quadrature mirror filter', 'QMF', 'lattice filter bank', 'encoding scheme', 'natural selection', 'offspring survival rate', 'chromosome codon', 'signal processing', 'perfect reconstruction', 'channel bank filters', 'encoding', 'filtering theory', 'genetic algorithms', 'lattice filters', 'quadrature mirror filters', 'signal reconstruction']","['novel genetic algorithm', 'genetic operations', 'filter', 'new features', 'set values', 'coefficient lattice structure', 'new scheme', 'new GA', 'genetic', 'novel']",1001,180,18,1002,177,10,629,177,3
"autonomous detection of crack initiation using surface-mounted piezotransducers in this paper we report on the application of an in situ health monitoring system, comprising an array of piezoceramic wafer elements, to the detection of fatigue degradation in metallic specimens exposed to cyclic loading. lamb waves, transmitted through a beam test coupon, are sensed using small surface-mounted piezotransducer elements, and the signals are then autonomously analysed for indications relating to the onset of structural degradation. the experimental results confirm the efficacy of the approach and provide a demonstration of good robustness under realistic loading conditions, emphasizing the great potential for developing an automated in situ structural health monitoring system for application to fatigue-prone operational structures, such as aircraft ","Autonomous detection of crack initiation using surface-mounted piezotransducers In this paper we report on the application of an in situ health monitoring system, comprising an array of piezoceramic wafer elements, to the detection of fatigue degradation in metallic specimens exposed to cyclic loading. Lamb waves, transmitted through a beam test coupon, are sensed using small surface-mounted piezotransducer elements, and the signals are then autonomously analysed for indications relating to the onset of structural degradation. The experimental results confirm the efficacy of the approach and provide a demonstration of good robustness under realistic loading conditions, emphasizing the great potential for developing an automated in situ structural health monitoring system for application to fatigue-prone operational structures, such as aircraft","['in situ health monitoring', 'piezoceramic wafer elements', 'fatigue degradation', 'metallic specimens', 'cyclic loading', 'Lamb waves', 'surface-mounted piezotransducer elements', 'structural degradation', 'robustness', 'loading conditions', 'automated in situ structural health monitoring', 'fatigue operational structures', 'aircraft', 'aircraft testing', 'automatic testing', 'computerised monitoring', 'condition monitoring', 'crack detection', 'fatigue testing', 'intelligent actuators', 'piezoceramics', 'piezoelectric transducers', 'surface acoustic wave transducers', 'surface acoustic waves']","['surface-mounted piezotransducers', 'piezoceramic wafer elements', 'structural degradation', 'Autonomous detection', 'fatigue degradation', 'crack initiation', 'detection', 'Autonomous', 'surface-mounted', 'piezotransducers']",739,118,24,739,117,10,0,0,7
"quantum learning and universal quantum matching machine suppose that three kinds of quantum systems are given in some unknown states |f>/sup (x)n/, |g/sub 1/>/sup (x)k/, and |g/sub 2/>/sup (x)k/, and we want to decide which template state |g/sub 1/> or |g/sub 2/>, each representing the feature of the pattern class c/sub 1/ or c/sub 2/, respectively, is closest to the input feature state |f>. this is an extension of the pattern matching problem into the quantum domain. assuming that these states are known a priori to belong to a certain parametric family of pure qubit systems, we derive two kinds of matching strategies. the first one is a semiclassical strategy that is obtained by the natural extension of conventional matching strategies and consists of a two-stage procedure: identification (estimation) of the unknown template states to design the classifier (learning process to train the classifier) and classification of the input system into the appropriate pattern class based on the estimated results. the other is a fully quantum strategy without any intermediate measurement, which we might call as the universal quantum matching machine. we present the bayes optimal solutions for both strategies in the case of k=1, showing that there certainly exists a fully quantum matching procedure that is strictly superior to the straightforward semiclassical extension of the conventional matching strategy based on the learning process ","Quantum learning and universal quantum matching machine Suppose that three kinds of quantum systems are given in some unknown states. f>/sup (X)N/, |g/sub 1/>/sup (X)K/, and |g/sub 2/>/sup (X)K/, and we want to decide which template state |g/sub 1/> or |g/sub 2I>, each representing the feature of the pattern class C/sub 1/ or Cisub 2/, respectively, is closest to the input feature state |t>. This is an extension of the pattern matching problem into the quantum domain. Assuming that these states are known a priori to belong to a certain parametric family of pure qubit systems, we derive two kinds of matching strategies. The first one is a semiclassical strategy that is obtained by the natural extension of conventional matching strategies and consists of a two-stage procedure: identification (estimation) of the unknown template states to design the classifier (learning process to train the classifier) and classification of the input system into the appropriate pattern class based on the estimated results. The other is a fully quantum strategy without any intermediate measurement, which we might call as the universal quantum matching machine. We present the Bayes optimal solutions for both strategies in the case of K=1, showing that there certainly exists a fully quantum matching procedure that is strictly superior to the straightforward semiclassical extension of the conventional matching strategy based on the learning process","['quantum learning', 'universal quantum matching machine', 'pattern class', 'pattern matching problem', 'quantum domain', 'qubit systems', 'matching strategies', 'semiclassical strategy', 'two-stage procedure', 'quantum strategy', 'Bayes optimal solutions', 'quantum matching procedure', 'semiclassical extension', 'matching strategy', 'learning process', 'learning systems', 'quantum theory']","['quantum', 'universal quantum', 'conventional matching strategies', 'unknown template states', 'semiclassical strategy', 'quantum strategy', 'Quantum learning', 'quantum systems', 'unknown states', 'quantum domain']",1225,225,17,1225,224,10,9,5,3
"the role of speech input in wearable computing speech recognition seems like an attractive input mechanism for wearable computers, and as we saw in this magazine's first issue, several companies are promoting products that use limited speech interfaces for specific tasks. however, we must overcome several challenges to using speech recognition in more general contexts, and interface designers must be wary of applying the technology to situations where speech is inappropriate ","The role of speech input in wearable computing Speech recognition seems like an attractive input mechanism for wearable computers, and as we saw in this magazine's first issue, several ‘companies are promoting products that use limited speech interfaces for specific tasks. However, we must overcome several challenges to using speech recognition in more general contexts, and interface designers must be wary of applying the technology to situations where speech is inappropriate","['wearable computing', 'speech input', 'speech recognition', 'wearable computer', 'speech recognizers', 'mobile speech recognition', 'background noise', 'speech interfaces', 'mobile computing', 'portable computers', 'speech recognition', 'speech-based user interfaces']","['speech recognition', 'speech', 'attractive input mechanism', 'limited speech interfaces', 'wearable computers', 'several challenges', 'several companies', 'speech input', 'input', 'wearable']",409,72,12,410,71,10,9,1,0
"a gendered view of computer professionals: preliminary results of a survey the under-representation of women in the computing profession in many parts the western world has received our attention through numerous publications, the noticeable low representation of women at computer science conferences and in the lecture halls. over the past two decades, the situation had become worse. this paper seeks to add to the dialogue by presenting preliminary findings from a research project conducted in four countries. the aim of this research was to gain an insight into the perceptions future computer professionals hold on the category of employment loosely defined under the term of ""a computer professional."" one goal was to get insight into whether or not there is a difference between female and mate students regarding their view of computer professionals. other goals were to determine if there was any difference between female and male students in different parts of the world, as well as who or what most influences the students to undertake their courses in computing ","‘A gendered view of computer professionals: preliminary results of a survey The under-representation of women in the computing profession in many parts the western world has received our attention through numerous publications, the noticeable low representation of women at computer science conferences and in the lecture halls. Over the past two decades, the situation had become worse. This paper seeks to add to the dialogue by presenting preliminary findings from a research project conducted in four countries. The aim of this research was to gain an insight into the perceptions future computer professionals hold on the category of ‘employment loosely defined under the term of ""a computer professional.” One goal was to get insight into whether or not there is a difference between female and mate students regarding their view of computer professionals. Other goals were to determine if there was any difference between female and male students in different parts of the world, as well as who or what most influences the students to undertake their courses in computing","['women under-representation', 'computing profession', 'future computer professional perceptions', 'employment', 'mate students', 'female students', 'computing courses', 'computer science education', 'employment', 'gender issues', 'professional aspects']","['computer', 'future computer professionals', 'computer science conferences', 'computing profession', 'preliminary results', 'different parts', 'mate students', 'gendered view', 'professionals', 'computer professionals']",907,171,11,909,170,10,12,3,2
"mothball mania [3g licences] telefonica moviles has frozen its 3g operations in germany, austria, italy and switzerland. with other 3g licence holders questioning the logic of entering already saturated markets with unproven technology, emma mcclune asks if the mothball effect is set to snowball any further ","Mothball mania [3G licences] Telefonica Moviles has frozen its 3G operations in Germany, Austria, Italy and Switzerland. With other 3G licence holders questioning the logic of entering already saturated markets with unproven technology, Emma McClune asks if the mothball effect is set to snowball any further","['3G licence holders', 'saturated markets', 'mothball', 'mobile telephony', 'cellular radio', 'mobile computing']","['germany austria Italy', 'saturated markets', 'Mothball mania', 'g operations', 'Telefonica', 'licences', 'big', 'g', 'mania', 'Mothball']",263,47,6,263,46,10,0,0,1
"x-rite: more than a graphic arts company although it is well known as a maker of densitometers and spectrophotometers, x-rite is active in measuring light and shape in many industries. among them are automobile finishes, paint and home improvements, scientific instruments, optical semiconductors and even cosmetic dentistry ","X-Rite: more than a graphic arts company Although it is well known as a maker of densitometers and spectrophotometers, Rite is active in measuring light and shape in many industries. Among them are automobile finishes, paint and home improvements, scientific instruments, optical semiconductors and even cosmetic dentistry","['X-Rite', 'graphic arts', 'colour measurement', 'colour graphics', 'publishing']","['spectrophotometers Rite', 'graphic arts company', 'many industries', 'densitometer', 'x-rated', 'maker', 'more', 'arts', 'graphic', 'company']",278,48,5,276,47,10,4,1,1
"non-optimal universal quantum deleting machine we verify the non-existence of some standard universal quantum deleting machine. then a non-optimal universal quantum deleting machine is constructed and we emphasize the difficulty for improving its fidelity. in a way, our results complement the universal quantum cloning machine established by buzek and hillery (1996), and manifest some of their distinctions ","Non-optimal universal quantum deleting machine We verify the non-existence of some standard universal quantum deleting machine. Then a non-optimal universal quantum deleting machine is constructed and we emphasize the difficulty for improving its fidelity. In a way, our results complement the universal quantum cloning machine established by Buzek and Hillery (1996), and manifest some of their distinctions","['nonoptimal universal quantum deleting machine', 'fidelity', 'NUQDM', 'universal quantum cloning machine', 'bound states', 'information theory', 'quantum computing', 'quantum cryptography', 'quantum theory']","['machine', 'non-optional universal quantum', 'standard universal quantum', 'non-existence', 'difficulty', 'fidelity', 'universal', 'quantum', 'standard', 'non-optional']",352,58,9,352,57,10,0,0,0
debugging web applications the author considers how one can save time tracking down bugs in web-based applications by arming yourself with the right tools and programming practices. a wide variety of debugging tools have been written with web developers in mind ,Debugging Web applications The author considers how one can save time tracking down bugs in Web-based applications by arming yourself with the right tools and programming practices. A wide variety of debugging tools have been written with Web developers in mind,"['Web application debugging tools', 'programming', 'Internet', 'program debugging', 'software tools']","['pen-based applications', 'programming practices', 'Web applications', 'debugging tools', 'Web developers', 'wide variety', 'right tools', 'tools', 'applications', 'Web']",221,42,5,221,41,10,0,0,0
"from free to fee [online advertising market] as the online advertising market continues to struggle, many online content marketers are wrestling with the issue of how to add at least some level of paid subscription income to their revenue mix in order to reach or improve profitability. since the business of selling content online is still in its infancy, and many consumers clearly still think of web content as simply and rightfully free, few roadmaps are available to show the way to effective marketing strategies, but some guiding principles have emerged ","From FREE to FEE [online advertising market] As the online advertising market continues to struggle, many online content marketers are wrestling with the issue of how to add at least some level of paid subscription income to their revenue mix in order to Teach or improve profitability. Since the business of selling content online is still in its infancy, and many consumers clearly still think of Web content as simply and rightfully free, few roadmaps are available to show the way to effective marketing strategies, but some guiding principles have emerged","['online advertising market', 'paid subscription income', 'selling content online', 'marketing strategies', 'advertising', 'Internet', 'marketing']","['online advertising market', 'effective marketing strategies', 'FREE', 'many consumers', 'content online', 'Web content', 'content', 'online', 'marketing', 'advertising']",470,92,7,470,91,10,1,1,1
"quantum-state information retrieval in a rydberg-atom data register we analyze a quantum search protocol to retrieve phase information from a rydberg-atom data register using a subpicosecond half-cycle electric field pulse. calculations show that the half-cycle pulse can perform the phase retrieval only within a range of peak field values. by varying the phases of the constituent orbitals of the rydberg wave packet register, we demonstrate coherent control of the phase retrieval process. by specially programming the phases of the orbitals comprising the initial wave packet, we show that it is possible to use the search method as a way to synthesize single energy eigenstates ","Quantum-state information retrieval in a Rydberg-atom data register We analyze a quantum search protocol to retrieve phase information from a Rydberg-atom data register using a subpicosecond half-cycle electric field pulse. Calculations show that the half-cycle pulse can perform the phase retrieval only within a range of peak field values. By varying the phases of the constituent orbitals of the Rydberg wave packet register, we demonstrate coherent control of the phase retrieval process. By specially programming the phases of the orbitals comprising the initial wave packet, we show that it is possible to use the search method as a way to synthesize single energy eigenstates","['quantum-state information retrieval', 'Rydberg-atom data register', 'quantum search protocol', 'phase information', 'subpicosecond half-cycle electric field pulse', 'phase retrieval', 'peak field values', 'constituent orbitals', 'Rydberg wave packet register', 'coherent control', 'initial wave packet', 'search method', 'single energy eigenstates', 'half-cycle pulse', 'information theory', 'quantum computing', 'quantum theory', 'Rydberg states', 'storage media']","['Rydberg-atom data', 'Quantum-state information retrieval', 'phase', 'quantum search protocol', 'phase retrieval process', 'phase information', 'peak field values', 'half-cycles pulse', 'retrieval', 'phase retrieval']",579,105,19,579,104,10,0,0,5
"recognizing groups g/sub 2/(3/sup n/) by their element orders it is proved that a finite group that is isomorphic to a simple non-abelian group g = g/sub 2/(3/sup n/) is, up to isomorphism, recognized by a set omega (g) of its element orders, that is, h approximately= g if omega (h) = omega (g) for some finite group h ","Recognizing groups G/sub 2/(3/sup n/) by their element orders Itis proved that a finite group that is isomorphic to a simple non-Abelian group G = G/sub 2/(3/sup n/) is, up to isomorphism, recognized by a set ‘omega (G) of its element orders, that is, H approximately= G if omega (H) = omega (G) for some finite group H","['element orders', 'finite group', 'isomorphism', 'formal logic', 'group theory']","['element orders', '3/sup n/n', 'finite group H', 'groups gosub', 'group', '= omega', '= gosub', 'omega', 'n/n', 'finite group']",261,60,5,262,58,10,121,49,1
"an efficient parallel algorithm for the calculation of canonical mp2 energies we present the parallel version of a previous serial algorithm for the efficient calculation of canonical mp2 energies. it is based on the saebo-almlof direct-integral transformation, coupled with an efficient prescreening of the ao integrals. the parallel algorithm avoids synchronization delays by spawning a second set of slaves during the bin-sort prior to the second half-transformation. results are presented for systems with up to 2000 basis functions. mp2 energies for molecules with 400-500 basis functions can be routinely calculated to microhartree accuracy on a small number of processors (6-8) in a matter of minutes with modern pc-based parallel computers ","An efficient parallel algorithm for the calculation of canonical MP2 energies We present the parallel version of a previous serial algorithm for the efficient calculation of canonical MP2 energies. It is based on the Saebo-Almiof direct-integral transformation, coupled with an efficient prescreening of the AO integrals. The parallel algorithm avoids synchronization delays by spawning a second set of slaves during the bin-sort prior to the second halt-transformation. Results are presented for systems with up to 2000 basis functions. MP2 energies for molecules with 400-500 basis functions can be routinely calculated to microhartree accuracy on a small number of processors (6-8) in a matter of minutes with modern PC-based parallel computers","['parallel algorithm', 'canonical MP2 energies', 'Saebo-Almlof direct-integral transformation', 'AO integrals', 'synchronization delays', 'second half-transformation', 'basis functions', 'MP2 energies', 'microhartree accuracy', 'PC-based parallel computers', 'chemistry computing', 'orbital calculations', 'parallel algorithms', 'physics computing', 'transforms']","['canonical mp energies', 'efficient parallel algorithm', 'previous serial algorithm', 'efficient prescreening', 'efficient calculation', 'parallel version', 'parallel', 'efficient', 'algorithm', 'parallel algorithm']",638,111,15,638,110,10,2,2,6
"reconstruction of time-varying 3-d left-ventricular shape from multiview x-ray cineangiocardiograms this paper reports on the clinical application of a system for recovering the time-varying three-dimensional (3-d) left-ventricular (lv) shape from multiview x-ray cineangiocardiograms. considering that x-ray cineangiocardiography is still commonly employed in clinical cardiology and computational costs for 3-d recovery and visualization are rapidly decreasing, it is meaningful to develop a clinically applicable system for 3-d lv shape recovery from x-ray cineangiocardiograms. the system is based on a previously reported closed-surface method of shape recovery from two-dimensional occluding contours with multiple views. to apply the method to ""real"" lv cineangiocardiograms, user-interactive systems were implemented for preprocessing, including detection of lv contours, calibration of the imaging geometry, and setting of the lv model coordinate system. the results for three real lv angiographic image sequences are presented, two with fixed multiple views (using supplementary angiography) and one with rotating views. 3-d reconstructions utilizing different numbers of views were compared and evaluated in terms of contours manually traced by an experienced radiologist. the performance of the preprocesses was also evaluated, and the effects of variations in user-specified parameters on the final 3-d reconstruction results were shown to be sufficiently small. these experimental results demonstrate the potential usefulness of combining multiple views for 3-d recovery from ""real"" lv cineangiocardiograms ","Reconstruction of time-varying 3-D left-ventricular shape from multiview X-ray cineangiocardiograms This paper reports on the clinical application of a system for recovering the time-varying three-dimensional (3-D) left-ventricular (LV) shape from multiview X-ray cineangiocardiograms. Considering that X-ray cineangiocardiography is still commonly employed in clinical cardiology and computational costs for 3-D recovery and visualization are rapidly decreasing, it is meaningful to develop a clinically applicable system for 3-D LV shape recovery from X-ray cineangiocardiograms. The system is based on a previously reported closed-surface method of shape recovery from two-dimensional occluding contours with multiple views. To apply the method to “real” LV cineangiocardiograms, user-interactive systems were implemented for preprocessing, including detection of LV contours, calibration of the imaging geometry, and setting of the LV model coordinate system. The results for three real LV angiographic image sequences are presented, two with fixed multiple views (using supplementary angiography) and one with rotating views. 3-D reconstructions utilizing different numbers of views were compared and evaluated in terms of contours manually traced by an experienced radiologist. The performance of the preprocesses was also evaluated, and the effects of variations in user-specified parameters on the final 3-D reconstruction results were shown to be sufficiently small. These experimental results demonstrate the potential usefulness of combining multiple views for 3-D recovery from ""real"" LV cineangiocardiograms","['medical diagnostic imaging', 'time-varying 3-D left-ventricular shape reconstruction', 'multiview X-ray cineangiocardiograms', 'clinical cardiology', 'two-dimensional occluding contours', 'arterial septal defect', 'B-spline', 'computational costs', 'user-interactive systems', 'angiographic image sequences', 'fixed multiple views', 'experienced radiologist', 'user-specified parameters variations', 'angiocardiography', 'calibration', 'image reconstruction', 'image sequences', 'medical image processing', 'shape measurement']","['multiview X-ray cineangiocardiograms', 'real LV cineangiocardiograms', 'shape recovery', 'd-d recovery', 'left-ventricular lvf shape', 'd-d reconstructions', 'applicable system', 'd-d', 'real LV', 'X-ray cineangiocardiograms']",1406,216,19,1406,215,10,2,1,10
"diagnosis of the technical state of heat systems a step-by-step approach to the diagnosis of the technical state of heat systems is stated. the class of physical defects is supplemented by the behavioral defects of objects, which are related to the disturbance of the modes of their operation. the implementation of the approach is illustrated by an example of the solution of a specific problem of the diagnosis of a closed heat consumption system ","Diagnosis of the technical state of heat systems step-by-step approach to the diagnosis of the technical state of heat systems is stated. The class of physical defects is supplemented by the behavioral defects of objects, which are related to the disturbance of the modes of their operation. The implementation of the approach is illustrated by an example of the solution of a specific problem of the diagnosis of a closed heat consumption system","['heat system technical state diagnosis', 'step-by-step diagnosis', 'operational mode disturbance', 'closed heat consumption system diagnosis', 'district heating', 'fault diagnosis']","['diagnosis', 'technical state', 'heat systems', 'step-by-step approach', 'behavioral defects', 'physical defects', 'heat', 'state', 'system', 'technical']",375,75,6,374,73,10,169,65,0
"accuracy and stability of splitting with stabilizing corrections this paper contains a convergence analysis for the method of stabilizing corrections, which is an internally consistent splitting scheme for initial-boundary value problems. to obtain more accuracy and a better treatment of explicit terms several extensions are regarded and analyzed. the relevance of the theoretical results is tested for convection-diffusion-reaction equations ","Accuracy and stability of splitting with Stabilizing Corrections This paper contains a convergence analysis for the method of stabilizing corrections, which is an internally consistent splitting scheme for initial-boundary value problems. To obtain more accuracy and a better treatment of explicit terms several extensions are regarded and analyzed. The relevance of the theoretical results is tested for convection-diffusion-reaction equations","['stability', 'convergence analysis', 'stabilizing corrections', 'splitting scheme', 'initial-boundary value problems', 'convection-diffusion-reaction equations', 'eigenvalues and eigenfunctions', 'initial value problems', 'matrix algebra', 'stability']","['Corrections', 'Accuracy', 'initial-boundary value problems', 'consistent splitting scheme', 'convergence analysis', 'stability', 'paper', 'analysis', 'splitting', 'convergence']",386,60,10,386,59,10,0,0,1
"well-posed anisotropic diffusion for image denoising a nonlinear iterative smoothing filter based on a second-order partial differential equation is introduced. it smooths out the image according to an anisotropic diffusion process. the approach is based on a smooth approximation of the total variation (tv) functional which overcomes the non-differentiability of the tv functional at the origin. in particular, the authors perform linear smoothing over smooth areas but selective smoothing over candidate edges. by relating the smoothing parameter to the time step, they arrive at a cfl condition which guarantees the causality of the discrete scheme. this allows the adoption of higher time discretisation steps, while ensuring the absence of artefacts deriving from the non-smooth behaviour of the tv functional at the origin. in particular, it is shown that the proposed approach avoids the typical staircase effects in smooth areas which occur in the standard time-marching tv scheme ","Well-posed anisotropic diffusion for image denoising nonlinear iterative smoothing filter based on a second-order partial differential equation is introduced. It smooths out the image according to an anisotropic diffusion process. The approach is based on a smooth approximation of the total variation (TV) functional which overcomes the non-differentiability of the TV functional at the origin. In particular, the authors perform linear smoothing over smooth areas but selective smoothing over candidate edges. By relating the smoothing parameter to the time step, they arrive at a CFL condition which guarantees the causality of the discrete scheme. This allows the adoption of higher time discretisation steps, while ensuring the absence of artefacts deriving from the non-smooth behaviour of the TV functional at the origin. In particular, it is shown that the proposed approach avoids the typical staircase effects in smooth areas which ‘occur in the standard time-marching TV scheme","['image denoising', 'well-posed anisotropic diffusion', 'nonlinear iterative smoothing filter', 'second-order partial differential equation', 'total variation functional', 'linear smoothing', 'selective smoothing', 'CFL condition', 'discrete scheme', 'causality', 'higher time discretisation steps', 'image restoration problem', 'random Gaussian noise', 'causality', 'filtering theory', 'functional analysis', 'Gaussian noise', 'image restoration', 'iterative methods', 'nonlinear filters', 'partial differential equations', 'smoothing methods']","['smooth areas', 'smoothing', 'well-loved anisotropic diffusion', 'anisotropic diffusion process', 'image', 'smooth approximation', 'smoothing parameter', 'selective smoothing', 'linear smoothing', 'anisotropic']",843,148,22,843,146,10,466,140,6
"component support in plt scheme plt scheme (drscheme and mzscheme) supports the component object model (com) standard with two pieces of software. the first piece is mzcom, a com class that makes a scheme evaluator available to com clients. with mzcom, programmers can embed scheme code in programs written in mainstream languages such as c++ or visual basic. some applications can also be used as mzcom clients. the other piece of component-support software is mysterx, which makes com classes available to plt scheme programs. when needed, mysterx uses a programmable web browser to display com objects. we describe the technical issues encountered in building these two systems and sketch some applications ","Component support in PLT scheme PLT Scheme (DrScheme and MzScheme) supports the Component Object Model (COM) standard with two pieces of software. The first piece is MZCOM, a COM. class that makes a Scheme evaluator available to COM clients. With MzCOM, programmers can embed Scheme code in programs written in mainstream languages such as C++ or Visual BASIC. Some applications can also be used as MZCOM clients. The other piece of component-support software is MysterX, which makes COM classes available to PLT Scheme programs. When needed, MysterX uses a programmable Web browser to. display COM objects. We describe the technical issues encountered in building these two systems and sketch some applications","['PLT Scheme', 'Component Object Model', 'MzCOM', 'reuse', 'Web browser', 'distributed object management', 'online front-ends', 'software reusability']","['put scheme', 'Scheme', 'Component Object Model', 'Component support', 'Scheme evaluator', 'Scheme programs', 'first piece', 'Scheme code', 'COM classes', 'Component']",599,112,8,601,111,10,0,2,2
"relation between glare and driving performance the present study investigated the effects of discomfort glare on driving behavior. participants (old and young; us and europeans) were exposed to a simulated low- beam light source mounted on the hood of an instrumented vehicle. participants drove at night in actual traffic along a track consisting of urban, rural, and highway stretches. the results show that the relatively low glare source caused a significant drop in detecting simulated pedestrians along the roadside and made participants drive significantly slower on dark and winding roads. older participants showed the largest drop in pedestrian detection performance and reduced their driving speed the most. the results indicate that the de boer rating scale, the most commonly used rating scale for discomfort glare, is practically useless as a predictor of driving performance. furthermore, the maximum us headlamp intensity (1380 cd per headlamp) appears to be an acceptable upper limit ","Relation between glare and driving performance The present study investigated the effects of discomfort glare on driving behavior. Participants (old and young; US and Europeans) were exposed to a simulated low- beam light source mounted on the hood of an instrumented vehicle. Participants drove at night in actual traffic along a track consisting of urban, rural, and highway stretches. The results show that the relatively low glare source caused a significant drop in detecting simulated pedestrians along the roadside and made Participants drive significantly slower on dark and winding roads. Older participants showed the largest drop in pedestrian detection performance and reduced their driving speed the most. The results indicate that the de Boer rating scale, the most commonly used rating scale for discomfort glare, is practically useless as a predictor of driving performance. Furthermore, the maximum US headlamp intensity (1380 cd per headlamp) appears to be an acceptable upper limit","['glare', 'driving performance', 'discomfort glare', 'simulated low-beam light source', 'road traffic', 'urban road', 'rural road', 'highway', 'deBoer rating scale', 'human factors', 'road vehicles', 'visual perception']","['discomfort glare', 'rating scale', 'instrumented vehicle Participants', 'pedestrian detection performance', 'behavior Participants', 'beam light source', 'low glare source', 'present study', 'glare', 'performance']",850,152,12,850,151,10,0,0,1
"explanations for the perpetration of and reactions to deception in a virtual community cases of identity deception on the internet are not uncommon. several cases of a revealed identity deception have been reported in the media. the authors examine a case of deception in an online community composed primarily of information technology professionals. in this case, an established community member (df) invented a character (nowheremom) whom he fell in love with and who was eventually killed in a tragic accident. when other members of the community eventually began to question nowheremom's actual identity, df admitted that he invented her. the discussion board was flooded with reactions to df's revelation. the authors propose several explanations for the perpetration of identity deception, including psychiatric illness, identity play, and expressions of true self. they also analyze the reactions of community members and propose three related explanations (social identity, deviance, and norm violation) to account for their reactions. it is argued that virtual communities' reactions to such threatening events provide invaluable clues for the study of group processes on the internet ","Explanations for the perpetration of and reactions to deception in a virtual ‘community Cases of identity deception on the Internet are not uncommon. Several cases of a revealed identity deception have been reported in the media. The authors examine a case of deception in an online community composed primarily of information technology professionals. In this case, an established community member (DF) invented a character (Nowheremom) whom he fell in love with and who was eventually Killed in a tragic accident. When other members of the community eventually began to question Nowheremom's actual identity, DF admitted that he invented her. The discussion board was flooded with reactions to DF's revelation. The authors propose several explanations for the perpetration of identity deception, including psychiatric illness, identity play, and expressions of true self. They also analyze the reactions of community members and propose three related explanations (social identity, deviance, and norm violation) to account for their reactions. It is argued that virtual communities’ reactions to such threatening events provide invaluable clues for the study of group processes on the Internet","['virtual community', 'identity deception', 'Internet', 'online community', 'information technology professionals', 'psychiatric illness', 'group processes', 'social processes', 'Web sites', 'psychology', 'bulletin boards', 'electronic mail', 'human factors', 'information resources', 'Internet', 'psychology', 'social aspects of automation']","['identity deception', 'identity', 'virtual communities reactions', 'social identity deviance', 'virtual community Cases', 'actual identity DF', 'community members', 'online community', 'reactions', 'deception']",1018,178,17,1019,177,10,9,2,5
"a new architecture for implementing pipelined fir adf based on classification of coefficients in this paper, we propose a new method for implementing pipelined finite-impulse response (fir) adaptive digital filter (adf), with an aim of reducing the maximum delay of the filtering portion of conventional delayed least mean square (dlms) pipelined adf. we achieve a filtering section with a maximum delay of one by simplifying a pre-upsampled and a post-downsampled fir filter using the concept of classification of coefficients. this reduction is independent of the order of the filter, which is an advantage when the order of the filter is very large, and as a result the method can also be applied to infinite impulse response (iir) filters. furthermore, when the proposed method is compared with the transpose adf, which has a filtering section with zero delay, it is realized that it significantly reduces the maximum delay associated with updating the coefficients of fir adf. the effect of this is that, the proposed method exhibits a higher convergence speed in comparison to the transpose fir adf ","Anew architecture for implementing pipelined FIR ADF based on classification of coefficients In this paper, we propose a new method for implementing pipelined finite-impulse response (FIR) adaptive digital filter (ADF), with an aim of reducing the maximum delay of the filtering portion of conventional delayed least mean square (DLMS) pipelined ADF. We achieve a filtering section with a maximum delay of one by simplifying a pre-upsampled and a post-downsampled FIR filter using the concept of classification of coefficients. This reduction is independent of the order of the filter, which is an advantage when the order of the filter is very large, and as a result the method can also be applied to infinite impulse response (IIR) fiers. Furthermore, when the proposed method is compared with the transpose ADF, which has a filtering section with zero delay, itis realized that it significantly reduces the maximum delay associated with updating the coefficients of FIR ADF. The effect of this is that, the proposed method exhibits a higher convergence speed in comparison to the transpose FIR ADF","['pipelined FIR ADF', 'coefficient classification', 'adaptive digital filter', 'maximum delay', 'delayed least mean square filter', 'pre-upsampled filter', 'post-downsampled filter', 'convergence speed', 'adaptive filters', 'delays', 'digital filters', 'FIR filters', 'least mean squares methods', 'pipeline processing']","['maximum delay', 'post-downsampled FIR filter', 'adaptive digital filter', 'transpose FIR ADF', 'pipeline FIR ADF', 'pipeline adf', 'FIR', 'new method', 'pipeline', 'FIR adf']",929,177,14,927,174,10,529,174,1
"in search of a general enterprise model many organisations, particularly smes, are reluctant to invest time and money in models to support decision making. such reluctance could be overcome if a model could be used for several purposes rather than using a traditional ""single perspective"" model. this requires the development of a ""general enterprise model"" (gem), which can be applied to a wide range of problem domains with unlimited scope. current enterprise modelling frameworks only deal effectively with nondynamic modelling issues whilst dynamic modelling issues have traditionally only been addressed at the operational level. although the majority of research in this area relates to manufacturing companies, the framework for a gem must be equally applicable to service and public sector organisations. the paper identifies five key design issues that need to be considered when constructing a gem. a framework for such a gem is presented based on a ""plug and play"" methodology and demonstrated by a simple case study ","In search of a general enterprise model Many organisations, particularly SMEs, are reluctant to invest time and money in models to support decision making. Such reluctance could be overcome if a model could be used for several purposes rather than using a traditional ""single perspective"" model. This requires the development ofa ""general enterprise model"" (GEM), which can be applied to a wide range of problem domains with unlimited scope. Current enterprise modelling frameworks only deal effectively with nondynamic modelling issues whilst dynamic modelling issues have traditionally only been addressed at the operational level. Although the majority of research in this area relates to manufacturing companies, the framework for a GEM must be equally applicable to service and public sector organisations. The paper identifies five key design issues that need to be considered when constructing a GEM. A framework for such a GEM is presented based on a ""plug and play"" methodology and demonstrated by a simple case study","['general enterprise model', 'business process re-engineering', 'SMEs', 'decision making', 'single perspective model', 'GEM', 'problem domains', 'enterprise modelling frameworks', 'operational level', 'dynamic modelling issues', 'public sector organisations', 'service sector organisations', 'plug and play methodology', 'case study', 'commerce', 'decision support systems', 'systems re-engineering']","['general enterprise model', 'model', 'public sector organisations', 'nondynamic modelling issues', 'Many organisations', 'key design issues', 'gen. A framework', 'Such reluctance', 'general', 'enterprise']",869,160,17,869,158,10,371,108,6
"a robust h/sub infinity / control approach for induction motors this paper deals with the robustness and stability of an induction motor control structure against internal and external disturbances. in the proposed control scheme, we have used an h/sub infinity / controller with field orientation and input-output linearization to achieve the above-specified features. simulation results are included to illustrate the control approach performances ","A robust H/sub infinity / control approach for induction motors This paper deals with the robustness and stability of an induction motor control structure against internal and external disturbances. In the proposed control scheme, we have used an H/sub infinity / controller with field orientation and input-output linearization to achieve the above-specified features. Simulation results are included to illustrate the control approach performances","['robust H/sub infinity / control', 'induction motors control', 'robustness', 'stability', 'internal disturbances', 'external disturbances', 'field orientation', 'input-output linearization', 'H/sup infinity / control', 'induction motors', 'linearisation techniques', 'machine control', 'robust control']","['control', 'control approach performances', 'robust sub infinity', 'induction motors', 'control scheme', 'robustness', 'motors', 'induction', 'sub infinity', 'control approach']",387,64,13,387,63,10,0,0,4
"documentum completes cm trifecta daily, people participating in clinical trials for drug companies fill out forms describing how they feel physically and emotionally. for some trials, there are hundreds, possibly thousands, of participants. the drug companies must compile all the forms and submit them electronically to the fda. that's where documentum comes in. ""we've streamlined the whole process of managing clinical trial content for companies, such as johnson & johnson, bristol myers squibb, and pfizer,"" notes documentum's president and ceo dave de walt. ""and by the way, the fda also is one of our customers, as well as the epa and the faa."" and there are about 1,300 other organizations in various industries worldwide that rely on documentum's technologies, consulting, and training services. the company's products are designed to manage digital content and facilitate online transactions, partner and supplier relationships, and ebusiness interactions ","Documentum completes CM Trifecta Daily, people participating in clinical trials for drug companies fill out forms describing how they feel physically and emotionally. For some trials, there are hundreds, possibly thousands, of participants. The drug companies must compile all the forms and submit them electronically to the FDA. That's where Documentum comes in. ""We've streamlined the whole process of managing clinical trial content for ‘companies, such as Johnson & Johnson, Bristol Myers Squibb, and Prizer."" notes Documentum's president and CEO Dave De Walt. ""And by the way, the FDA also is one of our customers, as well as the EPA and the FAA."" And there are about 1,300 other organizations in various industries worldwide that rely on Documentum's technologies, consulting, and training services. The company's products are designed to manage digital content and facilitate online transactions, partner and supplier relationships, and ebusiness interactions","['clinical trials', 'drug companies', 'FDA', 'Documentum', 'clinical trial content', 'training services', 'consulting services', 'medical administrative data processing', 'pharmaceutical industry', 'transaction processing']","['document', 'drug companies', 'clinical trial content', 'companys products', 'clinical trials', 'daily people', 'CM tribeca', 'clinical', 'trials', 'companies']",823,144,10,824,143,10,12,2,3
"real-time quasi-2-d inversion of array resistivity logging data using neural network we present a quasi-2-d real-time inversion algorithm for a modern galvanic array tool via dimensional reduction and neural network simulation. using reciprocity and superposition, we apply a numerical focusing technique to the unfocused data. the numerically focused data are much less subject to 2-d and layering effects and can be approximated as from a cylindrical 1-d earth. we then perform 1-d inversion on the focused data to provide approximate information about the 2-d resistivity structure. a neural network is used to perform forward modeling in the 1-d inversion, which is several hundred times faster than conventional numerical forward solutions. testing our inversion algorithm on both synthetic and field data shows that this fast inversion algorithm is useful for providing formation resistivity information at a well site ","Real-time quasi-2-D inversion of array resistivity logging data using neural network We present a quasi-2-D real-time inversion algorithm for a modern galvanic array tool via dimensional reduction and neural network simulation. Using reciprocity and superposition, we apply a numerical focusing technique to the unfocused data. The numerically focused data are much less subject to 2-D and layering effects and can be approximated as from a cylindrical 1-D Earth. We then perform 1-D inversion on the focused data to provide approximate information about the 2-D resistivity structure. A neural network is used to perform forward modeling in the 1-D inversion, which is several hundred times faster than conventional numerical forward solutions. Testing our inversion algorithm on both synthetic and field data shows that this fast inversion algorithm is useful for providing formation resistivity information at a well site","['real-time quasi-2-D inversion', 'array resistivity logging data', 'neural network', 'real-time inversion algorithm', 'galvanic array tool', 'dimensional reduction', 'reciprocity', 'superposition', 'numerical focusing technique', 'unfocused data', '1-D inversion', 'focused data', 'forward modeling', 'formation resistivity', 'well site', 'array signal processing', 'Earth crust', 'geophysical prospecting', 'geophysical signal processing', 'inverse problems', 'neural nets', 'terrestrial electricity']","['inversion', 'd-d inversion', 'real-time inversion algorithm', 'Real-time quasi-2-D inversion', 'neural network simulation', 'fast inversion algorithm', 'array resistivity', 'field data', 'inversion algorithm', 'neural network']",788,138,22,788,137,10,0,0,6
"client satisfaction in a feasibility study comparing face-to-face interviews with telepsychiatry we carried out a pilot study comparing satisfaction levels between psychiatric patients seen face to face (ftf) and those seen via videoconference. patients who consented were randomly assigned to one of two groups. one group received services in person (ftf from the visiting psychiatrist) while the other was seen using videoconferencing at 128 kbit/s. one psychiatrist provided all the ftf and videoconferencing assessment and follow-up visits. a total of 24 subjects were recruited. three of the subjects (13%) did not attend their appointments and two subjects in each group were lost to follow-up. thus there were nine in the ftf group and eight in the videoconferencing group. the two groups were similar in most respects. patient satisfaction with the services was assessed using the client satisfaction questionnaire (csq-8), completed four months after the initial consultation. the mean scores were 25.3 in the ftf group and 21.6 in the videoconferencing group. although there was a trend in favour of the ftf service, the difference was not significant. patient satisfaction is only one component of evaluation. the efficacy of telepsychiatry must also be measured relative to that of conventional, ftf care before policy makers can decide how extensively telepsychiatry should be implemented ","Client satisfaction in a feasibility study comparing face-to-face interviews with telepsychiatry We carried out a pilot study comparing satisfaction levels between psychiatric Patients seen face to face (FTF) and those seen via videoconference. Patients who consented were randomly assigned to one of two groups. One group received services in person (FTF from the visiting psychiatrist) while the other was seen using videoconferencing at 128 kbit/s. One psychiatrist provided all the FTF and videoconferencing assessment and follow-up visits. A total of 24 subjects were recruited. Three of the subjects (13%) did not attend their appointments and two subjects in each group were lost to follow-up. Thus there were nine in the FTF group and eight in the videoconferencing group. The two groups were similar in most respects. Patient satisfaction with the services was assessed using the Client Satisfaction Questionnaire (CSQ-8), completed four months after the initial consultation. The mean scores were 25.3, in the FTF group and 21.6 in the videoconferencing group. Although there was a trend in favour of the FTF service, the difference was not significant. Patient satisfaction is only one component of evaluation. The efficacy of telepsychiatry must also be measured relative to that of conventional, FTF care before policy makers can decide how extensively telepsychiatry should be implemented","['client satisfaction', 'face-to-face interviews', 'telepsychiatry', 'psychiatric patient satisfaction', 'human factors', 'videoconference', 'Client Satisfaction Questionnaire', 'telemedicine', '128 kbit/s', 'human factors', 'medical computing', 'patient treatment', 'psychology', 'teleconferencing', 'telemedicine']","['videoconferencing group', 'satisfaction', 'ft group', 'Client Satisfaction Questionnaire', 'significant Patient satisfaction', 'satisfaction levels', 'feasibility study', 'pilot study', 'ft service', 'Client satisfaction']",1191,212,15,1192,211,10,0,1,4
blind source separation applied to image cryptosystems with dual encryption blind source separation (bss) is explored to add another encryption level besides the existing encryption methods for image cryptosystems. the transmitted images are covered with a noise image by specific mixing before encryption and then recovered through bss after decryption. simulation results illustrate the validity of the proposed method ,Blind source separation applied to image cryptosystems with dual encryption Blind source separation (BSS) is explored to add another encryption level besides the existing encryption methods for image cryptosystems. The transmitted images are covered with a noise image by specific mixing before encryption and then recovered through BSS after decryption Simulation results illustrate the validity of the proposed method,"['blind source separation', 'transmitted images', 'noise image', 'image cryptosystems', 'dual encryption', 'cryptography', 'image coding', 'statistical analysis']","['Blind source separation', 'image cryptosystems', 'transmitted images', 'encryption methods', 'encryption level', 'noise image', 'encryption', 'image', 'Blind', 'source']",362,60,8,361,59,10,0,1,0
"estimation of an n-l-n hammerstein-wiener model estimation of a single-input single-output block-oriented model is studied. the model consists of a linear block embedded between two static nonlinear gains. hence, it is called an n-l-n hammerstein-wiener model. first, the model structure is motivated and the disturbance model is discussed. the paper then concentrates on parameter estimation. a relaxation iteration scheme is proposed by making use of a model structure in which the error is bilinear-in-parameters. this leads to a simple algorithm which minimizes the original loss function. the convergence and consistency of the algorithm are studied. in order to reduce the variance error, the obtained linear model is further reduced using frequency weighted model reduction. a simulation study is used to illustrate the method ","Estimation of an N-L-N Hammerstein-Wiener model Estimation of a single-input single-output block-oriented model is studied. The model consists of a linear block embedded between two static nonlinear gains. Hence, itis called an N-L-N Hammerstein-Wiener model. First, the model structure is motivated and the disturbance model is discussed. The paper then concentrates on parameter estimation. A relaxation iteration scheme is proposed by making use of a model structure in which the error is bilinear-in-parameters. This leads to a simple algorithm which minimizes the original loss function. The convergence and consistency of the algorithm are studied. In order to reduce the variance error, the obtained linear model is further reduced using frequency weighted model reduction. A simulation study is used to illustrate the method","['N-L-N Hammerstein-Wiener model', 'single-input single-output block-oriented model', 'linear block', 'static nonlinear gains', 'model structure', 'disturbance model', 'parameter estimation', 'relaxation iteration scheme', 'bilinear-in-parameters error', 'convergence', 'consistency', 'variance error', 'frequency weighted model reduction', 'nonlinear process', 'iterative methods', 'parameter estimation', 'reduced order systems', 'relaxation theory']","['model', 'Estimation', 'nylon Hammerstein-Wiener model', 'model structure', 'static nonlinear gains', 'disturbance model', 'linear model', 'linear block', 'nylon', 'Hammerstein-Wiener']",711,124,18,711,122,10,288,93,4
"all change [agile business] what does it take for an organisation to become an agile business? its employees probably need to adhere to new procurement policies, work more closely with colleagues in other departments, meet more exacting sales targets, and offer higher standards of customer service and support. in short, they need to change the way they work. implementing technologies to support agile business models and underpin new practices is a complex task in itself. but getting employees to adopt new practices is far harder, and one that requires careful handling, says barry o'connell, general manager of business-to-employee (b2e) solutions at systems vendor hewlett-packard (hp) ","All change [agile business] What does it take for an organisation to become an agile business? Its employees probably need to adhere to new procurement policies, work more closely with colleagues in other departments, meet more exacting sales targets, and offer higher standards of customer service and support. In short, they need to change the way they work. Implementing technologies to support agile business models and underpin new practices is a complex task in itself. But getting employees to adopt new practices is far harder, and one that requires careful handling, says Barry O'Connell, general manager of business-to-employee (B2E) solutions at systems vendor Hewlett-Packard (HP)","['agile business', 'corporate transformation', 'organisational change', 'management information systems']","['new practices', 'employees', 'change', 'new procurement policies', 'exacting sales targets', 'agile business models', 'new', 'business', 'agile', 'agile business']",588,106,4,588,105,10,0,0,0
"a self-organizing context-based approach to the tracking of multiple robot trajectories we have combined competitive and hebbian learning in a neural network designed to learn and recall complex spatiotemporal sequences. in such sequences, a particular item may occur more than once or the sequence may share states with another sequence. processing of repeated/shared states is a hard problem that occurs very often in the domain of robotics. the proposed model consists of two groups of synaptic weights: competitive interlayer and hebbian intralayer connections, which are responsible for encoding respectively the spatial and temporal features of the input sequence. three additional mechanisms allow the network to deal with shared states: context units, neurons disabled from learning, and redundancy used to encode sequence states. the network operates by determining the current and the next state of the learned sequences. the model is simulated over various sets of robot trajectories in order to evaluate its storage and retrieval abilities; its sequence sampling effects; its robustness to noise and its tolerance to fault ","A self-organizing context-based approach to the tracking of multiple robot trajectories We have combined competitive and Hebbian learning in a neural network designed to learn and recall complex spatiotemporal sequences. In such sequences, a particular item may occur more than once or the sequence may share states with another sequence. Processing of repeated/shared states is a hard problem that occurs very often in the domain of robotics. The proposed model consists of two groups of synaptic weights: competitive interlayer and Hebbian intralayer connections, which are responsible for encoding respectively the spatial and temporal features of the input sequence. Three additional mechanisms allow the network to deal with shared states: context units, neurons disabled from learning, and redundancy used to encode sequence states. The network operates by determining the current and the next state of the learned sequences. The model is simulated over various sets of robot trajectories in order to evaluate its storage and retrieval abilities; its sequence sampling effects; its robustness to noise and its tolerance to fault","['self-organizing context-based approach', 'trajectories tracking', 'competitive learning', 'Hebbian learning', 'complex spatiotemporal sequences', 'synaptic weights', 'competitive interlayer connections', 'Hebbian intralayer connections', 'shared states', 'context units', 'sequence states', 'robot trajectories', 'unsupervised learning', 'storage abilities', 'retrieval abilities', 'sequence sampling effects', 'fault tolerance', 'fault tolerance', 'Hebbian learning', 'position control', 'robots', 'self-adjusting systems', 'unsupervised learning']","['self-organizing content-based approach', 'complex spatio-temporal sequences', 'multiple robot trajectories', 'sequence sampling effects', 'sequence Processing', 'sequence states', 'such sequences', 'input sequence', 'sequences', 'robot trajectories']",966,170,23,966,169,10,0,0,6
"who wants to be a millionaire(r): the classroom edition this paper introduces a version of the internationally popular television game show who wants to be a millionaire(r) that has been created for use in the classroom using microsoft powerpoint(r). a suggested framework for its classroom use is presented, instructions on operating and editing the classroom version of who wants to be a millionaire(r) are provided, and sample feedback from students who have played the classroom version of who wants to be a millionaire(r) is offered ","Who Wants To Be A Millionaire(R): The classroom edition This paper introduces a version of the internationally popular television game show Who Wants To Be A Millionaire(R) that has been created for use in the classroom using Microsoft PowerPoint(R). A suggested framework for its classroom use is presented, instructions on operating and editing the classroom version of Who Wants To Be A Millionaire(R) are provided, and sample feedback from students who have played the classroom version ‘of Who Wants To Be A Millionaire(R) is offered","['classroom', 'Who Wants To Be A Millionaire(R)', 'classroom version', 'undergraduate business students', 'student contestants', 'computer aided instruction', 'management education']","['Millionaire', 'classroom version', 'classroom', 'R', 'popular television game', 'classroom edition', 'classroom use', 'editing', 'use', 'version']",453,86,7,454,85,10,2,1,1
"multivariable h/sub infinity // mu feedback control design for high-precision wafer stage motion conventional pid-like siso controllers are still the most common in industry, but with performance requirements becoming tighter there is a growing need for advanced controllers. for the positioning devices in ic-manufacturing, plant interaction is a major performance-limiting factor. mimo control can be invoked to tackle this problem. a practically feasible procedure is presented to design mimo feedback controllers for electromechanical positioning devices, using h/sub infinity // mu techniques. weighting filters are proposed to straightforwardly and effectively impose performance and uncertainty specifications. experiments show that mimo control can considerably improve upon the performance with multiloop siso control. some problems are highlighted that are important for industrial practice, but lacking a workable solution ","Multivariable H/sub infinity // mu feedback control design for high-precision wafer stage motion Conventional PID-like SISO controllers are still the most common in industry, but with performance requirements becoming tighter there is a growing need for advanced controllers. For the positioning devices in IC-manufacturing, plant interaction is a major performance-limiting factor. MIMO control can be invoked to tackle this problem. A practically feasible procedure is presented to design MIMO feedback controllers for electromechanical positioning devices, using H/sub infinity / mu techniques. Weighting filters are proposed to straightforwardly and effectively impose performance and uncertainty specifications. Experiments show that MIMO control can considerably improve upon the performance with multiloop SISO control. Some problems are highlighted that are important for industrial practice, but lacking a workable solution","['IC manufacture', 'multivariable control systems', 'weighting filters', 'MIMO systems', 'H/sub infinity / control', 'feedback', 'servo systems', 'model uncertainty', 'motion control', 'mechatronics', 'mu synthesis', 'feedback', 'H/sup infinity / control', 'integrated circuit manufacture', 'MIMO systems', 'motion control', 'multivariable control systems', 'servomechanisms']","['sub infinity', 'memo control', 'memo feedback controllers', 'performance requirements', 'multiload sis control', 'advanced controllers', 'mu techniques', 'controllers', 'sub', 'infinity']",810,125,18,809,124,10,0,1,1
"academic libraries and community: making the connection i explore the theme of academic libraries serving and reaching out to the broader community. i highlight interesting projects reported on in the literature (such as the through our parents' eyes project) and report on others. i look at challenges to community partnerships and recommendations for making them succeed. although i focus on links with the broader community, i also took at methods for increasing cooperation among various units on campus, so that the needs of campus community groups-such as distance education students or disabled students-are effectively addressed. though academic libraries are my focus, we can learn a lot from the community building efforts of public libraries ","Academic libraries and community: making the connection explore the theme of academic libraries serving and reaching out to the broader community. | highlight interesting projects reported on in the literature (such as the Through Our Parents' Eyes project) and report ‘on others. | look at challenges to community partnerships and recommendations for making them succeed. Although I focus on links with the broader community, | also took at methods for increasing cooperation among various units on campus, so that the needs of campus ‘community groups-such as distance education students or disabled students-are effectively addressed. Though academic libraries are my focus, we can learn a lot from the community building efforts of public libraries","['academic libraries', 'community partnerships', 'campus community groups', 'distance education students', 'disabled students', 'public libraries', 'academic libraries']","['broader community |', 'campus community groups-such', 'community building efforts', 'community partnerships', 'parents Eyes project', 'public libraries', 'libraries', 'academic libraries', 'Academic libraries', 'community']",639,115,7,640,113,10,364,106,2
"robust output feedback model predictive control using off-line linear matrix inequalities a fundamental question about model predictive control (mpc) is its robustness to model uncertainty. in this paper, we present a robust constrained output feedback mpc algorithm that can stabilize plants with both polytopic uncertainty and norm-bound uncertainty. the design procedure involves off-line design of a robust constrained state feedback mpc law and a state estimator using linear matrix inequalities (lmis). since we employ an off-line approach for the controller design which gives a sequence of explicit control laws, we are able to analyze the robust stabilizability of the combined control laws and estimator, and by adjusting the design parameters, guarantee robust stability of the closed-loop system in the presence of constraints. the algorithm is illustrated with two examples ","Robust output feedback model predictive control using off-line linear matrix inequalities ‘A fundamental question about model predictive control (MPC) is its robustness to model uncertainty. In this paper, we present a robust constrained ‘output feedback MPC algorithm that can stabilize plants with both polytopic uncertainty and norm-bound uncertainty. The design procedure involves off-line design of a robust constrained state feedback MPC law and a state estimator using linear matrix inequalities (LMIs). Since we employ an off-line approach for the controller design which gives a sequence of explicit control laws, we are able to analyze the robust stabilizabilty of the combined control laws and estimator, and by adjusting the design parameters, guarantee robust stability of the closed-loop system in the presence of constraints. The algorithm is illustrated with two examples","['robust output feedback model predictive control', 'off-line linear matrix inequalities', 'model uncertainty robustness', 'robust constrained output feedback MPC algorithm', 'polytopic uncertainty', 'norm-bound uncertainty', 'controller design procedure', 'robust constrained state feedback MPC law', 'state estimator', 'explicit control law sequence', 'closed-loop system', 'asymptotically stable invariant ellipsoid', 'asymptotic stability', 'closed loop systems', 'control system synthesis', 'matrix algebra', 'predictive control', 'robust control', 'state estimation', 'state feedback']","['linear matrix inequalities', 'predictive control mpc', 'robust stabilizabilty', 'explicit control laws', 'combined control laws', 'model uncertainty', 'controller design', 'robust stability', 'off-line design', 'predictive control']",758,130,20,759,129,10,9,3,3
"a multi-agent system infrastructure for software component marketplace: an ontological perspective in this paper, we introduce a multi-agent system architecture and an implemented prototype for a software component marketplace. we emphasize the ontological perspective by discussing ontology modeling for the component marketplace, uml extensions for ontology modeling, and the idea of ontology transfer which makes the multi-agent system adapt itself to dynamically changing ontologies ","‘A multiagent system infrastructure for software component marketplace: an ‘ontological perspective In this paper, we introduce a multi-agent system architecture and an implemented prototype for a software component marketplace. We emphasize the ontological perspective by discussing ontology modeling for the component marketplace, UML extensions for ontology modeling, and the idea of ontology transfer which makes the multi-agent system adapt itself to dynamically changing ontologies","['multi-agent system architecture', 'software component marketplace', 'ontology modeling', 'UML extensions', 'ontology transfer', 'dynamically changing ontologies', 'adaptation', 'electronic commerce', 'knowledge engineering', 'multi-agent systems', 'object-oriented programming', 'software libraries', 'specification languages']","['software component marketplace', 'ontological perspective', 'ontology modelling', 'multiagent system infrastructure', 'multi-agency system architecture', 'ontology transfer', 'system', 'multi-agency system', 'component', 'ontological']",423,65,13,424,64,10,17,3,2
"a parallelized indexing method for large-scale case-based reasoning case-based reasoning (cbr) is a problem solving methodology commonly seen in artificial intelligence. it can correctly take advantage of the situations and methods in former cases to find out suitable solutions for new problems. cbr must accurately retrieve similar prior cases for getting a good performance. in the past, many researchers proposed useful technologies to handle this problem. however, the performance of retrieving similar cases may be greatly influenced by the number of cases. in this paper, the performance issue of large-scale cbr is discussed and a parallelized indexing architecture is then proposed for efficiently retrieving similar cases in large-scale cbr. several algorithms for implementing the proposed architecture are also described. some experiments are made and the results show the efficiency of the proposed method ","A parallelized indexing method for large-scale case-based reasoning Case-based reasoning (CBR) is a problem solving methodology commonly seen in artificial intelligence. It can correctly take advantage of the situations and methods in former cases to find out suitable solutions for new problems. CBR must accurately retrieve similar prior cases for getting a good performance. In the past, many researchers proposed useful technologies to handle this problem. However, the performance of retrieving similar cases may be greatly influenced by the number of cases. In this paper, the performance issue of large-scale CBR is discussed and a parallelized indexing architecture is then proposed for efficiently retrieving similar cases in large-scale CBR. Several algorithms for implementing the proposed architecture are also described. Some experiments are made and the results show the efficiency of the proposed method","['parallelized indexing method', 'large-scale case-based reasoning', 'problem solving methodology', 'artificial intelligence', 'bitwise indexing', 'similar prior case retrieval', 'performance', 'experiments', 'case-based reasoning', 'database indexing', 'deductive databases', 'parallel processing', 'problem solving']","['similar cases', 'large-scale cash-based reasoning', 'cash-based reasoning cbr', 'indexing architecture', 'similar prior cases', 'new problems CBR', 'large-scale cor', 'large-scale CBR', 'indexing method', 'former cases']",786,134,13,786,133,10,0,0,5
"pdf subscriptions bolster revenue in 1999 sd times offered prospective subscribers the option of receiving their issues as adobe acrobat pdf files. what set the proposal apart from what other publishers were doing electronically on the web was that readers would get the entire version of the paper-including both advertising and editorial just as it looked when it was laid out and went to press. sd times is only one of a small, but growing, number of publications that are taking on the electronic world and finding success. in the past six months alone, the new york times, popular mechanics, trade magazine electronic buyers' news, and the harvard business review have launched digital versions of their newspapers and magazines to augment their online and print versions. the reasons are as varied as the publishers themselves. some companies are finding that readers don't like their web-based versions either due to poor navigation or missing graphics and images. others want to expand their publications nationally and internationally, but don't want the added cost of postage and printing. still others are looking for ways to give advertisers additional visibility and boost advertising and subscription revenues. no matter what the reason, it's a trend worth watching ","PDF subscriptions bolster revenue In 1999 SD Times offered prospective subscribers the option of receiving their issues as Adobe Acrobat PDF files. What set the proposal apart from what other publishers were doing electronically on the Web was that readers would get the entire version of the paper-including both advertising and editorial just as it looked when it was laid out and went to press. SD Times is only one of a small, but growing, number of Publications that are taking on the electronic world and finding success. In the past six months alone, the New York Times, Popular Mechanics, trade magazine Electronic Buyers' News, and the Harvard Business Review have launched digital versions of their newspapers and magazines to augment their online and print versions. The reasons are as varied as the publishers themselves. Some companies are finding that readers don't like their Web-based versions either due to poor navigation or missing graphics and images. Others want to expand their publications nationally and internationally, but don't want the added cost of postage and printing. Still others are looking for ways to give advertisers additional visibility and boost advertising and subscription revenues. No matter what the reason, it's a trend worth watching","['PDF subscriptions', 'SD Times', 'Adobe Acrobat PDF files', 'newspaper', 'electronic issue', 'digital versions', 'magazines', 'electronic publishing']","['SD Times', 'prospective subscribers', 'subscription revenues', 'boost advertising', 'PDF subscriptions', 'Adobe Acrobat PDF', 'other publishers', 'print versions', 'entire version', 'PDF']",1078,203,8,1078,202,10,0,0,2
"a humanist's legacy in medical informatics: visions and accomplishments of professor jean-raoul scherrer the objective is to report on the work of prof. jean-raoul scherrer, and show how his humanist vision, medical skills and scientific background have enabled and shaped the development of medical informatics over the last 30 years. starting with the mainframe-based patient-centred hospital information system diogene in the 70s, prof. scherrer developed, implemented and evolved innovative concepts of man-machine interfaces, distributed and federated environments, leading the way with information systems that obstinately focused on the support of care providers and patients. through a rigorous design of terminologies and ontologies, the diogene data would then serve as a basis for the development of clinical research, data mining, and lead to innovative natural language processing techniques. in parallel, prof. scherrer supported the development of medical image management, ranging from a distributed picture archiving and communication systems (pacs) to molecular imaging of protein electrophoreses. recognizing the need for improving the quality and trustworthiness of medical information of the web, prof. scherrer created the health-on-the net (hon) foundation. these achievements, made possible thanks to his visionary mind, deep humanism, creativity, generosity and determination, have made of prof. scherrer a true pioneer and leader of the human-centered, patient-oriented application of information technology for improving healthcare ","‘A humanist's legacy in medical informatics: visions and accomplishments of Professor Jean-Raoul Scherrer The objective is to report on the work of Prof. Jean-Raoul Scherrer, and show how his humanist vision, medical skills and scientific background have enabled and shaped the development of medical informatics over the last 30 years. Starting with the maintrame-based patient-centred hospital information system DIOGENE in the 70s, Prof. Scherrer developed, implemented and evolved innovative concepts of man-machine interfaces, distributed and federated environments, leading the way with information systems that obstinately focused on the support of care providers and patients. Through a rigorous design of terminologies and ontologies, the DIOGENE data would then serve as a basis for the development of clinical research, data mining, and lead to innovative natural language processing techniques. In parallel, Prof. Scherrer supported the development of medical image management, ranging from a distributed picture archiving and communication systems (PACS) to molecular imaging of protein electrophoreses. Recognizing the need for improving the quality and trustworthiness of medical information of the Web, Prof. Scherrer created the Health-On-the Net (HON) foundation. These achievements, made possible thanks to his visionary mind, deep humanism, creativity, generosity and determination, have made of Prof. Scherrer a true pioneer and leader of the human-centered, patient-oriented application of information technology for improving healthcare","['Professor Jean-Raoul Scherrer', 'Medical Informatics', 'mainframe based patient centered hospital information system', 'medical image management', 'PACS', 'Internet', 'DIOGENE system', 'man-machine interfaces', 'distributed systems', 'federated systems', 'data mining', 'natural language processing', 'biographies', 'health care', 'history', 'medical image processing', 'medical information systems', 'user interfaces']","['medical informatics visions', 'Professor jean-paul scherer', 'medical image management', 'Prof. jean-paul scherer', 'parallel Prof. scherer', 'medical information', 'webb Prof. scherer', 'humanist vision', 'Prof. scherer', 'medical informatics']",1346,214,18,1347,213,10,2,2,4
"bluetooth bites back it is now more than four years since we started to hear about bluetooth, and from the user's point of view very little seems to have happened since then. paul haddlesey looks at the progress, and the role bluetooth may eventually play in your firm's communications strategy ","Bluetooth bites back It is now more than four years since we started to hear about Bluetooth, and from the user's point of view very little seems to have happened since then. Paul Haddlesey looks at the progress, and the role Bluetooth may eventually play in your firm's communications strategy","['Bluetooth', 'communications strategy', 'wireless connection', 'mobile', 'mobile computing', 'wireless LAN']","['firms communications strategy', 'role Bluetooth', 'Paul Haddlesey', 'users point', 'years', 'more', 'bluetooth', 'Bluetooth', 'users', 'point']",245,51,6,245,50,10,0,0,0
"robustness evaluation of a minimal rbf neural network for nonlinear-data-storage-channel equalisation the authors present a performance-robustness evaluation of the recently developed minimal resource allocation network (mran) for equalisation in highly nonlinear magnetic recording channels in disc storage systems. unlike communication systems, equalisation of signals in these channels is a difficult problem, as they are corrupted by data-dependent noise and highly nonlinear distortions. nair and moon (1997) have proposed a maximum signal to distortion ratio (msdr) equaliser for data storage channels, which uses a specially designed neural network, where all the parameters of the neural network are determined theoretically, based on the exact knowledge of the channel model parameters. in the present paper, the performance of the msdr equaliser is compared with that of the mran equaliser using a magnetic recording channel model, under conditions that include variations in partial erasure, jitter, width and noise power, as well as model mismatch. results from the study indicate that the less complex mran equaliser gives consistently better performance robustness than the msdr equaliser in terms of signal to distortion ratios (sdrs) ","Robustness evaluation of a minimal RBF neural network for nonlinear-data-storage-channel equalisation The authors present a performance-robustness evaluation of the recently developed minimal resource allocation network (MRAN) for equalisation in highly nonlinear magnetic recording channels in disc storage systems. Unlike communication systems, equalisation of signals in these channels is a difficult problem, as they are corrupted by data-dependent noise and highly nonlinear distortions. Nair and Moon (1997) have proposed a maximum signal to distortion ratio (MSDR) equaliser for data storage channels, which uses a specially designed neural network, where all the parameters of the neural network are determined theoretically, based on the exact knowledge of the channel model parameters. In the present paper, the performance of the MSDR equaliser is compared with that of the MRAN equaliser using a magnetic recording channel model, under Conditions that include variations in partial erasure, jitter, width and noise power, as well as model mismatch. Results from the study indicate that the less complex MRAN equaliser gives consistently better performance robustness than the MSDR equaliser in terms of signal to distortion ratios (SDRs)","['robustness evaluation', 'minimal resource allocation network', 'highly nonlinear magnetic recording channels', 'disc storage systems', 'nonlinear-data-storage-channel equalisation', 'data-dependent noise', 'highly nonlinear distortions', 'maximum signal to distortion ratio equaliser', 'RBF neural network', 'MRAN equaliser', 'MSDR equaliser', 'digital magnetic recording', 'jitter noise', 'digital magnetic recording', 'equalisers', 'magnetic disc storage', 'magnetic recording noise', 'radial basis function networks']","['neural network', 'sdr equaliser', 'nonlinear-data-storage-channel equalisation', 'communication systems equalisation', 'performance-robustness evaluation', 'channel model parameters', 'complex man equaliser', 'minimal ref', 'man equaliser', 'Robustness evaluation']",1072,179,18,1072,178,10,0,0,3
"upper bound analysis of oblique cutting with nose radius tools a generalized upper bound model for calculating the chip flow angle in oblique cutting using flat-faced nose radius tools is described. the projection of the uncut chip area on the rake face is divided into a number of elements parallel to an assumed chip flow direction. the length of each of these elements is used to find the length of the corresponding element on the shear surface using the ratio of the shear velocity to the chip velocity. the area of each element is found as the cross product of the length and its width along the cutting edge. summing up the area of the elements along the shear surface, the total shear surface area is obtained. the friction area is calculated using the similarity between orthogonal and oblique cutting in the 'equivalent' plane that includes both the cutting velocity and chip velocity. the cutting power is obtained by summing the shear power and the friction power. the actual chip flow angle and chip velocity are obtained by minimizing the cutting power with respect to both these variables. the shape of the curved shear surface, the chip cross section and the cutting force obtained from this model are presented ","Upper bound analysis of oblique cutting with nose radius tools A generalized upper bound model for calculating the chip flow angle in oblique cutting using flat-faced nose radius tools is described. The projection of the uncut chip area on the rake face is divided into a number of elements parallel to an assumed chip flow direction. The length of each of these elements is used to find the length of the corresponding element on the shear surface using the ratio of the shear velocity to the chip velocity. The area of each element is found as the cross product of the length and its width along the cutting edge. Summing up the area of the elements along the shear surface, the total shear surface area is obtained. The friction area is calculated using the similarity between orthogonal and oblique cutting in the ‘equivalent plane that includes both the cutting velocity and chip velocity. The cutting power is obtained by summing the shear power and the friction power. The actual chip flow angle and chip velocity are obtained by minimizing the cutting power with respect to both these variables. The shape of the curved shear surface, the chip cross section and the cutting force obtained from this model are presented","['upper bound analysis', 'oblique cutting', 'nose radius tools', 'chip flow angle', 'uncut chip area', 'shear surface', 'shear velocity', 'chip velocity', 'friction area', 'cutting', 'friction', 'machine tools', 'machining', 'mechanical engineering', 'shear strength']","['oblique cutting', 'chip velocity', 'chip', 'nose radius tools', 'chip flow angle', 'curved shear surface', 'chip flow direction', 'chip cross section', 'uncut chip area', 'shear velocity']",1019,210,15,1018,209,10,1,1,2
"a better ballot box? election officials are examining technologies to address a wide range of voting issues. the problems observed in the november 2000 us election accelerated existing trends to get rid of lever machines, punch-cards, and hand-counted paper ballots and replace them with mark-sense balloting, internet, and automatic teller machine (atm) kiosk style computer-based systems. an estimated us $2-$4 billion will be spent in the united states and canada to update voting systems during the next decade. voting online might enable citizens to vote even if they are unable to get to the polls. yet making these methods work right turns out to be considerably more difficult than originally thought. new electronic voting systems pose risks as well as solutions. as it turns out, many of the voting products currently for sale provide less accountability, poorer reliability, and greater opportunity for widespread fraud than those already in use. this paper discusses the technology available and how to ensure accurate ballots ","Abetter ballot box? Election officials are examining technologies to address a wide range of voting issues. The problems observed in the November 2000 US election accelerated existing trends to get rid of lever machines, punch-cards, and hand-counted paper ballots and replace them with mark-sense balloting, Internet, and automatic teller machine (ATM) kiosk style ‘computer-based systems. An estimated US $2-$4 billion will be spent in the United States and Canada to update voting systems during the next decade. Voting online might enable citizens to vote even if they are unable to get to the polls. Yet making these methods work right turns ‘out to be considerably more difficult than originally thought. New electronic voting systems pose risks as well as solutions. As it turns ‘out, many of the voting products currently for sale provide less accountability, poorer reliability, and greater opportunity for widespread fraud than those already in use. This paper discusses the technology available and how to ensure accurate ballots","['ballot box', 'mark-sense balloting', 'automatic teller machine computer-based voting system', 'ATM kiosk style computer-based voting systems', 'electronic voting', 'online voting', 'Internet', 'public administration']","['mark-sense balloting internet', 'hand-counted paper ballots', 'lever machines punchcards', 'electronic voting systems', 'accurate ballots', 'US election', 'wide range', 'technology', 'ballots', 'voting systems']",878,162,8,881,160,10,576,160,1
"topology-reducing surface simplification using a discrete solid representation this paper presents a new approach for generating coarse-level approximations of topologically complex models. dramatic topology reduction is achieved by converting a 3d model to and from a volumetric representation. our approach produces valid, error-bounded models and supports the creation of approximations that do not interpenetrate the original model, either being completely contained in the input solid or bounding it. several simple to implement versions of our approach are presented and discussed. we show that these methods perform significantly better than other surface-based approaches when simplifying topologically-rich models such as scene parts and complex mechanical assemblies ","Topology-reducing surface simplification using a discrete solid representation This paper presents a new approach for generating coarse-level approximations of topologically complex models. Dramatic topology reduction is achieved by converting a 3D model to and from a volumetric representation. Our approach produces valid, error-bounded models and supports the creation of approximations that do not interpenetrate the original model, either being completely contained in the input solid or bounding it. Several simple to implement versions of our approach are presented and discussed. We show that these methods perform significantly better than other surface-based approaches when simplifying topologically-rich models such as scene parts and complex mechanical assemblies","['coarse-level approximations', 'topologically complex models', 'discrete solid representation', 'topology-reducing surface simplification', '3D model', 'volumetric representation', 'error-bounded models', 'scene parts', 'complex mechanical assemblies', 'computational geometry', 'computer graphics']","['model', 'Topology-reducing surface simplification', 'other surface-based approaches', 'discrete solid representation', 'valid error-bounded models', 'topologically-rich models', 'original model', 'complex models', 'new approach', 'd model']",673,105,11,673,104,10,0,0,2
"computation of unmeasured third-generation vct views from measured views we compute unmeasured cone-beam projections from projections measured by a third-generation helical volumetric computed tomography system by solving a characteristic problem for an ultrahyperbolic differential equation [john (1938)]. by working in the fourier domain, we convert the second-order pde into a family of first-order ordinary differential equations. a simple first-order integration is used to solve the odes ","Computation of unmeasured third-generation VCT views from measured views ‘We compute unmeasured cone-beam projections from projections measured by a third-generation helical volumetric computed tomography system by solving a characteristic problem for an ultrahyperbolic differential equation [John (1938)]. By working in the Fourier domain, we convert the second-order PDE into a family of first-order ordinary differential equations. A simple first-order integration is used to solve the ODES","['unmeasured third-generation VCT views computation', 'measured views', 'cone-beam projections', 'characteristic problem solution', 'ultrahyperbolic differential equation', 'Fourier domain', 'first-order ordinary differential equations', 'simple first-order integration', 'medical diagnostic imaging', 'range conditions', 'third-generation helical volumetric computed tomography system', 'computerised tomography', 'differential equations', 'image reconstruction', 'medical image processing']","['third-generation helical volumetric', 'unmeasured cone-beam projections', 'views', 'tomography system', 'Computation', 'act', 'cone-beam', 'unmeasured', 'projections', 'third-generation']",428,67,15,429,66,10,2,1,5
"domesticating computers and the internet the people who use computers and the ways they use them have changed substantially over the past 25 years. in the beginning highly educated people, mostly men, in technical professions used computers for work, but over time a much broader range of people are using computers for personal and domestic purposes. this trend is still continuing, and over a shorter time scale has been replicated with the use of the internet. the paper uses data from four national surveys to document how personal computers and the internet have become increasingly domesticated since 1995 and to explore the mechanisms for this shift. now people log on more often from home than from places of employment and do so for pleasure and for personal purposes rather than for their jobs. analyses comparing veteran internet users to novices in 1998 and 2000 and analyses comparing the change in use within a single sample between 1995 and 1996 support two complementary explanations for how these technologies have become domesticated. women, children, and less well-educated individuals are increasingly using computers and the internet and have a more personal set of motives than well-educated men. in addition, the widespread diffusion of the pc and the internet and the response of the computing industry to the diversity in consumers has led to a rich set of personal and domestic services ","Domesticating computers and the Internet The people who use computers and the ways they use them have changed substantially over the past 25 years. In the beginning highly educated people, mostly men, in technical professions used computers for work, but over time a much broader range of people are using computers for personal and domestic purposes. This trend is still continuing, and ‘over a shorter time scale has been replicated with the use of the Internet. The paper uses data from four national surveys to document how personal computers and the Internet have become increasingly domesticated since 1995 and to explore the mechanisms for this shift. Now people log on more often from home than from places of employment and do so for pleasure and for personal purposes rather than for their jobs. Analyses comparing veteran Internet users to novices in 1998 and 2000 and analyses comparing the change in use within a single sample between 1995 and 1996 support two complementary explanations for how these technologies have become domesticated. Women, children, and less well-educated individuals are increasingly using computers and the Internet and have a more personal set of motives than well-educated men. In addition, the widespread diffusion of the PC and the Internet and the response of the computing industry to the diversity in consumers has led to a rich set of personal and domestic services","['computer domestication', 'Internet', 'highly educated people', 'technical professions', 'domestic purposes', 'personal usage', 'national surveys', 'personal computers', 'veteran Internet users', 'novices', 'women', 'children', 'personal motives', 'PC diffusion', 'computing industry', 'domestic services', 'demographics', 'online behavior', 'human factors', 'Internet', 'personal computing', 'social aspects of automation', 'user interfaces']","['domesticated women children', 'domestication computers', 'veteran Internet users', 'personal computers', 'computing industry', 'personal purposes', 'domestic services', 'domestic purposes', 'computing', 'domesticated']",1185,229,23,1186,228,10,4,1,4
"loop restructuring for data i/o minimization on limited on-chip memory embedded processors in this paper, we propose a framework for analyzing the flow of values and their reuse in loop nests to minimize data traffic under the constraints of limited on-chip memory capacity and dependences. our analysis first undertakes fusion of possible loop nests intra-procedurally and then performs loop distribution. the analysis discovers the closeness factor of two statements which is a quantitative measure of data traffic saved per unit memory occupied if the statements were under the same loop nest over the case where they are under different loop nests. we then develop a greedy algorithm which traverses the program dependence graph to group statements together under the same loop nest legally to promote maximal reuse per unit of memory occupied. we implemented our framework in petit, a tool for dependence analysis and loop transformations. we compared our method with one based on tiling of fused loop nest and one based on a greedy strategy to purely maximize reuse. we show that our methods work better than both of these strategies in most cases for processors such as tms320cxx, which have a very limited amount of on-chip memory. the improvements in data i/o range from 10 to 30 percent over tiling and from 10 to 40 percent over maximal reuse for jpeg loops ","Loop restructuring for data /O minimization on limited on-chip memory embedded processors In this paper, we propose a framework for analyzing the flow of values and their reuse in loop nests to minimize data traffic under the constraints of limited on-chip memory capacity and dependences. Our analysis first undertakes fusion of possible loop nests intra-procedurally and then performs loop distribution. The analysis discovers the closeness factor of two statements which is a quantitative measure of data traffic saved per unit memory occupied if the statements were under the same loop nest over the case where they are under different loop nests. We then develop a greedy algorithm which traverses the program dependence graph to group statements together under the same loop nest legally to promote maximal reuse per unit of memory occupied. We implemented our framework in Petit, a tool for dependence analysis and loop transformations. We compared our method with one based on tiling of fused loop nest and one based on a greedy strategy to purely maximize reuse. We show that our methods work better than both of these strategies in most cases for processors such as TMS320Cxx, which have a very limited amount of on-chip memory. The improvements in data /O range from 10 to 30 percent over tiling and from 10 to 40 percent over maximal reuse for JPEG loops","['loop restructuring', 'data I/O minimization', 'on-chip memory', 'data traffic', 'embedded processors', 'loop fusion', 'data locality', 'program dependence graph', 'Petit', 'fused loop nest', 'closeness factor', 'DSP', 'embedded systems', 'graph theory', 'microprocessor chips', 'program control structures', 'storage management chips']","['limited on-chip memory', 'loop nest', 'same loop nest', 'data traffic', 'different loop nests', 'possible loop nests', 'Loop restructuring', 'loop distribution', 'unit memory', 'on-chip memory']",1145,225,17,1143,224,10,4,2,4
"comprehensive encoding and decoupling solution to problems of decoherence and design in solid-state quantum computing proposals for scalable quantum computing devices suffer not only from decoherence due to the interaction with their environment, but also from severe engineering constraints. here we introduce a practical solution to these major concerns, addressing solid-state proposals in particular. decoherence is first reduced by encoding a logical qubit into two qubits, then completely eliminated by an efficient set of decoupling pulse sequences. the same encoding removes the need for single-qubit operations, which pose a difficult design constraint. we further show how the dominant decoherence processes can be identified empirically, in order to optimize the decoupling pulses ","Comprehensive encoding and decoupling solution to problems of decoherence and design in solid-state quantum computing Proposals for scalable quantum computing devices suffer not only from decoherence due to the interaction with their environment, but also from severe engineering constraints. Here we introduce a practical solution to these major concerns, addressing solid-state proposals in particular. Decoherence is first reduced by encoding a logical qubit into two qubits, then completely eliminated by an efficient set of decoupling pulse sequences. The same encoding removes the need for single-qubit operations, which pose a difficult design constraint. We further show how the dominant decoherence processes can be identified empirically, in order to optimize the decoupling pulses","['solid-state quantum computing', 'decoherence', 'logical qubit encoding', 'pulse sequence decoupling', 'engineering constraints', 'decoupling pulse optimization', 'scalable quantum computing devices', 'exchange Hamiltonian', 'encoding', 'exchange interactions (electron)', 'quantum computing', 'semiconductor quantum dots']","['severe engineering constraints', 'dominant coherence processes', 'difficult design constraints', 'coupling pulse sequences', 'particular coherence', 'practical solution', 'coupling solution', 'coupling pulses', 'coherence', 'coupling']",681,112,12,681,111,10,0,0,5
"study of ambiguities inherent to the spectral analysis of voigt profiles-a modified simplex approach in pulsed spectrometries, temporal transients are often analyzed directly in the temporal domain, assuming they consist only of purely exponentially decaying sinusoids. when experimental spectra actually consist of gaussian or voigt profiles (gauss-lorentz profiles), we show that the direct methods may erroneously interpret such lines as the sum of two or more lorentzian profiles. using a nelder and mead simplex method, modified by introducing new means to avoid degeneracies and quenchings in secondary minima, we demonstrate that a large number of different solutions can be obtained with equivalent accuracy over the limited acquisition time interval, with final peak parameters devoid of physical or chemical meaning ","Study of ambiguities inherent to the spectral analysis of Voigt profiles-a modified Simplex approach In pulsed spectrometries, temporal transients are often analyzed directly in the temporal domain, assuming they consist only of purely exponentially decaying sinusoids. When experimental spectra actually consist of Gaussian or Voigt profiles (Gauss-Lorentz profiles), we show that the, direct methods may erroneously interpret such lines as the sum of two ‘or more Lorentzian profiles. Using a Nelder and Mead Simplex method, modified by introducing new means to avoid degeneracies and quenchings in secondary minima, we demonstrate that a large number of different solutions can be obtained with equivalent accuracy over the limited acquisition time interval, with final peak parameters devoid of physical or chemical meaning","['pulsed spectrometries', 'temporal transients', 'spectral analysis', 'Voigt profiles', 'Gaussian profiles', 'Gauss-Lorentz profiles', 'Nelder and Mead Simplex method', 'accuracy', 'limited acquisition time interval', 'final peak parameters', 'spectral analysis', 'spectral line breadth', 'spectroscopy computing']","['Voigt profiles', 'more Lorentzian profiles', 'Gauss-Lorentz profiles', 'Mead Simplex method', 'spectral analysis', 'profiles', 'Simplex approach', 'inherent', 'Study', 'spectral']",707,120,13,709,119,10,2,2,5
"soft options for software upgrades? several new products claim to take the work out of installing software and patches, and even migrating operating systems. software migration products fall into two broad categories. the drive imaging type is designed to make exact copies of a hard disk, either an entire drive or certain directories, so you can use it to back up data. the application management type is designed for more incremental upgrades and often provides additional features such as the ability to monitor or control users' access to applications ","Soft options for software upgrades? Several new products claim to take the work out of installing software and patches, and even migrating operating systems. Software migration products fall into two broad categories. The drive imaging type is designed to make exact copies of a hard disk, either an entire drive or certain directories, so you can use it to back up data. The application management type is designed for more incremental upgrades and often provides additional features such as the ability to monitor or control users' access to applications","['software installation', 'software upgrades', 'Microsoft Windows', 'operating systems migration', 'business data processing', 'DP management', 'operating systems (computers)']","['application management type', 'more incremental upgrades', 'Several new products', 'drive imaging type', 'software upgrades', 'Soft options', 'Soft', 'upgrades', 'software', 'products']",468,90,7,468,89,10,0,0,0
"community spirit it companies that contribute volunteers, resources or funding to charities and local groups not only make a real difference to their communities but also add value to their businesses. so says a new coalition of it industry bodies formed to raise awareness of the options for community involvement, promote the business case, and publicise examples of best practice. the bcs, intellect (formed from the merger of the computing services and software association and the federation of the electronics industry) and the worshipful company of information technologists plan to run advisory seminars and provide guidelines on how companies of all sizes can transform their local communities using their specialist it skills and resources while reaping business benefits ","Community spirit IT companies that contribute volunteers, resources or funding to charities and local groups not only make a real difference to their communities but also add value to their businesses. So says a new coalition of IT industry bodies formed to raise awareness of the options for community involvement, promote the business case, and publicise examples of best practice. The BCS, Intellect (formed from the merger of the Computing Services and Software Association and the Federation of the Electronics Industry) and the Worshipful Company of Information Technologists plan to run advisory seminars and provide guidelines on how companies of all sizes can transform their local communities using their specialist IT skills and resources while reaping business benefits","['IT companies', 'volunteer staff', 'resource contribution', 'charity projects', 'community projects', 'staff development', 'business benefits', 'best practice', 'human resource management', 'personnel', 'social aspects of automation']","['companies', 'community involvement', 'volunteers resources', 'local communities', 'Community spirit', 'business case', 'local groups', 'community', 'Community', 'resources']",664,119,11,664,118,10,0,0,1
"modeling of torsional vibration induced by extension-twisting coupling of anisotropic composite laminates with piezoelectric actuators in this paper we present a dynamic analytical model for the torsional vibration of an anisotropic piezoelectric laminate induced by the extension-twisting coupling effect. in the present approach, we use the hamilton principle and a reduced bending stiffness method for the derivation of equations of motion. as a result, the in-plane displacements are not involved and the out-of-plane displacement of the laminate is the only quantity to be calculated. therefore, the proposed method turns the twisting of a laminate with structural coupling into a simplified problem without losing its features. we give analytical solutions of the present model with harmonic excitation. a parametric study is performed to demonstrate the present approach ","Modeling of torsional vibration induced by extension-twisting coupling of anisotropic composite laminates with piezoelectric actuators In this paper we present a dynamic analytical model for the torsional vibration of an anisotropic piezoelectric laminate induced by the extension-twisting coupling effect. In the present approach, we use the Hamilton principle and a reduced bending stiffness method for the derivation of equations of motion. As a result, the in-plane displacements are not involved and the out-of-plane displacement of the, laminate is the only quantity to be calculated. Therefore, the proposed method turns the twisting of a laminate with structural coupling into a simplified problem without losing its features. We give analytical solutions of the present model with harmonic excitation. A parametric study is performed to demonstrate the present approach","['torsional vibration', 'extension -twisting coupling', 'anisotropic composite laminates', 'piezoelectric actuators', 'dynamic analytical model', 'anisotropic piezoelectric laminate', 'extension-twisting coupling effect', 'Hamilton principle', 'reduced bending stiffness', 'equations of motion', 'in-plane displacements', 'out-of-plane displacement', 'twisting', 'structural coupling', 'harmonic excitation', 'parametric study', 'composite laminate', 'material anisotropy', 'PZT', 'bending', 'elastic constants', 'harmonic analysis', 'intelligent actuators', 'intelligent materials', 'laminates', 'piezoelectric actuators', 'piezoelectric materials', 'torsion', 'vibrations']","['torsional vibration', 'extension-twisting coupling effect', 'anisotropic piezoelectric laminate', 'anisotropic composite laminates', 'dynamic analytical model', 'structural coupling', 'present model', 'modelling', 'laminate', 'extension-twisting coupling']",752,127,29,753,126,10,0,1,10
"monitoring the news online the author looks at how we can focus on what we want, finding small stories in vast oceans of news. there is no one tool that will scan every news resource available and give alerts on new available materials. every one has a slightly different focus. some are paid sources, while many are free. if used wisely, an excellent news monitoring system for a large number of topics can be set up for surprisingly little cost ","Monitoring the news online The author looks at how we can focus on what we want, finding small stories in vast oceans of news. There is no one tool that will scan every news resource available and give alerts on new available materials. Every ‘one has a slightly different focus. Some are paid sources, while many are free. If used wisely, an excellent news monitoring system for a large number of topics can be set up for surprisingly little cost","['news monitoring', 'online news', 'Internet', 'information resources', 'publishing']","['new available materials', 'small stories', 'news resource', 'vast oceans', 'news online', 'one tool', 'news', 'one', 'small', 'available']",367,81,5,368,80,10,3,1,0
surface micromachined paraffin-actuated microvalve normally-open microvalves have been fabricated and tested which use a paraffin microactuator as the active element. the entire structure with nominal dimension of phi 600 mu m * 30 mu m is batch-fabricated by surface micromachining the actuator and channel materials on top of a single substrate. gas flow rates in the 0.01-0.1 sccm range have been measured for several devices with actuation powers ranging from 50 to 150 mw on glass substrates. leak rates as low as 500 mu sccm have been measured. the normally-open blocking microvalve structure has been used to fabricate a precision flow control system of microvalves consisting of four blocking valve structures. the control valve is designed to operate over a 0.01-5.0 sccm flow range at a differential pressure of 800 torr. flow rates ranging from 0.02 to 4.996 sccm have been measured. leak rates as low as 3.2 msccm for the four valve system have been measured ,Surface micromachined paraffin-actuated microvalve Normally-open microvalves have been fabricated and tested which use a paraffin microactuator as the active element. The entire structure with nominal dimension of phi 600 mu m * 30 mu m is batch-fabricated by surface micromachining the actuator and channel materials on top of a single substrate. Gas flow rates in the 0.01-0.1 sccm range have been measured for several devices with actuation powers ranging from 50 to 150 mw on glass substrates. Leak rates as low as 500 mu sccm have been measured. The normally-open blocking microvalve structure has been used to fabricate a precision flow control system of microvalves consisting of four blocking valve structures. The control valve is designed to operate over a 0.01-5.0 sccm flow range at a differential pressure of 800 torr. Flow rates ranging from 0.02 to 4.996 sccm have been measured. Leak rates as low as 3.2 msccm for the four valve system have been measured,"['surface micromachined microvalve', 'normally-open microvalves', 'paraffin microactuator', 'active element', 'channel materials', 'gas flow rates', 'actuation powers', 'leak rates', 'blocking valve structures', 'differential pressure', 'flow rates', '600 micron', '30 micron', '50 to 150 mW', '800 torr', 'microactuators', 'microfluidics', 'micromachining', 'microvalves']","['microwave', 'paraffin-actuated microwave', 'Normally-open microwaves', 'microwave structure', 'valve structures', 'mu m', 'entire structure', 'scum flow range', 'scum range', 'mu scum']",814,158,19,814,157,10,0,0,10
"control centers are here to stay despite changes with different structures, market rules, and uncertainties, a control center must always be in place to maintain the security, reliability, and quality of electric service. this article focuses on the energy management system (ems) control center, identifying the major functions that have become standard components of every application software package. the two most important control center functions, security control and load-following control, guarantee the continuity of electric service, which after all, is the end-product of the utility business. new technology trends in the design of control center infrastructures are emerging in the liberalized environment of the energy market. an example of a control center infrastructure is described. the article ends with a concern for the security of the control center itself ","Control centers are here to stay Despite changes with different structures, market rules, and uncertainties, a control center must always be in place to maintain the security, reliability, and quality of electric service. This article focuses on the energy management system (EMS) control center, identifying the major functions that have become standard components of every application software package. The two most important control center functions, security control and load-following control, guarantee the continuity of electric service, which after all, is the end-product of the utility business. New technology trends in the design of control center infrastructures are emerging in the liberalized environment of the energy market. An example of a control center infrastructure is described. The article ends with a concern for the security of the, control center itself","['EMS control centers', 'energy management system', 'standard components', 'application software package', 'security control', 'load-following control', 'electric service continuity', 'control center infrastructures', 'liberalized environment', 'energy market', 'control facilities', 'energy management systems', 'load regulation', 'power system control', 'power system security']","['control', 'control center', 'control center infrastructures', 'important control center', 'load-following control', 'security reliability', 'security control', 'Control centers', 'energy market', 'centers']",751,130,15,752,129,10,0,1,3
"from information gateway to digital library management system: a case analysis this paper discusses the design, implementation and evolution of the cornell university library gateway using the case analysis method. it diagnoses the gateway within the conceptual framework of definitions and best practices associated with information gateways, portals, and emerging digital library management systems, in particular the product encompass ","From information gateway to digital library management system: a case analysis This paper discusses the design, implementation and evolution of the Cornell University Library Gateway using the case analysis method. It diagnoses the Gateway within the conceptual framework of definitions and best practices associated with information gateways, portals, and emerging digital library management systems, in particular the product ENCompass","['digital library management system', 'Cornell University Library Gateway', 'information gateways', 'portals', 'ENCompass', 'metadata', 'academic libraries', 'digital libraries', 'network servers']","['library', 'management', 'digital', 'information gateways portals', 'case analysis method', 'best practices', 'information', 'gateways', 'case analysis', 'information gateway']",379,60,9,379,59,10,0,0,1
"defending against flooding-based distributed denial-of-service attacks: a tutorial flooding-based distributed denial-of-service (ddos) attack presents a very serious threat to the stability of the internet. in a typical ddos attack, a large number of compromised hosts are amassed to send useless packets to jam a victim, or its internet connection, or both. in the last two years, it was discovered that ddos attack methods and tools are becoming more sophisticated, effective, and also more difficult to trace to the real attackers. on the defense side, current technologies are still unable to withstand large-scale attacks. the main purpose of this article is therefore twofold. the first one is to describe various ddos attack methods, and to present a systematic review and evaluation of the existing defense mechanisms. the second is to discuss a longer-term solution, dubbed the internet-firewall approach, that attempts to intercept attack packets in the internet core, well before reaching the victim ","Defending against flooding-based distributed denial-of-service attacks: a tutorial Flooding-based distributed denial-of-service (DDoS) attack presents a very serious threat to the stability of the Internet. In a typical DDoS attack, a large number of compromised hosts are amassed to send useless Packets to jam a victim, or its Internet connection, or both. In the last two years, it was discovered that DDoS attack methods and tools are becoming more sophisticated, effective, and also more difficult to trace to the real attackers. On the defense side, current technologies are still unable to withstand large-scale attacks. The main purpose of this article is therefore twofold. The first one is to describe various DDoS attack methods, and to present a systematic review and evaluation of the existing defense mechanisms. The second is to discuss a longer-term solution, dubbed the Internet-firewall approach, that attempts to intercept attack packets in the Internet core, well before reaching the victim","['flooding-based distributed denial-of-service attacks', 'tutorial', 'Internet stability', 'DDoS attack methods', 'DDoS attack tools', 'large-scale attacks', 'Internet firewall', 'attack packets interception', 'reflector attacks', 'distributed attack detection', 'authorisation', 'Internet', 'packet switching', 'telecommunication security']","['attack', 'dos attack methods', 'flooding-based', 'denial-of-service attacks', 'large-scale attacks', 'serious threat', 'real attackers', 'attack packets', 'typical dos', 'denial-of-service']",859,153,14,859,152,10,0,0,2
on the accuracy of polynomial interpolation in hilbert space with disturbed nodal values of the operator the interpolation accuracy of polynomial operators in a hilbert space with a measure is estimated when nodal values of these operators are given approximately ,On the accuracy of polynomial interpolation in Hilbert space with disturbed nodal values of the operator The interpolation accuracy of polynomial operators in a Hilbert space with a measure is estimated when nodal values of these operators are given approximately,"['polynomial interpolation', 'Hilbert space', 'disturbed nodal values', 'polynomial operators', 'Hilbert spaces', 'interpolation', 'polynomials']","['Hilbert space', 'polynomial interpolation', 'interpolation accuracy', 'disturbed nodal values', 'polynomial operators', 'interpolation', 'accuracy', 'operators', 'nodal values', 'Hilbert']",224,41,7,224,40,10,0,0,0
"modeling the labor market as an evolving institution: model artemis a stylized french labor market is modeled as an endogenously evolving institution. boundedly rational firms and individuals strive to decrease the cost or increase utility. the labor market is coordinated by a search process and decentralized setting of hiring standards, but intermediaries can speed up matching. the model reproduces the dynamics of the gross flows and spectacular changes in mobility patterns of some demographic groups when the oil crisis in the 1970's occurred, notably the sudden decline of the integration in good jobs. the internal labor markets of large firms are shown to increase unemployment if the secondary (temporary or bad) jobs do not exist ","Modeling the labor market as an evolving institution: model ARTEMIS Astylized French labor market is modeled as an endogenously evolving institution. Boundedly rational firms and individuals strive to decrease the cost or increase utility. The labor market is coordinated by a search process and decentralized setting of hiring standards, but intermediaries can speed up matching. The model reproduces the dynamics of the gross flows and spectacular changes in mobility patterns of some demographic groups when the oil crisis in the 1970's occurred, notably the sudden decline of the integration in good jobs. The internal labor markets of large firms are shown to increase unemployment if the secondary (temporary or bad) jobs do not exist","['ARTEMIS model', 'French labor market', 'endogenously evolving institution', 'simulation model', 'jobs', 'endogenous intermediary', 'spectacular changes', 'mobility patterns', 'demographic groups', 'demography', 'employment', 'simulation', 'social sciences']","['institutions model ARTEMIS', 'internal labor markets', 'increase utility', 'rational firms', 'large firms', 'labor', 'market', 'model', 'labor market', 'institutions']",627,116,13,627,114,10,335,104,6
"a hybrid ml-em algorithm for calculation of maximum likelihood estimates in semiparametric shared frailty models this paper describes a generalised hybrid ml-em algorithm for the calculation of maximum likelihood estimates in semiparametric shared frailty models, the cox proportional hazard models with hazard functions multiplied by a (parametric) frailty random variable. this hybrid method is much faster than the standard em method and faster than the standard direct maximum likelihood method (ml, newton-raphson) for large samples. we have previously applied this method to semiparametric shared gamma frailty models, and verified by simulations the asymptotic and small sample statistical properties of the frailty variance estimates. let theta /sub 0/ be the true value of the frailty variance parameter. then the asymptotic distribution is normal for theta /sub 0/>0 while it is a 50-50 mixture between a point mass at zero and a normal random variable on the positive axis for theta /sub 0/=0. for small samples, simulations suggest that the frailty variance estimates are approximately distributed as an x-(100-x)% mixture, 0<or=x<or=50, between a point mass at zero and a normal random variable on the positive axis even for theta /sub 0/>0. we apply this method and verify by simulations these statistical results for semiparametric shared log-normal frailty models. we also apply the semiparametric shared gamma and log-normal frailty models to busselton health study coronary heart disease data ","A hybrid ML-EM algorithm for calculation of maximum likelinood estimates in semiparametric shared frailty models This paper describes a generalised hybrid ML-EM algorithm for the calculation ‘of maximum likelinood estimates in semiparametric shared frailty models, the Cox proportional hazard models with hazard functions multiplied by a (parametric) frailty random variable. This hybrid method is much faster than the standard EM method and faster than the standard direct maximum likelihood method (ML, Newton-Raphson) for large samples. We have previously applied this method to semiparametric shared gamma frailty models, and verified by simulations the asymptotic and small sample statistical properties of the frailty variance estimates. Let theta /sub 0/ be the true value of the frailty variance parameter. Then the asymptotic distribution is normal for theta /sub 0/>0 while it is a 50-50 mixture between a point mass at zero and a normal random variable on the positive axis for theta /sub 0/=0. For small samples, simulations suggest that the frailty variance estimates are approximately distributed as an x-(100-x)% mixture, O<or=x<or=50, between a point mass at zero and a normal random variable on the positive axis even for theta /sub 0/>0. We apply this method and verify by simulations these statistical results for semiparametric shared log-normal frailty models. We also apply the semiparametric shared gamma and log-normal frailty models to Busselton Health Study coronary heart disease data","['hybrid ML-EM algorithm', 'maximum likelihood estimates', 'Cox proportional hazard models', 'Busselton Health Study', 'coronary heart disease data', 'data analysis', 'hazard functions', 'simulations', 'frailty variance estimates', 'asymptotic distribution', 'normal distribution', 'normal random variable', 'semiparametric shared log-normal frailty models', 'data analysis', 'log normal distribution', 'maximum likelihood estimation', 'medical administrative data processing', 'normal distribution']","['maximum likelihood', 'semiparametric shared gamma', 'frailty variance estimates', 'log-normal frailty models', 'hybrid modem algorithm', 'frailty variance parameters', 'proportional hazard models', 'parametric frailty', 'hybrid method', 'frailty variance']",1286,226,18,1287,225,10,5,4,5
"a new approach to the decomposition of boolean functions by the method of q-partitions.ii. repeated decomposition for pt.i. see upr. sist. mash., no. 6, p. 29-42 (1999). a new approach to the decomposition of boolean,functions that depend on n variables and are represented in various forms is considered. the approach is based on the method of q-partitioning of minterms and on the introduced concept of a decomposition clone. the theorem on simple disjunctive decomposition of full and partial functions is formulated. the approach proposed is illustrated by examples ","‘Anew approach to the decomposition of Boolean functions by the method of partitions. II. Repeated decomposition For pt.l. see Upr. Sist. Mash., no. 6, p. 29-42 (1999). A new approach to the, decomposition of Boolean,functions that depend on n variables and are represented in various forms is considered. The approach is based on the method of q-partitioning of minterms and on the introduced concept ‘of a decomposition clone. The theorem on simple disjunctive decomposition of full and partial functions is formulated. The approach proposed is illustrated by examples","['Boolean functions decomposition', 'minterms', 'decomposition clone', 'disjunctive decomposition', 'partial functions', 'logic synthesis', 'q-partitions', 'Boolean functions', 'logic design']","['decomposition', 'new approach', 'method', 'simple disjunctive decomposition', 'Repeated decomposition', 'decomposition clones', 'partial functions', 'Boolean functions', 'Boolean', 'approach']",482,89,9,483,88,10,38,17,2
"mathematical modelling of the work of the system of wells in a layer with the exponential law of permeability variation and the mobile liquid interface we construct and study a two-dimensional model of the work of the system of wells in a layer with the mobile boundary between liquids of various viscosity. we use a 'plunger' displacement model of liquids. the boundaries of the filtration region of these liquids are modelled by curves of the lyapunov class. unlike familiar work, we solve two-dimensonal problems in an inhomogeneous layer when the mobile boundary and the boundaries of the filtration region are modelled by curves of the lyapunov class. we show the practical convergence of the numerical solution of the problems studied ","Mathematical modelling of the work of the system of wells in a layer with the exponential law of permeability variation and the mobile liquid interface We construct and study a two-dimensional model of the work of the system of wells in a layer with the mobile boundary between liquids of various viscosity. We use a ‘plunger’ displacement model of liquids. The boundaries of the filtration region of these liquids are modelled by curves of the Lyapunov class. Unlike familiar work, we solve two-dimensonal problems in an inhomogeneous layer when the mobile boundary and the boundaries of the filtration region are modelled by curves of the Lyapunov class. We show the practical convergence of the numerical solution of the problems studied","['2D model', 'work', 'well system', 'mathematical modelling', 'exponential law', 'permeability variation', 'mobile liquid interface', 'mobile boundary', 'viscosity', 'plunger displacement model', 'filtration region boundaries', 'Lyapunov class curves', 'inhomogeneous layer', 'convergence', 'numerical solution', 'computational fluid dynamics', 'convergence of numerical methods', 'differential equations', 'elliptic equations', 'filtration', 'flow through porous media', 'Lyapunov methods', 'permeability', 'viscosity']","['mobile boundary', 'plunger displacement model', 'mobile liquid interface', 'Mathematical modelling', 'two-dimensional model', 'inhomogeneous layer', 'familiar work', 'work', 'liquid', 'modelling']",621,121,24,621,120,10,2,1,8
"the congenial talking philosophers problem in computer networks group mutual exclusion occurs naturally in situations where a resource can be shared by processes of the same group, but not by processes of different groups. for example, suppose data is stored in a cd-jukebox. then, when a disc is loaded for access, users that need data on the disc can concurrently access the disc, while users that need data on a different disc have to wait until the current disc is unloaded. the design issues for group mutual exclusion have been modeled as the congenial talking philosophers problem, and solutions for shared memory models have been proposed (y.-j. young, 2000; p. keane and m. moir, 1999). as in ordinary mutual exclusion and many other problems in distributed systems, however, techniques developed for shared memory do not necessarily apply to message passing (and vice versa). we investigate solutions for congenial talking philosophers in computer networks where processes communicate by asynchronous message passing. we first present a solution that is a straightforward adaptation from g. ricart and a.k. agrawala's (1981) algorithm for ordinary mutual exclusion. then we show that the simple modification suffers a severe performance degradation that could cause the system to behave as though only one process of a group can be in the critical section at a time. we then present a more efficient and highly concurrent distributed algorithm for the problem, the first such solution in computer networks ","The congenial talking philosophers problem in computer networks Group mutual exclusion occurs naturally in situations where a resource can be shared by processes of the same group, but not by processes of different groups. For example, suppose data is stored in a CD-jukebox. Then, when a disc is loaded for access, users that need data on the disc can concurrently access the disc, while users that need data on a different disc have to wait until the current disc is unloaded. The design issues for group mutual exclusion have been modeled as the Congenial Talking Philosophers problem, and solutions for shared memory models have been proposed (Y.-J. Young, 2000; P. Keane and M. Moir, 1999). As in ordinary mutual exclusion and many other problems in distributed systems, however, techniques developed for shared memory do not necessarily apply to message passing (and vice versa). We investigate solutions for Congenial Talking Philosophers in computer networks where processes communicate by asynchronous message passing. We first present a solution that is a straightforward adaptation from G. Ricart and AK. Agrawala's (1981) algorithm for ordinary mutual exclusion. Then we show that the simple modification suffers a severe performance degradation that could cause the system to behave as though only one process of a group can be in the critical section at a time. We then present a more efficient and highly concurrent distributed algorithm for the problem, the first such solution in computer networks","['congenial talking philosophers problem', 'computer networks', 'group mutual exclusion', 'resource sharing', 'shared-memory models', 'distributed systems', 'process communication', 'asynchronous message passing', 'critical section', 'concurrent distributed algorithm', 'computer networks', 'concurrency control', 'distributed algorithms', 'message passing', 'processor scheduling', 'resource allocation', 'shared memory systems']","['ordinary mutual exclusion', 'computer networks Group', 'many other problems', 'first such solution', 'different disc', 'problem', 'computer networks', 'mutual exclusion', 'philosophers problem', 'Philosophers problem']",1277,240,17,1276,239,10,2,1,5
"towards the globalisation of the is/it function the is/it function has recently emerged from the peripheral aspects of the finance department to the centre of critical organisational change. there is an increasing dependency on its activities as systems extend beyond supporting the internal efficiency of the organisation to augmenting global performance. the growth of wide and local networks has resulted in communication possibilities that were not possible a few years ago. e-commerce challenges the achievements of the is/it function and is very prominent in the globalisation of modern organisations. the complexity and diversity of electronic exchange is also well documented (hackney et al., 2000). this has a number of impacts on the development and implementation of is/it solutions for organisations involved in international trade. it is a conjecture that the is/it function is critically important for the alignment of the business to meet the demands of global competition, through building internal marketing strategies and creating knowledge based communities. there is clear evidence that is/it can lead to improved business performance and potentially for sustained competitive advantage. this is obviously true through the advent of new and emerging technologies such as the internet ","Towards the globalisation of the IS/IT function The IS/IT function has recently emerged from the peripheral aspects of the finance department to the centre of critical organisational change. There is an increasing dependency on its activities as systems extend beyond supporting the internal efficiency of the organisation to augmenting global performance. The growth of wide and local networks has resulted in communication possibilities that were not possible a few years ago. E-commerce challenges the achievements of the IS/IT function and is very prominent in the globalisation of modern organisations. The complexity and diversity of electronic exchange is, also well documented (Hackney et al., 2000). This has a number of impacts on the development and implementation of IS/IT solutions for organisations involved in international trade. It is a conjecture that the IS/IT function is critically important for the alignment of the business to meet the demands of global competition, through building internal marketing strategies and creating knowledge based communities. There is clear evidence that IS/IT can lead to improved business performance and potentially for sustained competitive advantage. This, is obviously true through the advent of new and emerging technologies such as the Internet","['globalisation', 'IS/IT function', 'local area networks', 'wide area networks', 'e-commerce', 'electronic exchange', 'international trade', 'internal marketing strategies', 'knowledge based communities', 'Internet', 'business communication', 'electronic commerce', 'information systems', 'international trade', 'Internet', 'management of change']","['visit function', 'globalisation', 'critical organisational change', 'internal marketing strategies', 'modern organisations', 'international trade', 'visit solutions', 'visit', 'organisational', 'function']",1112,193,16,1114,192,10,0,2,5
"using k-nearest-neighbor classification in the leaves of a tree we construct a hybrid (composite) classifier by combining two classifiers in common use - classification trees and k-nearest-neighbor (k-nn). in our scheme we divide the feature space up by a classification tree, and then classify test set items using the k-nn rule just among those training items in the same leaf as the test item. this reduces somewhat the computational load associated with k-nn, and it produces a classification rule that performs better than either trees or the usual k-nn in a number of well-known data sets ","Using k-nearest-neighbor classification in the leaves of a tree We construct a hybrid (composite) classifier by combining two classifiers in ‘common use - classification trees and k-nearest-neighbor (k-NN). In our scheme we divide the feature space up by a classification tree, and then classify test set items using the K-NN rule just among those training items in the same leaf as the test item. This reduces somewhat the computational load associated with k-NN, and it produces a classification rule that performs better than either trees or the usual k-NN in a number of well-known data sets","['k-nearest-neighbor classification', 'tree leaves', 'hybrid composite classifier', 'classification trees', 'feature space division', 'computational load', 'data sets', 'k-NN rule', 'learning (artificial intelligence)', 'pattern classification', 'statistical analysis', 'tree data structures']","['classification trees', 'classification', 'k-nearest-neighbor classification', 'composite classifier', 'classification rule', 'training items', 'test items', 'kind rule', 'tree', 'k-nearest-neighbor']",499,97,12,500,96,10,5,1,2
"three-dimensional particle image tracking for dilute particle-liquid flows in a pipe a three-dimensional (3d) particle image tracking technique was used to study the coarse spherical particle-liquid flows in a pipe. the flow images from both the front view and the normal side view, which was reflected into the front view by a mirror, were recorded with a ccd camera and digitized by a pc with an image grabber card. an image processing program was developed to enhance and segment the flow image, and then to identify the particles. over 90% of all the particles can be identified and located from the partially overlapped particle images using the circular hough transform. then the 3d position of each detected particle was determined by matching its front view image to its side view image. the particle velocity was then obtained by pairing its images in successive video fields. the measurements for the spherical expanded polystyrene particle-oil flows show that the particles, like the spherical bubbles in laminar bubbly flows, tend to conglomerate near the pipe wall and to line up to form the particle clusters. as liquid velocity decreases, the particle clusters disperse and more particles are distributed in the pipe centre region ","Three-dimensional particle image tracking for dilute particle-liquid flows in a pipe A three-dimensional (3D) particle image tracking technique was used to study the coarse spherical particle-liquid flows in a pipe. The flow images from both the front view and the normal side view, which was reflected into the front view by a mirror, were recorded with a CCD camera and digitized by a PC with an image grabber card. An image processing program was developed to enhance and segment the flow image, and then to identify the particles. Over 90% of all the particles can be identified and located from the partially overlapped particle images using the circular Hough transform. Then the 3D position of each detected particle was determined by matching its front view image to its side view image. The particle velocity was then obtained by pairing its images in successive video fields. The measurements for the spherical expanded polystyrene particle-oll flows show that the Particles, like the spherical bubbles in laminar bubbly flows, tend to conglomerate near the pipe wall and to line up to form the particle clusters. As liquid velocity decreases, the particle clusters disperse and more particles are distributed in the pipe centre region","['three-dimensional particle image tracking', 'dilute particle-liquid flows', 'two-phase flow', 'pipe flow', 'stereo-imaging technique', 'phase distribution', 'CCD camera', '3D position', 'spherical expanded polystyrene particle', 'particle clusters', 'spherical bubble', 'Wiener filter', 'image segmentation', 'region growing technique', 'image recognition', 'image matching', 'Hough transform', 'CCD image sensors', 'flow visualisation', 'Hough transforms', 'image recognition', 'image segmentation', 'mechanical engineering computing', 'optical images', 'physics computing', 'pipe flow', 'stereo image processing', 'tracers', 'two-phase flow', 'velocity measurement']","['particle', 'image', 'particle clusters', 'image processing program', 'image grabber card', 'particle velocity', 'front view image', 'side view image', 'particle images', 'flow images']",1046,201,30,1046,200,10,1,1,5
"performance, design and control of a series-parallel (cl/sup 2/-type) resonant dc/dc converter the three-element resonant network has various topological alternatives, one of which, a prospective compound topology, is investigated in detail. the converter uses one capacitor (c) and two inductors (l/sup 2/), to form a compound type cl/sup 2/ network. various advantages and limitations of the converter are detailed, and a new design procedure for such converters is also introduced. the converter may be controlled by varying the switching frequency or by pulse-width modulation. an experimental prototype has been produced and an excellent performance in the lagging power-factor mode has been confirmed ","Performance, design and control of a series-parallel (CL/sup 2/-type) resonant DCIDC converter The three-element resonant network has various topological alternatives, one of which, a prospective compound topology, is investigated in detail. The converter uses one capacitor (C) and two inductors (L/sup 2/), to form ‘a compound type CL/sup 2/ network. Various advantages and limitations of the converter are detailed, and a new design procedure for such converters is also introduced. The converter may be controlled by varying the switching frequency or by pulse-width modulation. An experimental prototype has been produced and an excellent performance in the lagging power-factor mode has been confirmed","['series-parallel resonant DC/DC power converter', 'three-element resonant network', 'capacitor', 'inductors', 'design procedure', 'switching frequency', 'pulse-width modulation', 'performance', 'lagging power factor mode', 'bridge circuits', 'DC-DC power convertors', 'electric current control', 'PWM power convertors', 'resonant power convertors', 'switching circuits', 'voltage control']","['various topological alternatives', 'three-element resonant network', 'prospective compound topology', 'lagging power-factor mode', 'excellent performance', 'new design procedure', 'performance design', 'such converters', 'performance', 'converter']",605,103,16,606,102,10,2,2,4
"radianz and savvis look to expand service in wake of telecom scandals [finance] with confidence in network providers waning, radianz and savvis try to prove their stability. savvis and radianz, which both specialize in providing the data-extranet components of telecommunication infrastructures, may see more networking doors open at investment banks, brokerage houses, exchanges and alternative-trading systems ","Radianz and Sawis look to expand service in wake of telecom scandals [finance] With confidence in network providers waning, Radianz and Sawis try to prove their stability. Savvis and Radianz, which both specialize in providing the data-extranet components of telecommunication infrastructures, may see more networking doors open at investment banks, brokerage houses, exchanges and alternative-trading systems,","['network providers', 'Savvis', 'Radianz', 'data-extranet', 'telecommunication infrastructures', 'investment banks', 'brokerage houses', 'exchanges', 'alternative-trading systems', 'electronic trading', 'investment', 'telecommunication']","['radiant', 'telecommunication infrastructures', 'telecom scandals finance', 'data-extranet components', 'more networking doors', 'network providers', 'saws', 'stability davis', 'wake', 'network']",356,57,12,355,56,10,6,3,7
"hermite interpolation by rotation-invariant spatial pythagorean-hodograph curves the interpolation of first-order hermite data by spatial pythagorean-hodograph curves that exhibit closure under arbitrary 3-dimensional rotations is addressed. the hodographs of such curves correspond to certain combinations of four polynomials, given by dietz et al. (1993), that admit compact descriptions in terms of quaternions - an instance of the ""ph representation map"" proposed by choi et al. (2002). the lowest-order ph curves that interpolate arbitrary first-order spatial hermite data are quintics. it is shown that, with ph quintics, the quaternion representation yields a reduction of the hermite interpolation problem to three ""simple"" quadratic equations in three quaternion unknowns. this system admits a closed-form solution, expressing all ph quintic interpolants to given spatial hermite data as a two-parameter family. an integral shape measure is invoked to fix these two free parameters ","Hermite interpolation by rotation-invariant spatial Pythagorean-hodograph curves The interpolation of first-order Hermite data by spatial Pythagorean-hodograph curves that exhibit closure under arbitrary 3-dimensional rotations is addressed. The hodographs of such curves correspond to certain combinations of four polynomials, given by Dietz et al. (1993), that admit compact descriptions in terms of quaternions - an instance of the ""PH representation map"" proposed by Choi et al. (2002). The lowest-order PH curves that interpolate arbitrary first-order spatial Hermite data are quintics. It is shown that, with PH quintics, the quaternion representation yields a reduction of the Hermite interpolation problem to three ""simple"" quadratic equations in three quaternion unknowns. This system admits a closed-form solution, expressing all PH quintic interpolants to given spatial Hermite data as a two-parameter family. An integral shape measure is invoked to fix these two free parameters","['interpolation', 'spatial Pythagorean hodograph curves', 'first-order Hermite data', 'quaternions', 'polynomials', 'closed-form solution', 'integral shape measure', 'interpolation', 'invariance', 'polynomials']","['spatial Pythagorean-hodograph curves', 'spatial hermit data', 'hermit', 'hermit interpolation problem', 'first-order hermit data', 'lowest-order PH curves', 'such curves', 'curves', 'spatial', 'hermit interpolation']",853,139,10,853,138,10,0,0,5
action aggregation and defuzzification in mamdani-type fuzzy systems discusses the issues of action aggregation and defuzzification in mamdani-type fuzzy systems. the paper highlights the shortcomings of defuzzification techniques associated with the customary interpretation of the sentence connective 'and' by means of the set union operation. these include loss of smoothness of the output characteristic and inaccurate mapping of the fuzzy response. the most appropriate procedure for aggregating the outputs of different fuzzy rules and converting them into crisp signals is then suggested. the advantages in terms of increased transparency and mapping accuracy of the fuzzy response are demonstrated ,Action aggregation and defuzzification in Mamdani-type fuzzy systems Discusses the issues of action aggregation and detuzzification in Mamdani-type fuzzy systems. The paper highlights the shortcomings of defuzzification techniques associated with the customary interpretation of the sentence connective ‘and’ by means of the set union operation. These include loss of smoothness of the output characteristic and inaccurate mapping of the fuzzy response. The most appropriate procedure for aggregating the outputs of different fuzzy rules and converting them into crisp signals is then suggested. The advantages in terms of increased transparency and mapping accuracy of the fuzzy response are demonstrated,"['action aggregation', 'defuzzification', 'Mamdani-type fuzzy systems', 'sentence connective', 'set union operation', 'fuzzy rules', 'fuzzy response', 'crisp signals', 'transparency', 'mapping accuracy', 'fuzzy logic', 'fuzzy set theory', 'fuzzy systems']","['Mamdani-type fuzzy systems', 'fuzzy response', 'defuzzification techniques', 'different fuzzy rules', 'fuzzy', 'aggregation', 'defuzzification', 'Mamdani-type', 'action aggregation', 'Action aggregation']",608,99,13,608,98,10,3,2,5
"banks pin their back-office hopes on successors to screen scrapers the big name in account aggregation has been yodlee, based in redwood shores, ca. it pioneered the art of screen scraping, or pulling data off web sites and aggregating it into a single statement. that data, however, is a snapshot and does not include a customer's investment history. also, because web sites update data at different times, scraping them can provide an inaccurate picture of a customer's financial situation, making it difficult for reps seeking to provide timely and accurate advice. the objective is to access both fresh and historical data across a client's financial spectrum, from investments to checking accounts and loans to insurance policies, a complete customer balance sheet. at least two technology vendors are progressing in that direction, each coming from different directions. one is advent, based in san francisco, another is fincentric, out of vancouver ","Banks pin their back-office hopes on successors to screen scrapers The big name in account aggregation has been Yodlee, based in Redwood Shores, CA. It pioneered the art of screen scraping, or pulling data off Web sites and aggregating it into a single statement. That data, however, is a snapshot and does not include a customer's investment history. Also, because Web sites update data at different times, scraping them can provide an inaccurate picture of a customer's financial situation, making it difficult for reps seeking to provide timely and accurate advice. The objective is to access both fresh and historical data across a client's financial spectrum, from investments to checking accounts and loans to insurance policies, a Complete Customer balance sheet. At least two technology vendors are progressing in that direction, each coming from different directions. One is Advent, based in San Francisco, another is Fincentric, out of Vancouver","['account aggregation', 'Yodlee', 'screen scraping', 'investment', 'Web sites', 'checking', 'loans', 'insurance', 'Advent', 'Fincentric', 'bankers', 'banking', 'insurance', 'investment']","['account aggregation', 'back-office hopes', 'Redwood shores CA', 'successors', 'scrapers', 'big name', 'Banks', 'big', 'hopes', 'back-office']",807,150,14,807,149,10,0,0,11
genetic algorithm for input/output selection in mimo systems based on controllability and observability indices a time domain optimisation algorithm using a genetic algorithm in conjunction with a linear search scheme has been developed to find the smallest or near-smallest subset of inputs and outputs to control a multi-input-multi-output system. experimental results have shown that this proposed algorithm has a very fast convergence rate and high computation efficiency ,Genetic algorithm for input/output selection in MIMO systems based on controllability and observability indices A time domain optimisation algorithm using a genetic algorithm in conjunction with a linear search scheme has been developed to find the smallest or near-smallest subset of inputs and outputs to control a multi-input-multi-output system. Experimental results have shown that this proposed algorithm has a very fast convergence rate and high computation efficiency,"['genetic algorithm', 'input/output selection', 'MIMO systems', 'controllability indices', 'observability indices', 'time domain optimisation algorithm', 'linear search scheme', 'near-smallest subset', 'smallest subset', 'multi-input-multi-output system', 'very fast convergence', 'high computation efficiency', 'multivariable control systems', 'control system analysis', 'controllability', 'convergence of numerical methods', 'genetic algorithms', 'MIMO systems', 'multivariable control systems', 'observability', 'time-domain analysis']","['multi-input-multi-output system', 'input/output selection', 'observability indices', 'fast convergence rate', 'linear search scheme', 'memo systems', 'system', 'algorithm', 'genetic algorithm', 'Genetic algorithm']",409,68,21,409,67,10,0,0,4
"adaptive thinning for bivariate scattered data this paper studies adaptive thinning strategies for approximating a large set of scattered data by piecewise linear functions over triangulated subsets. our strategies depend on both the locations of the data points in the plane, and the values of the sampled function at these points - adaptive thinning. all our thinning strategies remove data points one by one, so as to minimize an estimate of the error that results by the removal of a point from the current set of points (this estimate is termed ""anticipated error""). the thinning process generates subsets of ""most significant"" points, such that the piecewise linear interpolants over the delaunay triangulations of these subsets approximate progressively the function values sampled at the original scattered points, and such that the approximation errors are small relative to the number of points in the subsets. we design various methods for computing the anticipated error at reasonable cost, and compare and test the performance of the methods. it is proved that for data sampled from a convex function, with the strategy of convex triangulation, the actual error is minimized by minimizing the best performing measure of anticipated error. it is also shown that for data sampled from certain quadratic polynomials, adaptive thinning is equivalent to thinning which depends only on the locations of the data points - nonadaptive thinning. based on our numerical tests and comparisons, two practical adaptive thinning algorithms are proposed for thinning large data sets, one which is more accurate and another which is faster ","Adaptive thinning for bivariate scattered data This paper studies adaptive thinning strategies for approximating a large set of scattered data by piecewise linear functions over triangulated subsets. Our strategies depend on both the locations of the data points in the plane, and the values of the sampled function at these points - adaptive thinning. All our thinning strategies remove data points one by one, so as to minimize an estimate of the error that results by the removal of a point from the current set of points (this estimate is termed ""anticipated error""). The thinning process generates subsets of ""most significant"" points, such that the piecewise linear interpolants over the Delaunay triangulations of these subsets approximate progressively the function values sampled at the original scattered points, and such that the approximation errors are small relative to the number of points in the subsets. We design various methods for ‘computing the anticipated error at reasonable cost, and compare and test the performance of the methods. It is proved that for data sampled from a convex function, with the strategy of convex triangulation, the actual error is minimized by minimizing the best performing measure of anticipated error. It is also shown that for data sampled from certain quadratic polynomials, adaptive thinning is equivalent to thinning which depends only on the locations of the data points - nonadaptive thinning. Based on our numerical tests and comparisons, two practical adaptive thinning algorithms are proposed for thinning large data sets, ‘one which is more accurate and another which is faster","['adaptive thinning', 'scattered data', 'piecewise linear functions', 'triangulated subsets', 'error', 'Delaunay triangulations', 'convex function', 'data reduction', 'error analysis', 'interpolation', 'mesh generation', 'piecewise linear techniques']","['anticipated error', 'data points', 'point', 'piecewise linear functions', 'original scattered points', 'most significant points', 'approximation errors', 'data', 'paper studies', 'large data']",1382,256,12,1384,255,10,12,2,4
"creating web-based listings of electronic journals without creating extra work creating up-to-date listings of electronic journals is challenging due to frequent changes in titles available and in urls for electronic journal titles. however, many library users may want to browse web pages which contain listings of electronic journals arranged by title and/or academic disciplines. this case study examines the development of a system which automatically exports data from the online catalog and incorporates it into dynamically-generated web sites. these sites provide multiple access points for journals, include web-based interfaces enabling subject specialists to manage the list of titles which appears in their subject area. because data are automatically extracted from the catalog, overlap in updating titles and urls is avoided. following the creation of this system, usage of electronic journals dramatically increased and feedback has been positive. future challenges include developing more frequent updates and motivating subject specialists to more regularly monitor new titles ","Creating Web-based listings of electronic journals without creating extra work Creating up-to-date listings of electronic journals is challenging due to frequent changes in titles available and in URLs for electronic journal titles. However, many library users may want to browse Web pages which contain listings of electronic journals arranged by title and/or academic disciplines. This case study examines the development of a system which automatically exports data from the online catalog and incorporates it into dynamically-generated Web sites. These sites provide multiple access points for journals, include Web-based interfaces enabling subject specialists to manage the list of titles, which appears in their subject area. Because data are automatically extracted from the catalog, overlap in updating titles and URLs is avoided. Following the creation of this system, usage of electronic journals dramatically increased and feedback has been positive. Future challenges include developing more frequent updates and motivating subject specialists to more regularly monitor new titles","['Web-based listings', 'electronic journals', 'URL', 'library', 'technical services', 'public services partnerships', 'Web pages', 'case study', 'online catalog', 'Web sites', 'feedback', 'cataloguing', 'electronic publishing', 'information resources', 'Internet', 'library automation']","['electronic journals', 'electronic journal titles', 'pen-based interfaces', 'up-to-date listings', 'pen-based listings', 'list', 'titles', 'new titles', 'electronic', 'journals']",939,155,16,940,154,10,0,1,7
"generalized predictive control for non-uniformly sampled systems in this paper, we study digital control systems with non-uniform updating and sampling patterns, which include multirate sampled-data systems as special cases. we derive lifted models in the state-space domain. the main obstacle for generalized predictive control (gpc) design using the lifted models is the so-called causality constraint. taking into account this design constraint, we propose a new gpc algorithm, which results in optimal causal control laws for the non-uniformly sampled systems. the solution applies immediately to multirate sampled-data systems where rates are integer multiples of some base period ","Generalized predictive control for non-uniformly sampled systems In this paper, we study digital control systems with non-uniform updating and sampling patterns, which include multirate sampled-data systems as special cases. We derive lifted models in the state-space domain. The main obstacle for generalized predictive control (GPC) design using the lifted models is the so-called causality constraint. Taking into account this design constraint, we propose a new GPC algorithm, which results in optimal causal control laws for the non-uniformly sampled systems. The solution applies immediately to multirate sampled-data systems where rates are integer multiples of some base period","['generalized predictive control design', 'nonuniformly sampled systems', 'digital control systems', 'nonuniform updating patterns', 'nonuniform sampling patterns', 'multirate sampled-data systems', 'state-space models', 'GPC', 'causality constraint', 'optimal causal control laws', 'integer multiples', 'control system synthesis', 'digital control', 'multivariable control systems', 'optimal control', 'predictive control', 'sampled data systems', 'state-space methods']","['non-uniform', 'ultimate sampled-data systems', 'predictive control', 'Generalized', 'so-called causality constraints', 'digital control systems', 'design constraints', 'systems', 'control', 'predictive']",590,97,18,590,96,10,0,0,2
"winning post [mail systems] businesses that take their mail for granted can end up wasting money as well as opportunities. mike stecyk, vp of marketing and lines of business at pitney bowes, suggests strategies for making more of a great opportunity ","Winning post [mail systems] Businesses that take their mail for granted can end up wasting money as well as ‘opportunities. Mike Stecyk, VP of marketing and lines of business at Pitney Bowes, suggests strategies for making more of a great ‘opportunity","['mail', 'Pitney Bowes', 'strategies', 'franking machines', 'folders', 'inserters', 'direct mail shots', 'mailing systems']","['opportunities Mike Stecyk', 'great opportunity', 'mail', 'Pitney bowes', 'Businesses', 'systems', 'money', 'post', 'Mike', 'opportunity']",209,42,8,211,41,10,23,2,1
"a second order characteristic finite element scheme for convection-diffusion problems a new characteristic finite element scheme is presented for convection-diffusion problems. it is of second order accuracy in time increment, symmetric, and unconditionally stable. optimal error estimates are proved in the framework of l/sup 2/-theory. numerical results are presented for two examples, which show the advantage of the scheme ","Assecond order characteristic finite element scheme for convection-diffusion problems Anew characteristic finite element scheme is presented for convection-diffusion problems. It is of second order accuracy in time increment, symmetric, and unconditionally stable. Optimal error estimates are proved in the framework of L/sup 2/-theory. Numerical results are presented for two examples, which show the advantage of the scheme","['second order characteristic finite element scheme', 'convection-diffusion problems', 'second order accuracy', 'optimal error estimates', 'L/sup 2/ -theory', 'error analysis', 'finite element analysis']","['convection-diffusion problems', 'scheme', 'characteristic', 'element', 'finite', 'second order accuracy', 'order', 'second', 'second order', 'convection-diffusion']",368,60,7,369,57,10,208,57,1
"clioweb, cliorequest, and clio database: enhancing patron and staff satisfaction faced with increased demand from students and faculty for a speedier and more user-friendly method of obtaining materials from other institutions, the interlibrary loan (ill) department sought to implement a management system which would accomplish the task. students wanted remote interconnectivity to the system and staff wanted increased workflow efficiency, reduced paper work, and better data management. this paper focuses on washington college's experience in selecting and implementing an interlibrary loan system, which would enhance student satisfaction as well as that of the library staff ","Clioweb, ClioRequest, and Clio database: enhancing patron and staff satisfaction Faced with increased demand from students and faculty for a speedier and more user-friendly method of obtaining materials from other institutions, the interlibrary loan (ILL) department sought to implement a management system which would accomplish the task. Students wanted remote interconnectivity to the system and staff wanted increased workflow efficiency, reduced paper work, and better data management. This paper focuses on Washington College's experience in selecting and implementing an interlibrary loan system, which would enhance student satisfaction as well as that of the library staff","['Clio database', 'ClioRequest', 'ClioWeb', 'staff satisfaction', 'patron satisfaction', 'faculty', 'students', 'interlibrary loan department', 'user-friendly method', 'management system', 'remote interconnectivity', 'workflow efficiency', 'data management', 'Washington College', 'academic libraries', 'document delivery', 'interlibrary loan', 'management of change', 'software selection']","['inter-library loan system', 'student satisfaction', 'staff satisfaction', 'management system', 'library staff', 'Clio database', 'Clioweb', 'staff', 'Clio', 'database']",587,96,19,587,95,10,0,0,7
"evaluation of existing and new feature recognition algorithms. 2. experimental results for pt.1 see ibid., p.839-851. this is the second of two papers investigating the performance of general-purpose feature detection techniques. the first paper describes the development of a methodology to synthesize possible general feature detection face sets. six algorithms resulting from the synthesis have been designed and implemented on a sun workstation in c++ using acis as the geometric modelling system. in this paper, extensive tests and comparative analysis are conducted on the feature detection algorithms, using carefully selected components from the public domain, mostly from the national design repository. the results show that the new and enhanced algorithms identify face sets that previously published algorithms cannot detect. the tests also show that each algorithm can detect, among other types, a certain type of feature that is unique to it. hence, most of the algorithms discussed in this paper would have to be combined to obtain complete coverage ","Evaluation of existing and new feature recognition algorithms. 2. Experimental results For pt.1 see ibid., p.839-851. This is the second of two papers investigating the performance of general-purpose feature detection techniques. The first paper describes the development of a methodology to synthesize possible general feature detection face sets. Six algorithms resulting from the synthesis have been designed and implemented on a SUN Workstation in C++ using ACIS as the geometric modelling system. In this paper, extensive tests and comparative analysis are conducted on the feature detection algorithms, using carefully selected components from the public domain, mostly from the National Design Repository. The results show that the new and enhanced algorithms identify face sets that previously published algorithms cannot detect. The tests also show that each algorithm can detect, among other types, a certain type of feature that is unique to it. Hence, most of the algorithms discussed in this paper would have to be combined to obtain complete coverage","['feature recognition algorithms', 'general-purpose feature detection techniques', 'National Design Repository', 'face sets', 'convex hull', 'concavity', 'feature extraction', 'mechanical engineering']","['algorithms', 'face sets', 'feature detection algorithms', 'Experimental results', 'algorithms cannon', 'first paper', 'Evaluation', 'new', 'paper', 'feature']",906,160,8,906,159,10,0,0,1
"implementation and performance evaluation of a fifo queue class library for time warp the authors describe the implementation, use, and performance evaluation of a fifo queue class library by means of a high-performance, easy-to-use interface employed for queuing simulations in parallel discrete simulations based on the time warp method. various general-purpose simulation libraries and languages have been proposed, and among these some have the advantage of not requiring users to define anything other than the state vector, and not needing awareness of rollback under a platform which performs state control based on copies. however, because the state vectors must be defined as simple data structures without pointers, dynamic data structures such as a fifo queue cannot be handled directly. under the proposed class library, both the platform and the user can handle such structures in the same fashion that embedded data structures are handled. in addition, instead of all stored data, just the operational history can be stored and recovered efficiently at an effectively minimal cost by taking advantage of the first-in-first-out characteristics of the above data structures. when the kernel deletes past state histories during a simulation, garbage collection is also performed transparently using the corresponding method ","Implementation and performance evaluation of a FIFO queue class library for time warp The authors describe the implementation, use, and performance evaluation of a FIFO queue class library by means of a high-performance, easy-to-use interface employed for queuing simulations in parallel discrete simulations based on the time warp method. Various general-purpose simulation libraries and languages have been proposed, and among these some have the advantage of not requiring users to define anything other than the state vector, and not needing awareness of rollback under a platform which performs state control based on copies. However, because the state vectors must be defined as simple data structures without pointers, dynamic data structures such as a FIFO queue cannot be handled directly. Under the proposed class library, both the platform and the user can handle such structures in the same fashion that embedded data structures are handled. In addition, instead of all stored data, just the operational history can be stored and recovered efficiently at an effectively minimal cost by taking advantage of the first-in-first-out characteristics of the above data structures. When the kernel deletes past state histories during a simulation, garbage collection is also performed transparently using the corresponding method","['FIFO queue', 'class library', 'time warp simulation', 'performance evaluation', 'easy-to-use interface', 'queuing simulations', 'parallel discrete simulations', 'general-purpose simulation libraries', 'simulation languages', 'state vectors', 'dynamic data structures', 'embedded data structures', 'operational history', 'first-in-first-out characteristics', 'garbage collection', 'object oriented method', 'state management', 'data structures', 'object-oriented programming', 'queueing theory', 'simulation languages', 'software libraries', 'software performance evaluation', 'storage management', 'time warp simulation']","['class library', 'performance evaluation', 'data structures', 'dynamic data structures', 'simple data structures', 'above data structures', 'FIFO queue cannon', 'time warp method', 'FIFO', 'time warp']",1137,199,25,1137,198,10,0,0,8
computer mediated communication and university international students the design for the preliminary study presented was based on the experiences of the international students and faculty members of a small southwest university being surveyed and interviewed. the data collection procedure blends qualitative and quantitative data. a strong consensus was found that supports the study's premise that there is an association between the use of computer mediated communication (cmc) and teaching and learning performance of international students. both groups believe cmc to be an effective teaching and learning tool by: increasing the frequency and quality of communication between students and instructors; improving language skills through increased writing and communication opportunities; allowing students and instructors to stay current and to compete effectively; providing alternative teaching and learning methods to increase students' confidence in their ability to communicate effectively with peers and instructors; and improving the instructors' pedagogical focus and questioning techniques ,Computer mediated communication and university international students The design for the preliminary study presented was based on the experiences of the international students and faculty members of a small southwest university being surveyed and interviewed. The data collection procedure blends qualitative and quantitative data. A strong consensus was found that supports the study's premise that there is an association between the use of computer mediated communication (CMC) and teaching and learning performance of international students. Both groups believe CMC to be an effective teaching and learning tool by: increasing the frequency and quality of communication between students and instructors; improving language skills through increased writing and communication opportunities; allowing students and instructors to stay current and to compete effectively; providing alternative teaching and learning methods to increase students’ confidence in their ability to communicate effectively with peers and instructors; and improving the instructors’ pedagogical focus and questioning techniques,"['computer mediated communication', 'university international students', 'faculty members', 'small southwest university', 'data collection procedure', 'quantitative data', 'qualitative data', 'CMC', 'teaching', 'learning performance', 'language skills', 'communication opportunities', 'instructors', 'student confidence', 'peers', 'pedagogical focus', 'questioning techniques', 'educational technology', 'electronic mail', 'teaching', 'teleconferencing']","['international students', 'communication opportunities', 'small southwest university', 'data collection procedure', 'students confidence', 'preliminary study', 'communication cmc', 'communication', 'students', 'international']",956,149,21,956,148,10,2,2,9
"global stability of the attracting set of an enzyme-catalysed reaction system the essential feature of enzymatic reactions is a nonlinear dependency of reaction rate on metabolite concentration taking the form of saturation kinetics. recently, it has been shown that this feature is associated with the phenomenon of ""loss of system coordination"" (liu, 1999). in this paper, we study a system of ordinary differential equations representing a branched biochemical system of enzyme-mediated reactions. we show that this system can become very sensitive to changes in certain maximum enzyme activities. in particular, we show that the system exhibits three distinct responses: a unique, globally-stable steady-state, large amplitude oscillations, and asymptotically unbounded solutions, with the transition between these states being almost instantaneous. it is shown that the appearance of large amplitude, stable limit cycles occurs due to a ""false"" bifurcation or canard explosion. the subsequent disappearance of limit cycles corresponds to the collapse of the domain of attraction of the attracting set for the system and occurs due to a global bifurcation in the flow, namely, a saddle connection. subsequently, almost all nonnegative data become unbounded under the action of the dynamical system and correspond exactly to loss of system coordination. we discuss the relevance of these results to the possible consequences of modulating such systems ","Global stability of the attracting set of an enzyme-catalysed reaction system The essential feature of enzymatic reactions is a nonlinear dependency of reaction rate on metabolite concentration taking the form of saturation kinetics. Recently, it has been shown that this feature is associated with the phenomenon of ""loss of system coordination"" (Liu, 1999). In this paper, we study a system of ordinary differential equations representing a branched biochemical system of enzyme-mediated reactions. We show that this system can become very sensitive to changes in certain maximum enzyme activities. In particular, we show that the system exhibits three distinct responses: a unique, globally-stable steady-state, large amplitude oscillations, and asymptotically unbounded solutions, with the transition between these states being almost instantaneous. It is shown that the appearance of large amplitude, stable limit cycles occurs due to a ""false"" bifurcation or canard explosion. The subsequent disappearance of limit cycles corresponds to the collapse of the domain of attraction of the attracting set for the system and occurs due to a global bifurcation in the flow, namely, a saddle connection. Subsequently, almost all nonnegative data become unbounded under the action of the dynamical system and correspond exactly to loss of system coordination. We discuss the relevance of these results to the possible consequences of modulating such systems,","['enzymatic reactions', 'nonlinear dependency', 'metabolite concentration', 'saturation kinetics', 'biochemical system', 'ordinary differential equations', 'enzyme-mediated reactions', 'saddle connection', 'stable limit cycles', 'bifurcation', 'bifurcation', 'biochemistry', 'catalysis', 'differential equations', 'proteins', 'reaction kinetics theory', 'stability']","['system', 'system coordination', 'enzyme-catalysed reaction system', 'enzyme-mediated reactions', 'enzymatic reactions', 'biochemical system', 'dynamical system', 'Global stability', 'reaction rate', 'such systems']",1242,214,17,1243,213,10,0,1,9
"books on demand: just-in-time acquisitions the purdue university libraries interlibrary loan unit proposed a pilot project to purchase patrons' loan requests from amazon. com, lend them to the patrons, and then add the titles to the collection. staff analyzed previous monograph loans, developed ordering criteria, implemented the proposal as a pilot project for six months, and evaluated the resulting patron comments, statistics, and staff perceptions. as a result of enthusiastic patron comments and a review of the project statistics, the program was extended ","Books on demand: just-in-time acquisitions. The Purdue University Libraries Interlibrary Loan unit proposed a pilot project to purchase patrons’ loan requests from Amazon. com, lend them to the patrons, and then add the titles to the collection. Staff analyzed previous monograph loans, developed ordering criteria, implemented the proposal as a pilot project for six months, and evaluated the resulting patron comments, statistics, and staff perceptions. As a result of enthusiastic patron comments and a review of the project statistics, the program was extended","['Purdue University Libraries Interlibrary Loan unit', 'monograph loans', 'ordering criteria', 'staff perceptions', 'patron comments', 'publication on demand', 'academic libraries', 'library automation']","['pilot project', 'enthusiastic patron comments', 'Purdue University Libraries', 'patron comments statistics', 'just-in-time acquisitions', 'inter-library Loan unit', 'patrons loan requests', 'Books', 'just-in-time', 'acquisitions']",481,84,8,482,83,10,1,2,3
"negotiating the semantics of agent communication languages this article presents a formal framework and outlines a method that autonomous agents can use to negotiate the semantics of their communication language at run-time. such an ability is needed in open multi-agent systems so that agents can ensure they understand the implications of the utterances that are being made and so that they can tailor the meaning of the primitives to best fit their prevailing circumstances. to this end, the semantic space framework provides a systematic means of classifying the primitives along multiple relevant dimensions. this classification can then be used by the agents to structure their negotiation (or semantic fixing) process so that they converge to the mutually agreeable semantics that are necessary for coherent social interactions ","Negotiating the semantics of agent communication languages This article presents a formal framework and outlines a method that autonomous agents can use to negotiate the semantics of their communication language at run-time. Such an ability is needed in open multi-agent systems so that agents can ensure they understand the implications of the utterances that are being made and so that they can tailor the meaning of the primitives to best fit their prevailing circumstances. To this end, the semantic space framework provides a systematic means of classifying the primitives along multiple relevant dimensions. This classification can then be used by the agents to structure their negotiation (or semantic fixing) process so that they converge to the mutually agreeable semantics that are necessary for coherent social interactions","['autonomous agents', 'multi-agent systems', 'communication language', 'semantic fixing', 'semantic space', 'social interactions', 'formal languages', 'multi-agent systems', 'programming language semantics']","['agent communication languages', 'open multi-agency systems', 'semantic space framework', 'agreeable semantics', 'autonomous agents', 'formal framework', 'agents', 'semantics', 'languages', 'communication language']",709,127,9,709,126,10,0,0,1
"permission grids: practical, error-bounded simplification we introduce the permission grid, a spatial occupancy grid which can be used to guide almost any standard polygonal surface simplification algorithm into generating an approximation with a guaranteed geometric error bound. in particular, all points on the approximation are guaranteed to be within some user-specified distance from the original surface. such bounds are notably absent from many current simplification methods, and are becoming increasingly important for applications in scientific computing and adaptive level of detail control. conceptually simple, the permission grid defines a volume in which the approximation must lie, and does not permit the underlying simplification algorithm to generate approximations outside the volume. the permission grid makes three important, practical improvements over current error-bounded simplification methods. first, it works on arbitrary triangular models, handling all manners of mesh degeneracies gracefully. further, the error tolerance may be easily expanded as simplification proceeds, allowing the construction of an error-bounded level of detail hierarchy with vertex correspondences among all levels of detail. and finally, the permission grid has a representation complexity independent of the size of the input model, and a small running time overhead, making it more practical and efficient than current methods with similar guarantees ","Permission grids: practical, error-bounded simplification We introduce the permission grid, a spatial occupancy grid which can be used to guide almost any standard polygonal surface simplification algorithm into generating an approximation with a guaranteed geometric error bound. In particular, all points on the approximation are guaranteed to be within some user-specified distance from the original surface. Such bounds are notably absent from many current simplification methods, and are becoming increasingly important for applications in scientific ‘computing and adaptive level of detail control. Conceptually simple, the permission grid defines a volume in which the approximation must lie, and does not permit the underlying simplification algorithm to generate approximations outside the volume. The permission grid makes three important, practical improvements over current error-bounded simplification methods. First, it works on arbitrary triangular models, handling all manners of mesh degeneracies gracefully. Further, the error tolerance may be easily expanded as simplification proceeds, allowing the construction of an error-bounded level of detail hierarchy with vertex correspondences among all levels of detail. And finally, the permission grid has a representation complexity independent of the. size of the input model, and a small running time overhead, making it more practical and efficient than current methods with similar guarantees","['permission grid', 'spatial occupancy grid', 'polygonal surface simplification algorithm', 'guaranteed geometric error bound', 'approximation', 'error-bounded simplification', 'user-specified distance', 'scientific computing', 'adaptive level of detail control', 'arbitrary triangular models', 'mesh degeneracies', 'error tolerance', 'vertex correspondences', 'representation complexity', 'running time overhead', 'computational complexity', 'computational geometry', 'computer graphics']","['simplification', 'simplification algorithm', 'practical error-bounded simplification', 'important practical improvements', 'simplification proceeds', 'spatial occupancy grid', 'error-bounded level', 'Permission grids', 'grid', 'permission grid']",1260,202,18,1262,201,10,9,2,10
"a digital-to-analog converter based on differential-quad switching a high-conversion-rate high-resolution oversampling digital-to-analog converter (dac) for direct digital modulation is addressed in this paper. a new type of switching scheme, called differential-quad switching, is presented. to verify the feasibility of this scheme, essential parts with some auxiliary circuitry for interfacing were fabricated in a 0.8- mu m cmos technology. measured results show that the switching scheme provides 11-b resolution at 100 msamples/s and 6-b at 1 gsamples/s. the degradation in signal-to-noise ratio is not observed for the variation of the supply voltage down to 1.5 v, which means the proposed scheme is suitable for low-voltage applications ","A digital-to-analog converter based on differential-quad switching A high-conversion-rate high-resolution oversampling digital-to-analog converter (DAC) for direct digital modulation is addressed in this paper. A new type of switching scheme, called differential-quad switching, is presented. To verity the feasibility of this scheme, essential parts with some auxiliary circuitry for interfacing were fabricated in a 0.8- mu m CMOS technology. Measured results show that the switching scheme provides 11-b resolution at 100 MSamples/s and 6-b at 1 GSamples/s. The degradation in signal-to-noise ratio is not observed for the variation of the supply voltage down to 1.5 V, which means the proposed scheme is suitable for low-voltage applications,","['high-conversion-rate DAC', 'high-resolution DAC', 'oversampling DAC', 'digital-to-analog converter', 'CMOS technology', 'direct digital modulation', 'differential-quad switching', 'signal-to-noise ratio', 'SNR', '1.5 V', '0.8 micron', 'CMOS integrated circuits', 'digital-analogue conversion', 'high-speed integrated circuits', 'integrated circuit design', 'modulation', 'switching circuits']","['differential-quad switching', 'switching scheme', 'high-conversion-rate high-resolution oversampling', 'digital-to-analog converter dace', 'direct digital modulation', 'switching', 'converter', 'digital-to-analog', 'digital-to-analog converter', 'scheme']",641,106,17,642,105,10,1,2,3
"java portability put to the test sun microsystems' recently launched java verification program aims to enable companies to assess the cross-platform portability of applications written in java, and to help software vendors ensure that their solutions can run in heterogenous j2ee application server environments ","Java portability put to the test Sun Microsystems’ recently launched Java Verification Program aims to enable companies to assess the cross-platform portability of applications written in Java, and to help software vendors ensure that their solutions can run in heterogenous J2EE application server environments","['Sun Microsystems', 'Java Verification Program', 'cross-platform portability', 'application program interfaces', 'DP management', 'Java']","['application server environments', 'cross-platform portability', 'Java Verification Program', 'test Sun microsystems', 'software vendors', 'Java portability', 'Java', 'portability', 'Sun', 'microsystems']",268,45,6,268,44,10,1,1,0
"continuous-time linear systems: folklore and fact we consider a family of continuous input-output maps representing linear time-invariant systems that take a set of signals into itself. it is shown that this family contains maps whose impulse response is the zero function, but which take certain inputs into nonzero outputs. it is shown also that this family contains members whose input-output properties are not described by their frequency domain response functions, and that the maps considered need not even commute ","Continuous-time linear systems: folklore and fact We consider a family of continuous input-output maps representing linear time-invariant systems that take a set of signals into itself. Itis shown that this family contains maps whose impulse response is the zero function, but which take certain inputs into nonzero outputs. It is shown also that this family contains members whose input-output properties are not described by their frequency domain response functions, and that the maps considered need not even commute","['continuous-time systems', 'linear systems', 'continuous input-output maps', 'time-invariant systems', 'impulse response', 'zero function', 'frequency domain response', 'commutation', 'signal processing', 'continuous time systems', 'frequency-domain analysis', 'linear systems', 'poles and zeros', 'signal processing', 'transient response']","['family', 'linear time-invariant systems', 'continuous input-output maps', 'input-output properties', 'impulse response', 'Continuous-time', 'input-output', 'maps', 'linear', 'systems']",443,80,15,443,78,10,191,52,1
"a virtual test facility for the simulation of dynamic response in materials the center for simulating dynamic response of materials at the california institute of technology is constructing a virtual shock physics facility for studying the response of various target materials to very strong shocks. the virtual test facility (vtf) is an end-to-end, fully three-dimensional simulation of the detonation of high explosives (he), shock wave propagation, solid material response to pressure loading, and compressible turbulence. the vtf largely consists of a parallel fluid solver and a parallel solid mechanics package that are coupled together by the exchange of boundary data. the eulerian fluid code and lagrangian solid mechanics model interact via a novel approach based on level sets. the two main computational packages are integrated through the use of pyre, a problem solving environment written in the python scripting language. pyre allows application developers to interchange various computational models and solver packages without recompiling code, and it provides standardized access to several data visualization engines and data input mechanisms. in this paper, we outline the main components of the vtf, discuss their integration via pyre, and describe some recent accomplishments in large-scale simulation using the vtf ","A Virtual Test Facility for the simulation of dynamic response in materials The Center for Simulating Dynamic Response of Materials at the California Institute of Technology is constructing a virtual shock physics facllty for studying the response of various target materials to very strong shocks. The Virtual Test Facility (VTF) is an end-to-end, fully three-dimensional simulation of the detonation of high explosives (HE), shock wave propagation, solid material response to pressure loading, and compressible turbulence. The VTF largely consists of a parallel fluid solver and a parallel solid mechanics package that are coupled together by the exchange of boundary data. The Eulerian fluid code and Lagrangian solid mechanics model interact via a novel approach based on level sets. The two main computational packages are integrated through the use of Pyre, a problem solving environment written in the Python scripting language. Pyre allows application developers to interchange various computational models and solver packages without recompiling code, and it provides standardized access to several data visualization engines and data input mechanisms. In this paper, we outline the main ‘components of the VTF, discuss their integration via Pyre, and describe some recent accomplishments in large-scale simulation using the VTF","['virtual shock physics facility', 'Virtual Test Facility', 'high explosives', 'shock wave propagation', 'solid material response', 'pressure loading', 'compressible turbulence', 'parallel fluid solver', 'parallel solid mechanics', 'shock physics simulation', 'data visualization', 'Pyre', 'problem solving environment', 'Python scripting language', 'digital simulation', 'dynamic response', 'materials science', 'shock waves', 'virtual reality']","['Virtual Test Facility', 'dynamic response', 'three-dimensional simulation', 'various target materials', 'solid material response', 'shock wave propagation', 'large-scale simulation', 'strong shocks', 'Test', 'Virtual']",1142,197,19,1142,196,10,13,2,8
"taking back control [scada system] most common way to implement a scada system is to go outside. however, in the author's opinion, to truly take control of a scada project, in-house personnel should handle as much of the job as possible. this includes design, equipment specification, installation, and programming. the more of these tasks one does in-house, the more control and ownership one has. to accomplish this, we first evaluated the existing scada system and investigated new technologies to establish a list of features the new system needed to incorporate ","Taking back control [SCADA system] Most common way to implement a SCADA system is to go outside. However, in the author's opinion, to truly take control of a SCADA project, in-house Personnel should handle as much of the job as possible. This includes design, equipment specification, installation, and programming. The more of these tasks one does in-house, the more control and ownership ‘one has. To accomplish this, we first evaluated the existing SCADA system and investigated new technologies to establish a list of features the new system needed to incorporate","['SCADA', 'supervisory control', 'data acquisition', 'in-house integration', 'compatibility', 'programmable logic controllers', 'process control', 'programmable controllers', 'SCADA systems']","['scala system', 'control escada system', 'in-house Personnel', 'authors opinion', 'scala project', 'system', 'new system', 'common way', 'scala', 'control']",477,91,9,478,90,10,3,1,1
"towards strong stability of concurrent repetitive processes sharing resources the paper presents a method for design of stability conditions of concurrent, repetitive processes sharing common resources. steady-state behaviour of the system with m cyclic processes utilising a resource with the mutual exclusion is considered. based on a recurrent equations framework necessary and sufficient conditions for the existence of maximal performance steady-state are presented. it was shown that if the conditions hold then the m-process system is marginally stable, i.e., a steady-state of the system depends on the perturbations. the problem of finding the relative positions of the processes leading to waiting-free (maximal efficiency) steady-states of the system is formulated as a constraint logic programming problem. an example illustrating the solving of the problem for a 3-process system using object-oriented, constraint logic programming language oz is presented. a condition sufficient for strong stability of the m-process system is given. when the condition holds then for any initial phases of the processes a waiting-free steady-state will be reached ","Towards strong stability of concurrent repetitive processes sharing resources The paper presents a method for design of stability conditions of concurrent, repetitive processes sharing common resources. Steady-state behaviour of the system with m cyclic processes utilising a resource with the mutual exclusion is considered. Based on a recurrent equations framework necessary and sufficient conditions for the existence of maximal performance steady-state are presented. It was shown that if the conditions hold then the m-process system is marginally stable, le., a steady-state of the system depends on the perturbations. The problem of finding the relative positions of the processes leading to waiting-free (maximal efficiency) steady-states of the system is formulated as a constraint logic programming problem. An example illustrating the solving of the problem for a 3-process system using object-oriented, constraint logic programming language Oz is presented A condition sufficient for strong stability of the m-process system is given. When the condition holds then for any initial phases of the processes a waiting-free steady-state will be reached","['strong stability', 'concurrent repetitive processes', 'common resources', 'steady-state behaviour', 'cyclic processes', 'mutual exclusion', 'recurrent equations framework', 'necessary and sufficient conditions', 'maximal performance steady-state', 'waiting-free steady-states', 'constraint logic programming', '3-process system', 'Oz language', 'concurrency theory', 'constraint handling', 'logic programming languages', 'object-oriented methods', 'stability']","['process system', 'concurrent repetitive processes', 'strong stability', 'maximal performance steady-state', 'sufficient conditions', 'stability conditions', 'cyclic processes', 'processes', 'conditions', 'system']",997,167,18,995,166,10,4,2,4
"personality research on the internet: a comparison of web-based and traditional instruments in take-home and in-class settings students, faculty, and researchers have become increasingly comfortable with the internet, and many of them are interested in using the web to collect data. few published studies have investigated the differences between web-based data and data collected with more traditional methods. in order to investigate these potential differences, two important factors were crossed in this study: whether the data were collected on line or not and whether the data were collected in a group setting at a fixed time or individually at a time of the respondent's choosing. the visions of morality scale (shelton and mcadams, 1990) was used, and the participants were assigned to one of four conditions: in-class web survey, in-class paper-and-pencil survey; take-home web survey, and take-home paper-and-pencil survey. no significant differences in scores were found for any condition; however, response rates were affected by the type of survey administered, with the take-home web-based instrument having the lowest response rate. therefore, researchers need to be aware that different modes of administration may affect subject attrition and may, therefore, confound investigations of other independent variables ","Personality research on the Internet: a comparison of Web-based and traditional instruments in take-home and in-class settings Students, faculty, and researchers have become increasingly comfortable with the Internet, and many of them are interested in using the Web to collect data. Few published studies have investigated the differences between Web-based data and data collected with more traditional methods. In order to investigate these potential differences, two important factors were crossed in this study: whether the data were collected on line or not and whether the data were collected in a group setting at a fixed time or individually at a time of the respondent's choosing. The Visions of Morality scale (Shelton and McAdams, 1990) was used, and the participants were assigned to one of four conditions: in-class Web survey, in-class paper-and-pencil survey; take-home Web survey, and take-home paper-and-pencil survey. No significant differences in scores were found for any condition; however, response rates were affected by the type of survey administered, with the take-home Web-based instrument having the lowest response rate. Therefore, researchers need to be aware that different modes of administration may affect subject attrition and may, therefore, confound investigations of other independent variables","['personality research', 'Internet', 'Web-based instruments', 'data collection', 'Visions of Morality scale', 'in-class Web survey', 'in-class paper-and-pencil survey', 'take-home Web survey', 'take-home paper-and-pencil survey', 'response rates', 'administration', 'subject attrition', 'computer aided instruction', 'Internet', 'psychology']","['take-home paper-and-pencil survey', 'take-home pen-based instrument', 'traditional instruments', 'take-home Web survey', 'lowest response rate', 'Personality research', 'pen-based data', 'data', 'research', 'paper-and-pencil survey']",1139,195,15,1139,194,10,0,0,5
"optimization of cutting conditions for single pass turning operations using a deterministic approach an optimization analysis, strategy and cam software for the selection of economic cutting conditions in single pass turning operations are presented using a deterministic approach. the optimization is based on criteria typified by the maximum production rate and includes a host of practical constraints. it is shown that the deterministic optimization approach involving mathematical analyses of constrained economic trends and graphical representation on the feed-speed domain provides a clearly defined strategy that not only provides a unique global optimum solution, but also the software that is suitable for on-line cam applications. a numerical study has verified the developed optimization strategies and software and has shown the economic benefits of using optimization ","Optimization of cutting conditions for single pass turning operations using a deterministic approach An optimization analysis, strategy and CAM software for the selection of economic cutting conditions in single pass turning operations are presented using a deterministic approach. The optimization is based on criteria typified by the maximum production rate and includes a host of practical constraints. It is shown that the deterministic optimization approach involving mathematical analyses of constrained economic trends and graphical representation on the feed-speed domain provides a clearly defined strategy that not only provides a unique global optimum. solution, but also the software that is suitable for on-line CAM applications. A numerical study has verified the developed optimization strategies and software and has shown the economic benefits of using optimization","['cutting conditions optimization', 'single pass turning operations', 'deterministic approach', 'CAM software', 'economic cutting conditions', 'maximum production rate', 'mathematical analyses', 'constrained economic trends', 'process planning', 'computer aided production planning', 'computerised numerical control', 'cutting', 'machine tools', 'machining', 'optimisation']","['optimization', 'deterministic approach', 'single pass', 'deterministic optimization approach', 'developed optimization strategies', 'optimization analysis strategy', 'economic cutting conditions', 'conditions', 'pass', 'single']",758,125,15,759,124,10,0,1,3
"differential algebraic systems anew it is proposed to figure out the leading term in differential algebraic systems more precisely. low index linear systems with those properly stated leading terms are considered in detail. in particular, it is asked whether a numerical integration method applied to the original system reaches the inherent regular ode without conservation, i.e., whether the discretization and the decoupling commute in some sense. in general one cannot expect this commutativity so that additional difficulties like strong stepsize restrictions may arise. moreover, abstract differential algebraic equations in infinite-dimensional hilbert spaces are introduced, and the index notion is generalized to those equations. in particular, partial differential algebraic equations are considered in this abstract formulation ","Differential algebraic systems anew Itis proposed to figure out the leading term in differential algebraic systems more precisely. Low index linear systems with those properly stated leading terms are considered in detail. In particular, it is asked whether a numerical integration method applied to the original system reaches the inherent regular ODE without conservation, i.e., whether the discretization and the decoupling commute in some sense. In general ‘one cannot expect this commutativity so that additional difficulties like strong stepsize restrictions may arise. Moreover, abstract differential algebraic equations in infinite-dimensional Hilbert spaces are introduced, and the index notion is generalized to those equations. In particular, partial differential algebraic equations are considered in this abstract formulation","['differential algebraic systems', 'low index linear systems', 'numerical integration method', 'inherent regular ODE', 'commutativity', 'stepsize restrictions', 'abstract differential algebraic equations', 'differential equations', 'matrix algebra', 'stability']","['numerical integration method', 'term', 'original system', 'linear systems', 'Low index', 'systems', 'differential', 'algebraic', 'differential algebraic systems', 'Differential algebraic systems']",724,116,10,725,114,10,406,110,3
"learning weights for the quasi-weighted means we study the determination of weights for quasi-weighted means (also called quasi-linear means) when a set of examples is given. we consider first a simple case, the learning of weights for weighted means, and then we extend the approach to the more general case of a quasi-weighted mean. we consider the case of a known arbitrary generator f. the paper finishes considering the use of parametric functions that are suitable when the values to aggregate are measure values or ratio ","Learning weights for the quasi-weighted means. We study the determination of weights for quasi-weighted means (also called quasi-linear means) when a set of examples is given. We consider first a simple case, the learning of weights for weighted means, and then we extend the approach to the more general case of a quasi-weighted mean. We consider the case of a known arbitrary generator f. The paper finishes considering the use of parametric functions that are suitable when the values to aggregate are measure values or ratio","['quasi-weighted means', 'quasi-linear means', 'learning', 'parametric functions', 'measure values', 'ratio values', 'learning (artificial intelligence)', 'matrix algebra', 'minimisation']","['quasi-weighted mean', 'weighted means', 'quasi-linear means', 'Learning weights', 'general case', 'simple case', 'case', 'means', 'weights', 'quasi-weighted']",442,87,9,443,86,10,0,1,2
2002 in-house fulfillment systems report [publishing] cm's 13th annual survey of in-house fulfillment system suppliers brings you up to date on the current capabilities of the leading publication software packages ,2002 in-house fulfillment systems report [publishing] CM's 13th annual survey of in-house fulfillment system suppliers brings you up to date on the current capabilities of the leading publication software Packages,"['survey', 'in-house fulfillment system', 'suppliers', 'publication software packages', ""buyer's guides"", 'publishing', 'software packages']","['publication software Packages', 'in-house fulfillment systems', 'current capabilities', 'annual survey', 'c', 'survey', 'annual', 'systems', 'in-house', 'fulfillment']",184,31,7,184,30,10,0,0,2
"bioone: a new model for scholarly publishing this article describes a unique electronic journal publishing project involving the university of kansas, the big 12 plus libraries consortium, the american institute of biological sciences, allen press, and sparc, the scholarly publishing and academic resources coalition. this partnership has created bioone, a database of 40 full-text society journals in the biological and environmental sciences, which was launched in april, 2001. the genesis and development of the project is described and financial, technical, and intellectual property models for the project are discussed. collaborative strategies for the project are described ","BioOne: a new model for scholarly publishing This article describes a unique electronic journal publishing project involving the University of Kansas, the Big 12 Plus Libraries Consortium, the American Institute of Biological Sciences, Allen Press, and SPARC, the Scholarly Publishing and Academic Resources Coalition. This partnership has created BioOne, a database of 40 fulltext society journals in the biological and environmental sciences, which was launched in April, 2001. The genesis and development of the project is described and financial, technical, and intellectual property models for the project are discussed. Collaborative strategies for the project are described","['BioOne full-text society journal database', 'electronic journal publishing project', 'scholarly publishing model', 'University of Kansas', 'Big 12 Plus Libraries Consortium', 'American Institute of Biological Sciences', 'Allen Press', 'SPARC', 'Scholarly Publishing and Academic Resources Coalition', 'biological sciences', 'environmental sciences', 'intellectual property models', 'technical models', 'financial models', 'collaborative strategies', 'academic libraries', 'biology computing', 'electronic publishing', 'environmental science computing', 'full-text databases', 'scientific information systems']","['project', 'scholarly publishing', 'intellectual property models', 'Academic Resources coalition', 'full-text society journals', 'boone', 'environmental sciences', 'new model', 'models', 'publishing']",586,97,21,585,96,10,4,1,5
"a suggestion of fractional-order controller for flexible spacecraft attitude control a controller design method for flexible spacecraft attitude control is proposed. the system is first described by a partial differential equation with internal damping. then the frequency response is analyzed, and the three basic characteristics of the flexible system, namely, average function, lower bound and upper bound are defined. on this basis, a fractional-order controller is proposed, which functions as phase stabilization control for lower frequency and smoothly enters to amplitude stabilization at higher frequency by proper amplitude attenuation. it is shown that the equivalent damping ratio increases in proportion to the square of frequency ","A suggestion of fractional-order controller for flexible spacecraft attitude control A controller design method for flexible spacecraft attitude control is proposed. The system is first described by a partial differential equation with internal damping. Then the frequency response is analyzed, and the three basic characteristics of the flexible system, namely, average function, lower bound and upper bound are defined. On this basis, a fractiona-order controller is proposed, which functions. as phase stabilization control for lower frequency and smoothly enters to amplitude stabilization at higher frequency by proper amplitude attenuation. It is shown that the equivalent damping ratio increases in proportion to the square of frequency","['fractional-order controller', 'flexible spacecraft attitude control', 'partial differential equation', 'internal damping', 'frequency response', 'phase stabilization control', 'amplitude stabilization', 'damping ratio', 'aerospace instrumentation', 'attitude control', 'controllers', 'damping', 'flexible manipulators', 'frequency response', 'frequency stability', 'nonlinear control systems', 'nonlinear dynamical systems', 'partial differential equations', 'space vehicles']","['fractional-order controller', 'control', 'phase stabilization control', 'controller design method', 'frequency response', 'higher frequency', 'lower frequency', 'flexible system', 'flexible', 'fractional-order']",639,106,19,639,105,10,6,2,4
the information age interview - capital one credit card company capital one attributes its rapid customer growth to the innovative use of cutting-edge technology. european cio catherine doran talks about the systems that have fuelled that runaway success ,The Information Age interview - Capital One Credit card company Capital One attributes its rapid customer growth to the innovative use of cutting-edge technology. European ClO Catherine Doran talks about the systems that have fuelled that runaway success,"['credit card company', 'Capital One', 'customer growth', 'cutting-edge technology', 'credit transactions', 'marketing']","['Capital', 'Information Age interview', 'rapid customer growth', 'innovative use', 'company', 'Credit', 'card', 'Age', 'interview', 'Information']",217,39,6,217,38,10,1,1,1
"influence of advertising expenses on the characteristics of functioning of an insurance company the basic characteristics of the functioning of an insurance company, including the average capital, ruin and survival probabilities, and the conditional time before ruin, are examined with allowance for advertising expenses ","Influence of advertising expenses on the characteristics of functioning of an insurance company The basic characteristics of the functioning of an insurance company, including the average capital, ruin and survival probabilities, and the conditional time before ruin, are examined with allowance for advertising expenses","['advertising expenses influence', 'insurance company functioning characteristics', 'average capital', 'ruin probabilities', 'survival probabilities', 'conditional time', 'advertising', 'costing', 'insurance', 'probability']","['advertising expenses', 'insurance company', 'basic characteristics', 'conditional time', 'average capital', 'characteristics', 'company', 'expenses', 'insurance', 'advertising']",277,45,10,277,44,10,0,0,1
"end-user perspectives on the uptake of computer supported cooperative working researchers in information systems have produced a rich collection of meta-analyses and models to further understanding of factors influencing the uptake of information technologies. in the domain of cscw, however, these models have largely been neglected, and while there are many case studies, no systematic account of uptake has been produced. we use findings from information systems research to structure a meta-analysis of uptake issues as reported in cscw case studies, supplemented by a detailed re-examination of one of our own case studies from this perspective. this shows that while there are some factors which seem to be largely specific to cscw introductions, many of the case study results are very similar to standard is findings. we conclude by suggesting how the two communities of researchers might build on each other's work, and finally propose activity theory as a means of integrating the two perspectives ","End-user perspectives on the uptake of computer supported cooperative working Researchers in information systems have produced a rich collection of meta-analyses and models to further understanding of factors influencing the uptake of information technologies. In the domain of CSCW, however, these models have largely been neglected, and while there are many case studies, no systematic account of uptake has been produced. We use findings from information systems research to structure a meta-analysis of uptake issues as reported in CSCW case studies, supplemented by a detailed re-examination of one of our own case studies from this perspective. This shows that while there are some factors which seem to be largely specific to CSCW introductions, many of the case study results are very similar to standard IS findings. We conclude by suggesting how the two communities of researchers might build on each other's work, and finally propose activity theory as a means of integrating the two perspectives","['computer supported cooperative work', 'end-user perspectives', 'information systems', 'meta-analyses', 'information technology', 'CSCW', 'activity theory', 'business data processing', 'groupware', 'information systems', 'personal computing']","['information systems research', 'information technologies', 'End-user perspectives', 'case study results', 'many case studies', 'csce case studies', 'own case studies', 'uptake issues', 'uptake', 'information systems']",853,156,11,853,155,10,0,0,4
"exploiting randomness in quantum information processing we consider how randomness can be made to play a useful role in quantum information processing-in particular, for decoherence control and the implementation of quantum algorithms. for a two-level system in which the decoherence channel is non-dissipative, we show that decoherence suppression is possible if memory is present in the channel. random switching between two potentially harmful noise sources can then provide a source of stochastic control. such random switching can also be used in an advantageous way for the implementation of quantum algorithms ","Exploiting randomness in quantum information processing We consider how randomness can be made to play a useful role in quantum information processing-in particular, for decoherence control and the implementation of quantum algorithms. For a two-level system in which the decoherence channel is non-dissipative, we show that decoherence suppression is possible if memory is present in the channel. Random switching between two potentially harmful noise sources can then provide a source of stochastic control. Such random switching can also be used in an advantageous way for the implementation of quantum algorithms","['quantum information processing', 'randomness', 'decoherence control', 'quantum algorithms', 'two-level system', 'random switching', 'noise', 'stochastic control', 'information theory', 'quantum communication', 'stochastic processes']","['quantum algorithms', 'randomness', 'quantum information processing-in', 'channel Random switching', 'coherence suppression', 'Such random switching', 'coherence control', 'coherence channel', 'quantum', 'quantum information processing']",527,91,11,527,90,10,0,0,2
"emotion and self-control a biology-based model of choice is used to examine time-inconsistent preferences and the problem of self-control. emotion is shown to be the biological substrate of choice, in that emotional systems assign value to 'goods' in the environment and also facilitate the learning of expectations regarding alternative options for acquiring those goods. a third major function of the emotional choice systems is motivation. self-control is shown to be the result of a problem with the inhibition of the motive force of emotion, where this inhibition is necessary for higher level deliberation ","Emotion and self-control A biology-based model of choice is used to examine time-inconsistent preferences and the problem of self-control. Emotion is shown to be the. biological substrate of choice, in that emotional systems assign value to ‘goods’ in the environment and also facilitate the learning of expectations regarding alternative options for acquiring those goods. A third major function of the emotional choice systems is motivation. Self-control is shown to be the result of a problem with the inhibition of the motive force of emotion, where this inhibition is necessary for higher level deliberation","['choice model', 'inhibition', 'learning', 'time-inconsistent preferences', 'self-control', 'emotional choice systems', 'emotion', 'psychology']","['Emotion', 'time-inconsistent preferences', 'emotional choice systems', 'motivation Self-control', 'self-control Emotion', 'biological substrate', 'biology-based model', 'emotional systems', 'self-control', 'choice']",519,94,8,520,93,10,2,2,2
"knowledge acquisition and ontology modelling for construction of a control and monitoring expert system this paper presents the processes of knowledge acquisition and ontology development for structuring the knowledge base of an expert system. ontological engineering is a process that facilitates construction of the knowledge base of an intelligent system. ontology is the study of the organization and classification of knowledge. ontological engineering in artificial intelligence has the practical goal of constructing frameworks for knowledge that allow computational systems to tackle knowledge-intensive problems and it supports knowledge sharing and reuse. to illustrate the process of conceptual modelling using the inferential modelling technique as a basis for ontology construction, the tool and processes are applied to build an expert system in the domain of monitoring of a petroleum-production facility ","Knowledge acquisition and ontology modelling for construction of a control and monitoring expert system This paper presents the processes of knowledge acquisition and ontology development for structuring the knowledge base of an expert system. Ontological engineering is a process that facilitates construction of the knowledge base of an intelligent system. Ontology is the study of the organization and classification of knowledge. Ontological engineering in artificial intelligence has the practical goal of constructing frameworks for knowledge that allow computational systems, to tackle knowledge-intensive problems and it supports knowledge sharing and reuse. To illustrate the process of conceptual modelling using the Inferential Modelling Technique as a basis for ontology construction, the tool and processes are applied to build an expert system in the domain of monitoring of a petroleum-production facility","['knowledge acquisition', 'knowledge base', 'intelligent system', 'ontological engineering', 'artificial intelligence', 'knowledge reuse', 'inferential modelling technique', 'petroleum-production facility', 'ontology modelling', 'control and monitoring expert system', 'artificial intelligence', 'expert systems', 'knowledge acquisition', 'software reusability']","['knowledge base', 'intelligent system Ontology', 'monitoring expert system', 'ontology construction', 'ontology development', 'ontology modelling', 'knowledge sharing', 'expert system', 'knowledge acquisition', 'Knowledge acquisition']",792,129,14,793,128,10,0,1,4
"a brief history of electronic reserves electronic reserves has existed as a library service for barely ten years, yet its history, however brief, is important as an indicator of the direction being taken by the profession of librarianship as a whole. recent improvements in technology and a desire to provide better service to students and faculty have resulted in the implementation of e-reserves by ever greater numbers of academic libraries. yet a great deal of confusion still surrounds the issue of copyright compliance. negotiation, litigation, and legislation in particular have framed the debate over the application of fair use to an e-reserves environment, and the question of whether or not permission fees should be paid to rights holders, but as of yet no definitive answers or standards have emerged ","A brief history of electronic reserves Electronic reserves has existed as a library service for barely ten years, yet its history, however brief, is important as an indicator of the direction being taken by the profession of Librarianship as a whole. Recent improvements in technology and a desire to provide better service to students and faculty have resulted in the implementation of e-reserves by ever greater numbers of academic libraries. Yet a great deal of confusion still surrounds the issue of copyright compliance. Negotiation, litigation, and legislation in particular have framed the debate over the application of fair use to an e-reserves environment, and the question of whether or not permission fees should be paid to rights holders, but as of yet no definitive answers or standards have emerged","['electronic reserves', 'library service', 'librarianship', 'students', 'faculty', 'academic libraries', 'copyright compliance', 'negotiation', 'litigation', 'legislation', 'e-reserves environment', 'permission fees', 'academic libraries', 'copyright', 'history', 'legislation', 'library automation']","['reserves environment', 'academic libraries', 'library service', 'better service', 'brief history', 'reserves', 'brief', 'history', 'electronic reserves', 'Electronic reserves']",685,130,17,685,129,10,0,0,10
"causes of the decline of the business school management science course the business school management science course is suffering serious decline. the traditional model- and algorithm-based course fails to meet the needs of mba programs and students. poor student mathematical preparation is a reality, and is not an acceptable justification for poor teaching outcomes. management science ph.d.s are often poorly prepared to teach in a general management program, having more experience and interest in algorithms than management. the management science profession as a whole has focused its attention on algorithms and a narrow subset of management problems for which they are most applicable. in contrast, mba's rarely encounter problems that are suitable for straightforward application of management science tools, living instead in a world where problems are ill-defined, data is scarce, time is short, politics is dominant, and rational ""decision makers"" are non-existent. the root cause of the profession's failure to address these issues seems to be (in russell ackoff's words) a habit of professional introversion that caused the profession to be uninterested in what mba's really do on the job and how management science can help them ","Causes of the decline of the business school management science course The business school management science course is suffering serious decline. The traditional model- and algorithm-based course fails to meet the needs ‘of MBA programs and students. Poor student mathematical preparation is, a reality, and is not an acceptable justification for poor teaching ‘outcomes. Management science Ph.D.s are often poorly prepared to teach ina general management program, having more experience and interest in algorithms than management. The management science profession as a whole has focused its attention on algorithms and a narrow subset of management problems for which they are most applicable. In contrast, MBA's rarely encounter problems that are suitable for straightforward application of management science tools, living instead in a world where problems are ill-defined, data is scarce, time is short, politics is dominant, and rational ""decision makers"" are non-existent. The root cause of the profession's failure to address these issues seems to be (in Russell Ackoft's words) a habit of professional introversion that caused the profession to be uninterested in what MBA's really do on the job and how management science can help them","['business school management science course', 'MBA programs', 'MBA students', 'management science', 'profession', 'educational courses', 'management science', 'professional aspects']","['management', 'management science', 'management science profession', 'general management program', 'management science tools', 'algorithm-based course', 'management problems', 'serious decline', 'science', 'course']",1057,189,8,1060,187,10,421,127,1
"center-crossing recurrent neural networks for the evolution of rhythmic behavior a center-crossing recurrent neural network is one in which the null(hyper)surfaces of each neuron intersect at their exact centers of symmetry, ensuring that each neuron's activation function is centered over the range of net inputs that it receives. we demonstrate that relative to a random initial population, seeding the initial population of an evolutionary search with center-crossing networks significantly improves both the frequency and the speed with which high-fitness oscillatory circuits evolve on a simple walking task. the improvement is especially striking at low mutation variances. our results suggest that seeding with center-crossing networks may often be beneficial, since a wider range of dynamics is more likely to be easily accessible from a population of center-crossing networks than from a population of random networks ","Center-crossing recurrent neural networks for the evolution of rhythmic behavior Accenter-crossing recurrent neural network is one in which the null(hyper)surfaces of each neuron intersect at their exact centers of symmetry, ensuring that each neuron's activation function is centered ‘over the range of net inputs that it receives. We demonstrate that relative to a random initial population, seeding the initial population of an evolutionary search with center-crossing networks significantly improves both the frequency and the speed with which high-fitness oscillatory circuits evolve on a simple walking task. The improvement is especially striking at low mutation variances. Our results suggest that seeding with center-crossing networks may often be beneficial, since a wider range of dynamics is more likely to be easily accessible from a population of center-crossing networks than from a population of random networks","['center-crossing recurrent neural networks', 'rhythmic behavior evolution', 'null surfaces', 'symmetry', 'activation function', 'evolutionary algorithm', 'random initial population', 'evolutionary search', 'high-fitness oscillatory circuits', 'learning', 'low mutation variance', 'random networks', 'evolutionary computation', 'learning (artificial intelligence)', 'recurrent neural nets', 'search problems']","['center-crossing networks', 'recurrent neural network', 'rhythmic behavior Accenter-crossing', 'neurons activation function', 'random initial population', 'random networks', 'networks', 'initial population', 'neural', 'recurrent']",793,135,16,795,133,10,416,123,4
"vendor qualifications for it staff and networking in some cases, vendor-run accreditation schemes can offer an objective measure of a job applicant's skills, but they do not always indicate the true extent of practical abilities ","Vendor qualifications for IT staff and networking In some cases, vendor-run accreditation schemes can offer an objective measure of a job applicant's skills, but they do not always indicate the true extent of practical abilities","['vendor-run accreditation schemes', 'job applicant', 'IT staff', 'network administrators', 'practical abilities', 'accreditation', 'certification', 'computer network management', 'personnel']","['vendor-run accreditation schemes', 'job applicants skills', 'Vendor qualifications', 'objective measure', 'networking', 'staff', 'cases', 'Vendor', 'vendor-run', 'qualifications']",194,36,9,194,35,10,0,0,1
optimal time of switching between portfolios of securities optimal time of switching between several portfolios of securities are found for the purpose of profit maximization. two methods of their determination are considered. the cases with three and n portfolios are studied in detail ,Optimal time of switching between portfolios of securities Optimal time of switching between several portfolios of securities are found for the purpose of profit maximization. Two methods of their determination are considered. The cases with three and n portfolios are studied in detail,"['optimal time', 'portfolios of securities', 'profit maximization', 'commodity trading']","['n portfolios', 'Optimal time', 'securities', 'switching', 'profit maximization', 'several portfolios', 'portfolios', 'several', 'time', 'Optimal']",244,44,4,244,43,10,0,0,0
"wavelet-based level-of-detail representation of 3d objects in this paper, we propose a 3d object lod (level of detail) modeling system that constructs a mesh from range images and generates the mesh of various lod using the wavelet transform. in the initial mesh generation, we use the marching cube algorithm. we modify the original algorithm to apply it to construct the mesh from multiple range images efficiently. to get the base mesh we use the decimation algorithm which simplifies a mesh with preserving the topology. finally, when reconstructing new mesh which is similar to initial mesh we calculate the wavelet coefficients by using the wavelet transform. we solve the critical problem of wavelet-based methods - the surface crease problem - by using the mesh simplification as the base mesh generation method ","Wavelet-based level-of-detail representation of 3D objects In this paper, we propose a 3D object LOD (Level of Detail) modeling system that constructs a mesh from range images and generates the mesh of various LOD using the wavelet transform. In the initial mesh generation, we use the marching cube algorithm. We modify the original algorithm to apply it to construct the mesh from multiple range images efficiently. To get the base mesh we use the decimation algorithm which simplifies a mesh with preserving the topology. Finally, when reconstructing new mesh which is similar to initial mesh we calculate the wavelet coefficients by using the wavelet transform. We solve the critical problem of wavelet-based methods - the surface crease problem - by using the mesh simplification as the base mesh generation method","['3D object level of detail modeling system', 'wavelet-based level-of-detail representation', 'range images', 'wavelet transform', 'marching cube algorithm', 'base mesh', 'decimation algorithm', 'wavelet coefficients', 'critical problem', 'surface crease problem', 'mesh simplification', 'hierarchy transformation', 'mesh generation', 'solid modelling', 'topology', 'wavelet transforms']","['mesh', 'base mesh', 'd object', 'Wavelet-based level-of-detail representation', 'initial mesh generation', 'mesh simplification', 'new mesh', 'objects', 'd', 'initial mesh']",690,131,16,690,130,10,0,0,8
"upsilon: universal programming system with incomplete lazy object notation this paper presents a new model of computation that differs from prior models in that it emphasizes data over flow control, has no named variables and has an object-oriented flavor. we prove that this model is a complete and confluent acceptable programming system and has a usable type theory. a new data synchronization primitive is introduced in order to achieve the above properties. subtle variations of the model are shown to fall short of having all these necessary properties ","UPSILON: universal programming system with incomplete lazy object notation This paper presents a new model of computation that differs from prior models in that it emphasizes data over flow control, has no named variables and has an object-oriented flavor. We prove that this model is a complete and confluent acceptable programming system and has a usable type theory. A new data synchronization primitive is introduced in order to achieve the above properties. Subtle variations of the model are shown to fall short of having all these necessary properties","['UPSILON', 'universal programming system', 'object-oriented flavor', 'programming system', 'usable type theory', 'data synchronization primitive', 'incomplete lazy object notation', 'computational complexity', 'object-oriented programming', 'type theory']","['universal programming system', 'new data synchronization', 'usable type theory', 'prior models', 'new model', 'upsilon', 'model', 'system', 'universal', 'programming']",471,89,10,471,88,10,0,0,2
"high-performance numerical pricing methods the pricing of financial derivatives is an important field in finance and constitutes a major component of financial management applications. the uncertainty of future events often makes analytic approaches infeasible and, hence, time-consuming numerical simulations are required. in the aurora financial management system, pricing is performed on the basis of lattice representations of stochastic multidimensional scenario processes using the monte carlo simulation and backward induction methods, the latter allowing for the exploitation of shared-memory parallelism. we present the parallelization of a backward induction numerical pricing kernel on a cluster of smps using hpf+, an extended version of high-performance fortran. based on language extensions for specifying a hierarchical mapping of data onto an smp cluster, the compiler generates a hybrid-parallel program combining distributed-memory and shared-memory parallelism. we outline the parallelization strategy adopted by the vfc compiler and present an experimental evaluation of the pricing kernel on an nec sx-5 vector supercomputer and a linux smp cluster, comparing a pure mpi version to a hybrid-parallel mpi/openmp version ","High-performance numerical pricing methods The pricing of financial derivatives is an important field in finance and constitutes a major component of financial management applications. The uncertainty of future events often makes analytic approaches infeasible and, hence, time-consuming numerical simulations are required. In the Aurora Financial Management System, pricing is performed on the basis of lattice representations of stochastic multidimensional scenario processes using the Monte Carlo simulation and Backward Induction methods, the latter allowing for the exploitation of shared-memory parallelism. We present the parallelization of a Backward Induction numerical pricing kernel on a cluster of SMPs using HPF +, an extended version of High-Performance Fortran. Based on language extensions for specifying a hierarchical mapping of data onto an SMP cluster, the compiler generates a hybrid-parallel program combining distributed-memory and shared-memory parallelism. We outline the parallelization strategy adopted by the VFC compiler and present an ‘experimental evaluation of the pricing kernel on an NEC SX-5 vector supercomputer and a Linux SMP cluster, comparing a pure MPI version to a hybrid-parallel MP/OpenMP version","['finance', 'financial management', 'Aurora Financial Management System', 'pricing', 'stochastic processes', 'Monte Carlo simulation', 'Backward Induction methods', 'numerical pricing kernel', 'derivative pricing', 'investment strategies', 'financial data processing', 'FORTRAN', 'investment', 'parallel programming']","['time-consuming numerical simulations', 'financial management applications', 'numerical pricing kernel', 'Monte Carlo simulation', 'financial derivatives', 'Induction methods', 'pricing', 'numerical', 'methods', 'pricing kernel']",1071,170,14,1071,170,10,251,74,2
"aggregate bandwidth estimation in stored video distribution systems multimedia applications like video on demand, distance learning, internet video broadcast, etc. will play a fundamental role in future broadband networks. a common aspect of such applications is the transmission of video streams that require a sustained relatively high bandwidth with stringent requirements of quality of service. in this paper various original algorithms for evaluating, in a video distribution system, a statistical estimation of aggregate bandwidth needed by a given number of smoothed video streams are proposed and discussed. the variable bit rate traffic generated by each video stream is characterized by its marginal distribution and by conditional probabilities between rates of temporary closed streams. the developed iterative algorithms evaluate an upper and lower bound of needed bandwidth for guaranteeing a given loss probability. the obtained results are compared with simulations and with other results, based on similar assumptions, already presented in the literature. some considerations on the developed algorithms are made, in order to evaluate the effectiveness of the proposed methods ","Aggregate bandwidth estimation in stored video distribution systems Multimedia applications like video on demand, distance learning, Internet video broadcast, etc. will play a fundamental role in future broadband networks. A common aspect of such applications is the transmission of video streams that require a sustained relatively high bandwidth with stringent requirements of quality of service. In this paper various original algorithms for evaluating, in a video distribution system, a statistical estimation of aggregate bandwidth needed by a given number of smoothed video streams are proposed and discussed. The variable bit rate traffic generated by each video stream is characterized by its marginal distribution and by conditional probabilities between rates of temporary closed streams. The developed iterative algorithms evaluate an upper and lower bound of needed bandwidth for guaranteeing a given loss probability. The obtained results are compared with simulations and with other results, based on similar assumptions, already presented in the literature. Some considerations on the developed algorithms are made, in order to evaluate the effectiveness of the proposed methods","['aggregate bandwidth estimation', 'stored video distribution systems', 'multimedia applications', 'video on demand', 'distance learning', 'Internet video broadcast', 'broadband networks', 'video streams transmission', 'quality of service', 'statistical estimation', 'variable bit rate traffic', 'marginal distribution', 'conditional probabilities', 'temporary closed streams', 'iterative algorithms', 'upper bound', 'lower bound', 'loss probability', 'simulations', 'VoD', 'video coding', 'QoS', 'broadband networks', 'data compression', 'Internet', 'iterative methods', 'multimedia communication', 'probability', 'quality of service', 'statistical analysis', 'telecommunication traffic', 'variable rate codes', 'video coding', 'video on demand', 'visual communication']","['video distribution system', 'video streams', 'video', 'Aggregate bandwidth estimation', 'temporary closed streams', 'Internet video broadcast', 'statistical estimation', 'bandwidth', 'high bandwidth', 'aggregate bandwidth']",1024,171,35,1024,170,10,0,0,13
"operational phase-space probability distribution in quantum communication theory operational phase-space probability distributions are useful tools for describing quantum mechanical systems, including quantum communication and quantum information processing systems. it is shown that quantum communication channels with gaussian noise and quantum teleportation of continuous variables are described by operational phase-space probability distributions. the relation of operational phase-space probability distribution to the extended phase-space formalism proposed by chountasis and vourdas (1998) is discussed ","Operational phase-space probability distribution in quantum communication theory Operational phase-space probability distributions are useful tools for describing quantum mechanical systems, including quantum communication and quantum information processing systems. It is shown that quantum ‘communication channels with Gaussian noise and quantum teleportation of continuous variables are described by operational phase-space probability distributions. The relation of operational phase-space probability distribution to the extended phase-space formalism proposed by Chountasis and Vourdas (1998) is discussed","['operational phase-space probability distribution', 'quantum communication theory', 'quantum mechanical systems', 'quantum information processing systems', 'Gaussian noise', 'quantum teleportation', 'continuous variables', 'extended phase-space formalism', 'Gaussian noise', 'information theory', 'probability', 'quantum communication', 'quantum theory']","['probability', 'Operational', 'quantum', 'distributions', 'quantum communication channels', 'quantum communication theory', 'quantum teleportation', 'phase-space formalism', 'quantum communication', 'phase-space']",540,72,13,541,71,10,12,1,3
"least load dispatching algorithm for parallel web server nodes a least load dispatching algorithm for distributing requests to parallel web server nodes is described. in this algorithm, the load offered to a node by a request is estimated based on the expected transfer time of the corresponding reply through the internet. this loading information is then used by the algorithm to identify the least load node of the web site. by using this algorithm, each request will always be sent for service at the earliest possible time. performance comparison using nasa and clarknet access logs between the proposed algorithm and commonly used dispatching algorithms is performed. the results show that the proposed algorithm gives 10% higher throughput than that of the commonly used random and round-robin dispatching algorithms ","Least load dispatching algorithm for parallel Web server nodes Aleast load dispatching algorithm for distributing requests to parallel Web server nodes is described. In this algorithm, the load offered to a node by a request is estimated based on the expected transfer time of the corresponding reply through the Internet. This loading information is then used by the algorithm to identify the least load node of the Web site. By using this algorithm, each request will always be sent for service at the earliest possible time. Performance comparison using NASA and ClarkNet access logs between the proposed algorithm and ‘commonly used dispatching algorithms is performed. The results show that the proposed algorithm gives 10% higher throughput than that of the commonly used random and round-robin dispatching algorithms","['least load dispatching algorithm', 'parallel Web server nodes', 'Internet', 'transfer time', 'NASA access logs', 'ClarkNet access logs', 'throughput', 'round-robin dispatching algorithms', 'random dispatching algorithms', 'World Wide Web server', 'client-server systems', 'file servers', 'Internet']","['request', 'algorithms', 'parallel', 'server', 'least load node', 'Web site', 'load', 'Web', 'node', 'least load']",696,129,13,697,127,10,392,118,5
"instability phenomena in the gas-metal arc welding self-regulation process arc instability is a very important determinant of weld quality. the instability behaviour of the gas-metal arc welding (gmaw) process is characterized by strong oscillations in arc length and current. in the paper, a model of the gmaw process is developed using an exact arc voltage characteristic. this model is used to study stability of the self-regulation process and to develop a simulation program that helps to understand the transient or dynamic nature of the gmaw process and relationships among current, electrode extension and contact tube-work distance. the process is shown to exhibit instabilities at both long electrode extension and normal extension. results obtained from simulation runs of the model were also experimentally confirmed by the present author, as reported in this study. in order to explain the concept of the instability phenomena, the metal transfer mode and the arc voltage-current characteristic were examined. based on this examination, the conclusion of this study is that their combined effects lead to the oscillations in arc current and length ","Instability phenomena in the gas-metal arc welding self-regulation process Arc instability is a very important determinant of weld quality. The instability behaviour of the gas-metal arc welding (GMAW) process is, characterized by strong oscillations in arc length and current. In the Paper, a model of the GMAW process is developed using an exact arc voltage characteristic. This model is used to study stability of the self-regulation process and to develop a simulation program that helps to understand the transient or dynamic nature of the GMAW process and relationships among current, electrode extension and contact tube-work distance. The process is shown to exhibit instabilities at both long electrode extension and normal extension. Results obtained from simulation runs of the model were also experimentally confirmed by the present author, as reported in this study. In order to explain the concept of the instability phenomena, the metal transfer mode and the arc voltage-current characteristic were examined. Based on this, examination, the conclusion of this study is that their combined effects lead to the oscillations in arc current and length","['instability phenomena', 'gas-metal arc welding', 'self-regulation process', 'arc instability', 'weld quality', 'GMAW process', 'exact arc voltage characteristic', 'metal transfer mode', 'adaptive control', 'arc welding', 'process control', 'self-adjusting systems', 'stability']","['self-regulation process', 'gas-metal arc welding', 'maw process', 'instability behaviour', 'weld quality', 'arc length', 'arc', 'process', 'instability phenomena', 'Instability phenomena']",985,177,13,987,176,10,0,2,2
"robust wavelet neuro control for linear brushless motors design, simulation and experimental implementation of a wavelet basis function network learning controller for linear brushless dc motors (lbdcm) are considered. stability robustness with position tracking is the primary concern. the proposed controller deals mainly with external disturbances, e.g. nonlinear friction force and payload variation in motion control of linear motors. it consists of two parts, one is a state feedback component, and the other one is a learning feedback component. the state feedback controller is designed on the basis of a simple linear model, and the learning feedback component is a wavelet neural controller. the attenuation effect of wavelet neural networks on friction force is first verified by the numerical method. the learning effect of wavelet neural networks on friction force is also shown in the numerical results. then, a wavelet neural network is applied on a real lbdcm to on-line suppress the friction force, which may be variable due to the different lubrication. the effectiveness of the proposed control schemes is demonstrated by simulated and experimental results ","Robust wavelet neuro control for linear brushless motors, Design, simulation and experimental implementation of a wavelet basis function network learning controller for linear brushless dc motors (LBDCM) are considered. Stability robustness with position tracking is the primary concern. The proposed controller deals mainly with external disturbances, e.g. nonlinear friction force and payload variation in motion control of linear motors. It consists of two parts, one is a state feedback component, and the other one is a learning feedback component. The state feedback controller is designed on the basis of a simple linear model, and the learning feedback component is a wavelet neural controller. The attenuation effect of wavelet neural networks on friction force is first verified by the numerical method. The learning effect of wavelet neural networks on friction force is also shown in the numerical results. Then, a wavelet neural network is applied on a real LBDCM to on-line suppress the friction force, which may be variable due to the different lubrication. The effectiveness of the proposed control schemes is demonstrated by simulated and experimental results","['robust wavelet neuro control', 'linear brushless motors', 'wavelet basis function network', 'LBDCM', 'stability robustness', 'position tracking', 'external disturbances', 'nonlinear friction force', 'payload variation', 'motion control', 'state feedback component', 'learning feedback component', 'attenuation effect', 'friction force', 'lubrication', 'brushless DC motors', 'linear motors', 'neurocontrollers', 'position control', 'robust control', 'state feedback', 'wavelet transforms']","['wavelets', 'neural networks', 'controller', 'state feedback controller', 'state feedback components', 'proposed controller deals', 'neural controller', 'control schemes', 'motion control', 'linear motors']",999,178,22,1000,177,10,0,1,10
"information security policy - what do international information security standards say? one of the most important information security controls, is the information security policy. this vital direction-giving document is, however, not always easy to develop and the authors thereof battle with questions such as what constitutes a policy. this results in the policy authors turning to existing sources for guidance. one of these sources is the various international information security standards. these standards are a good starting point for determining what the information security policy should consist of, but should not be relied upon exclusively for guidance. firstly, they are not comprehensive in their coverage and furthermore, tending to rather address the processes needed for successfully implementing the information security policy. it is far more important the information security policy must fit in with the organisation's culture and must therefore be developed with this in mind ","Information security policy - what do international information security standards say? One of the most important information security controls, is the information security policy. This vital direction-giving document is, however, not always easy to develop and the authors thereof battle with questions such as what constitutes a policy. This results in the policy authors turning to existing sources for guidance. One of these sources is the various international information security standards. These standards are a good starting point for determining what the information security Policy should consist of, but should not be relied upon exclusively for guidance. Firstly, they are not comprehensive in their coverage and furthermore, tending to rather address the processes needed for successfully implementing the information security policy. It is far more important the information security policy must fit in with the organisation's culture and must therefore be developed with this in mind","['information security policy', 'international information security standards', 'DP management', 'information systems', 'information technology', 'security of data', 'standards']","['standards', 'international', 'vital direction-giving document', 'policy authors', 'policy', 'information', 'security', 'information security policy', 'information security Policy', 'Information security policy']",854,147,7,854,146,10,0,0,0
"on-line homework/quiz/exam applet: freely available java software for evaluating performance on line the homework/quiz/exam applet is a freely available java program that can be used to evaluate student performance on line for any content authored by a teacher. it has database connectivity so that student scores are automatically recorded. it allows several different types of questions. each question can be linked to images and detailed story problems. three levels of feedback are provided to student responses. it allows teachers to randomize the sequence of questions and to randomize which of several options is the correct answer in multiple-choice questions. the creation and editing of questions involves menu selections, button presses, and the typing of content; no programming knowledge is required. the code is open source in order to encourage modifications that will meet individual pedagogical needs ","On-line Homework/Quiz/Exam applet: freely available Java software for evaluating performance on line The Homework/Quiz/Exam applet is a freely available Java program that can be used to evaluate student performance on line for any content authored by a teacher. It has database connectivity so that student scores are automatically recorded. It allows several different types of questions. Each question can be linked to images and detailed story problems. Three levels of feedback are provided to student responses. It allows teachers to randomize the sequence of questions and to randomize which of several options is the correct answer in multiple-choice questions. The creation and editing of questions involves menu selections, button presses, and the typing of content; no programming knowledge is required. The code is open source in order to encourage modifications that will meet individual pedagogical needs","['online Homework/Quiz/Exam applet', 'freely available Java software', 'online student performance evaluation', 'teacher authored content', 'database connectivity', 'automatic student score recording', 'images', 'detailed story problems', 'feedback', 'randomized question sequence', 'multiple-choice questions', 'question editing', 'question creation', 'menu selections', 'button presses', 'typing content', 'individual pedagogical needs', 'courseware', 'Java', 'public domain software']","['On-line Homework/Quiz/Exam apple', 'available Java software', 'available Java program', 'student performance', 'student scores', 'available', 'Java', 'apple', 'Homework/Quiz/Exam', 'Homework/Quiz/Exam apple']",782,137,20,782,136,10,0,0,8
"deadlock-free scheduling in flexible manufacturing systems using petri nets this paper addresses the deadlock-free scheduling problem in flexible manufacturing systems. an efficient deadlock-free scheduling algorithm was developed, using timed petri nets, for a class of fmss called systems of sequential systems with shared resources (s/sup 4/ r). the algorithm generates a partial reachability graph to find the optimal or near-optimal deadlock-free schedule in terms of the firing sequence of the transitions of the petri net model. the objective is to minimize the mean flow time (mft). an efficient truncation technique, based on the siphon concept, has been developed and used to generate the minimum necessary portion of the reachability graph to be searched. it has been shown experimentally that the developed siphon truncation technique enhances the ability to develop deadlock-free schedules of systems with a high number of deadlocks, which cannot be achieved using standard petri net scheduling approaches. it may be necessary, in some cases, to relax the optimality condition for large fmss in order to make the search effort reasonable. hence, a user control factor (ucf) was defined and used in the scheduling algorithm. the objective of using the ucf is to achieve an acceptable trade-off between the solution quality and the search effort. its effect on the mft and the cpu time has been investigated. randomly generated examples are used for illustration and comparison. although the effect of ucf did not affect the mean flow time, it was shown that increasing it reduces the search effort (cpu time) significantly ","Deadlock-free scheduling in flexible manufacturing systems using Petri nets This paper addresses the deadlock-free scheduling problem in Flexible Manufacturing Systems. An efficient deadlock-free scheduling algorithm, was developed, using timed Petri nets, for a class of FMSS called Systems of Sequential Systems with Shared Resources (S/sup 4/ R). The algorithm generates a partial reachability graph to find the optimal or near-optimal deadlock-free schedule in terms of the firing sequence of the transitions of the Petri net model. The objective is to minimize the mean flow time (MFT). An efficient truncation technique, based on the siphon concept, has been developed and used to generate the minimum necessary portion of the reachability graph to be searched. It has been shown experimentally that the developed siphon truncation technique enhances the ability to develop deadlock-free schedules of systems with a high number of deadlocks, which cannot be achieved using standard Petri net scheduling approaches. It may be necessary, in some cases, to relax the optimality condition for large FMSs in order to make the search effort reasonable. Hence, a User Control Factor (UCF) was defined and used in the scheduling algorithm. The objective of using the UCF is to achieve an acceptable trade-off between the solution quality and the search effort. Its effect on the MFT and the CPU time has been investigated. Randomly generated examples are used for illustration and comparison. Although the effect of UCF did not affect the mean flow time, it was shown that increasing it reduces the search effort (CPU time) significantly","['flexible manufacturing systems', 'deadlock-free scheduling', 'Petri nets', 'systems of sequential systems with shared resources', 'partial reachability graph', 'optimal deadlock-free schedule', 'near-optimal deadlock-free schedule', 'Petri net model transitions firing sequence', 'mean flow time minimization', 'siphon truncation technique', 'optimality condition relaxation', 'user control factor', 'CPU time', 'randomly generated examples', 'flexible manufacturing systems', 'minimisation', 'Petri nets', 'production control', 'reachability analysis', 'resource allocation']","['scheduling algorithm', 'Petri nets', 'near-optimal deadlock-free schedule', 'deadlock-free scheduling problem', 'deadlock-free schedules', 'scheduling', 'Petri net model', 'Deadlock-free scheduling', 'flexible manufacturing systems', 'Flexible Manufacturing systems']",1383,253,20,1384,252,10,0,1,5
"extracting linguistic dna: nstein goes to work for upi it's a tantalizing problem for categorization. united press international (upi) has more than 700 correspondents creating thousands of stories every week, running the gamut from business news to sports to entertainment to global coverage of america's war on terrorism. and while upi and others news services have mechanisms for adding keywords and categorizing their content, upi recognized a need to add more automation to the process. with the recent growth and improvement in tools for computer-aided indexing (cai), upi undertook a process of looking at its needs and evaluating the many cai tools out there. in the end, they chose technology from montreal-based nstein technologies. ""our main objective was to acquire the best cai tool to help improve our customers' access and interaction with our content,"" says steve sweet, cio at upi. ""we examined a number of solutions, and nstein's nserver suite clearly came out on top. the combination of speed, scalability, accuracy, and flexibility was what really sold us."" ","Extracting linguistic DNA: NStein goes to work for UPI It's a tantalizing problem for categorization. United Press International (UPI) has more than 700 correspondents creating thousands of stories every week, running the gamut from business news to sports to entertainment to global coverage of America's war on terrorism. And while UPI and others news services have mechanisms for adding keywords and categorizing their content, UPI recognized a need to add more automation to the process. With the recent growth and improvement in tools for Computer-Aided Indexing (CAI), UPI undertook a process of looking at its needs and evaluating the many CAI tools out there. In the end, they chose technology from Montreal-based NStein Technologies. ""Our main objective was to acquire the best CAI tool to help improve ‘our customers’ access and interaction with our content,"" says Steve Sweet, CIO at UPI. ""We examined a number of solutions, and NStein's Nserver suite clearly came out on top. The combination of speed, scalability, accuracy, and flexibility was what really sold us.”","['United Press International', 'UPI', 'electronic archive', 'wire service stories', 'Computer-Aided Indexing', 'NStein Technologies', 'indexing', 'information resources', 'text analysis']","['Montreal-based stein technologies', 'others news services', 'linguistic dna stein', 'tantalizing problem', 'many CAI tools', 'best CAI tool', 'content UPI', 'stein', 'UPI', 'linguistic']",909,170,9,910,169,10,5,3,3
exact controllability of shells in minimal time we prove an exact controllability result for thin cups using the fourier method and recent improvements of ingham (1936) type theorems ,Exact controllability of shells in minimal time We prove an exact controllability result for thin cups using the Fourier method and recent improvements of Ingham (1936) type theorems,"['controllability', 'shells', 'minimal time', 'thin cups', 'partial differential equations', 'Young modulus', 'Hilbert space', 'Fourier method', 'Ingham type theorems', 'controllability', 'eigenvalues and eigenfunctions', 'elasticity', 'Fourier analysis', 'Hilbert spaces', 'partial differential equations', ""Young's modulus""]","['exact controllability result', 'Fourier method', 'minimal time', 'thin cups', 'shells', 'Exact', 'controllability', 'time', 'minimal', 'Exact controllability']",155,29,16,155,28,10,0,0,0
"the canadian national site licensing project in january 2000, a consortium of 64 universities in canada signed a historic inter-institutional agreement that launched the canadian national site licensing project (cnslp), a three-year pilot project aimed at bolstering the research and innovation capacity of the country's universities. cnslp tests the feasibility of licensing, on a national scale, electronic versions of scholarly publications; in its initial phases the project is focused on full-text electronic journals and research databases in science, engineering, health and environmental disciplines. this article provides an overview of the cnslp initiative, summarizes organizational and licensing accomplishments to date, and offers preliminary observations on challenges and opportunities for subsequent phases of the project ","The Canadian National Site Licensing Project In January 2000, a consortium of 64 universities in Canada signed a historic inter-institutional agreement that launched the Canadian National Site Licensing Project (CNSLP), a three-year pilot project aimed at bolstering the research and innovation capacity of the country's universities. CNSLP tests the feasibility of licensing, on a national scale, electronic versions of scholarly publications; in its initial phases the project is focused on full-text electronic journals and research databases in science, engineering, health and environmental disciplines. This article provides an overview of the CNSLP initiative, summarizes organizational and licensing accomplishments to date, and offers preliminary observations on challenges and opportunities for subsequent phases of the project","['Canadian National Site Licensing Project', 'inter-institutional agreement', 'research and innovation', 'CNSLP', 'academic libraries', 'information resources', 'electronic scholarly publications', 'full-text electronic journals', 'research databases', 'academic libraries', 'contracts', 'electronic publishing', 'full-text databases', 'information resources', 'library automation']","['Licensing', 'National', 'Canadian', 'historic inter-institutional agreement', 'country universities insp', 'three-year pilot project', 'Site', 'Project', 'agreement', 'universities']",725,114,15,725,113,10,0,0,4
"warranty reserves for nonstationary sales processes estimation of warranty costs, in the event of product failure within the warranty period, is of importance to the manufacturer. costs associated with replacement or repair of the product are usually drawn from a warranty reserve fund created by the manufacturer. considering a stochastic sales process, first and second moments (and thereby the variance) are derived for the manufacturer's total discounted warranty cost of a single sale for single-component items under four different warranty policies from a manufacturer's point of view. these servicing strategies represent a renewable free-replacement, nonrenewable free-replacement, renewable pro-rata, and a nonrenewable minimal-repair warranty plans. the results are extended to determine the mean and variance of total discounted warranty costs for the total sales over the life cycle of the product. furthermore, using a normal approximation, warranty reserves necessary for a certain protection level, so that reserves are not completely depleted, are found. results and their managerial implications are studied through an extensive example ","Warranty reserves for nonstationary sales processes Estimation of warranty costs, in the event of product failure within the warranty period, is of importance to the manufacturer. Costs associated with replacement or repair of the product are usually drawn from a warranty reserve fund created by the manufacturer. Considering a stochastic sales process, first and second moments (and thereby the variance) are derived for the manufacturer's total discounted warranty cost of a single sale for single-component items under four different warranty policies from a manufacturer's point of view. These servicing strategies represent a renewable free-replacement, nonrenewable free-replacement, renewable pro-rata, and a nonrenewable minimal-repair warranty plans. The results are extended to determine the mean and variance of total discounted warranty costs for the total sales over the life cycle of the product. Furthermore, using a normal approximation, warranty reserves necessary for a certain protection level, so that reserves are not completely depleted, are found. Results and their managerial implications are studied through an extensive example","['nonstationary sales processes', 'warranty reserves', 'warranty costs estimation', 'product failure', 'product replacement', 'product repair', 'stochastic sales process', 'first moments', 'second moments', 'variance', 'total discounted warranty cost', 'single-component items', 'servicing strategies', 'renewable free-replacement', 'nonrenewable free-replacement', 'renewable pro-rata', 'nonrenewable minimal-repair warranty plans', 'total discounted warranty costs', 'product life cycle', 'normal approximation', 'managerial implications', 'life cycle costing', 'probability', 'retailing']","['warranty', 'Warranty reserves', 'warranty costs', 'different warranty policies', 'stochastic sales process', 'warranty reserve fund', 'warranty period', 'total sales', 'single sale', 'reserve']",992,164,24,992,163,10,0,0,10
"an improved self-organizing cpn-based fuzzy system with adaptive back-propagation algorithm this paper describes an improved self-organizing cpn-based (counter-propagation network) fuzzy system. two self-organizing algorithms iusocpn and issocpn, being unsupervised and supervised respectively, are introduced. the idea is to construct the neural-fuzzy system with a two-phase hybrid learning algorithm, which utilizes a cpn-based nearest-neighbor clustering scheme for both structure learning and initial parameters setting, and a gradient descent method with adaptive learning rate for fine tuning the parameters. the obtained network can be used in the same way as a cpn to model and control dynamic systems, while it has a faster learning speed than the original back-propagation algorithm. the comparative results on the examples suggest that the method is fairly efficient in terms of simple structure, fast learning speed, and relatively high modeling accuracy ","An improved self-organizing CPN-based fuzzy system with adaptive back-propagation algorithm This paper describes an improved self-organizing CPN-based (Counter-Propagation Network) fuzzy system. Two self-organizing algorithms IUSOCPN and ISSOCPN, being unsupervised and supervised respectively, are introduced. The idea is to construct the neural-fuzzy system with a two-phase hybrid learning algorithm, which utilizes a CPN-based nearest-neighbor clustering scheme for both structure learning and initial parameters setting, and a gradient descent method with adaptive learning rate for fine tuning the parameters. The obtained network can be used in the same way as a CPN to model and control dynamic systems, while it has a faster learning speed than the original back-propagation algorithm. The comparative results on the examples suggest that the method is fairly efficient in terms of simple structure, fast learning speed, and relatively high modeling accuracy","['self-organizing fuzzy system', 'Counter-Propagation Network', 'neural-fuzzy system', 'hybrid learning', 'gradient descent', 'structure learning', 'initial parameters setting', 'back-propagation learning scheme', 'backpropagation', 'fuzzy neural nets', 'learning (artificial intelligence)', 'self-adjusting systems']","['algorithm', 'original back-propagation algorithm', 'adaptive back-propagation algorithm', 'self-organizing algorithms IUSOCPN', 'adaptive learning rate', 'neural-fuzzy system', 'dynamic systems', 'system', 'fuzzy system', 'self-organizing']",834,135,12,834,134,10,0,0,3
"effect of insulation layer on transcribability and birefringence distribution in optical disk substrate as the need for information storage media with high storage density increases, digital video disks (dvds) with smaller recording marks and thinner optical disk substrates than those of conventional dvds are being required. therefore, improving the replication quality of land-groove or pit structure and reducing the birefringence distribution are emerging as important criteria in the fabrication of high-density optical disk substrates. we control the transcribability and distribution of birefringence by inserting an insulation layer under the stamper during injection-compression molding of dvd ram substrates. the effects of the insulation layer on the geometrical and optical properties, such as transcribability and birefringence distribution, are examined experimentally. the inserted insulation layer is found to be very effective in improving the quality of replication and leveling out the first peak of the gapwise birefringence distribution near the mold wall and reducing the average birefringence value, because the insulation layer retarded the growth of the solidified layer ","Effect of insulation layer on transcribability and birefringence distribution in optical disk substrate As the need for information storage media with high storage density increases, digital video disks (DVDs) with smaller recording marks and thinner optical disk substrates than those of conventional DVDs are being required. Therefore, improving the replication quality of land-groove or pit structure and reducing the birefringence distribution are emerging as important criteria in the fabrication of high-density optical disk substrates. We control the transcribability and distribution of birefringence by inserting an insulation layer under the stamper during injection-compression molding of DVD RAM substrates. The effects of the insulation layer on the geometrical and optical properties, such as transcribability and birefringence distribution, are examined experimentally. The inserted insulation layer is found to be very effective in improving the quality of replication and leveling out the first peak of the gapwise birefringence distribution near the mold wall and reducing the average birefringence value, because the insulation layer retarded the growth of the solidified layer","['optical disk substrate', 'transcribability', 'birefringence distribution', 'insulation layer', 'information storage media', 'high storage density', 'digital video disks', 'smaller recording marks', 'thinner optical disk substrates', 'replication quality', 'land-groove', 'pit structure', 'fabrication', 'stamper', 'injection-compression molding', 'DVD RAM substrates', 'geometrical properties', 'optical properties', 'gapwise birefringence distribution', 'mold wall', 'solidified layer growth retardation', 'polyimide thermal insulation layer', 'birefringence', 'moulding', 'optical disc storage', 'optical fabrication', 'replica techniques', 'substrates', 'thermal insulation', 'video discs']","['insulation layer', 'optical disk substrate', 'birefringence', 'baptise birefringence distribution', 'average birefringence value', 'digital video disks', 'optical properties', 'birefringence distribution', 'layer', 'insulation']",1031,167,30,1031,166,10,0,0,13
"optimization of requantization parameter for mpeg transcoding this paper considers transcoding in which an mpeg stream is converted to a low-bit-rate mpeg stream, and proposes a method in which the transcoding error can be reduced by optimally selecting the quantization parameter for each macroblock. in transcoding with a low compression ratio, it is crucial to prohibit transcoding with a requantization parameter which is 1 to 2 times the quantization parameter of the input stream. consequently, as the first step, an optimization method for the requantization parameter is proposed which cares for the error propagation effect by interframe prediction. then, the proposed optimization method is extended so that the method can also be applied to the case of a high compression ratio in which the rate-distortion curve is approximated for each macroblock in the range of requantization parameters larger than 2 times the quantization parameter. it is verified by a simulation experiment that the psnr is improved by 0.5 to 0.8 db compared to the case in which a 6 mbit/s mpeg stream is not optimized by twofold recompression ","Optimization of requantization parameter for MPEG transcoding This paper considers transcoding in which an MPEG stream is converted to a low-bit-rate MPEG stream, and proposes a method in which the transcoding error can be reduced by optimally selecting the quantization parameter for each macroblock. In transcoding with a low compression ratio, it is crucial to prohibit transcoding with a requantization parameter which is 1 to 2 times the quantization parameter of the input stream. Consequently, as the first step, an ‘optimization method for the requantization parameter is proposed which cares for the error propagation effect by interframe prediction. Then, the proposed optimization method is extended so that the method can also be applied to the case of a high compression ratio in which the rate-distortion curve is approximated for each macroblock in the range of requantization parameters larger than 2 times the quantization parameter. It is verified by a simulation experiment that the PSNR is improved by 0.5 to 0.8 dB compared to the case in which a 6 Mbit/s MPEG stream is not optimized by twofold recompression","['requantization parameter optimization', 'low-bit-rate MPEG stream', 'transcoding error', 'macroblock', 'compression ratio', 'error propagation effect', 'interframe prediction', 'rate-distortion curve', 'PSNR', 'simulation', 'twofold recompression', 'rate conversion', 'rate control', '6 Mbit/s', 'data compression', 'optimisation', 'quantisation (signal)', 'rate distortion theory', 'variable rate codes', 'video coding']","['quantization parameter', 'optimization method', 'parameters', 'low-bit-rate MPEG stream', 'low compression ratio', 'bits MPEG stream', 'input stream', 'MPEG', 'MPEG stream', 'quantization']",952,179,20,953,178,10,12,1,9
"speedera: web without the wait there's no greater testament to the utility of the internet than the fact that hundreds of millions of people worldwide are willing to wait for web pages as they build incrementally on screen. but while users may put up with the ""world wide wait,"" they definitely don't like it. that's where content delivery networks come in. cdns can't turn a footpath into a freeway, but they can help data in transit take advantage of shortcuts and steer clear of traffic jams. and while enhancing the responsiveness of web interaction, cdns also enhance the prospects of their clients, who need engaged visitors to keep their web-based business models afloat. ""our mission is to improve the quality of the internet experience for end-users,"" says gordon smith, vice president of marketing at speedera networks in santa clara, california, ""and to enable web-site operators to provide better delivery quality, performance, scalability, and security through an outsourced service model that slashes it costs."" ","Speedera: Web without the wait There's no greater testament to the utility of the Internet than the fact that hundreds of millions of people worldwide are willing to wait for Web pages as they build incrementally on screen. But while users may put up with the ""World Wide Wait,"" they definitely don't like it. That's where Content Delivery Networks come in. CDNs can't turn a footpath into a freeway, but they can help data in transit take advantage of shortcuts and steer clear of traffic jams. And while enhancing the responsiveness of Web interaction, CDNs also enhance the prospects of their clients, who need engaged visitors to keep their Web-based business models afloat. ""Our mission is to improve the quality of the Internet experience for end-users,"" says Gordon Smith, vice president of marketing at Speedera Networks in Santa Clara, California, ""and to enable Web-site operators to provide better delivery quality, performance, scalability, and security through an outsourced service model that slashes IT costs.”","['Content Delivery Networks', 'Web-site operators', 'delivery quality', 'scalability', 'security', 'outsourced service model', 'World Wide Web', 'Web interaction', 'Web-based business models', 'Internet experience', 'file servers', 'Internet', 'multimedia communication', 'outsourcing']","['Content Delivery Networks', 'Santa clara california', 'Web interaction cans', 'Internet experience', 'greater testament', 'speeder Networks', 'speeder Web', 'Web pages', 'Web', 'speeder']",863,164,14,863,163,10,1,1,5
"data management in location-dependent information services location-dependent information services have great promise for mobile and pervasive computing environments. they can provide local and nonlocal news, weather, and traffic reports as well as directory services. before they can be implemented on a large scale, however, several research issues must be addressed ","Data management in location-dependent information services Location-dependent information services have great promise for mobile and pervasive computing environments. They can provide local and nonlocal news, weather, and traffic reports as well as directory services. Before they can be implemented on a large scale, however, several research issues must be addressed","['location-dependent information services', 'wireless networks', 'pervasive computing', 'mobile computing', 'news', 'weather', 'traffic reports', 'data management', 'directory services', 'database management systems', 'information services', 'mobile computing']","['pervasive computing environments', 'non-local news weather', 'directory services', 'Data management', 'great promise', 'services', 'Data', 'information', 'location-dependent information services', 'Location-dependent information services']",319,51,12,319,50,10,0,0,1
"automated cerebrum segmentation from three-dimensional sagittal brain mr images we present a fully automated cerebrum segmentation algorithm for full three-dimensional sagittal brain mr images. first, cerebrum segmentation from a midsagittal brain mr image is performed utilizing landmarks, anatomical information, and a connectivity-based threshold segmentation algorithm as previously reported. recognizing that the cerebrum in laterally adjacent slices tends to have similar size and shape, we use the cerebrum segmentation result from the midsagittal brain mr image as a mask to guide cerebrum segmentation in adjacent lateral slices in an iterative fashion. this masking operation yields a masked image (preliminary cerebrum segmentation) for the next lateral slice, which may truncate brain region(s). truncated regions are restored by first finding end points of their boundaries, by comparing the mask image and masked image boundaries, and then applying a connectivity-based algorithm. the resulting final extracted cerebrum image for this slice is then used as a mask for the next lateral slice. the algorithm yielded satisfactory fully automated cerebrum segmentations in three-dimensional sagittal brain mr images, and had performance superior to conventional edge detection algorithms for segmentation of cerebrum from 3d sagittal brain mr images ","Automated cerebrum segmentation from three-dimensional sagittal brain MR images We present a fully automated cerebrum segmentation algorithm for full three-dimensional sagittal brain MR images. First, cerebrum segmentation from a midsagittal brain MR image is performed utilizing landmarks, anatomical information, and a connectivity-based threshold segmentation algorithm as previously reported. Recognizing that the cerebrum in laterally adjacent slices tends to have similar size and shape, we use the cerebrum segmentation result from the midsagittal brain MR image as a mask to guide cerebrum segmentation in adjacent lateral slices in an iterative fashion. This masking operation yields a masked image (preliminary cerebrum segmentation) for the next lateral slice, which may truncate brain region(s). Truncated regions are restored by first finding end points of their boundaries, by comparing the mask image and masked image boundaries, and then applying a connectivity-based algorithm. The resulting final extracted cerebrum image for this slice is then used as a mask for the next lateral slice. The algorithm yielded satisfactory fully automated cerebrum segmentations in three-dimensional sagittal brain MR images, and had performance superior to conventional edge detection algorithms for segmentation of cerebrum from 3D sagittal brain MR images","['fully automated cerebrum segmentation algorithm', 'full 3D sagittal brain MR images', 'midsagittal brain MR image', 'landmarks', 'anatomical information', 'connectivity-based threshold segmentation algorithm', 'laterally adjacent slices', 'masking operation', 'brain region truncation', 'boundary end points', 'masked image boundaries', 'connectivity-based algorithm', 'biomedical MRI', 'brain', 'edge detection', 'image segmentation', 'medical image processing', 'stereo image processing']","['cerebrum', 'cerebrum segmentation', 'segmentation', 'images', 'preliminary cerebrum segmentation', 'cerebrum segmentation algorithm', 'first cerebrum segmentation', 'image boundaries', 'cerebrum image', 'mask image']",1170,191,18,1170,190,10,0,0,6
"identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information the problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set theory, to correctly identify the current state of a system ","Identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information The problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. Some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set theory, to correctly identify the current state of a system","['complex systems states identification', 'admissible measurement errors', 'fuzzy information', 'informative attributes', 'measurement error', 'fuzzy set theory', 'fuzzy logic', 'fuzzy set theory']","['complex systems', 'states', 'admissible measurement errors', 'fuzzy information', 'admissible degree', 'fuzzy set theory', 'current state', 'fuzzy values', 'measurement error', 'systems']",384,71,8,384,70,10,0,0,1
"identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information the problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set theory, to correctly identify the current state of a system ","Identification of states of complex systems with estimation of admissible measurement errors on the basis of fuzzy information The problem of identification of states of complex systems on the basis of fuzzy values of informative attributes is considered. Some estimates of a maximally admissible degree of measurement error are obtained that make it possible, using the apparatus of fuzzy set theory, to correctly identify the current state of a system","['complex systems states identification', 'admissible measurement errors', 'fuzzy information', 'informative attributes', 'measurement error', 'fuzzy set theory', 'fuzzy logic', 'fuzzy set theory']","['complex systems', 'states', 'admissible measurement errors', 'fuzzy information', 'admissible degree', 'fuzzy set theory', 'current state', 'fuzzy values', 'measurement error', 'systems']",384,71,8,384,70,10,0,0,1
"down up [it projects] despite the second quarter's gloomy gdp report, savvy cios are forging ahead with big it projects that will position their companies to succeed when the economy soars again ","Down up [IT projects] Despite the second quarter's gloomy GDP report, savvy CIOs are forging ahead with big IT projects that will position their companies to succeed when the economy soars again","['strategic technology projects', 'Walgreen', 'Ford', 'Caterpillar', ""Victoria's Secret"", 'Morgan Stanley', 'Staples', 'DP management']","['gloomy GDP report', 'second quarters', 'savvy cos', 'cos', 'GDP', 'savvy', 'second', 'report', 'gloomy', 'quarters']",163,33,8,163,32,10,0,0,0
"content standards for electronic books: the oebf publication structure and the role of public interest participation in the emerging world of electronic publishing how we create, distribute, and read books will be in a large part determined by an underlying framework of content standards that establishes the range of technological opportunities and constraints for publishing and reading systems. but efforts to develop content standards based on sound engineering models must skillfully negotiate competing and sometimes apparently irreconcilable objectives if they are to produce results relevant to the rapidly changing course of technology. the open ebook forum's publication structure, an xml-based specification for electronic books, is an example of the sort of timely and innovative problem solving required for successful real-world standards development. as a result of this effort, the electronic book industry will not only happen sooner and on a larger scale than it would have otherwise, but the electronic books it produces will be more functional, more interoperable, and more accessible to all readers. public interest participants have a critical role in this process ","Content standards for electronic books: the OEBF publication structure and the role of public interest participation In the emerging world of electronic publishing how we create, distribute, and read books will be in a large part determined by an underlying framework of content standards that establishes the range of technological opportunities and constraints for publishing and reading systems. But efforts to develop content standards based on sound engineering models must skillfully negotiate competing and sometimes, apparently irreconcilable objectives if they are to produce results relevant to the rapidly changing course of technology. The Open eBook Forum's Publication Structure, an XML-based specrfication for electronic books, is an example of the sort of timely and innovative problem solving required for successful real-world standards development. As a result of this effort, the electronic book industry will not only happen sooner and on a larger scale than it would have otherwise, but the electronic books it produces will be more functional, more interoperable, and more accessible to all readers. Public interest participants have a critical role in this process","['electronic publishing', 'electronic books', 'content standards', 'OEBF Publication Structure', 'public interest participation', 'Open eBook Forum Publication Structure', 'XML-based specification', 'electronic publishing', 'hypermedia markup languages', 'standards']","['electronic books', 'electronic', 'public interest participation', 'Public interest participants', 'ebf publication structure', 'electronic book industry', 'electronic publishing', 'books', 'content standards', 'Content standards']",1013,176,10,1014,175,10,1,2,1
"knowledge model reuse: therapy decision through specialisation of a generic decision model we present the definition of the therapy decision task and its associated heuristic multi-attribute (hm) solving method, in the form of a kads-style specification. the goal of the therapy decision task is to identify the ideal therapy, for a given patient, in accordance with a set of objectives of a diverse nature constituting a global therapy-evaluation framework in which considerations such as patient preferences and quality-of-life results are integrated. we give a high-level overview of this task as a specialisation of the generic decision task, and additional decomposition methods for the subtasks involved. these subtasks possess some reflective capabilities for reasoning about self-models, particularly the learning subtask, which incrementally corrects and refines the model used to assess the effects of the therapies. this work illustrates the process of reuse in the framework of ai software development methodologies such as kads-commonkads in order to obtain new (more specialised but still generic) components for the analysis libraries developed in this context. in order to maximise reuse benefits, where possible, the therapy decision task and hm method have been defined in terms of regular components from the earlier-mentioned libraries. to emphasise the importance of using a rigorous approach to the modelling of domain and method ontologies, we make extensive use of the semi-formal object-oriented analysis notation uml, together with its associated constraint language ocl, to illustrate the ontology of the decision method and the corresponding specific one of the therapy decision domain, the latter being a refinement via inheritance of the former ","Knowledge model reuse: therapy decision through specialisation of a generic decision model We present the definition of the therapy decision task and its associated Heuristic Multi-Attribute (HM) solving method, in the form of a KADS-style specification. The goal of the therapy decision task is to identify the ideal therapy, for a given patient, in accordance with a set of objectives of a diverse nature constituting a global therapy-evaluation framework in which considerations such as patient preferences and quality-of-life results are integrated. We give a high-level overview of this task as a specialisation of the generic decision task, and additional decomposition methods for the subtasks involved. These subtasks possess some reflective capabilities for reasoning about self-models, particularly the learning subtask, which incrementally corrects and refines the model used to assess the effects of the therapies. This work illustrates the process of reuse in the framework of Al software development methodologies such as KADS-CommonkADS in order to obtain new (more specialised but still generic) components for the analysis libraries developed in this context. In order to maximise reuse benefits, where possible, the therapy decision task and HM method have been defined in terms of regular components from the earlier-mentioned libraries. To emphasise the importance of using a rigorous approach to the modelling of domain and method ontologies, we make extensive use of the semi-formal object-oriented analysis notation UML, together with its associated constraint language OCL, to illustrate the ontology of the decision method and the corresponding specific one of the therapy decision domain, the latter being a refinement via inheritance of the former","['knowledge model reuse', 'therapy decision task', 'KADS-style specification', 'global therapy-evaluation framework', 'patient preferences', 'reasoning', 'learning subtask', 'software development methodologies', 'CommonKADS', 'ontologies', 'object-oriented analysis notation', 'UML', 'constraint language', 'OCL', 'generic decision model specialisation', 'Heuristic Multi-Attribute solving method', 'decision support systems', 'heuristic programming', 'knowledge acquisition', 'medical expert systems', 'object-oriented methods', 'patient treatment', 'specification languages']","['decision', 'therapy decision task', 'therapy', 'additional decomposition methods', 'therapy decision domain', 'generic decision model', 'generic decision task', 'decision method', 'ideal therapy', 'model']",1514,262,23,1514,261,10,1,1,12
"construction of two-sided bounds for initial-boundary value problems this paper extends the bounding operator approach developed for boundary value problems to the case of initial-boundary value problems (ibvps). following the general principle of bounding operators enclosing methods for the case of partial differential equations are discussed. in particular, continuous discretization methods with an appropriate error bound controlled shift and monotone extensions of rothe's method for parabolic problems are investigated ","Construction of two-sided bounds for initial-boundary value problems This paper extends the bounding operator approach developed for boundary value problems to the case of initial-boundary value problems (IBVPS). Following the general principle of bounding operators enclosing methods for the case of partial differential equations are discussed. In particular, continuous discretization methods with an appropriate error bound controlled shift and monotone extensions of Rothe's method for parabolic problems are investigated","['two-sided bounds', 'initial-boundary value problems', 'bounding operator approach', 'bounding operators', 'partial differential equations', 'parabolic problems', 'initial value problems', 'partial differential equations']","['initial-boundary value problems', 'parabolic problems', 'operator approach', 'two-sided bounds', 'others method', 'Construction', 'problems', 'value', 'initial-boundary', 'boundary value problems']",458,70,8,458,69,10,0,0,3
"experimental investigations on monitoring and control of induction heating process for semi-solid alloys using the heating coil as sensor a method of monitoring the state of metal alloys during induction heating and control of the heating process utilizing the heating coil itself as a sensor is proposed, and its usefulness and effectiveness were experimentally investigated using aluminium a357 billets for the semi-solid metal (ssm) casting processes. the impedance of the coil containing the billet was continuously measured by the proposed method in the temperature range between room temperature and 700 degrees c. it was found that the reactance component of the impedance varied distinctively according to the billet state and could clearly monitor the deformation of the billet, while the resistance component increased with temperature, reflecting the variation of the resistivity of the billet which has strong correlation to the solid/liquid fraction of the billets. the measured impedance is very sensitive to the billet states such as temperature, deformation and solid/liquid fraction and could be used as a parameter to monitor and control the heating process for ssms ","Experimental investigations on monitoring and control of induction heating process for semi-solid alloys using the heating coil as sensor A method of monitoring the state of metal alloys during induction heating and control of the heating process utilizing the heating coil itself as a sensor is proposed, and its usefulness and effectiveness were experimentally investigated using aluminium A357 billets for the semi-solid metal (SSM) casting processes. The impedance of the coil containing the billet was continuously measured by the proposed method in the temperature range between room temperature and 700 degrees C. It was found that the reactance component of the impedance varied distinctively according to the billet state and could clearly monitor the deformation of the billet, while the resistance component increased with temperature, reflecting the variation of the resistivity of the billet which has strong correlation to the solid/liquid fraction of the billets. The measured impedance is very sensitive to the billet states such as temperature, deformation and solid/liquid fraction and could be used as a parameter to monitor and control the heating process for SSMs","['induction heating process', 'process monitoring', 'process control', 'semisolid alloys', 'semisolid metal casting', 'heating coil sensor', 'coil impedance', 'reactance component', 'billet state', 'billet deformation', 'resistance component', 'resistivity variation', 'solid/liquid fraction', 'solenoid coil', '20 to 700 C', 'casting', 'electric reactance measurement', 'electric resistance measurement', 'induction heating', 'metallurgical industries', 'process control', 'process monitoring', 'solenoids', 'temperature control']","['control', 'heating coil', 'billet state', 'heating', 'Experimental investigations', 'induction heating process', 'aluminium a35 billets', 'semisolid metal ssm', 'heating process', 'induction heating']",1007,179,24,1007,178,10,0,0,4
"the case for activity based management in today's stormy economic climate businesses need activity based management (abm) more than ever before. in an economic downturn it is a vital tool for pinpointing a business' most profitable customers, products, regions or channels, as well as uncovering the costs of individual business processes that may need to be improved in order to drive higher profit levels. changes may be afoot in the abm market, but armstrong laing group ceo mike sherratt argues that businesses need specialists with an abm focus to keep up with their requirements in such a climate. he looks at what benefits a `best-of-breed' abm system can offer businesses and contends that businesses must choose carefully when going down the abm route - and also ask themselves the question whether 'generalist' organisations will be able to deliver the best possible abm solution ","The case for activity based management In today's stormy economic climate businesses need Activity Based Management (ABM) more than ever before. In an economic downturn it is a vital tool for pinpointing a business! most profitable customers, products, regions or channels, as well as uncovering the costs of individual business processes that may need to be improved in order to drive higher profit levels. Changes may be afoot in the ABM market, but Armstrong Laing Group CEO Mike Sherratt argues that businesses need specialists with an ABM focus to keep up with their requirements in such a climate. He looks at what benefits a ‘best-of-breed’ ABM system can offer businesses and contends that businesses must choose carefully when going down the ABM route - and also ask themselves the question whether ‘generalist’ organisations will be able to deliver the best possible ABM solution","['activity based costing', 'best-of-breed ABM', 'Armstrong Laing Group', 'activity based management', 'accounting', 'costing']","['businesses', 'management', 'activity', 'individual business processes', 'economic downturn', 'ABM system', 'ABM focus', 'todays', 'case', 'economic']",747,144,6,747,143,10,5,3,1
"establishing an urban digital cadastre: analytical reconstruction of parcel boundaries a new method for generating a spatially accurate, legally supportive and operationally efficient cadastral database of the urban cadastral reality is described. the definition and compilation of an accurate cadastral database (achieving a standard deviation smaller than 0.1 m) is based on an analytical reconstruction of cadastral boundaries rather than on the conventional field reconstruction process. the new method is based on gps control points and traverse networks for providing the framework; the old field books for defining the links between the various original ground features; and a geometrical and cadastral adjustment process as the conceptual basis. a pilot project that was carried out in order to examine and evaluate the new method is described ","Establishing an urban digital cadastre: analytical reconstruction of parcel boundaries Anew method for generating a spatially accurate, legally supportive and operationally efficient cadastral database of the urban cadastral reality is described. The definition and compilation of an accurate cadastral database (achieving a standard deviation smaller than 0.1 m) is based on an analytical reconstruction of cadastral boundaries rather than on the conventional field reconstruction process. The new method is based on GPS control points and traverse networks for providing the framework; the old field books for defining the links between the various original ground features; and a geometrical and cadastral adjustment process as the conceptual basis. A pilot project that was carried out in order to examine and evaluate the new method is described","['urban digital cadastre', 'analytical reconstruction', 'parcel boundaries', 'spatially accurate cadastral database', 'urban cadastral reality', 'standard deviation', 'field reconstruction process', 'GPS control points', 'traverse networks', 'old field books', 'ground features', 'cadastral adjustment process', 'land information systems', 'LIS', 'geographic information systems', 'cartography', 'geographic information systems', 'terrain mapping', 'town and country planning', 'visual databases']","['cadastral', 'analytical reconstruction', 'efficient cadastral database', 'cadastral adjustment process', 'accurate cadastral database', 'urban cadastral reality', 'urban digital cadastre', 'cadastral boundaries', 'parcel boundaries', 'urban']",727,126,20,727,124,10,392,114,6
"bit-serial ab/sup 2/ multiplier using modified inner product this paper presents a new multiplication algorithm and, based on this algorithm, proposes a hardware architecture, called modified inner-product multiplier (mipm), which computes ab/sup 2/ multiplication based on a linear feedback shift register (lfsr). the algorithm is based on the property of the irreducible all one polynomial (aop) over the finite field gf(2/sup m/). the proposed architecture reduces the time and space complexity for computing ab/sup 2/. the proposed architecture has a potential application to implementing exponentiation architecture for a public-key cryptosystem ","Bit-serial AB/sup 2/ multiplier using modified inner product This paper presents a new multiplication algorithm and, based on this, algorithm, proposes a hardware architecture, called modified inner-product multiplier (MIPM), which computes AB/sup 2/ multiplication based on a linear feedback shift register (LFSR). The algorithm is based on the property of the irreducible all one polynomial (AOP) over the finite field GF(2/sup mi). The proposed architecture reduces the time and space complexity for computing AB/Sup 21. The proposed architecture has a potential application to implementing exponentiation architecture for a public-key cryptosystem","['bit-serial AB/sup 2/ multiplier', 'modified inner product', 'multiplication algorithm', 'hardware architecture', 'modified inner-product multiplier', 'linear feedback shift register', 'irreducible all one polynomial', 'space complexity', 'time complexity', 'public-key cryptosystem', 'computational complexity', 'cryptography', 'logic design', 'multiplying circuits', 'shift registers']","['r', 'new multiplication algorithm', 'exponentiation architecture', 'inner-product multiplier', 'hardware architecture', 'Bit-serial AB/sup r', 'inner product', 'AB/sup', 'AB/sup r', 'multiplier']",561,91,15,562,90,10,2,3,4
motion estimation using modified dynamic programming a new method for computing precise estimates of the motion vector field of moving objects in a sequence of images is proposed. correspondence vector-field computation is formulated as a matching optimization problem for multiple dynamic images. the proposed method is a heuristic modification of dynamic programming applied to the 2-d optimization problem. motion-vector-field estimates using real movie images demonstrate good performance of the algorithm in terms of dynamic motion analysis ,Motion estimation using modified dynamic programming Anew method for computing precise estimates of the motion vector field of moving objects in a sequence of images is proposed. Correspondence vector-field computation is formulated as a matching optimization problem for multiple dynamic images. The proposed method is a heuristic modification of dynamic programming applied to the 2-D optimization problem. Motion-vector-field estimates using real movie images demonstrate good performance of the algorithm in terms of dynamic motion analysis,"['modified dynamic programming', 'motion estimation', 'precise estimates', 'motion vector field', 'moving objects', 'image sequence', 'vector-field computation', 'matching optimization problem', 'multiple dynamic images', 'heuristic modification', 'dynamic programming', '2-D optimization problem', 'motion vector field estimates', 'real movie images', 'algorithm', 'dynamic motion analysis', 'dynamic programming', 'image matching', 'image sequences', 'motion estimation', 'robot vision', 'vectors']","['dynamic programming', 'matching optimization problem', 'Motion-vector-field estimates', 'd-d optimization problem', 'multiple dynamic images', 'dynamic motion analysis', 'real movie images', 'precise estimates', 'Motion estimation', 'dynamic']",470,77,22,470,75,10,259,69,4
"a new graphical user interface for fast construction of computation phantoms and mcnp calculations: application to calibration of in vivo measurement systems reports on a new utility for development of computational phantoms for monte carlo calculations and data analysis for in vivo measurements of radionuclides deposited in tissues. the individual properties of each worker can be acquired for a rather precise geometric representation of his (her) anatomy, which is particularly important for low energy gamma ray emitting sources such as thorium, uranium, plutonium and other actinides. the software enables automatic creation of an mcnp input data file based on scanning data. the utility includes segmentation of images obtained with either computed tomography or magnetic resonance imaging by distinguishing tissues according to their signal (brightness) and specification of the source and detector. in addition, a coupling of individual voxels within the tissue is used to reduce the memory demand and to increase the calculational speed. the utility was tested for low energy emitters in plastic and biological tissues as well as for computed tomography and magnetic resonance imaging scanning information ","‘Anew graphical user interface for fast construction of computation phantoms and MCNP calculations: application to calibration of in vivo measurement systems Reports on a new utility for development of computational phantoms for Monte Carlo calculations and data analysis for in vivo measurements of radionuclides deposited in tissues. The individual properties of each worker can be acquired for a rather precise geometric representation of his (her) anatomy, which is particularly important for low energy gamma ray emitting sources such as thorium, uranium, plutonium and other actinides. The software enables automatic creation of an MCNP input data file based on scanning data. The utility includes segmentation of images obtained with either computed tomography or magnetic resonance imaging by distinguishing tissues according to their signal (brightness) and specification of the source and detector. In addition, a coupling of individual voxels within the tissue is used to reduce the memory demand and to increase the calculational speed. The utility was tested for low energy emitters in plastic and biological tissues as well as for computed tomography and magnetic resonance imaging scanning information","['computational phantoms', 'Monte Carlo calculations', 'in vivo measurements', 'radionuclides', 'tissues', 'worker', 'precise geometric representation', 'MCNP input data file', 'scanning data', 'computed tomography', 'brightness', 'graphical user interface', 'computation phantoms', 'calibration', 'in vivo measurement systems', 'Th', 'U', 'Pu', 'signal', 'detector', 'individual voxels', 'memory demand', 'calculational speed', 'plastic', 'biological tissues', 'magnetic resonance imaging scanning information', 'anatomy', 'low energy gamma ray emitting sources', 'actinides', 'software', 'automatic creation', 'biological tissues', 'biomedical MRI', 'calibration', 'computerised tomography', 'graphical user interfaces', 'lung', 'Monte Carlo methods', 'physics computing', 'radioisotopes']","['computed tomography', 'mnp calculations application', 'graphical user interface', 'Monte Carlo calculations', 'computation phantoms', 'fast construction', 'user', 'fast', 'graphical', 'computation']",1038,180,40,1039,178,10,614,178,28
"simulation of evacuation processes using a bionics-inspired cellular automaton model for pedestrian dynamics we present simulations of evacuation processes using a recently introduced cellular automaton model for pedestrian dynamics. this model applies a bionics approach to describe the interaction between the pedestrians using ideas from chemotaxis. here we study a rather simple situation, namely the evacuation from a large room with one or two doors. it is shown that the variation of the model parameters allows to describe different types of behaviour, from regular to panic. we find a non-monotonic dependence of the evacuation times on the coupling constants. these times depend on the strength of the herding behaviour, with minimal evacuation times for some intermediate values of the couplings, i.e., a proper combination of herding and use of knowledge about the shortest way to the exit ","Simulation of evacuation processes using a bionics-inspired cellular automaton model for pedestrian dynamics We present simulations of evacuation processes using a recently introduced cellular automaton model for pedestrian dynamics. This model applies a bionics approach to describe the interaction between the pedestrians using ideas from chemotaxis. Here we study a rather simple situation, namely the evacuation from a large room with one or two doors. It is shown that the variation of the model parameters allows to describe different types of behaviour, from regular to panic. We find a non-monotonic dependence of the evacuation times on the coupling constants. These times depend on the strength of the herding behaviour, with minimal evacuation times for some intermediate values of the couplings, i.¢., a proper combination of herding and use of knowledge about the shortest way to the exit","['evacuation processes simulation', 'chemotaxis', 'nonmonotonic dependence', 'coupling constants', 'herding behaviour', 'bionics-inspired cellular automaton model', 'pedestrian dynamics', 'biocybernetics', 'cellular automata']","['cellular automaton model', 'evacuation processes', 'pedestrian dynamics', 'evacuation', 'minimal evacuation times', 'model parameters', 'model', 'evacuation times', 'processes', 'pedestrian']",765,138,9,765,137,10,1,1,3
"statistical analysis of nonlinearly reconstructed near-infrared tomographic images. ii. experimental interpretation for pt. i see ibid., vol. 21, no. 7, p. 755-63 (2002). image error analysis of a diffuse near-infrared tomography (nir) system has been carried out on simulated data using a statistical approach described in pt. i of this paper (pogue et al., 2002). the methodology is used here with experimental data acquired on phantoms with a prototype imaging system intended for characterizing breast tissue. results show that imaging performance is not limited by random measurement error, but rather by calibration issues. the image error over the entire field of view is generally not minimized when an accurate homogeneous estimate of the phantom properties is available; however, local image error over a target region of interest (roi) is reduced. the image reconstruction process which includes a levenberg-marquardt style regularization provides good minimization of the objective function, yet its reduction is not always correlated with an overall image error decrease. minimization of the bias in an roi which contains localized changes in the optical properties can be achieved through five to nine iterations of the algorithm. precalibration of the algorithm through statistical evaluation of phantom studies may provide a better measure of the image accuracy than that implied by minimization of the standard objective function ","Statistical analysis of nonlinearly reconstructed near-infrared tomographic images. I. Experimental interpretation For pt. I see ibid., vol. 21, no. 7, p. 755-63 (2002). Image error analysis of a diffuse near-infrared tomography (NIR) system has been carried out on simulated data using a statistical approach described in pt. | of this paper (Pogue et al. 2002). The methodology is used here with experimental data acquired on phantoms with a prototype imaging system intended for characterizing breast tissue. Results show that imaging performance is not limited by random measurement error, but rather by calibration issues. The image error over the entire field of view is generally not minimized when an accurate homogeneous estimate of the phantom properties is available; however, local image error over a target region of interest (ROI) is reduced. The image reconstruction process which includes a Levenberg-Marquardt style regularization provides good minimization of the objective function, yet its reduction is not always correlated with an overall image error decrease Minimization of the bias in an ROI which contains localized changes in the optical properties can be achieved through five to nine iterations of the algorithm. Precalibration of the algorithm through statistical evaluation of phantom studies may provide a better measure of the image accuracy than that implied by minimization of the standard objective function","['medical diagnostic imaging', 'nonlinearly reconstructed near-infrared tomographic images', 'image error', 'algorithm precalibration', 'hemoglobin', 'random measurement error', 'target region of interest', 'accurate homogeneous estimate', 'phantom properties', 'Levenberg-Marquardt style regularization', 'bias minimization', 'algorithm iterations', 'objective function minimization', 'calibration', 'image reconstruction', 'infrared imaging', 'measurement errors', 'medical image processing', 'optical tomography', 'statistical analysis']","['image', 'image reconstruction process', 'random measurement error', 'prototype imaging system', 'Statistical analysis', 'Image error analysis', 'imaging performance', 'local image error', 'image accuracy', 'image error']",1232,216,20,1229,215,10,2,4,6
"decomposition of additive cellular automata finite additive cellular automata with fixed and periodic boundary conditions are considered as endomorphisms over pattern spaces. a characterization of the nilpotent and regular parts of these endomorphisms is given in terms of their minimal polynomials. generalized eigenspace decomposition is determined and relevant cyclic subspaces are described in terms of symmetries. as an application, the lengths and frequencies of limit cycles in the transition diagram of the automaton are calculated ","Decomposition of additive cellutar automata Finite additive cellular automata with fixed and periodic boundary conditions are considered as endomorphisms over pattern spaces. A characterization of the nilpotent and regular parts of these endomorphism is given in terms of their minimal polynomials. Generalized eigenspace decomposition is determined and relevant cyclic subspaces are described in terms of symmetries. As an application, the lengths and frequencies of limit cycles in the transition diagram of the automaton are calculated","['cellular automata', 'finite cellular automaton', 'transition diagram', 'endomorphisms', 'computational complexity', 'cellular automata', 'computational complexity']","['additive cellular automata', 'Decomposition', 'periodic boundary conditions', 'pattern spaces', 'endomorphisms', 'Finite', 'periodic', 'cellular', 'automata', 'additive']",465,76,7,464,75,10,1,2,1
"parallel operation of capacity-limited three-phase four-wire active power filters three-phase four-wire active power filters (apfs) are presented that can be paralleled to enlarge the system capacity and reliability. the apf employs the pwm four-leg voltage-source inverter. a decoupling control approach for the leg connected to the neutral line is proposed such that the switching of all legs has no interaction. functions of the proposed apf include compensation of reactive power, harmonic current, unbalanced power and zero-sequence current of the load. the objective is to achieve unity power factor, balanced line current and zero neutral-line current. compensation of all components is capacity-limited, co-operating with the cascaded load current sensing scheme. multiple apfs can be paralleled to share the load power without requiring any control interconnection. in addition to providing the theoretic bases and detailed design of the apfs, two 6 kva apfs are implemented. the effectiveness of the proposed method is validated with experimental results ","Parallel operation of capacity-limited three-phase four-wire active power filters Three-phase four-wire active power fiters (APFs) are presented that can be paralleled to enlarge the system capacity and reliability. The APF employs the PWM four-leg voltage-source inverter. A decoupling control approach for the leg connected to the neutral line is proposed such that the switching of all legs has no interaction. Functions of the proposed APF include compensation of reactive power, harmonic current, unbalanced power and zero-sequence current of the load. The objective is to achieve unity power factor, balanced line current and zero neutral-line current. Compensation of all components is capacity-limited, co-operating with the cascaded load current sensing scheme. Multiple APF can be paralleled to share the load power without requiring any control interconnection. In addition to providing the theoretic bases and detailed design of the APFS, two 6 KVA APFs are implemented. The effectiveness of the proposed method is validated with experimental results","['capacity-limited three-phase four-wire active power filters', 'parallel operation', 'PWM four-leg voltage-source inverter', 'decoupling control approach', 'leg switching', 'control design', 'reactive power compensation', 'harmonic current compensation', 'unbalanced power compensation', 'zero-sequence load current compensation', 'unity power factor', 'balanced line current', 'zero neutral-line current', 'load power sharing', 'control performance', '6 kVA', 'active filters', 'compensation', 'control system synthesis', 'DC-AC power convertors', 'electric current control', 'power harmonic filters', 'power supply quality', 'power system control', 'power system harmonics', 'PWM invertors', 'reactive power control']","['capacity-limited', 'power', 'four-mile', 'neutral-line current Compensation', 'zero-sequence current', 'unity power factors', 'current', 'Parallel operation', 'reactive power', 'load power']",911,155,27,909,154,10,4,2,3
"new developments in inductive learning any intelligent system, whether natural or artificial, must have three characteristics: knowledge, reasoning, and learning. artificial intelligence (ai) studies these three aspects in artificial systems. briefly, we could say that knowledge refers to the system's world model, and reasoning to the manipulation of this knowledge. learning is slightly more complex; the system interacts with the world and as a consequence it builds onto and modifies its knowledge. this process of self-building and self-modifying is known as learning. this thesis is set within the field of artificial intelligence and focuses on learning. more specifically, it deals with the inductive learning of decision trees ","New developments in inductive learning Any intelligent system, whether natural or artificial, must have three characteristics: knowledge, reasoning, and learning. Artificial intelligence (Al) studies these three aspects in artificial systems. Briefly, we could say that knowledge refers to the system's world model, and reasoning to the manipulation of this knowledge. Learning is slightly more complex; the system interacts with the world and as a consequence it builds onto and modifies its knowledge. This process of self-building and self-moditying is known as learning. This thesis is set within the field of artificial intelligence and focuses on learning. More specifically, it deals with the inductive learning of decision trees","['inductive learning', 'new developments', 'intelligent system', 'knowledge', 'reasoning', 'artificial intelligence', 'decision trees', 'decision trees', 'learning by example']","['inductive learning', 'characteristics knowledge reasoning', 'artificial systems briefly', 'system', 'systems world model', 'knowledge Learning', 'intelligent system', 'New developments', 'artificial intelligence', 'Artificial intelligence']",630,108,9,630,107,10,2,2,2
"robust model-order reduction of complex biological processes this paper addresses robust model-order reduction of a high dimensional nonlinear partial differential equation (pde) model of a complex biological process. based on a nonlinear, distributed parameter model of the same process which was validated against experimental data of an existing, pilot-scale biological nutrient removal (bnr) activated sludge plant, we developed a state-space model with 154 state variables. a general algorithm for robustly reducing the nonlinear pde model is presented and, based on an investigation of five state-of-the-art model-order reduction techniques, we are able to reduce the original model to a model with only 30 states without incurring pronounced modelling errors. the singular perturbation approximation balanced truncating technique is found to give the lowest modelling errors in low frequency ranges and hence is deemed most suitable for controller design and other real-time applications ","Robust model-order reduction of complex biological processes This paper addresses robust model-order reduction of a high dimensional nonlinear partial differential equation (PDE) model of a complex biological process. Based on a nonlinear, distributed parameter model of the same process which was validated against experimental data of an existing, pilot-scale biological nutrient removal (BNR) activated sludge plant, we developed a state-space model with 154 state variables. A general algorithm for robustly reducing the nonlinear PDE model is presented and, based on an investigation of five state-of-the-art model-order reduction techniques, we are able to reduce the original model to a model with only 30 states without incurring pronounced modelling errors. The singular perturbation approximation balanced truncating technique is found to give the lowest modelling errors in low frequency ranges and hence is deemed most suitable for controller design and other real-time applications","['complex biological processes', 'robust model-order reduction', 'high dimensional nonlinear partial differential equation model', 'nonlinear distributed parameter model', 'pilot-scale BNR activated sludge plant', 'state-space model', 'singular perturbation approximation balanced truncating technique', 'modelling errors', 'controller design', 'Hankel singular values', 'biological nutrient removal activated sludge processes', 'biotechnology', 'control system synthesis', 'nonlinear control systems', 'nonlinear differential equations', 'partial differential equations', 'reduced order systems', 'robust control', 'singularly perturbed systems', 'state-space methods', 'water treatment']","['model', 'complex biological processes', 'pronounced modelling errors', 'lowest modelling errors', 'state-space model', 'parameter model', 'original model', 'model-order', 'robust model-order reduction', 'Robust model-order reduction']",855,141,21,855,140,10,0,0,2
"managing safety and strategic stocks to improve materials requirements planning performance this paper provides a methodology for managing safety and strategic stocks in materials requirements planning (mrp) environments to face uncertainty in market demand. a set of recommended guidelines suggest where to position, how to dimension and when to replenish both safety and strategic stocks. trade-offs between stock positioning and dimensioning and between stock positioning and replenishment order triggering are outlined. the study reveals also that most of the decisions are system specific, so that they should be evaluated in a quantitative manner through simulation. a case study is reported, where the benefits from adopting the new proposed methodology lie in achieving the target service level even under peak demand conditions, with the value of safety stocks as a whole growing only by about 20 per cent ","Managing safety and strategic stocks to improve materials requirements planning performance This paper provides a methodology for managing safety and strategic stocks in materials requirements planning (MRP) environments to face uncertainty in market demand. A set of recommended guidelines suggest where to Position, how to dimension and when to replenish both safety and strategic stocks. Trade-offs between stock positioning and dimensioning and between stock positioning and replenishment order triggering are outlined. The study reveals also that most of the decisions are system specific, so that they should be evaluated in a quantitative manner through simulation. A case study is reported, where the benefits from adopting the new proposed methodology lie in achieving the target service level even under peak demand conditions, with the value of safety stocks as a whole growing only by about 20 per cent","['MRP', 'materials requirements planning', 'market demand', 'strategic stocks', 'safety stocks', 'inventory management', 'variance control', 'stock replenishment', 'service level', 'peak demand', 'management', 'manufacturing resources planning', 'production control', 'stock control']","['materials requirements', 'stock positioning', 'strategic stocks Trade-offs', 'peak demand conditions', 'safety stocks', 'stocks', 'strategic', 'safety', 'strategic stocks', 'materials']",778,138,14,778,137,10,0,0,2
"micro-optical realization of arrays of selectively addressable dipole traps: a scalable configuration for quantum computation with atomic qubits we experimentally demonstrate novel structures for the realization of registers of atomic qubits: we trap neutral atoms in one- and two-dimensional arrays of far-detuned dipole traps obtained by focusing a red-detuned laser beam with a microfabricated array of microlenses. we are able to selectively address individual trap sites due to their large lateral separation of 125 mu m. we initialize and read out different internal states for the individual sites. we also create two interleaved sets of trap arrays with adjustable separation, as required for many proposed implementations of quantum gate operations ","Micro-optical realization of arrays of selectively addressable dipole traps: a scalable configuration for quantum computation with atomic qubits We experimentally demonstrate novel structures for the realization of registers of atomic qubits: We trap neutral atoms in one- and two-dimensional arrays of far-detuned dipole traps obtained by focusing a red-detuned laser beam with a microfabricated array of microlenses. We are able to selectively address individual trap sites due to their large lateral separation of 125 mu m. We initialize and read out different internal states for the individual sites. We also create two interleaved sets of trap arrays with adjustable separation, as required for many proposed implementations of quantum gate operations","['atomic qubits', 'registers', 'neutral atoms', 'far-detuned dipole traps', 'red-detuned laser beam', 'microfabricated array', 'microlenses', 'internal states', 'quantum gate operations', 'quantum computation', 'scalable configuration', 'quantum computing', 'trapped ions']","['trap', 'Micro-optical realization', 'far-detuned dipole traps', 'addressable dipole traps', 'two-dimensional arrays', 'microfabricated array', 'individual trap sites', 'neutral atoms', 'trap arrays', 'arrays']",648,111,13,648,110,10,0,0,8
"six common enterprise programming mistakes instead of giving you tips to use in your programming (at least directly), i want to look at some common mistakes made in enterprise programming. instead of focusing on what to do, i want to look at what you should not do. most programmers take books like mine and add in the good things, but they leave their mistakes in the very same programs! so i touch on several common errors i see in enterprise programming, and then briefly mention how to avoid those mistakes ","Six common enterprise programming mistakes Instead of giving you tips to use in your programming (at least directly), | want to look at some common mistakes made in enterprise programming, Instead of focusing on what to do, | want to look at what you should not do. Most programmers take books like mine and add in the good things, but they leave their mistakes in the very same programs! So | touch on several common errors | see in enterprise programming, and then briefly mention how to avoid those mistakes","['enterprise programming mistakes', 'common errors', 'data store', 'database', 'XML', 'Enterprise JavaBeans', 'vendor-specific programming', 'business data processing', 'programming']","['| touch', 'enterprise programming', 'several common errors', 'Most programmers', 'common mistakes', 'same programs', 'common', 'programs', 'mistakes', 'enterprise']",421,91,9,421,90,10,5,5,0
"mathematical properties of dominant ahp and concurrent convergence method this study discusses the mathematical structure of the dominant ahp and the concurrent convergence method which were originally developed by kinoshita and nakanishi. they introduced a new concept of a regulating alternative into an analyzing tool for a simple evaluation problem with a criterion set and an alternative set. although the original idea of the dominant ahp and the concurrent convergence method is unique, the dominant ahp and the concurrent convergence method are not sufficiently analyzed in mathematical theory. this study shows that the dominant ahp consists of a pair of evaluation rules satisfying a certain property of overall evaluation vectors. this study also shows that the convergence of concurrent convergence method is guaranteed theoretically ","Mathematical properties of dominant AHP and concurrent convergence method This study discusses the mathematical structure of the dominant AHP and the concurrent convergence method which were originally developed by Kinoshita and Nakanishi. They introduced a new concept of a regulating alternative into an analyzing tool for a simple evaluation problem with a criterion set and an alternative set. Although the original idea of the dominant AHP and the concurrent convergence method is unique, the dominant AHP and the concurrent convergence method are not sufficiently analyzed in mathematical theory. This study shows that the dominant AHP consists of a pair of evaluation rules satistying a certain property of overall evaluation vectors. This study also shows that the convergence of concurrent convergence method is guaranteed theoretically","['dominant AHP', 'concurrent convergence method', 'overall evaluation vectors', 'convergence', 'decision theory']","['concurrent convergence method', 'dominant ah', 'overall evaluation vectors', 'simple evaluation problem', 'Mathematical properties', 'ah', 'method', 'dominant', 'concurrent', 'convergence']",722,125,5,722,124,10,1,1,0
"selecting rail grade crossing investments with a decision support system the federal railroad administration (fra) has developed a series of rail and rail-related analysis tools that assist fra officials, metropolitan planning organizations (mpos), state department of transportation (dot), and other constituents in evaluating the cost and benefits of potential infrastructure projects. to meet agency objectives, the fra wants to add a high-speed rail grade crossing analysis tool to its package of rail and rail-related intermodal software products. this paper presents a conceptual decision support system (dss) that can assist officials in achieving this goal. the paper first introduces the fra's objectives and the role of cost benefit analysis in achieving these objectives. next, there is a discussion of the models needed to assess the feasibility of proposed high-speed rail grade crossing investments and the presentation of a decision support system (dss) that can deliver these models transparently to users. then, the paper illustrates a system session and examines the potential benefits from system use ","Selecting rail grade crossing investments with a decision support system The Federal Railroad Administration (FRA) has developed a series of rail and rai-related analysis tools that assist FRA officials, Metropolitan Planning Organizations (MPOs), state Department of Transportation (007), and other constituents in evaluating the cost and benefits of potential infrastructure projects. To meet agency objectives, the FRA wants to add a high-speed rail grade crossing analysis tool to its package of rail and rail-related intermodal software products. This Paper presents a conceptual decision support system (DSS) that can assist officials in achieving this goal. The paper first introduces the FRA's objectives and the role of cost benefit analysis in achieving these objectives. Next, there is a discussion of the models needed to assess the feasibility of proposed high-speed rail grade crossing investments and the presentation of a decision support system (DSS) that can deliver these models transparently to users. Then, the paper illustrates a system session and examines the potential benefits from system use","['rail grade crossing investment selection', 'decision support system', 'Federal Railroad Administration', 'Metropolitan Planning Organizations', 'Department of Transportation', 'infrastructure projects', 'high-speed rail grade crossing analysis tool', 'rail-related intermodal software products', 'rail intermodal software products', 'cost benefit analysis', 'cost-benefit analysis', 'decision support systems', 'investment', 'railways', 'town and country planning', 'transportation']","['decision support system', 'high-speed rail grade', 'rai-related analysis tools', 'cost benefit analysis', 'frans objectives', 'system session', 'system use', 'rail', 'analysis tool', 'rail grade']",956,165,16,955,164,10,11,2,7
"an efficient algorithm for sequential generation of failure states in a network with multi-mode components in this work, a new algorithm for the sequential generation of failure states in a network with multi-mode components is proposed. the algorithm presented in the paper transforms the state enumeration problem into a k-shortest paths problem. taking advantage of the inherent efficiency of an algorithm for shortest paths enumeration and also of the characteristics of the reliability problem in which it will be used, an algorithm with lower complexity than the best algorithm in the literature for solving this problem, was obtained. computational results will be presented for comparing the efficiency of both algorithms in terms of cpu time and for problems of different size ","An efficient algorithm for sequential generation of failure states in a network with multi-mode components In this work, a new algorithm for the sequential generation of failure states in a network with multi-mode components is proposed. The algorithm presented in the paper transforms the state enumeration problem into a k-shortest paths problem. Taking advantage of the inherent efficiency of an algorithm for shortest paths enumeration and also of the characteristics of the reliability problem in which it will be used, an algorithm with lower complexity than the best algorithm in the literature for solving this problem, was obtained. Computational results will be presented for comparing the efficiency of both algorithms in terms of CPU time and for problems of different size","['multi-mode components reliability', 'sequential failure states generation algorithm', 'network failure states', 'state enumeration problem', 'K-shortest paths problem', 'CPU time', 'engineering computing', 'failure analysis', 'reliability theory']","['sequential generation', 'failure states', 'shortest paths enumeration', 'state enumeration problem', 'shortest paths problem', 'inherent efficiency', 'efficient algorithm', 'best algorithm', 'new algorithm', 'algorithms']",665,122,9,665,121,10,0,0,1
"wireless-retail financial services: adoption can't justify the cost slow adoption by retail investors, costly services and bankrupt vendors has prompted banks and brokerage firms to turn off their wireless applications ","Wireless-retail financial services: adoption can't justify the cost Slow adoption by retail investors, costly services and bankrupt vendors has prompted banks and brokerage firms to turn off their wireless applications","['banks', 'brokerage firms', 'wireless applications', 'banking', 'investment', 'mobile computing']","['cost Slow adoption', 'retail investors', 'bankrupt vendors', 'costly services', 'brokerage firms', 'Wireless-retail', 'financial', 'costly', 'services', 'adoption']",189,31,6,189,30,10,0,0,2
"comparison of automated digital elevation model extraction results using along-track aster and across-track spot stereo images a digital elevation model (dem) can be extracted automatically from stereo satellite images. during the past decade, the most common satellite data used to extract dem was the across-track spot. recently, the addition of along-track aster data, which can be downloaded freely, provides another attractive alternative to extract dem data. this work compares the automated dem extraction results using an aster stereo pair and a spot stereo pair over an area of hilly mountains in drum mountain, utah, when compared to a usgs 7.5-min dem standard product. the result shows that spot produces better dem results in terms of accuracy and details, if the radiometric variations between the images, taken on subsequent satellite revolutions, are small. otherwise, the aster stereo pair is a better choice because of simultaneous along-track acquisition during a single pass. compared to the usgs 7.5-min dem, the aster and the spot extracted dems have a standard deviation of 11.6 and 4.6 m, respectively ","Comparison of automated digital elevation model extraction results using along-track ASTER and across-track SPOT stereo images A digital elevation model (DEM) can be extracted automatically from stereo satellite images. During the past decade, the most common satellite data used to extract DEM was the across-track SPOT. Recently, the addition of along-track ASTER data, which can be downloaded freely, provides another attractive alternative to extract DEM data. This work compares the automated DEM extraction results using an ASTER stereo pair and a SPOT stereo pair over an area of hilly mountains in Drum Mountain, Utah, when compared to a USGS 7.5-min DEM standard product. The result shows that SPOT produces better DEM results in terms of accuracy and details, if the radiometric variations between the images, taken on subsequent satellite revolutions, are small. Otherwise, the ASTER stereo pair is a better choice because of simultaneous along-track acquisition during a single pass. Compared to the USGS 7.5-min DEM, the ASTER and the SPOT extracted DEMs have a standard deviation of 11.6 and 4.6 m, respectively","['automated digital elevation model extraction', 'along-track ASTER data', 'across-track SPOT stereo images', 'stereo satellite images', 'ASTER stereo pair', 'SPOT stereo image pair', 'radiometric variations', 'simultaneous along-track acquisition', 'geophysical signal processing', 'height measurement', 'image resolution', 'stereo image processing', 'terrain mapping']","['digital elevation model', 'ASTER stereo pair', 'automated DEM extraction', 'stereo satellite images', 'along-track ASTER data', 'common satellite data', 'better DEM results', 'SPOT stereo pair', 'DEM data', 'along-track ASTER']",953,174,13,953,173,10,0,0,1
"yet some more complexity results for default logic we identify several new tractable subsets and several new intractable simple cases for reasoning in the propositional version of reiter's default logic. the majority of our findings are related to brave reasoning. by making some intuitive observations, most classes that we identify can be derived quite easily from some subsets of default logic already known in the literature. some of the subsets we discuss are subclasses of the so-called ""extended logic programs"". all the tractable subsets presented in this paper can be recognized in linear time ","Yet some more complexity results for default logic We identity several new tractable subsets and several new intractable simple cases for reasoning in the propositional version of Reiter's default logic. The majority of our findings are related to brave reasoning. By making some intuitive observations, most classes that we identify can be derived quite easily from some subsets of default logic already known in the literature. Some of the subsets we discuss are subclasses of the so-called ""extended logic programs"". All the tractable subsets presented in this paper can be recognized in linear time","['reasoning', 'default logic', 'complexity results', 'complexity classes', 'nonmonotonic reasoning', 'extended logic programs', 'tractable subsets', 'computational complexity', 'formal logic', 'nonmonotonic reasoning']","['tractable subsets', 'more complexity results', 'reuters default logic', 'logic', 'default', 'more', 'results', 'complexity', 'default logic', 'subsets']",509,95,10,509,94,10,1,1,1
"a unifying co-operative web caching architecture network caching of objects has become a standard way of reducing network traffic and latency in the web. however, web caches exhibit poor performance with a hit rate of about 30%. a solution to improve this hit rate is to have a group of proxies form co-operation where objects can be cached for later retrieval. a cooperative cache system includes protocols for hierarchical and transversal caching. the drawback of such a system lies in the resulting network load due to the number of messages that need to be exchanged to locate an object. this paper proposes a new co-operative web caching architecture, which unifies previous methods of web caching. performance results shows that the architecture achieve up to 70% co-operative hit rate and accesses the cached object in at most two hops. moreover, the architecture is scalable with low traffic and database overhead ","A.unifying co-operative Web caching architecture Network caching of objects has become a standard way of reducing network traffic and latency in the Web. However, Web caches exhibit poor performance with a hit rate of about 30%. A solution to improve this hit rate is to have a group of proxies form co-operation where objects can be cached for later retrieval. A cooperative cache system includes protocols for hierarchical and transversal caching. The drawback of such a system lies in the resulting network load due to the number of messages that need to be exchanged to locate an object. This paper proposes a new co-operative Web caching architecture, which unifies previous methods of Web caching. Performance results shows that the architecture achieve up to 70% co-operative hit rate and accesses the ‘cached object in at most two hops. Moreover, the architecture is scalable with low traffic and database overhead","['co-operative Web caching architecture', 'network caching', 'network traffic reduction', 'network latency reduction', 'co-operative hit rate', 'cooperative cache system', 'protocols', 'hierarchical caching', 'transversal caching', 'network load', 'scalable architecture', 'low traffic overhead', 'low database overhead', 'Web browser', 'World Wide Web', 'cache storage', 'file servers', 'Internet', 'memory protocols', 'online front-ends', 'telecommunication traffic']","['architecture', 'objects', 'cooperative cache system', 'Web caching Performance', 'co-operative hit rate', 'caching', 'transverse caching', 'Web', 'hit rate', 'co-operative']",773,150,21,775,148,10,453,148,2
"a fractional-flow model of serial manufacturing systems with rework and its reachability and controllability properties a dynamic fractional-flow model of a serial manufacturing system incorporating rework is considered. using some results on reachability and controllability of positive linear systems the ability of serial manufacturing systems with rework to ""move in space"", that is their reachability and controllability properties, are studied. these properties are important not only for optimising the performance of the manufacturing system, possibly off-line, but also to improve its functioning by using feedback control online ","A tractional-flow model of serial manufacturing systems with rework and its reachability and controllability properties, A dynamic fractional-flow model of a serial manufacturing system incorporating rework is considered. Using some results on reachability and controllability of positive linear systems the ability of serial manufacturing systems with rework to ""move in space"", that is their reachability and controllability properties, are studied. These properties are important not only for optimising the performance of the manufacturing system, possibly off-line, but also to improve its functioning by using feedback control online","['serial manufacturing systems', 'rework', 'reachability', 'controllability', 'dynamic fractional-flow model', 'positive linear systems', 'performance optimisation', 'feedback control', 'controllability', 'discrete time systems', 'graph theory', 'inspection', 'linear systems', 'Markov processes', 'matrix algebra', 'probability', 'production control', 'reachability analysis']","['serial manufacturing systems', 'controllability properties', 'dynamic fractional-flow model', 'positive linear systems', 'feedback control online', 'tractional-flow model', 'system', 'manufacturing', 'serial', 'manufacturing system']",552,88,18,553,87,10,1,2,2
"nonlinear adaptive control via sliding-mode state and perturbation observer the paper presents a nonlinear adaptive controller (nac) for single-input single-output feedback linearisable nonlinear systems. a sliding-mode state and perturbation observer is designed to estimate the system states and perturbation which includes the combined effect of system nonlinearities, uncertainties and external disturbances. the nac design does not require the details of the nonlinear system model and full system states. it possesses an adaptation capability to deal with system parameter uncertainties, unmodelled system dynamics and external disturbances. the convergence of the observer and the stability analysis of the controller/observer system are given. the proposed control scheme is applied for control of a synchronous generator, in comparison with a state-feedback linearising controller (flc). simulation study is carried out based on a single-generator infinite-bus power system to show the performance of the controller/observer system ","Nonlinear adaptive control via sliding-mode state and perturbation observer The paper presents a nonlinear adaptive controller (NAC) for single-input single-output feedback linearisable nonlinear systems. A sliding-mode state and perturbation observer is designed to estimate the system states and perturbation which includes the combined effect of system nonlinearities, uncertainties and external disturbances. The NAC design does not require the details of the nonlinear system model and full system states. It possesses an adaptation capability to deal with system parameter uncertainties, unmodeled system dynamics and external disturbances. The convergence of the observer and the stability analysis of the controller/observer system are given. The proposed control scheme is applied for control of a synchronous generator, in comparison with a state-feedback linearising controller (FLC). Simulation study is carried out based on a single-generator infinite-bus power system to show the performance of the controller/observer system","['nonlinear adaptive control', 'sliding-mode state observer', 'perturbation observer', 'NAC', 'SISO feedback linearisable nonlinear systems', 'parameter uncertainties', 'unmodelled system dynamics', 'external disturbances', 'convergence', 'synchronous generator control', 'state-feedback linearising controller', 'FLC', 'single-generator infinite-bus power system', 'adaptive control', 'control system analysis', 'convergence', 'feedback', 'linearisation techniques', 'nonlinear control systems', 'observers', 'perturbation techniques', 'stability', 'variable structure systems']","['controller/observer system', 'Nonlinear adaptive control', 'system states', 'state-feedback linearising controller', 'system parameter uncertainties', 'linearisable nonlinear systems', 'unmodeled system dynamics', 'nonlinear system model', 'system nonlinearities', 'full system']",901,141,23,900,140,10,2,1,8
"trust in online advice many people are now influenced by the information and advice they find on the internet, much of it of dubious quality. this article describes two studies concerned with those factors capable of influencing people's response to online advice. the first study is a qualitative account of a group of house-hunters attempting to find worthwhile information online. the second study describes a survey of more than 2,500 people who had actively sought advice over the internet. a framework for understanding trust in online advice is proposed in which first impressions are distinguished from more detailed evaluations. good web design can influence the first process, but three key factors-source credibility, personalization, and predictability-are shown to predict whether people actually follow the advice given ","Trust in online advice Many people are now influenced by the information and advice they find on the Internet, much of it of dubious quality. This article describes two studies concerned with those factors capable of influencing people's response to online advice. The first study is a qualitative account of a group of house-hunters attempting to find worthwhile information online. The second study describes a survey of more than 2,500 people who had actively sought advice over the Internet. A framework for understanding trust in online advice is proposed in which first impressions are distinguished from more detailed evaluations. Good Web design can influence the first process, but three key factors-source credibility, personalization, and predictability-are shown to predict whether people actually follow the advice given","['online advice trust', 'Internet', 'survey', 'online mortgage advice', 'Web design', 'source credibility', 'personalization', 'predictability', 'e-commerce', 'house buying advice', 'financial data processing', 'human factors', 'information resources', 'Internet', 'psychology', 'real estate data processing', 'social aspects of automation']","['online advice', 'worthwhile information online', 'internet A framework', 'peoples response', 'second study', 'first study', 'Many people', 'people', 'online', 'advice']",709,126,17,709,125,10,0,0,5
"hierarchical neuro-fuzzy quadtree models hybrid neuro-fuzzy systems have been in evidence during the past few years, due to its attractive combination of the learning capacity of artificial neural networks with the interpretability of the fuzzy systems. this article proposes a new hybrid neuro-fuzzy model, named hierarchical neuro-fuzzy quadtree (hnfq), which is based on a recursive partitioning method of the input space named quadtree. the article describes the architecture of this new model, presenting its basic cell and its learning algorithm. the hnfq system is evaluated in three well known benchmark applications: the sinc(x, y) function approximation, the mackey glass chaotic series forecast and the two spirals problem. when compared to other neuro-fuzzy systems, the hnfq exhibits competing results, with two major advantages it automatically creates its own structure and it is not limited to few input variables ","Hierarchical neuro-fuzzy quadtree models Hybrid neuro-fuzzy systems have been in evidence during the past few years, due to its attractive combination of the learning capacity of artificial neural networks with the interpretability of the fuzzy systems. This article proposes a new hybrid neuro-fuzzy model, named hierarchical neuro-fuzzy quadtree (HNFQ), which is based on a recursive partitioning method of the input space named quadtree. The article describes the architecture of this new model, presenting its basic cell and its learning algorithm. The HNFQ system is evaluated in three well known benchmark applications: the sinc(x, y) function approximation, the Mackey Glass chaotic series forecast and the two spirals problem. When compared to other neuro-fuzzy systems, the HNFQ exhibits competing results, with two major advantages it automatically creates its own structure and it is not limited to few input variables","['neuro-fuzzy systems', 'fuzzy systems', 'hierarchical neuro-fuzzy quadtree', 'quadtree', 'recursive partitioning', 'learning algorithm', 'Mackey Glass chaotic series', 'fuzzy neural nets', 'learning (artificial intelligence)', 'quadtrees']","['hierarchical neuro-fuzzy quatre', 'other neuro-fuzzy systems', 'past few years', 'neuro-fuzzy', 'new model', 'hq system', 'systems', 'model', 'fuzzy systems', 'quatre']",793,138,10,793,137,10,0,0,4
"using the small business innovation research program to turn your ideas into products the us government's small business innovation research program helps small businesses transform new ideas into commercial products. the program provides an ideal means for businesses and universities to obtaining funding for cooperative projects. rules and information for the program are readily available, and i will give a few helpful hints to provide guidance ","Using the Small Business Innovation Research Program to turn your ideas into products The US Government's Small Business Innovation Research Program helps small businesses transform new ideas into commercial products. The program, provides an ideal means for businesses and universities to obtaining funding for cooperative projects. Rules and information for the program are readily available, and | will give a few helpful hints to provide guidance","['Small Business Innovation Research Program', 'commercial product development', 'businesses', 'universities', 'funding', 'cooperative projects', 'US Government', 'USA', 'government policies', 'product development', 'research initiatives']","['Program', 'Innovation', 'Research', 'commercial products', 'small businesses', 'new ideas', 'Small', 'ideas', 'products', 'Business']",384,67,11,385,66,10,1,2,4
"friedberg numberings of families of n-computably enumerable sets we establish a number of results on numberings, in particular, on friedberg numberings, of families of d.c.e. sets. first, it is proved that there exists a friedberg numbering of the family of all d.c.e. sets. we also show that this result, patterned on friedberg's famous theorem for the family of all c.e. sets, holds for the family of all n-c.e. sets for any n > 2. second, it is stated that there exists an infinite family of d.c.e. sets without a friedberg numbering. third, it is shown that there exists an infinite family of c.e. sets (treated as a family of d.c.e. sets) with a numbering which is unique up to equivalence. fourth, it is proved that there exists a family of d.c.e. sets with a least numbering (under reducibility) which is friedberg but is not the only numbering (modulo reducibility) ","Friedberg numberings of families of n-computably enumerable sets We establish a number of results on numberings, in particular, on Friedberg numberings, of families of d.c.e. sets. First, itis proved that there exists a Friedberg numbering of the family of all d.c.e. sets. We also show that this result, patterned on Friedberg's famous theorem for the family of all c.e. sets, holds for the family of all n-c.e. sets for any n> 2. Second, it is stated that there exists an infinite family of d.ce. sets without a Friedberg numbering. Third, it is shown that there exists an infinite family of ce. sets (treated as a family of d.ce. sets) with a numbering which is unique up to equivalence. Fourth, it is proved that there exists a family of d.c.e. sets with a least numbering (under reducibility) which is Friedberg but is not the ‘only numbering (modulo reducibility)","['Friedberg numberings', 'infinite family', 'computability theory', 'families of n-computably enumerable sets', 'computability', 'number theory']","['freedberg numbering', 'd.c. sets', 'infinite family', 'c.e. sets', 'n-computably enumerable sets', 'numbering', 'least numbering', 'family', 'freedberg', 'c. sets']",725,150,6,723,147,10,352,120,1
the vibration reliability of poppet and contoured actuator valves the problem of selecting the shape of the actuator valve (the final control valve) itself is discussed; the solution to this problem will permit appreciable dynamic loads to be eliminated from the moving elements of the steam distribution system of steam turbines under all operating conditions ,The vibration reliability of poppet and contoured actuator valves The problem of selecting the shape of the actuator valve (the final control valve) itself is discussed; the solution to this problem will permit appreciable dynamic loads to be eliminated from the moving elements of the steam distribution system of steam turbines under all operating conditions,"['actuator valve shape selection', 'contoured actuator valves', 'poppet actuator valves', 'dynamic loads elimination', 'moving elements', 'steam distribution system', 'steam turbines', 'vibration reliability', 'actuators', 'reliability', 'steam turbines', 'valves', 'vibrations']","['problem', 'contoured actuators valves', 'appreciable dynamic loads', 'vibration reliability', 'final control valve', 'valve', 'actuators', 'vibration', 'contoured', 'actuators valve']",306,56,13,306,55,10,0,0,4
"matlab code for plotting ambiguity functions a matlab code capable of plotting ambiguity functions of many different radar signals is presented. the program makes use of matlab's sparse matrix operations, and avoids loops. the program could be useful as a pedagogical tool in radar courses teaching pulse compression ","MATLAB code for plotting ambiguity functions AMATLAB code capable of plotting ambiguity functions of many different radar signals is presented. The program makes use of MATLAB's sparse matrix operations, and avoids loops. The program could be useful as a pedagogical tool in radar courses teaching pulse compression","['MATLAB code', 'ambiguity functions plotting', 'radar signals', 'sparse matrix operations', 'pedagogical tool', 'radar courses', 'pulse compression', 'matched-filter response', 'Doppler-shifted signal version', 'functions', 'pulse compression', 'radar computing', 'radar signal processing', 'sparse matrices']","['ambiguity functions', 'sparse matrix operations', 'radar courses', 'malta code', 'capable', 'AMATLAB', 'code', 'malta', 'functions', 'ambiguity']",269,49,14,269,47,10,140,41,3
"the visible cement data set with advances in x-ray microtomography, it is now possible to obtain three-dimensional representations of a material's microstructure with a voxel size of less than one micrometer. the visible cement data set represents a collection of 3-d data sets obtained using the european synchrotron radiation facility in grenoble, france in september 2000. most of the images obtained are for hydrating portland cement pastes, with a few data sets representing hydrating plaster of paris and a common building brick. all of these data sets are being made available on the visible cement data set website at http://visiblecement.nist.gov. the website includes the raw 3-d datafiles, a description of the material imaged for each data set, example two-dimensional images and visualizations for each data set, and a collection of c language computer programs that will be of use in processing and analyzing the 3-d microstructural images. this paper provides the details of the experiments performed at the esrf, the analysis procedures utilized in obtaining the data set files, and a few representative example images for each of the three materials investigated ","The visible cement data set With advances in x-ray microtomography, it is now possible to obtain three-dimensional representations of a material's microstructure with a voxel size of less than one micrometer. The Visible Cement Data Set, represents a collection of 3-D data sets obtained using the European Synchrotron Radiation Facility in Grenoble, France in September 2000. Most of the images obtained are for hydrating portland cement pastes, with a few data sets representing hydrating Plaster of Paris and a ‘common building brick. All of these data sets are being made available ‘on the Visible Cement Data Set website at http://visiblecement nist gov. The website includes the raw 3-D datafiles, a description of the material imaged for each data set, ‘example two-dimensional images and visualizations for each data set, and a collection of C language computer programs that will be of use in processing and analyzing the 3-D microstructural images. This paper provides the details of the experiments performed at the ESRF, the analysis procedures utilized in obtaining the data set files, and a few representative example images for each of the three materials investigated","['X-ray microtomography', '3D representations', 'microstructure', 'voxel size', 'European Synchrotron Radiation Facility', 'hydrating portland cement pastes', 'Plaster of Paris', 'building brick', 'cement hydration', 'two-dimensional images', 'microstructural images', 'ESRF', 'cements (building materials)', 'crystal microstructure', 'image processing', 'Internet', 'measurement standards', 'nondestructive testing', 'synchrotron radiation', 'X-ray imaging', 'X-ray topography']","['data set', 'data', 'd-d microstructure images', 'materials microstructure', 'x-ray microtomography', 'data set files', 'few data sets', 'd-d data sets', 'visible cement data', 'Visible Cement Data']",998,183,21,1000,184,10,264,86,6
"connecting the business without busting the budget the ""multi-channel content delivery"" model (mccd) might be a new concept to you, but it is already beginning to replace traditional methods of business communications, print and content delivery, argues darren atkinson, cto, formscape ","Connecting the business without busting the budget The ""multi-channel content delivery"" model (MCCD) might be a new concept to you, but it is already beginning to replace traditional methods of business communications, print and content delivery, argues Darren Atkinson, CTO, FormScape","['multi-channel content delivery', 'FormScape', 'documents', 'distributed output management', 'business process management', 'archive', 'retrieval', 'content management', 'document handling', 'management information systems']","['content delivery', 'business communications print', 'traditional methods', 'multi-channel', 'new concept', 'budget', 'model', 'business', 'content', 'delivery']",245,42,10,245,41,10,0,0,1
"approximate confidence intervals for one proportion and difference of two proportions constructing a confidence interval for a binomial proportion or the difference of two proportions is a routine exercise in daily data analysis. the best-known method is the wald interval based on the asymptotic normal approximation to the distribution of the observed sample proportion, though it is known to have bad performance for small to medium sample sizes. agresti et al. (1998, 2000) proposed an adding-4 method: 4 pseudo-observations are added with 2 successes and 2 failures and then the resulting (pseudo-)sample proportion is used. the method is simple and performs extremely well. here we propose an approximate method based on a t-approximation that takes account of the uncertainty in estimating the variance of the observed (pseudo-)sample proportion. it follows the same line of using a t-test, rather than z-test, in testing the mean of a normal distribution with an unknown variance. for some circumstances our proposed method has a higher coverage probability than the adding-4 method ","Approximate confidence intervals for one proportion and difference of two proportions Constructing a confidence interval for a binomial proportion or the difference of two proportions is a routine exercise in daily data analysis. The best-known method is the Wald interval based on the asymptotic normal approximation to the distribution of the observed sample proportion, though it is known to have bad performance for small to medium sample sizes. Agresti et al. (1998, 2000) proposed an Adding-4 method: 4 pseudo-observations are added with 2 successes and 2 failures and then the resulting (pseudo-)sample proportion is used. The method is simple and performs extremely well. Here we propose an approximate method based on a t-approximation that takes account of the uncertainty in estimating the variance of the observed (pseudo-)sample proportion. It follows the same line of using a test, rather than z-test, in testing the mean of a normal distribution with an unknown variance. For some circumstances our proposed method has a higher coverage probability than the Adding-4 method","['approximate confidence intervals', 'binomial proportion', 'difference of two proportions', 'data analysis', 't-approximation', 'uncertainty', 'variance estimation', 't-test', 'normal distribution', 'coverage probability', 'pseudo-sample proportion', 'approximation theory', 'binomial distribution', 'data analysis', 'normal distribution', 'sampling methods']","['sample proportion', 'proportions', 'Approximate confidence intervals', 'asymptotic normal approximation', 'binomial proportion', 'approximate method', 'best-known method', 'wall interval', 'interval', 'confidence interval']",924,168,16,922,167,10,4,1,6
"simple nonlinear dual-window operator for edge detection we propose a nonlinear edge detection technique based on a two-concentric-circular-window operator. we perform a preliminary selection of edge candidates using a standard gradient and use the dual-window operator to reveal edges as zero-crossing points of a simple difference function depending only on the minimum and maximum values in the two windows. comparisons with other well-established techniques are reported in terms of visual appearance and computational efficiency. they show that detected edges are surely comparable with canny's and laplacian of gaussian algorithms, with a noteworthy reduction in terms of computational load ","Simple nonlinear dual-window operator for edge detection We propose a nonlinear edge detection technique based on a two-concentric-circular-window operator. We perform a preliminary selection of edge candidates using a standard gradient and use the ‘dual-window operator to reveal edges as zero-crossing points of a simple difference function depending only on the minimum and maximum, values in the two windows. Comparisons with other well-established techniques are reported in terms of visual appearance and computational efficiency. They show that detected edges are surely comparable with Canny's and Laplacian of Gaussian algorithms, with a noteworthy reduction in terms of computational load","['nonlinear dual-window operator', 'edge detection', 'nonlinear edge detection technique', 'two-concentric-circular-window operator', 'standard gradient', 'dual window operator', 'zero-crossing points', 'difference function', 'minimum values', 'maximum values', 'computational efficiency', 'detected edges', 'Laplacian algorithms', 'Gaussian algorithms', ""Canny's algorithms"", 'computational load', 'nonlinear processing', 'edge detection', 'Laplace equations', 'nonlinear optics']","['dual-window operator', 'edge detection', 'nonlinear', 'two-concentric-circular-window operator', 'other well-established techniques', 'edge candidates', 'edge', 'operator', 'detection', 'dual-window']",599,99,20,601,98,10,11,2,8
"evolution complexity of the elementary cellular automaton rule 18 cellular automata are classes of mathematical systems characterized by discreteness (in space, time, and state values), determinism, and local interaction. using symbolic dynamical theory, we coarse-grain the temporal evolution orbits of cellular automata. by means of formal languages and automata theory, we study the evolution complexity of the elementary cellular automaton with local rule number 18 and prove that its width 1-evolution language is regular, but for every n >or= 2 its width n-evolution language is not context free but context sensitive ","Evolution complexity of the elementary cellular automaton rule 18 Cellular automata are classes of mathematical systems characterized by discreteness (in space, time, and state values), determinism, and local interaction. Using symbolic dynamical theory, we coarse-grain the temporal evolution orbits of cellular automata. By means of formal languages and automata theory, we study the evolution complexity of the elementary cellular automaton with local rule number 18 and prove that its width 1-evolution language is regular, but for every n >or= 2 its width n-evolution language is not context free but context sensitive","['cellular automata', 'symbolic dynamical theory', 'formal languages', 'complexity', 'evolution complexity', 'elementary cellular automaton', 'cellular automata', 'computational complexity', 'evolutionary computation']","['elementary cellular automaton', 'width revolution language', 'temporal evolution orbits', 'local rule number', 'automata theory', 'cellular', 'cellular automata', 'Cellular automata', 'evolution complexity', 'Evolution complexity']",533,92,9,533,91,10,0,0,1
"mustering motivation to enact decisions: how decision process characteristics influence goal realization decision scientists tend to focus mainly on decision antecedents, studying how people make decisions. action psychologists, in contrast, study post-decision issues, investigating how decisions, once formed, are maintained, protected, and enacted. through the research presented here, we seek to bridge these two disciplines, proposing that the process by which decisions are reached motivates subsequent pursuit and benefits eventual realization. we identify three characteristics of the decision process (dp) as having motivation-mustering potential: dp effort investment, dp importance, and dp confidence. through two field studies tracking participants' decision processes, pursuit and realization, we find that after controlling for the influence of the motivational mechanisms of goal intention and implementation intention, the three decision process characteristics significantly influence the successful enactment of the chosen decision directly. the theoretical and practical implications of these findings are considered and future research opportunities are identified ","Mustering motivation to enact decisions: how decision process characteristics influence goal realization Decision scientists tend to focus mainly on decision antecedents, studying how people make decisions. Action psychologists, in contrast, study post-decision issues, investigating how decisions, once formed, are maintained, protected, and enacted. Through the research presented here, we seek to bridge these two disciplines, proposing that the process by which decisions are reached motivates subsequent pursuit and benefits eventual realization. We identify three characteristics of the decision process (DP) as having motivation-mustering potential: DP effort investment, DP importance, and DP confidence. Through two field studies tracking participants’ decision processes, pursuit and realization, we find that after controlling for the influence of the motivational mechanisms of goal intention and implementation intention, the three decision process characteristics significantly influence the successful enactment of the chosen decision directly. The theoretical and practical implications of these findings are considered and future research opportunities are identified","['decision enactment', 'motivation', 'goal realization', 'decision process characteristics', 'action psychologists', 'post-decision issues', 'motivation-mustering potential', 'decision process investment', 'decision process importance', 'decision process confidence', 'goal intention', 'research opportunities', 'decision scientists', 'behavioural sciences', 'decision theory', 'human resource management']","['decision', 'decision process characteristics', 'decisions Action psychologists', 'motivational mechanisms', 'participants decision', 'eventual realization', 'decision process gdp', 'decision antecedents', 'goal intention', 'process']",1033,153,16,1033,152,10,1,1,4
"the canonical dual frame of a wavelet frame we show that there exist wavelet frames that have nice dual wavelet frames, but for which the canonical dual frame does not consist of wavelets, i.e., cannot be generated by the translates and dilates of a single function ","The canonical dual frame of a wavelet frame We show that there exist wavelet frames that have nice dual wavelet frames, but for which the canonical dual frame does not consist of wavelets, Le., cannot be generated by the translates and dilates of a single function","['canonical dual frame', 'wavelet frame', 'Gabor frames', 'multiresolution hierarchy', 'compact support', 'wavelet transforms']","['canonical dual frame', 'wavelets frames', 'single function', 'translates', 'cannon', 'nice', 'frame', 'dual', 'wavelets', 'canonical']",220,47,6,219,46,10,4,1,0
"process planning for reliable high-speed machining of moulds a method of generating nc programs for the high-speed milling of moulds is investigated. forging dies and injection moulds, whether plastic or aluminium, have a complex surface geometry. in addition they are made of steels of hardness as much as 30 or even 50 hrc. since 1995, high-speed machining has been much adopted by the die-making industry, which with this technology can reduce its use of sinking electrodischarge machining (sedm). edm, in general, calls for longer machining times. the use of high-speed machining makes it necessary to redefine the preliminary stages of the process. in addition, it affects the methodology employed in the generation of nc programs, which requires the use of high-level cam software. the aim is to generate error-free programs that make use of optimum cutting strategies in the interest of productivity and surface quality. the final result is a more reliable manufacturing process. there are two risks in the use of high-speed milling on hardened steels. one of these is tool breakage, which may be very costly and may furthermore entail marks on the workpiece. the other is collisions between the tool and the workpiece or fixtures, the result of which may be damage to the ceramic bearings in the spindles. in order to minimize these risks it is necessary that new control and optimization steps be included in the cam methodology. there are three things that the firm adopting high-speed methods should do. it should redefine its process engineering, it should systematize access by its cam programmers to high-speed knowhow, and it should take up the use of process simulation tools. in the latter case, it will be very advantageous to use tools for the estimation of cutting forces. the new work methods proposed in this article have made it possible to introduce high speed milling (hsm) into the die industry. examples are given of how the technique has been applied with cam programming re-engineered as here proposed, with an explanation of the novel features and the results ","Process planning for reliable high-speed machining of moulds ‘A method of generating NC programs for the high-speed milling of moulds is investigated. Forging dies and injection moulds, whether plastic or aluminium, have a complex surface geometry. In addition they are made of steels of hardness as much as 30 or even 50 HRC. Since 1995, high-speed machining has been much adopted by the die-making industry, which with this technology can reduce its use of Sinking Electrodischarge Machining (SEDM). EDM, in general, calls for longer machining times. The use of high-speed machining makes it necessary to redefine the preliminary stages of the process. In addition, it affects the methodology employed in the generation of NC programs, which requires the use of high-level CAM software. The aim is to generate error-free programs that make use of optimum cutting strategies in the interest of productivity and surface quality. The final result is a more reliable manufacturing process. There are two risks in the use of high-speed milling on hardened steels. One of these is tool breakage, which may be very costly and may furthermore entail marks on the workpiece. The other is collisions between the tool and the workpiece or fixtures, the result of which may be damage to the ceramic bearings in the spindles. in order to minimize these risks it is necessary that new control and optimization steps be included in the CAM methodology. There are three things that the firm adopting high-speed methods should do. It should redefine its process engineering, it should systematize access by its CAM programmers to high-speed knowhow, and it should take Up the use of process simulation tools. In the latter case, it will be very advantageous to use tools for the estimation of cutting forces. The new work methods proposed in this article have made it possible to introduce high speed milling (HSM) into the die industry. Examples are given of how the technique has been applied with CAM programming re-engineered as here proposed, with an explanation of the novel features and the results","['moulds', 'reliable high-speed machining', 'process planning', 'NC programs', 'high-speed milling', 'forging dies', 'injection moulds', 'complex surface geometry', 'error-free programs', 'optimum cutting strategies', 'productivity', 'surface quality', 'hardened steels', 'tool breakage', 'tool workpiece collisions', 'ceramic bearings', 'CAM methodology', 'process engineering redefinition', 'process simulation tools', 'CAM programming re-engineering', 'cutting strategies', 'CAD/CAM', 'computer aided production planning', 'computerised numerical control', 'cutting', 'forging', 'machining', 'moulding', 'systems re-engineering']","['high-speed', 'high-speed milling', 'reliable manufacturing process', 'reliable high-speed machining', 'CAM programming re-engineer', 'longer machining times', 'high-speed methods', 'high-speed knowhow', 'injection moulds', 'high-speed machining']",1750,341,29,1751,340,10,1,1,13
estimation of the poisson stream intensity in a multilinear queue with an exponential job queue decay times the busy queue periods start are found for a multilinear queue with an exponential job queue decay and uniform resource allocation to individual servers. the stream intensity and the average job are estimated from observations of the times the queue busy periods start ,Estimation of the Poisson stream intensity in a multilinear queue with an exponential job queue decay Times the busy queue periods start are found for a multilinear queue with an exponential job queue decay and uniform resource allocation to individual servers. The stream intensity and the average job are estimated from observations of the times the queue busy periods start,"['Poisson stream intensity', 'multilinear queue', 'exponential job queue decay', 'busy queue periods start', 'uniform resource allocation', 'stream intensity', 'individual servers', 'queueing theory', 'resource allocation', 'stochastic processes']","['multilinear queue', 'Poisson stream intensity', 'busy queue periods', 'busy periods', 'queue', 'average job', 'stream', 'job', 'intensity', 'stream intensity']",317,61,10,317,60,10,0,0,3
"evolution of litigation support systems for original paper see ibid., vol. 12, no. 6: ""the e-mail of the species"". the author responds to that paper and argues that printing, scanning and imaging e-mails or other electronic (rather than paper) documents prior to listing and disclosure seems to be unnecessary, not 'proportionate' (from a costs point of view) and not particularly helpful, to either side. he asks how litigation support systems might evolve to help and support the legal team in their task ","Evolution of litigation support systems For original paper see ibid., vol. 12, no. 6: ""The E-mail of the Species"". The author responds to that paper and argues that printing, scanning and imaging E-mails or other electronic (rather than paper) documents prior to listing and disclosure seems to be unnecessary, not ‘proportionate’ (from a costs point of view) and not particularly helpful, to either side. He asks how litigation support systems might evolve to help and support the legal team in their task","['litigation support systems', 'E-mail', 'legal team', 'electronic mail', 'law administration']","['litigation support systems', 'paper documents', 'original paper', 'imaging e-mail', 'legal team', 'Evolution', 'paper', 'support', 'systems', 'litigation']",425,83,5,425,82,10,2,1,0
"extracting straight road structure in urban environments using ikonos satellite imagery we discuss a fully automatic technique for extracting roads in urban environments. the method has its bases in a vegetation mask derived from multispectral ikonos data and in texture derived from panchromatic ikonos data. these two techniques together are used to distinguish road pixels. we then move from individual pixels to an object-based representation that allows reasoning on a higher level. recognition of individual segments and intersections and the relationships among them are used to determine underlying road structure and to then logically hypothesize the existence of additional road network components. we show results on an image of san diego, california. the object-based processing component may be adapted to utilize other basis techniques as well, and could be used to build a road network in any scene having a straight-line structured topology ","Extracting straight road structure in urban environments using IKONOS satellite imagery We discuss a fully automatic technique for extracting roads in urban environments. The method has its bases in a vegetation mask derived from multispectral IKONOS data and in texture derived from panchromatic IKONOS data. These two techniques together are used to distinguish road pixels. We then move from individual pixels to an object-based representation that allows reasoning on a higher level. Recognition of individual segments and intersections and the relationships among them are used to determine underlying road structure and to then logically hypothesize the existence of additional road network components. We show results on an image of San Diego, California. The object-based processing component may be adapted to utilize other basis techniques as well, and could be used to build a road network in any scene having a straight-line structured topology","['straight road structure', 'urban environments', 'IKONOS satellite imagery', 'fully automatic technique', 'vegetation mask', 'texture', 'panchromatic IKONOS data', 'road pixels', 'object-based representation', 'higher level reasoning', 'individual segment recognition', 'road network components', 'San Diego', 'object-based processing component', 'straight-line structured topology', 'high-resolution imagery', 'large-scale feature extraction', 'vectorized road network', 'civil engineering computing', 'feature extraction', 'image classification', 'image representation', 'image texture', 'remote sensing']","['urban environments', 'road network', 'straight-line structured topology', 'multispectral nikonos data', 'panchromatic nikonos data', 'nikonos satellite imagery', 'straight road structure', 'road pixels', 'roads', 'road structure']",814,144,24,814,143,10,0,0,6
"transcripts: bane or boon? [law reporting] because judge-made law, by its very nature, is less immediately accessible than the law of codified, statutory systems, it calls for an efficient system of law reporting. of necessity, any such system will be selective, the majority of decisions going unreported. considerable power thereby comes to repose in the hands of the law reporters. the author shares his invaluable perception and extensive research on the difficulties which arise from the excess of access to judgments ","Transcripts: bane or boon? [law reporting] Because judge-made law, by its very nature, is less immediately accessible than the law of codified, statutory systems, it calls for an efficient system of law reporting. Of necessity, any such system will be selective, the majority of decisions going unreported. Considerable power thereby comes to repose in the hands of the law reporters. The author shares his invaluable perception and extensive research on the difficulties which arise from the excess of access to judgments","['transcripts', 'law reporting', 'judge-made law', 'judgments', 'information resources', 'law administration']","['statutory systems', 'transcripts bane', 'efficient system', 'judge-made law', 'law reporters', 'law', 'very nature', 'such system', 'boon law', 'system']",442,82,6,442,81,10,0,0,1
"identification of linear parameter varying models we consider identification of a certain class of discrete-time nonlinear systems known as linear parameter varying system. we assume that inputs, outputs and the scheduling parameters are directly measured, and a form of the functional dependence of the system coefficients on the parameters is known. we show how this identification problem can be reduced to a linear regression, and provide compact formulae for the corresponding least mean square and recursive least-squares algorithms. we derive conditions on persistency of excitation in terms of the inputs and scheduling parameter trajectories when the functional dependence is of polynomial type. these conditions have a natural polynomial interpolation interpretation, and do not require the scheduling parameter trajectories to vary slowly. this method is illustrated with a simulation example using two different parameter trajectories ","Identification of linear parameter varying models We consider identification of a certain class of discrete-time nonlinear systems known as linear parameter varying system. We assume that, inputs, outputs and the scheduling parameters are directly measured, and a form of the functional dependence of the system coefficients on the parameters is known. We show how this identification problem can be reduced to a linear regression, and provide compact formulae for the corresponding least mean square and recursive least-squares algorithms. We derive conditions on persistency of excitation in terms of the inputs and scheduling parameter trajectories when the functional dependence is of polynomial type. These conditions have a natural polynomial interpolation interpretation, and do not require the scheduling parameter trajectories to vary slowly. This method is illustrated with a simulation example using two different parameter trajectories","['linear parameter varying models', 'identification', 'discrete-time nonlinear systems', 'scheduling parameters', 'functional dependence', 'system coefficients', 'linear regression', 'least mean square algorithms', 'recursive least-squares algorithms', 'persistency of excitation conditions', 'scheduling parameter trajectories', 'polynomial interpolation interpretation', 'parameter trajectories', 'time-varying systems', 'discrete time systems', 'least mean squares methods', 'least squares approximations', 'nonlinear control systems', 'parameter estimation', 'polynomial approximation', 'recursive estimation', 'statistical analysis', 'time-varying systems']","['scheduling parameter trajectories', 'linear parameter', 'different parameter trajectories', 'discrete-time nonlinear systems', 'identification problem', 'scheduling parameters', 'system coefficients', 'linear regression', 'linear', 'parameters']",813,135,23,814,134,10,0,1,3
"airline base schedule optimisation by flight network annealing a system for rigorous airline base schedule optimisation is described. the architecture of the system reflects the underlying problem structure. the architecture is hierarchical consisting of a master problem for logical aircraft schedule optimisation and a sub-problem for schedule evaluation. the sub-problem is made up of a number of component sub-problems including connection generation, passenger choice modelling, passenger traffic allocation by simulation and revenue and cost determination. schedule optimisation is carried out by means of simulated annealing of flight networks. the operators for the simulated annealing process are feasibility preserving and form a complete set of operators ","Airline base schedule optimisation by flight network annealing Assystem for rigorous airline base schedule optimisation is described. The architecture of the system reflects the underlying problem structure. The architecture is hierarchical consisting of a master problem for logical aircraft schedule optimisation and a sub-problem for schedule evaluation. The sub-problem is made up of a number of component sub-problems including connection generation, passenger choice modelling, passenger traffic allocation by simulation and revenue and cost determination. Schedule optimisation is carried out by means of simulated annealing of flight networks. The operators for the simulated annealing process are feasibility preserving and form a complete set of operators","['airline base schedule optimisation', 'flight network annealing', 'system architecture', 'hierarchical architecture', 'master problem', 'logical aircraft schedule optimisation', 'schedule evaluation', 'connection generation', 'passenger choice modelling', 'passenger traffic allocation', 'cost determination', 'simulated annealing', 'operators', 'time complexity', 'air traffic', 'mathematical operators', 'scheduling', 'simulated annealing', 'travel industry']","['optimisation', 'simulated annealing process', 'component sub-problems', 'schedule evaluation', 'problem structure', 'base', 'flight networks', 'master problem', 'schedule', 'simulated annealing']",661,106,19,662,104,10,367,96,5
"the ultimate control group empirical research on the organization of firms requires that firms be classified on the basis of their control structures. this should be done in a way that can potentially be made operational. it is easy to identify the ultimate controller of a hierarchical organization, and the literature has largely focused on this case. however, many organizational structures mix hierarchy with collective choice procedures such as voting, or use circular structures under which superiors are accountable to their subordinates. the author develops some analytic machinery that can be used to map the authority structures of such organizations, and show that under mild restrictions there is a well-defined ultimate control group. the results are consistent with intuitions about the nature of control in familiar economic settings ","The ultimate control group Empirical research on the organization of firms requires that firms be classified on the basis of their control structures. This should be done in a way that can potentially be made operational. It is easy to identify the ultimate controller of a hierarchical organization, and the literature has largely focused on this case. However, many organizational structures mix hierarchy with collective choice procedures such as voting, or use circular structures under which superiors are accountable to their subordinates. The author develops some analytic machinery that can be used to map the authority structures of such organizations, and show that under mild restrictions there is a well-defined ultimate control group. The results are consistent with intuitions about the nature of control in familiar economic settings","['ultimate control group', 'hierarchical organization', 'organizational structures', 'authority structures', 'committees', 'control rights', 'firm organization', 'management', 'management science', 'set theory']","['ultimate control group', 'control', 'many organizational structures', 'hierarchical organization', 'authority structures', 'ultimate controller', 'circular structures', 'such organizations', 'control structures', 'ultimate']",721,129,10,721,128,10,0,0,0
"ethnography, customers, and negotiated interactions at the airport in the late 1990s, tightly coordinated airline schedules unraveled owing to massive delays resulting from inclement weather, overbooked flights, and airline operational difficulties. as schedules slipped, the delayed departures and late arrivals led to systemwide breakdowns, customers missed their connections, and airline work activities fell further out of sync. in offering possible answers, we emphasize the need to consider the customer as participant, following the human-centered computing model. our study applied ethnographic methods to understand the airline system domain and the nature of airline delays, and it revealed the deficiencies of the airline production system model of operations. the research insights that led us to shift from a production and marketing system perspective to a customer-as-participant view might appear obvious to some readers. however, we do not know of any airline that designs its operations and technologies around any other model than the production and marketing system view. our human-centered analysis used ethnographic methods to gather information, offering new insight into airline delays and suggesting effective ways to improve operations reliability ","Ethnography, customers, and negotiated interactions at the airport In the late 1990s, tightly coordinated airline schedules unraveled owing to massive delays resulting from inclement weather, overbooked flights, and airline operational difficulties. As schedules slipped, the delayed departures and late arrivals led to systemwide breakdowns, customers missed their connections, and airline work activities fell further out of sync. In offering possible answers, we emphasize the need to consider the customer as participant, following the human-centered ‘computing model. Our study applied ethnographic methods to understand the airline system domain and the nature of airline delays, and it revealed the deficiencies of the airline production system model of operations. The research insights that led us to shift from a production and marketing system perspective to a customer-as-participant view might appear obvious to some readers. However, we do not know of any airline that designs its operations and technologies around any other model than the production and marketing system view. Our human-centered analysis used ethnographic methods to. gather information, offering new insight into airline delays and suggesting effective ways to improve operations reliability","['human-centered computing model', 'customer trajectories', 'airports', 'employees', 'ethnography', 'negotiated interactions', 'airline delays', 'airline production system operations model', 'customer-as-participant view', 'operations reliability', 'airports', 'marketing data processing', 'negotiation support systems', 'personnel', 'reliability', 'travel industry', 'user centred design']","['airline', 'airline delays', 'system-wide breakdowns customers', 'airline work activities', 'marketing system view', 'ethnography customers', 'airline system domain', 'airline schedules', 'late arrivals', 'customer']",1095,180,17,1097,179,10,9,2,4
"ride quality evaluation of an actively-controlled stretcher for an ambulance this study considers the subjective evaluation of ride quality during ambulance transportation using an actively-controlled stretcher (acs). the ride quality of a conventional stretcher and an assistant driver's seat is also compared. braking during ambulance transportation generates negative foot-to-head acceleration in patients and causes blood pressure to rise in the patient's head. the acs absorbs the foot-to-head acceleration by changing the angle of the stretcher, thus reducing the blood pressure variation. however, the ride quality of the acs should be investigated further because the movement of the acs may cause motion sickness and nausea. experiments of ambulance transportation, including rapid acceleration and deceleration, are performed to evaluate the effect of differences in posture of the transported subject on the ride quality; the semantic differential method and factor analysis are used in the investigations. subjects are transported using a conventional stretcher with head forward, a conventional stretcher with head backward, the acs, and an assistant driver's seat for comparison with transportation using a stretcher. experimental results show that the acs gives the most comfortable transportation when using a stretcher. moreover, the reduction of the negative foot-to-head acceleration at frequencies below 0.2 hz and the small variation of the foot-to-head acceleration result in more comfortable transportation. conventional transportation with the head forward causes the worst transportation, although the characteristics of the vibration of the conventional stretcher seem to be superior to that of the acs ","Ride quality evaluation of an actively-controlled stretcher for an ambulance This study considers the subjective evaluation of ride quality during ambulance transportation using an actively-controlled stretcher (ACS). The ride quality of a conventional stretcher and an assistant driver's seat is also compared. Braking during ambulance transportation generates negative foot-to-head acceleration in patients and causes blood pressure to rise in the patient's head. The ACS absorbs the foot-to-head acceleration by changing the angle of the stretcher, thus reducing the blood pressure variation. However, the ride quality of the ACS should be investigated further because the movement of the ACS may cause motion sickness and nausea. Experiments of ambulance transportation, including rapid acceleration and deceleration, are performed to evaluate the effect of differences in posture of the transported subject on the ride quality; the semantic differential method and factor analysis are used in the investigations. Subjects are transported using a conventional stretcher with head forward, a conventional stretcher with head backward, the ACS, and an assistant driver's seat for comparison with transportation using a stretcher. Experimental results show that the ACS gives the most comfortable transportation when using a stretcher. Moreover, the reduction of the negative foot-to-head acceleration at frequencies below 0.2 Hz and the small variation of the foot-to-head acceleration result in more comfortable transportation. Conventional transportation with the head forward causes the worst transportation, although the characteristics of the vibration of the conventional stretcher seem to be superior to that of the ACS","['actively-controlled stretcher', 'ambulance', 'ride quality evaluation', 'subjective evaluation', 'ambulance transportation', 'conventional stretcher', 'assistant driver seat', 'braking', 'negative foot-to-head acceleration', 'blood pressure variation', 'patient head', 'stretcher angle', 'motion sickness', 'nausea', 'rapid acceleration', 'rapid deceleration', 'posture differences', 'transported subject', 'semantic differential method', 'factor analysis', 'head forward', 'head backward', 'comfortable transportation', 'vibration', 'braking', 'haemodynamics', 'patient care', 'position control', 'road vehicles']","['conventional stretcher', 'ambulance transportation', 'negative foot-to-head acceleration', 'actively-controlled stretcher', 'foot-to-head acceleration result', 'more comfortable transportation', 'stretcher', 'Ride quality evaluation', 'quality', 'ride quality']",1486,244,29,1486,243,10,0,0,11
"repeated games with lack of information on one side: the dual differential approach we introduce the dual differential game of a repeated game with lack of information on one side as the natural continuous time version of the dual game introduced by de meyer (1996). a traditional way to study the value of differential games is through discrete time approximations. here, we follow the opposite approach: we identify the limit value of a repeated game in discrete time as the value of a differential game. namely, we use the recursive structure for the finitely repeated version of the dual game to construct a differential game for which the upper values of the uniform discretization satisfy precisely the same property. the value of the dual differential game exists and is the unique viscosity solution of a first-order derivative equation with a limit condition. we identify the solution by translating viscosity properties in the primal ","Repeated games with lack of information on one side: the dual differential approach We introduce the dual differential game of a repeated game with lack of information on one side as the natural continuous time version of the dual game introduced by De Meyer (1996). A traditional way to study the value of differential games is through discrete time approximations. Here, we follow the opposite approach: We identify the limit value of a repeated game in discrete time as the value of a differential game. Namely, we use the recursive structure for the finitely repeated version of the dual game to construct a differential game for which the upper values of the uniform discretization satisty precisely the same property. The value of the dual differential game exists and is the unique viscosity solution of a first-order derivative equation with a limit condition. We identify the solution by translating viscosity properties in the primal","['repeated games', 'dual differential game', 'repeated game', 'discrete time approximations', 'limit value', 'discrete time', 'viscosity solution', 'limit condition', 'differential games', 'duality (mathematics)']","['game', 'dual differential game', 'dual game', 'discrete time approximations', 'differential', 'dual differential approach', 'Repeated games', 'limit value', 'dual', 'differential game']",791,154,10,791,153,10,1,1,1
"edison's direct current influenced ""broadway"" show lighting during the early decades of the 20 th century, midtown manhattan in new york city developed an extensive underground direct current (dc) power distribution system. this was a result of the original introduction of direct current by thomas edison's pioneering pearl street station in 1882. the availability of dc power in the theater district, led to the perpetuation of an archaic form of stage lighting control through nearly three-quarters of the 20 th century. this control device was known as a ""resistance dimmer."" it was essentially a series-connected rheostat, but it was wound with a special resistance ""taper"" so as to provide a uniform change in the apparent light output of typical incandescent lamps throughout the travel of its manually operated arm. the development and use of dc powered stage lighting is discussed in this article ","Edison's direct current influenced ""Broadway"" show lighting During the early decades of the 20 th century, midtown Manhattan in New York City developed an extensive underground direct current (DC) power distribution system. This was a result of the original introduction of direct current by Thomas Edison's pioneering Pearl Street Station in 1882. The availability of DC power in the theater district, led to the perpetuation of an archaic form of stage lighting control through nearly three-quarters of the 20 th century. This control device was known as a ""resistance dimmer."" It was essentially a series-connected rheostat, but it was wound with a special resistance ""taper"" so as to provide a uniform change in the apparent light output of typical incandescent lamps throughout the travel of its manually operated arm. The development and use of DC powered stage lighting is discussed in this article","['Broadway show lighting', 'Manhattan', 'New York City', 'underground direct current power distribution system', ""Thomas Edison's Pearl Street Station"", 'theater district', 'stage lighting control', 'resistance dimmer', 'series-connected rheostat', 'resistance taper', 'apparent light output', 'incandescent lamps', 'DC powered stage lighting', 'DC power transmission', 'history', 'lighting control']","['current', 'direct', 'century midtown Manhattan', 'stage lighting control', 'broadway show lighting', 'apparent light output', 'Thomas edison', 'New York City', 'edison', 'stage lighting']",763,144,16,763,143,10,0,0,4
web services boost integration microsoft and ibm have announced products to help their database software co-exist with competitors' offerings. the products use web services technology allowing users to improve integration between databases and application software from rival vendors ,Web services boost integration Microsoft and IBM have announced products to help their database software co-exist with competitors’ offerings. The products use web services technology allowing users to improve integration between databases and application software from rival vendors,"['web services technology', 'Microsoft', 'IBM', 'database software', 'database management systems', 'information resources']","['products', 'web services technology', 'integration Microsoft', 'competitors offerings', 'application software', 'database software', 'services', 'database', 'integration', 'Web services']",246,39,6,246,38,10,1,1,1
"limitations of delayed state feedback: a numerical study stabilization of a class of linear time-delay systems can be achieved by a numerical procedure, called the continuous pole placement method [michiels et al., 2000]. this method can be seen as an extension of the classical pole placement algorithm for ordinary differential equations to a class of delay differential equations. in [michiels et al., 2000] it was applied to the stabilization of a linear time-invariant system with an input delay using static state feedback. in this paper we study the limitations of such delayed state feedback laws. more precisely we completely characterize the class of stabilizable plants in the 2d-case. for that purpose we make use of numerical continuation techniques. the use of delayed state feedback in various control applications and the effect of its limitations are briefly discussed ","Limitations of delayed state feedback: a numerical study Stabilization of a class of linear time-delay systems can be achieved by a numerical procedure, called the continuous pole placement method [Michiels et al., 2000]. This method can be seen as an extension of the classical pole placement algorithm for ordinary differential equations to a class of delay differential equations. In [Michiels et al., 2000] it was applied to the stabilization of a linear time-invariant system with an input delay using static state feedback. In this paper we study the limitations of such delayed state feedback laws. More precisely we ‘completely characterize the class of stabilizable plants in the 2D-case. For that purpose we make use of numerical continuation techniques. The use of delayed state feedback in various control applications and the effect of its limitations are briefly discussed","['linear time-delay systems', 'continuous pole placement method', 'delay differential equations', 'static state feedback', 'delayed state feedback', 'numerical continuation', 'delay-differential systems', 'differential equations', 'iterative methods', 'state feedback']","['numerical continuation techniques', 'numerical study Stabilization', 'linear time-invariant system', 'linear time-delay systems', 'static state feedback', 'state feedback laws', 'numerical procedure', 'state feedback', 'feedback', 'state']",749,138,10,750,137,10,10,1,3
"dynamics of the firing probability of noisy integrate-and-fire neurons cortical neurons in vivo undergo a continuous bombardment due to synaptic activity, which acts as a major source of noise. we investigate the effects of the noise filtering by synapses with various levels of realism on integrate-and-fire neuron dynamics. the noise input is modeled by white (for instantaneous synapses) or colored (for synapses with a finite relaxation time) noise. analytical results for the modulation of firing probability in response to an oscillatory input current are obtained by expanding a fokker-planck equation for small parameters of the problem-when both the amplitude of the modulation is small compared to the background firing rate and the synaptic time constant is small compared to the membrane time constant. we report the detailed calculations showing that if a synaptic decay time constant is included in the synaptic current model, the firing-rate modulation of the neuron due to an oscillatory input remains finite in the high-frequency limit with no phase lag. in addition, we characterize the low-frequency behavior and the behavior of the high-frequency limit for intermediate decay times. we also characterize the effects of introducing a rise time to the synaptic currents and the presence of several synaptic receptors with different kinetics. in both cases, we determine, using numerical simulations, an effective decay time constant that describes the neuronal response completely ","Dynamics of the firing probability of noisy integrate-and-fire neurons Cortical neurons in vivo undergo a continuous bombardment due to synaptic activity, which acts as a major source of noise. We investigate the effects of the noise filtering by synapses with various levels of realism on integrate-and-fire neuron dynamics. The noise input is modeled by white (for instantaneous synapses) or colored (for synapses with a finite relaxation time) noise. Analytical results for the Modulation of firing probability in response to an oscillatory input current are obtained by expanding a Fokker-Pianck equation for small parameters of the problem-when both the amplitude of the modulation is small compared to the background firing rate and the synaptic time constant is small compared to the membrane time constant. We report the detailed calculations showing that if a synaptic decay time constant is included in the synaptic current model, the firing-rate modulation of the neuron due to an oscillatory input remains finite in the high-frequency limit with no phase lag. In addition, we characterize the low-frequency behavior and the behavior of the high-frequency limit for intermediate decay times. We also characterize the effects of introducing a rise time to the synaptic currents and the presence of several synaptic receptors with different kinetics. In both cases, we determine, using numerical simulations, an effective decay time constant that describes the neuronal response completely","['firing probability', 'noisy integrate-and-fire neurons', 'cortical neurons', 'synaptic activity', 'noise filtering', 'white noise', 'colored noise', 'Fokker-Planck equation', 'synaptic time constant', 'membrane time constant', 'phase lag', 'synaptic receptors', 'numerical simulation', 'bioelectric potentials', 'Fokker-Planck equation', 'neural nets', 'neurophysiology', 'noise', 'probability']","['firing probability', 'integrate-and-fire neuron dynamics', 'noisy integrate-and-fire neurons', 'intermediate decay times', 'synaptic current model', 'effective decay time', 'synaptic decay time', 'neuronal response', 'Cortical neurons', 'synaptic time']",1274,226,19,1274,225,10,1,1,6
"mpeg-4 video object-based rate allocation with variable temporal rates in object-based coding, bit allocation is performed at the object level and temporal rates of different objects may vary. the proposed algorithm deals with these two issues when coding multiple video objects (mvos). the proposed algorithm is able to successfully achieve the target bit rate, effectively code arbitrarily shaped mvos with different temporal rates, and maintain a stable buffer level ","MPEG-4 video object-based rate allocation with variable temporal rates. In object-based coding, bit allocation is performed at the object level and temporal rates of different objects may vary. The proposed algorithm, deals with these two issues when coding multiple video objects (MVOs), The proposed algorithm is able to successfully achieve the target bit rate, effectively code arbitrarily shaped MVOs with different temporal rates, and maintain a stable buffer level","['MPEG-4 video coding', 'bit allocation', 'multiple video objects', 'rate-distortion encoding', 'object-based rate allocation', 'variable temporal rates', 'rate distortion theory', 'video coding']","['object-based rate allocation', 'different temporal rates', 'variable temporal rates', 'multiple video objects', 'different objects', 'target bit rate', 'object level', 'mpeg video', 'temporal rates', 'rates']",401,70,8,403,69,10,1,3,0
"the art of the cross-sell [accounting software] with the market for accounting software nearing saturation, vendors are training resellers in the subtleties of the cross-sell. the rewards can be great. the key is knowing when to focus, and when to partner ","The art of the cross-sell [accounting software] With the market for accounting software nearing saturation, vendors are training resellers in the subtleties of the cross-sell. The rewards can be great. The key is knowing when to focus, and when to partner","['accounting software', 'resellers', 'cross-selling', 'accounting']","['cross-sell accounting software', 'saturation vendors', 'art', 'market', 'software', 'accounting', 'vendors', 'saturation', 'cross-sell', 'accounting software']",215,42,4,215,41,10,0,0,1
"training multilayer perceptrons via minimization of sum of ridge functions motivated by the problem of training multilayer perceptrons in neural networks, we consider the problem of minimizing e(x)= sigma /sub i=1//sup n/ f/sub i/( xi /sub i/.x), where xi /sub i/ in r/sup s/, 1<or=i<or=n, and each f/sub i/( xi /sub i/.x) is a ridge function. we show that when n is small the problem of minimizing e can be treated as one of minimizing univariate functions, and we use the gradient algorithms for minimizing e when n is moderately large. for a large n, we present the online gradient algorithms and especially show the monotonicity and weak convergence of the algorithms ","Training multilayer perceptrons via minimization of sum of ridge functions Motivated by the problem of training multilayer perceptrons in neural networks, we consider the problem of minimizing E(x)= Sigma /sub i=1//sup n/ fisub il( xi /Sub i/-x), where xi /sub i/in R/SUp S/, 1<or=i<or=n, and each f/sub i( xi /sub Vx) is a ridge function. We show that when n is small the problem of minimizing E can be treated as one of minimizing univariate functions, and we use the gradient algorithms for minimizing E when n is moderately large. For a large n, we present the online gradient algorithms and especially show the monotonicity and weak convergence of the algorithms","['multilayer perceptrons', 'online gradient algorithms', 'ridge functions', 'minimization', 'neural networks', 'gradient algorithms', 'univariate functions', 'monotonicity', 'weak convergence', 'convergence', 'gradient methods', 'learning (artificial intelligence)', 'minimisation', 'multilayer perceptrons']","['multilayer perceptrons', 'problem', 'ridge function', 'univariate functions', '= Sigma sub', 'xi sub vxm', 'ill xi sub', '= n', 'multilayer', 'perceptrons']",560,113,14,557,111,10,191,74,7
"development of railway vr safety simulation system abnormal conditions occur in railway transportation due to trouble or accidents and it affects a number of passengers. it is very important, therefore, to quickly recover and return to normal train operation. for this purpose we developed a system, ""computer vr simulation system for the safety of railway transportation."" it is a new type simulation system to evaluate measures to be taken under abnormal conditions. users of this simulation system cooperate with one another to correct the abnormal conditions that have occurred in virtual reality. this paper reports the newly developed simulation system ","Development of railway VR safety simulation system Abnormal conditions occur in railway transportation due to trouble or accidents and it affects a number of passengers. It is very important, therefore, to quickly recover and return to normal train operation. For this purpose we developed a system, ""Computer VR Simulation System for the Safety of Railway Transportation.” It is a new type simulation system to evaluate measures to be taken under abnormal conditions. Users of this simulation system cooperate with one another to correct the abnormal conditions that have occurred in virtual reality. This paper reports the newly developed simulation system","['railway transportation', 'accidents', 'normal train operation', 'Computer VR Simulation System', 'virtual reality simulation system', 'abnormal conditions correction', 'accidents', 'digital simulation', 'railways', 'traffic engineering computing', 'virtual reality']","['simulation system', 'abnormal conditions Users', 'VR', 'railway', 'abnormal conditions', 'Abnormal conditions', 'railway transportation', 'Railway transportation', 'system', 'simulation']",559,101,11,559,100,10,1,1,4
"scheduling schemes for an integrated flight and propulsion control system we describe two schemes for scheduling an integrated flight and propulsion control system for an experimental vertical/short take-off and landing (v/stol) aircraft concept in the acceleration from hover (0-120 kn) flight phase. multivariable integrated flight and propulsion controllers are designed at several points over the v/stol envelope and implemented as exact plant observers with state feedback. in the first scheduling scheme, the values of the state feedback and observer gain matrices are interpolated between the fixed-point designs as a function of aircraft speed. in the second approach, the control signals produced by the different fixed-point controllers are blended, allowing a significant reduction in the order of the scheduled controllers. both scheduling schemes are shown in nonlinear simulation to provide excellent handling qualities as the aircraft accelerates from the hover ","Scheduling schemes for an integrated flight and propulsion control system We describe two schemes for scheduling an integrated flight and propulsion control system for an experimental vertical/short take-off and landing (VISTOL) aircraft concept in the acceleration from hover (0-120 kn) flight phase. Multivariable integrated flight and propulsion controllers are designed at several points over the V/STOL envelope and implemented as exact plant observers with state feedback. In the first scheduling scheme, the values of the state feedback and observer gain matrices are interpolated between the fixed-point designs as a function of aircraft speed. In the second approach, the control signals produced by the different fixed-point controllers are blended, allowing a significant reduction in the order of the scheduled controllers. Both scheduling schemes are shown in nonlinear simulation to provide excellent handling qualities as the aircraft accelerates from the hover","['vertical short take-off landing aircraft', 'VSTOL aircraft', 'propulsion control', 'flight control', 'scheduling', 'multivariable control systems', 'observers', 'state feedback', 'fixed-point controllers', 'aerospace propulsion', 'aircraft control', 'multivariable control systems', 'observers', 'scheduling', 'state feedback']","['propulsion control system', 'scheduling schemes', 'different fixed-point controllers', 'first scheduling scheme', 'propulsion controllers', 'kn flight phase', 'control signals', 'controllers', 'flight', 'schemes']",838,140,15,838,139,10,1,1,4
"generic simulation approach for multi-axis machining. part 1: modeling methodology this paper presents a new methodology for analytically simulating multi-axis machining of complex sculptured surfaces. a generalized approach is developed for representing an arbitrary cutting edge design, and the local surface topology of a complex sculptured surface. a nurbs curve is used to represent the cutting edge profile. this approach offers the advantages of representing any arbitrary cutting edge design in a generic way, as well as providing standardized techniques for manipulating the location and orientation of the cutting edge. the local surface topology of the part is defined as those surfaces generated by previous tool paths in the vicinity of the current tool position. the local surface topology of the part is represented without using a computationally expensive cad system. a systematic prediction technique is then developed to determine the instantaneous tool/part interaction during machining. the methodology employed here determines the cutting edge in-cut segments by determining the intersection between the nurbs curve representation of the cutting edge and the defined local surface topology. these in-cut segments are then utilized for predicting instantaneous chip load, static and dynamic cutting forces, and tool deflection. part 1 of this paper details the modeling methodology and demonstrates the capabilities of the simulation for machining a complex surface ","Generic simulation approach for multi-axis machining. Part 1: modeling methodology This paper presents a new methodology for analytically simulating multi-axis machining of complex sculptured surfaces. A generalized approach is developed for representing an arbitrary cutting edge design, and the local surface topology of a complex sculptured surface. A NURBS curve is used to represent the cutting edge profile. This approach offers the advantages of representing any arbitrary cutting edge design ina generic way, as well as providing standardized techniques for Manipulating the location and orientation of the cutting edge. The local surface topology of the part is defined as those surfaces generated by previous tool paths in the vicinity of the current tool Position. The local surface topology of the part is represented without using a computationally expensive CAD system. A systematic prediction technique is then developed to determine the instantaneous toolpart interaction during machining. The methodology employed here determines the cutting edge in-cut segments by determining the intersection between the NURBS curve representation of the cutting edge and the defined local surface topology. These in-cut segments are then utilized for predicting instantaneous chip load, static and dynamic cutting forces, and tool deflection. Part 1 of this paper details the modeling methodology and demonstrates the capabilities of the simulation for machining a complex surface","['multiple axis machining', 'generic modeling', 'tool path specification', 'complex surface machining', 'complex sculptured surfaces', 'systematic prediction', 'cutting edge profile', 'surface topology', 'NURBS curve', 'digital simulation', 'machining', 'path planning', 'production engineering computing']","['local surface topology', 'complex sculptured surface', 'modelling methodology', 'Generic simulation approach', 'multi-axis machining Part', 'edge input segments', 'edge design ina', 'complex surface', 'edge design', 'multi-axis machining']",1272,216,13,1271,214,10,514,144,2
"hypothesis-based concept assignment in software maintenance software maintenance accounts for a significant proportion of the lifetime cost of a software system. software comprehension is required in many parts of the maintenance process and is one of the most expensive activities. many tools have been developed to help the maintainer reduce the time and cost of this task, but of the numerous tools and methods available one group has received relatively little attention: those using plausible reasoning to address the concept assignment problem. we present a concept assignment method for cobol ii: hypothesis-based concept assignment (hb-ca). an implementation of a prototype tool is described, and results from a comprehensive evaluation using commercial cobol ii sources are summarised. in particular, we identify areas of a standard maintenance process where such methods would be appropriate, and discuss the potential cost savings that may result ","Hypothesis-based concept assignment in software maintenance Software maintenance accounts for a significant proportion of the lifetime cost of a software system. Software comprehension is required in many parts of the maintenance process and is one of the most expensive activities. Many tools have been developed to help the maintainer reduce the time and cost of this task, but of the numerous tools and methods available ‘one group has received relatively little attention: those using plausible reasoning to address the concept assignment problem. We present a concept assignment method for COBOL I: hypothesis-based concept assignment (HB-CA). An implementation of a prototype tool is described, and results from a comprehensive evaluation using commercial COBOL II sources are summarised. In particular, we identity areas of a standard maintenance process where such methods would be appropriate, and discuss the potential cost savings that may result","['hypothesis-based concept assignment', 'software maintenance', 'lifetime cost', 'COBOL II', 'scalability', 'COBOL', 'software engineering', 'software maintenance']","['assignment', 'standard maintenance process', 'concept assignment problem', 'concept assignment method', 'concept', 'lifetime cost', 'maintenance', 'maintenance process', 'hypothesis-based concept assignment', 'Hypothesis-based concept assignment']",817,142,8,817,141,10,5,3,2
"scalable hybrid computation with spikes we outline a hybrid analog-digital scheme for computing with three important features that enable it to scale to systems of large complexity: first, like digital computation, which uses several one-bit precise logical units to collectively compute a precise answer to a computation, the hybrid scheme uses several moderate-precision analog units to collectively compute a precise answer to a computation. second, frequent discrete signal restoration of the analog information prevents analog noise and offset from degrading the computation. third, a state machine enables complex computations to be created using a sequence of elementary computations. a natural choice for implementing this hybrid scheme is one based on spikes because spike-count codes are digital, while spike-time codes are analog. we illustrate how spikes afford easy ways to implement all three components of scalable hybrid computation. first, as an important example of distributed analog computation, we show how spikes can create a distributed modular representation of an analog number by implementing digital carry interactions between spiking analog neurons. second, we show how signal restoration may be performed by recursive spike-count quantization of spike-time codes. third, we use spikes from an analog dynamical system to trigger state transitions in a digital dynamical system, which reconfigures the analog dynamical system using a binary control vector; such feedback interactions between analog and digital dynamical systems create a hybrid state machine (hsm). the hsm extends and expands the concept of a digital finite-state-machine to the hybrid domain. we present experimental data from a two-neuron hsm on a chip that implements error-correcting analog-to-digital conversion with the concurrent use of spike-time and spike-count codes. we also present experimental data from silicon circuits that implement hsm-based pattern recognition using spike-time synchrony. we outline how hsms may be used to perform learning, vector quantization, spike pattern recognition and generation, and how they may be reconfigured ","Scalable hybrid computation with spikes We outline a hybrid analog-digital scheme for computing with three important features that enable it to scale to systems of large complexity: First, like digital computation, which uses several one-bit precise logical Units to collectively compute a precise answer to a computation, the hybrid scheme uses several moderate-precision analog units to collectively compute a precise answer to a computation. Second, frequent discrete signal restoration of the analog information prevents analog noise and offset from degrading the computation. Third, a state machine enables complex computations to be created using a sequence of elementary computations. A natural choice for implementing this hybrid scheme is one based on spikes because spike-count codes are digital, while spike-time codes are analog. We illustrate how spikes afford easy ways to implement all three components of scalable hybrid computation. First, as an important example of distributed analog computation, we show how spikes can create a distributed modular representation of an analog number by implementing digital carry interactions between spiking analog neurons. Second, we show how signal restoration may be performed by recursive spike-count quantization of spike-time codes. Third, we use spikes from an analog dynamical system to trigger state transitions in a digital dynamical system, which reconfigures the analog dynamical system using a binary control vector; such feedback interactions between analog and digital dynamical systems create a hybrid state machine (HSM). The HSM extends and expands the concept of a digital finite-state-machine to the hybrid domain. We present experimental data from a two-neuron HSM on a chip that implements error-correcting analog-to-digital conversion with the concurrent use of spike-time and spike-count codes. We also present experimental data from silicon circuits that implement HSM-based pattern recognition using spike-time synchrony. We outline how HSMs may be used to perform learning, vector quantization, spike pattern recognition and generation, and how they may be reconfigured","['scalable hybrid computation', 'spikes', 'hybrid analog-digital scheme', 'moderate-precision analog units', 'frequent discrete signal restoration', 'analog noise', 'spike-count codes', 'finite-state-machine', 'distributed analog computation', 'spike-time codes', 'digital carry interactions', 'binary control vector', 'feedback interactions', 'two neuron hybrid state machine', 'error-correcting analog-to-digital conversion', 'silicon circuits', 'pattern recognition', 'learning', 'vector quantization', 'analogue-digital conversion', 'digital arithmetic', 'error correction', 'finite state machines', 'learning (artificial intelligence)', 'neural chips', 'pattern recognition', 'signal restoration', 'vector quantisation']","['computation', 'hybrid scheme', 'digital dynamical systems', 'elementary computations', 'hybrid state machine', 'complex computations', 'digital computation', 'analog computation', 'scalable hybrid computation', 'Scalable hybrid computation']",1843,309,28,1843,308,10,0,0,14
"a blog in every law firm? you don't know today what you'll want to know next year. rather than trying to solve that problem, focus on providing simple tools to users that create valuable content across the firm. individual contributions will be more visible, and you will have a searchable archive of your institutional memory and a simplified process for ensuring everyone is up to speed. whether you embrace weblogs for their individual or institutional benefits, one thing is certain: they will become powerful tools for those who seek ways to more efficiently and intelligently manage information ","Blog in every law firm? You don't know today what you'll want to know next year. Rather than trying to solve that problem, focus on providing simple tools to users that create valuable content across the firm. Individual contributions will be more visible, and you will have a searchable archive of your institutional memory and a simplified process for ensuring everyone is up to speed. Whether you embrace weblogs for their individual or institutional benefits, one thing is certain: They will become powerful tools for those who seek ways to more efficiently and intelligently manage information","['law firm', 'weblogs', 'institutional memory', 'Web site', 'information resources', 'law administration']","['firm Individual contributions', 'powerful tools', 'problem focus', 'simple tools', 'next years', 'law firm', 'today', 'blow', 'firm', 'law']",504,98,6,503,96,10,325,96,2
"failures and successes: notes on the development of electronic cash between 1997 and 2001, two mid-sized communities in canada hosted north america's most comprehensive experiment to introduce electronic cash and, in the process, replace physical cash for casual, low-value payments. the technology used was mondex, and its implementation was supported by all the country's major banks. it was launched with an extensive publicity campaign to promote mondex not only in the domestic but also in the global market, for which the canadian implementation was to serve as a ""showcase."" however, soon after the start of the first field test it became apparent that the new technology did not work smoothly. on the contrary, it created a host of controversies, in areas as varied as computer security, consumer privacy, and monetary policy. in the following years, few of these controversies could be resolved and mondex could not be established as a widely used payment mechanism. in 2001, the experiment was finally terminated. using the concepts developed in recent science and technology studies (sts), the article analyzes these controversies as resulting from the difficulties of fitting electronic cash, a new sociotechnical system, into the complex setting of the existing payment system. the story of mondex not only offers lessons on why technologies fail, but also offers insight into how short-term failures can contribute to long-term transformations. this suggests the need to rethink the dichotomy of success and failure ","Failures and successes: notes on the development of electronic cash Between 1997 and 2001, two mid-sized communities in Canada hosted North America's most comprehensive experiment to introduce electronic cash and, in the process, replace physical cash for casual, low-value payments. The technology used was Mondex, and its implementation was, supported by all the country's major banks. It was launched with an extensive publicity campaign to promote Mondex not only in the domestic but also in the global market, for which the Canadian implementation was to serve as a ""showcase."" However, soon after the start of the first field test it became apparent that the new technology did not work smoothly. On the contrary, it created a host of controversies, in areas as varied as computer security, consumer privacy, and monetary Policy. In the following years, few of these controversies could be resolved and Mondex could not be established as a widely used payment mechanism. In 2001, the experiment was finally terminated. Using the concepts developed in recent science and technology studies (STS), the article analyzes these controversies as resulting from the difficulties of fitting electronic cash, a new sociotechnical system, into the complex setting of the existing payment system. The story of Mondex not only offers lessons on why technologies fail, but also offers insight into how short-term failures can contribute to long-term transformations. This suggests the need to rethink the dichotomy of success and failure","['electronic cash', 'Canada', 'low-value payments', 'Mondex', 'major banks', 'publicity campaign', 'global market', 'Canadian implementation', 'computer security', 'consumer privacy', 'monetary policy', 'payment mechanism', 'science and technology studies', 'sociotechnical system', 'short-term failures', 'long-term transformations', 'bibliographies', 'electronic commerce', 'electronic money', 'government policies', 'management of change', 'socio-economic effects']","['Failures', 'casual low-value payments', 'fitting electronic cash', 'successes notes', 'new technology', 'physical cash', 'cash', 'electronic', 'successes', 'electronic cash']",1294,237,22,1295,236,10,0,1,12
"super high definition image (whd: wide/double hd) transmission system this paper describes a whd image transmission system constructed from a display projector, codecs, and a camera system imaging a super high definition image (whd: wide/double hd) corresponding to two screen portions of common high-vision images. this system was developed as a transmission system to communicate with or transmit information giving a reality-enhanced feeling to a remote location by using images of super high definition. in addition, the correction processing for the distortions of images occurring due to the structure of the camera system, an outline of the transmission experiments using the proposed system, and subjective evaluation experiments using whd images are presented ","Super high definition image (WHD: Wide/Double HD) transmission system This paper describes a WHD image transmission system constructed from a display projector, CODECs, and a camera system imaging a super high definition image (WHD: Wide/Double HD) corresponding to two screen portions of ‘common high-vision images. This system was developed as a transmission system to communicate with or transmit information giving a reality-enhanced feeling to a remote location by using images of super high definition. In addition, the correction processing for the distortions of images occurring due to the structure of the camera system, an outline of the transmission experiments using the proposed system, and subjective evaluation experiments using WHD images are presented","['super high definition image transmission system', 'WHD image transmission system', 'CODECs', 'camera system imaging', 'reality-enhanced feeling', 'asynchronous transfer mode', 'high definition television', 'image processing']","['super high definition', 'system', 'high definition image', 'image', 'image transmission system', 'common high-vision images', 'camera system imaging', 'proposed system', 'camera system', 'transmission system']",657,113,8,658,112,10,5,1,2
"lossy spice models produce realistic averaged simulations in previous averaged models, the state-space averaging technique or switch waveforms analysis were usually applied over perfect elements, non-inclusive of the ohmic losses. however, if these elements play an active role in the dc transfer function, they affect the small-signal ac analysis by introducing various damping effects. a model is introduced in a boost voltage-mode application ","Lossy SPICE models produce realistic averaged simulations In previous averaged models, the state-space averaging technique or switch waveforms analysis were usually applied over perfect elements, non- inclusive of the ohmic losses. However, if these elements play an active role in the DC transfer function, they affect the small-signal AC analysis by introducing various damping effects. A model is introduced in a boost voltage-mode application","['lossy SPICE models', 'realistic averaged simulations', 'state-space averaging technique', 'switch waveforms analysis', 'damping effects', 'boost voltage-mode application', 'ohmic losses', 'DC transfer function', 'circuit simulation', 'DC-DC power convertors', 'losses', 'SPICE', 'state-space methods', 'transfer functions', 'waveform analysis']","['state-space averaging technique', 'switch waveforms analysis', 'small-signal AC analysis', 'DC transfer function', 'loss SPICE models', 'perfect elements', 'comic losses', 'model', 'losses', 'elements']",383,64,15,383,64,10,111,39,3
"relevance of web documents: ghosts consensus method the dominant method currently used to improve the quality of internet search systems is often called ""digital democracy."" such an approach implies the utilization of the majority opinion of internet users to determine the most relevant documents: for example, citation index usage for sorting of search results (google.com) or an enrichment of a query with terms that are asked frequently in relation with the query's theme. ""digital democracy"" is an effective instrument in many cases, but it has an unavoidable shortcoming, which is a matter of principle: the average intellectual and cultural level of internet users is very low; everyone knows what kind of information is dominant in internet query statistics. therefore, when one searches the internet by means of ""digital democracy"" systems, one gets answers that reflect an underlying assumption that the user's mind potential is very low, and that his cultural interests are not demanding. thus, it is more correct to use the term ""digital ochlocracy"" to refer to internet search systems with ""digital democracy."" based on the well-known mathematical mechanism of linear programming, we propose a method to solve the indicated problem ","Relevance of Web documents: ghosts consensus method The dominant method currently used to improve the quality of Internet search systems is often called ""digital democracy."" Such an approach implies the utilization of the majority opinion of Internet users to determine the most relevant documents: for example, citation index usage for sorting of search results (google.com) or an enrichment of a query with terms that are asked frequently in relation with the query's theme. ""Digital democracy’ is an effective instrument in many cases, but it has an unavoidable shortcoming, which is a matter of principle: the average intellectual and cultural level of Internet users is very low; everyone knows what kind of information is dominant in Internet query statistics. Therefore, when one searches the Internet by means of “digital democracy"" systems, one gets answers that reflect an underlying assumption that the user's mind potential is very low, and that his cultural interests are not demanding. Thus, it is more correct to use the term ""digital ochlocracy"" to refer to Internet search systems with ""digital democracy."" Based on the well-known mathematical mechanism of linear programming, we propose a method to solve the indicated problem","['Internet search systems', 'digital democracy', 'majority opinion', 'citation index usage', 'search results', 'Internet query statistics', 'digital ochlocracy', 'linear programming', 'ghosts consensus method', 'World Wide Web', 'Internet', 'linear programming', 'relevance feedback', 'search engines']","['Internet search systems', 'Internet users', 'Internet', 'digital democracy systems', 'Internet query statistics', 'relevant documents', 'dominant method', 'method', 'documents', 'digital democracy']",1053,193,14,1053,192,10,2,2,4
"office essentials [stationery suppliers] make purchasing stationery a relatively simple task through effective planning and management of stock, and identifying the right supplier ","Office essentials [stationery suppliers] Make purchasing stationery a relatively simple task through effective planning and management of stock, and identifying the right supplier","['stationery suppliers', 'purchasing', 'planning', 'management of stock', 'computer stationery', 'paper', 'purchasing', 'stock control']","['stationery', 'effective planning', 'right supplier', 'simple task', 'essentials', 'Office', 'task', 'simple', 'supplier', 'effective']",157,24,8,157,23,10,0,0,3
"disability-related special libraries one of the ways that the federal government works to improve services to people with disabilities is to fund disability-related information centers and clearinghouses that provide information resources and referrals to disabled individuals, their family members, service providers, and the general public. the teaching research division of western oregon university operates two federally funded information centers for people with disabilities: obirn (the oregon brain injury resource network) and db-link (the national information clearinghouse on children who are deaf-blind). both have developed in-depth library collections and services in addition to typical clearinghouse services. the authors describe how obirn and db-link were designed and developed, and how they are currently structured and maintained. both information centers use many of the same strategies and tools in day-to-day operations, but differ in a number of ways, including materials and clientele ","Disability-related special libraries One of the ways that the federal government works to improve services to people with disabilities is to fund disability-related information centers and clearinghouses that provide information resources and referrals to disabled individuals, their family members, service providers, and the general public. The Teaching Research Division of Western Oregon University operates two federally funded information centers for people with disabilities: OBIRN (the Oregon Brain Injury Resource Network) and DB-LINK (the National Information Clearinghouse on Children who are Deaf-Blind). Both have developed in-depth library collections and services in addition to typical clearinghouse services. The authors describe how OBIRN and DB-LINK were designed and developed, and how they are currently structured and maintained. Both information centers use many of the same strategies and tools in day-to-day operations, but differ in a number of ways, including materials and clientele","['disability-related special libraries', 'federal government', 'disability-related information centers', 'disability-related clearinghouses', 'information resources', 'information referrals', 'Western Oregon University', 'OBIRN', 'Oregon Brain Injury Resource Network', 'DB-LINK', 'National Information Clearinghouse on Children who are Deaf-Blind', 'library collections', 'handicapped aids', 'information centres', 'information needs', 'special libraries']","['disability-related information centers', 'Disability-related special libraries', 'typical clearinghouse services', 'in-depth library collections', 'information resources', 'federal government', 'ways', 'services', 'libraries', 'information centers']",872,140,16,872,139,10,0,0,4
"on the p-adic birch, swinnerton-dyer conjecture for non-semistable reduction in this paper, we examine the iwasawa theory of elliptic curves e with additive reduction at an odd prime p. by extending perrin-riou's theory to certain nonsemistable representations, we are able to convert kato's zeta-elements into p-adic l-functions. this allows us to deduce the cotorsion of the selmer group over the cyclotomic z/sub p/-extension of q, and thus prove an inequality in the p-adic birch and swinnerton-dyer conjecture at primes p whose square divides the conductor of e ","On the p-adic Birch, Swinnerton-Dyer Conjecture for non-semistable reduction In this paper, we examine the Iwasawa theory of elliptic curves E with additive reduction at an odd prime p. By extending Perrin-Riou's theory to certain nonsemistable representations, we are able to convert Kato's zeta-elements into p-adic L-functions. This allows us to deduce the cotorsion of the Selmer group over the cyclotomic Z/sub p/-extension of Q, and thus prove an inequality in the p-adic Birch and Swinnerton-Dyer conjecture at primes p whose square divides the conductor of E","['p-adic Birch', 'Swinnerton-Dyer conjecture', 'nonsemistable reduction', 'lwasawa theory', 'elliptic curves', 'additive reduction', ""Perrin-Riou's theory"", 'p-adic L-functions', 'cotorsion', 'Selmer group', 'cyclotomic Z/sub p/-extension', 'interpolation', 'number theory']","['Swinnerton-Dyer conjecture', 'panic Birch', 'non-semistable reduction', 'additive reduction', 'panic L-functions', 'elliptic curves E', 'cyclotomic sub p', 'Iwasawa theory', 'odd prime p.', 'panic']",480,88,13,480,87,10,0,0,2
"adaptive image enhancement for retinal blood vessel segmentation retinal blood vessel images are enhanced by removing the nonstationary background, which is adaptively estimated based on local neighbourhood information. the result is a much better segmentation of the blood vessels with a simple algorithm and without the need to obtain a priori illumination knowledge of the imaging system ","Adaptive image enhancement for retinal blood vessel segmentation Retinal blood vessel images are enhanced by removing the nonstationary background, which is adaptively estimated based on local neighbourhood information. The result is a much better segmentation of the blood vessels with a simple algorithm and without the need to obtain a priori illumination knowledge of the imaging system","['adaptive image enhancement', 'retinal blood vessel images', 'local neighbourhood information', 'nonstationary background removal', 'image segmentation', 'personal identification', 'security applications', 'blood vessels', 'eye', 'image enhancement', 'image segmentation']","['local neighbourhood information', 'prior illumination knowledge', 'Adaptive image enhancement', 'better segmentation', 'imaging system', 'blood vessels', 'images', 'blood', 'vessels', 'segmentation']",334,58,11,334,57,10,0,0,0
"distribution software: roi is king middle-market accounting software vendors are taking to the open road, by way of souped-up distribution suites that can track product as it wends its way from warehouse floor to customer site. integration provides efficiencies, and cost savings ","Distribution software: ROI is king Middle-market accounting software vendors are taking to the open road, by way of souped-up distribution suites that can track product as it wends its way from warehouse floor to customer site. Integration provides. efficiencies, and cost savings","['accounting software', 'warehouse management', 'distribution', 'accounting', 'goods distribution', 'stock control', 'warehouse automation']","['souped-up distribution suites', 'Distribution software ROI', 'king', 'Middle-market', 'accounting', 'way', 'open road', 'Distribution', 'ROI', 'software']",238,43,7,239,42,10,0,1,0
"model intestinal microflora in computer simulation: a simulation and modeling package for host-microflora interactions the ecology of the human intestinal microflora and its interaction with the host are poorly understood. though more and more data are being acquired, in part using modern molecular methods, development of a quantitative theory has not kept pace with this increase in observing power. this is in part due to the complexity of the system and to the lack of simulation environments in which to test what the ecological effect of a hypothetical mechanism of interaction would be, before resorting to laboratory experiments. the mimics project attempts to address this through the development of a cellular automaton for simulation of the intestinal microflora. in this paper, the design and evaluation of this simulator is discussed ","Model intestinal microflora in computer simulation: a simulation and modeling Package for host-microflora interactions The ecology of the human intestinal microflora and its interaction with the host are poorly understood. Though more and more data are being acquired, in part using modern molecular methods, development of a quantitative theory has not kept pace with this increase in observing power. This is in part due to the complexity of the system and to the lack of simulation environments in which to test what the ecological effect of a hypothetical mechanism of interaction would be, before resorting to laboratory experiments. The MIMICS project attempts to address this through the development of a cellular automaton for simulation of the intestinal microflora. In this paper, the design and evaluation of this simulator is discussed","['human intestines', 'intestinal microflora', 'molecular methods', 'observing power', 'system complexity', 'microbial ecology', 'parallel computing', 'host-microflora interactions', 'quantitative theory', 'MIMICS project', 'complex microbial ecosystem', 'biological organs', 'cellular automata', 'digital simulation', 'medical computing', 'microorganisms', 'physiological models', 'software packages']","['host-microflora interactions', 'human intestinal microflora', 'simulation environments', 'computer simulation', 'ecological effect', 'microflora', 'simulator', 'intestinal', 'interaction', 'intestinal microflora']",718,131,18,718,130,10,0,0,4
iscsi poised to lower san costs it managers building storage area networks or expanding their capacity may be able to save money by using iscsi and ip systems rather than fibre channel technologies ,ISCSI poised to lower SAN costs IT managers building storage area networks or expanding their capacity may be able to save money by using iSCSI and IP systems rather than Fibre Channel technologies,"['SAN costs', 'storage area networks', 'iSCSI', 'IP systems', 'computer network management', 'digital storage']","['Fibre Channel technologies', 'storage area networks', 'scsi', 'IP systems', 'lower SAN', 'managers', 'SAN', 'area', 'lower', 'storage']",165,34,6,165,33,10,0,0,1
"ecological interface design: progress and challenges ecological interface design (eid) is a theoretical framework for designing human-computer interfaces for complex socio-technical systems. its primary aim is to support knowledge workers in adapting to change and novelty. this literature review shows that in situations requiring problem solving, eid improves performance when compared with current design approaches in industry. eid has been applied to industry-scale problems in a broad variety of application domains (e.g., process control, aviation, computer network management, software engineering, medicine, command and control, and information retrieval) and has consistently led to the identification of new information requirements. an experimental evaluation of eid using a full-fidelity simulator with professional workers has yet to be conducted, although some are planned. several significant challenges remain as obstacles to the confident use of eid in industry. promising paths for addressing these outstanding issues are identified. actual or potential applications of this research include improving the safety and productivity of complex socio-technical systems ","Ecological interface design: progress and challenges Ecological interface design (EID) is a theoretical framework for designing human-computer interfaces for complex socio-technical systems. Its primary aim is to support knowledge workers in adapting to change and novelty. This literature review shows that in situations requiring problem solving, EID improves performance when compared with current design approaches in industry. EID has been applied to industry-scale problems in a broad variety of application domains (e.g., process control, aviation, computer network management, software engineering, medicine, command and control, and information retrieval) and has consistently led to the identification of new information requirements. An experimental evaluation of EID using a full-fidelity simulator with professional workers has yet to be conducted, although some are planned. Several significant challenges remain as obstacles to the confident use of EID in industry. Promising paths for addressing these outstanding issues are identified. Actual or potential applications of this research include improving the safety and productivity of complex socio-technical systems,","['ecological interface design', 'human-computer interfaces', 'complex social technical systems', 'industry', 'productivity', 'user interface', 'human factors', 'ergonomics', 'factory automation', 'human factors', 'large-scale systems', 'user interfaces']","['complex socio-technical systems', 'Ecological', 'Several significant challenges', 'human-computer interfaces', 'current design approaches', 'theoretical framework', 'progress', 'design', 'interface', 'challenges']",1025,160,12,1026,159,10,0,1,2
"a survey of interactive mesh-cutting techniques and a new method for implementing generalized interactive mesh cutting using virtual tools in our experience, mesh-cutting methods can be distinguished by how their solutions address the following major issues: definition of the cut path, primitive removal and re-meshing, number of new primitives created, when re-meshing is performed, and representation of the cutting tool. many researchers have developed schemes for interactive mesh cutting with the goals of reducing the number of new primitives created, creating new primitives with good aspect ratios, avoiding a disconnected mesh structure between primitives in the cut path, and representing the path traversed by the tool as accurately as possible. the goal of this paper is to explain how, by using a very simple framework, one can build a generalized cutting scheme. this method allows for any arbitrary cut to be made within a virtual object, and can simulate cutting surface, layered surface or tetrahedral objects using a virtual scalpel, scissors, or loop cautery tool. this method has been implemented in a real-time, haptic-rate surgical simulation system allowing arbitrary cuts to be made on high-resolution patient-specific models ","A survey of interactive mesh-cutting techniques and a new method for implementing generalized interactive mesh cutting using virtual tools In our experience, mesh-cutting methods can be distinguished by how their solutions address the following major issues: definition of the cut path, primitive removal and re-meshing, number of new primitives created, when re-meshing is performed, and representation of the cutting tool. Many researchers have developed schemes for interactive mesh cutting with the goals of reducing the number of new primitives created, creating new primitives with good aspect ratios, avoiding a disconnected mesh structure between primitives in the cut path, and representing the path traversed by the tool as accurately as possible. The goal of this paper is to explain how, by using a very simple framework, one can build a generalized cutting scheme. This method allows for any arbitrary cut to be made within a virtual object, and can simulate cutting surface, layered surface or tetrahedral objects using a virtual scalpel, scissors, or loop cautery tool. This method has been implemented in a real-time, haptic-rate surgical simulation system allowing arbitrary cuts to be made on high-resolution patient-specific models","['generalized interactive mesh cutting', 'virtual tools', 'cut path definition', 're-meshing', 'cutting tool', 'disconnected mesh structure', 'virtual object', 'tetrahedral objects', 'layered surface', 'real-time system', 'haptic-rate surgical simulation system', 'high-resolution patient-specific models', 'rendering', 'haptic interfaces', 'cutting', 'digital simulation', 'haptic interfaces', 'medical computing', 'real-time systems', 'rendering (computer graphics)', 'surgery']","['new primitives', 'arbitrary cut', 'cut path', 'interactive mesh-cutting techniques', 'mesh-cutting methods', 'virtual tools', 'cutting tools', 'new method', 'interactive', 'interactive mesh']",1064,188,21,1064,187,10,0,0,5
"four-terminal quantum resistor network for electron-wave computing interconnected ultrathin conducting wires or, equivalently, interconnected quasi-one-dimensional electron waveguides, which form a quantum resistor network, are presented here in four-terminal configurations. the transmission behaviors through such four-terminal networks are evaluated and classified. in addition, we show that such networks can be used as the basic building blocks for a possible massive wave computing machine in the future. in a network, each interconnection, a node point, is an elastic scatterer that routes the electron wave. routing and rerouting of electron waves in a network is described in the framework of quantum transport from landauer-buttiker theory in the presence of multiple elastic scatterers. transmissions through various types of four-terminal generalized clean aharonov-bohm rings are investigated at zero temperature. useful logic functions are gathered based on the transmission probability to each terminal with the use of the buttiker symmetry rule. in the generalized rings, even and odd numbers of terminals can possess some distinctly different transmission characteristics as we have shown here and earlier. just as an even or odd number of atoms in a ring is an important quantity for classifying the transmission behavior, we show here that whether the number of terminals is an even or an odd number is just as important in understanding the physics of transmission through such a ring. furthermore, we show that there are three basic classes of four-terminal rings and the scaling relation for each class is provided. in particular, the existence of equitransmission among all four terminals is shown here. this particular physical phenomena cannot exist in any three-terminal ring. comparisons and discussions of transmission characteristics between three-terminal and four-terminal rings are also presented. the node-equation approach by considering the kirchhoff current conservation law at each node point is used for this analysis. many useful logic functions for electron-wave computing are shown here. in particular, we show that a full adder can be constructed very simply using the equitransmission property of the four-terminal ring. this is in sharp contrast with circuits based on transistor logic ","Four-terminal quantum resistor network for electron-wave computing Interconnected ultrathin conducting wires or, equivalently, interconnected quasi-one-dimensional electron waveguides, which form a quantum resistor network, are presented here in four-terminal configurations. The transmission behaviors through such four-terminal networks are evaluated and classified. In addition, we show that such networks can be used as the basic building blocks for a possible massive wave ‘computing machine in the future. In a network, each interconnection, a node point, is an elastic scatterer that routes the electron wave Routing and rerouting of electron waves in a network is described in the framework of quantum transport from Landauer-Buttiker theory in the presence of multiple elastic scatterers. Transmissions through various types of four-terminal generalized clean Aharonov-Bohm rings are investigated at zero temperature. Useful logic functions are gathered based on the transmission probability to each terminal with the use of the Buttiker symmetry rule. In the generalized rings, even and odd numbers of terminals can possess some distinctly different transmission characteristics as we have shown here and earlier. Just as an even or ‘odd number of atoms in a ring is an important quantity for classifying the transmission behavior, we show here that whether the number of terminals is an even or an odd number is just as important in understanding the physics of transmission through such a ring Furthermore, we show that there are three basic classes of four-terminal rings and the scaling relation for each class is provided. In particular, the existence of equitransmission among all four terminals is shown here. This particular physical phenomena cannot exist in any three-terminal ring. Comparisons and discussions of transmission characteristics between three-terminal and four-terminal rings are also presented. The node-equation approach by considering the Kirchhoff current conservation law at each node point is used for this analysis. Many useful logic functions for electron-wave computing are shown here. In particular, we show that a full adder can be constructed very simply using the equitransmission property of the four-terminal ring. This is in sharp contrast with circuits based on transistor logic","['four-terminal quantum resistor network', 'electron-wave computing', 'interconnected ultrathin conducting wires', 'quasi1D electron waveguides', 'rerouting', 'Landauer-Buttiker theory', 'multiple elastic scatterers', 'Aharonov-Bohm rings', 'logic functions', 'transmission probability', 'Buttiker symmetry rule', 'transmission behavior', 'Kirchhoff current conservation law', 'equitransmission property', 'Aharonov-Bohm effect', 'electron waveguides', 'quantum computing', 'resistors']","['quantum resistor network', 'electron-wave computing', 'four-terminal rings', 'network', 'such four-terminal networks', 'electron wave Routing', 'quantum transport', 'electron waves', 'such networks', 'quantum']",1991,340,18,1991,339,10,11,4,10
"santera targets independents in major strategy overhaul [telecom] with big carriers slashing capital expense budgets, santera systems is broadening the reach of its next-generation switching platform to include independent telcos. this week, the vendor will announce that it has signed a deal with kerman, calif-based kerman telephone co. furthermore, the company is angling for inclusion in the rural utilities service's approved equipment list, hoping to sell its class 5 replacement boxes to the smallest carriers. the move is almost a complete reversal for the plano, texas-based vendor, which previously focused solely on large carriers, including the rbocs ","Santera targets independents in major strategy overhaul [telecom] With big carriers slashing capital expense budgets, Santera Systems is broadening the reach of its next-generation switching platform to include independent telcos. This week, the vendor will announce that it has signed a deal with Kerman, Calif-based Kerman Telephone Co. Furthermore, the company is angling for inclusion in the Rural Utilities Service's approved equipment list, hoping to sell its Class 5 replacement boxes to the smallest carriers. The move is almost a complete reversal for the Plano, Texas-based vendor, which previously focused solely on large carriers, including the RBOCs","['Santera Systems', 'switching', 'Kerman Telephone', 'Rural Utilities Service', 'telecommunication switching']","['next-generation switching platform', 'santer targets independents', 'capital expense budgets', 'independent tescos', 'smallest carriers', 'large carriers', 'big carriers', 'major', 'santer', 'independents']",566,98,5,566,97,10,0,0,3
"modeling frequently accessed wireless data with weak consistency to reduce the response times of wireless data access in a mobile network, caches are utilized in wireless handheld devices. if the original data entry has been updated, the cached data in the handheld device becomes stale. thus, a mechanism is required to predict when the cached copy will expire. this paper studies a weakly consistent data access mechanism that computes the time-to-live (ttl) interval to predict the expiration time. we propose an analytic model to investigate this ttl-based algorithm for frequently accessed data. the analytic model is validated against simulation experiments. our study quantitatively indicates how the ttl-based algorithm reduces the wireless communication cost by increasing the probability of stale accesses. depending on the requirements of the application, appropriate parameter values can be selected based on the guidelines provided ","Modeling frequently accessed wireless data with weak consistency To reduce the response times of wireless data access in a mobile network, caches are utilized in wireless handheld devices. If the original data entry has been updated, the cached data in the handheld device becomes stale. Thus, a mechanism is required to predict when the cached copy will expire. This paper studies a weakly consistent data access mechanism that computes the time-to-live (TTL) interval to predict the expiration time. We propose an analytic model to investigate this TTL-based algorithm for frequently accessed data. The analytic model is validated against simulation experiments. Our study quantitatively indicates how the TTL-based algorithm reduces the wireless ‘communication cost by increasing the probability of stale accesses. Depending on the requirements of the application, appropriate parameter values can be selected based on the guidelines provided","['frequently accessed wireless data modeling', 'weak consistency', 'response time reduction', 'wireless data access', 'mobile network', 'caches', 'wireless handheld devices', 'data entry', 'time-to-live interval', 'expiration time prediction', 'analytic model', 'simulation experiments', 'wireless communication cost', 'stale access probability', 'cache storage', 'client-server systems', 'mobile communication', 'mobile computing', 'portable computers']","['data', 'wireless', 'wireless communication cost', 'wireless handheld devices', 'mobile network caches', 'wireless data access', 'original data entry', 'stale accesses', 'cached data', 'wireless data']",807,139,19,808,138,10,12,1,3
"median partitioning: a novel method for the selection of representative subsets from large compound pools a method termed median partitioning (mp) has been developed to select diverse sets of molecules from large compound pools. unlike many other methods for subset selection, the mp approach does not depend on pairwise comparison of molecules and can therefore be applied to very large compound collections. the only time limiting step is the calculation of molecular descriptors for database compounds. mp employs arrays of property descriptors with little correlation to divide large compound pools into partitions from which representative molecules can be selected. in each of n subsequent steps, a population of molecules is divided into subpopulations above and below the median value of a property descriptor until a desired number of 2/sup n/ partitions are obtained. for descriptor evaluation and selection, an entropy formulation was embedded in a genetic algorithm. mp has been applied to generate a subset of the available chemicals directory, and the results have been compared with cell-based partitioning ","Median partitioning: a novel method for the selection of representative subsets from large compound pools A method termed median partitioning (MP) has been developed to select diverse sets of molecules from large compound pools. Unlike many other methods for subset selection, the MP approach does not depend on pairwise ‘comparison of molecules and can therefore be applied to very large ‘compound collections. The only time limiting step is the calculation of molecular descriptors for database compounds. MP employs arrays of property descriptors with litle correlation to divide large compound Pools into partitions from which representative molecules can be selected. In each of n subsequent steps, a population of molecules is divided into subpopulations above and below the median value of a property descriptor until a desired number of 2/sup n/ partitions are obtained. For descriptor evaluation and selection, an entropy formulation was embedded in a genetic algorithm. MP has been applied to generate a subset of the Available Chemicals Directory, and the results have been compared with cell-based partitioning","['median partitioning', 'large compound pools', 'representative subset selection', 'molecules', 'time limiting step', 'molecular descriptors', 'database compounds', 'property descriptor array', 'entropy formulation', 'genetic algorithm', 'Available Chemicals Directory', 'cell-based partitioning', 'chemistry computing', 'entropy', 'genetic algorithms', 'organic compounds', 'scientific information systems', 'very large databases']","['large compound collections', 'representative molecules', 'representative subsets', 'database compounds MP', 'many other methods', 'subset selection', 'novel method', 'method', 'large compound pools', 'large compound Pools']",953,170,18,954,169,10,20,3,8
contentment management andersen's william yarker and richard young outline the route to a successful content management strategy ,Contentment management Andersen's William Yarker and Richard Young outline the route to a successful content management strategy,"['Andersen Consulting', 'content management strategy', 'document handling', 'information resources']","['management', 'Richard Young', 'Contentment', 'successful', 'andersen', 'William', 'parker', 'route', 'Young', 'Richard']",112,18,4,112,17,10,0,0,0
"reinventing broadband many believe that broadband providers need to change their whole approach. the future, then, is in reinventing broadband. that means tiered pricing to make broadband more competitive with dial-up access and livelier, more distinct content: video on demand, mp3, and other features exclusive to the fat-pipe superhighway ","Reinventing broadband Many believe that broadband providers need to change their whole approach. The future, then, is in reinventing broadband. That means tiered pricing to make broadband more competitive with dial-up access and livelier, more distinct content: video on demand, MP3, and other features exclusive to the fat-pipe superhighway","['MP3', 'business plans', 'video on demand', 'tiered pricing', 'broadband', 'broadband networks', 'costing', 'economics', 'Internet', 'technological forecasting']","['distinct content video', 'broadband providers', 'whole approach', 'tiered pricing', 'dial-up access', 'Many', 'whole', 'approach', 'broadband', 'providers']",293,50,10,293,49,10,0,0,1

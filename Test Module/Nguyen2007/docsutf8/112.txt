Categorizing Web Queries According to Geographical Locality
ABSTRACT
Web pages (and resources, in general) can be characterized
according to their geographical locality. For example, a web
page with general information about wildflowers could be
considered a global page, likely to be of interest to a ge-ographically
broad audience. In contrast, a web page with
listings on houses for sale in a specific city could be regarded
as a local page, likely to be of interest only to an audience
in a relatively narrow region. Similarly, some search
engine queries (implicitly) target global pages, while other
queries are after local pages.
For example, the best results
for query [wildflowers] are probably global pages about
wildflowers such as the one discussed above. However, local
pages that are relevant to, say, San Francisco are likely
to be good matches for a query [houses for sale] that was
issued by a San Francisco resident or by somebody moving
to that city. Unfortunately, search engines do not analyze
the geographical locality of queries and users, and hence
often produce sub-optimal results. Thus query [wildflowers
] might return pages that discuss wildflowers in specific
U.S. states (and not general information about wildflowers),
while query [houses for sale] might return pages with real
estate listings for locations other than that of interest to the
person who issued the query. Deciding whether an unseen
query should produce mostly local or global pages--without
placing this burden on the search engine users--is an important
and challenging problem, because queries are often
ambiguous or underspecify the information they are after.
In this paper, we address this problem by first defining how
to categorize queries according to their (often implicit) geographical
locality. We then introduce several alternatives
for automatically and efficiently categorizing queries in our
scheme, using a variety of state-of-the-art machine learning
tools. We report a thorough evaluation of our classifiers using
a large sample of queries from a real web search engine,
and conclude by discussing how our query categorization
approach can help improve query result quality.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKM'03, November 3­8, 2003, New Orleans, Louisiana, USA.
Copyright 2003 ACM 1-58113-723-0/03/0011 ...
$
5.00.
Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information
Search and Retrieval--query formulation, search process

General Terms
Algorithms, Experimentation, Human Factors

INTRODUCTION
Web pages (and resources, in general) can be characterized
according to their geographical locality. For example, a
web page with general information about wildflowers could
be considered a global page, likely to be of interest to a ge-ographically
broad audience. In contrast, a web page with
listings on houses for sale in a specific city could be regarded
as a local page, likely to be of interest only to an audience in
a relatively narrow region. Earlier research [9] has addressed
the problem of automatically computing the "geographical
scope" of web resources.
Often search engine queries (implicitly) target global web
pages, while other queries are after local pages. For example,
the best results for query [wildflowers] are probably global
pages about wildflowers discussing what types of climates
wildflowers grow in, where wildflowers can be purchased,
or what types of wildflower species exist. In contrast, local
pages that are relevant to, say, San Francisco are likely to
be good matches for a query [houses for sale] that was issued
by a San Francisco resident, or by somebody moving
to San Francisco, even if "San Francisco" is not mentioned
in the query. The user's intent when submitting a query
may not be always easy to determine, but if underspecified
queries such as [houses for sale] can be detected, they can
be subsequently modified by adding the most likely target
geographical location or by getting further user input to cus-tomize
the results.
Unfortunately, search engines do not analyze the geographical
locality of queries and users, and hence often produce
sub-optimal results, even if these results are on-topic
and reasonably "popular" or "authoritative." Thus query
[wildflowers] might return pages that discuss wildflowers in
specific U.S. states (and not general information about wildflowers
). In fact, as of the writing of this paper, the first
10 results that Google provides for this query include 5
pages each of which discusses wildflowers in only one U.S.
325
state (e.g., "Texas Wildflowers"). Similarly, the top 10 results
that Google returns for query [houses for sale] include
real estate pages for Tuscany, United Kingdom, and New
Zealand.
These pages are likely to be irrelevant to, say,
somebody interested in San Francisco real estate who types
such an underspecified query.
Deciding whether a query posed by a regular search engine
user should produce mostly local or global pages is an important
and challenging problem, because queries are often
ambiguous or underspecify the information they are after,
as in the examples above. By identifying that, say, query
[wildflowers] is likely after "global" information, a search
engine could rank the results for this query so that state-specific
pages do not appear among the top matches. By
identifying that, say, query [houses for sale] is likely after
"local" information, a search engine could filter out pages
whose geographical locality is not appropriate for the user
who issued the query. Note that deciding which location
is of interest to a user who wrote an underspecified query
such as [houses for sale] is an orthogonal, important issue
that we do not address in this paper. Our focus is on identifying
that such a query is after "local" pages in nature,
and should therefore be treated differently by a search engine
than queries that are after "global" pages. By knowing
that a user query is after local information, a search engine
might choose to privilege pages whose geographical locality
coincides with that of the user's or, alternatively, attempt
to obtain further input from the user on what location is of
interest.
In this paper, we first define how to categorize user queries
according to their (often implicit) geographical locality. We
then introduce several alternatives for automatically and efficiently
classifying queries according to their locality, using
a variety of state-of-the-art machine learning tools. We report
a thorough evaluation of our classifiers using a large
sample of queries from a real web search engine query log.
Finally, we discuss how our query categorization approach
can help improve query result quality. The specific contributions
of this paper are as follows:
· A discussion on how to categorize user queries according
to their geographical locality, based on a careful
analysis of a large query log from the Excite web site
(Section 3).
· A feature representation for queries; we derive the feature
representation of a query from the results produced
for the query by a web search engine such as
Google (Section 4.1).
· A variety of automatic query classification strategies
that use our feature representation for queries (Section
4.2).
· A large-scale experimental evaluation of our strategies
over real search engine queries (Section 5).
· Preliminary query reformulation and page re-ranking
strategies that exploit our query classification techniques
to improve query result quality (Section 6).
RELATED WORK
Traditional information-retrieval research has studied how
to best answer keyword-based queries over collections of text
documents [18].
These collections are typically assumed
to be relatively uniform in terms of, say, their quality and
scope. With the advent of the web, researchers are studying
other "dimensions" to the data that help separate useful resources
from less-useful ones in an extremely heterogeneous
environment like the web. Notably, the Google search engine
[4] and the HITS algorithm [7, 13] estimate the "impor-tance"
of web pages by analyzing the hyperlinks that point
to them, thus capturing an additional dimension to the web
data, namely how important or authoritative the pages are.
Ding et al. [9] extract yet another crucial dimension of the
web data, namely the geographical scope of web pages. For
example, the Stanford Daily newspaper has a geographical
scope that consists of the city of Palo Alto (where Stanford
University is located), while the New York Times newspaper
has a geographical scope that includes the entire U.S.
To compute the geographical scope of a web page, Ding et
al. propose two complementary strategies: a technique based
on the geographical distribution of HTML links to the page,
and a technique based on the distribution of geographical
references in the text of the page. Ding et al. report on
a search-engine prototype that simply filters out from the
results for a user query any pages not in the geographical
scope of the user. This technique does not attempt to determine
whether a query is best answered with "global" or
"local" pages, which is the focus of our paper. Ding et al.
built on the work by Buyukkokten et al. [6], who discussed
how to map a web site (e.g., http://www-db.stanford.edu)
to a geographical location (e.g., Palo Alto) and presented a
tool to display the geographical origin of the HTML links
to a given web page. This tool then helps visualize the geographical
scope of web pages [6].
A few commercial web sites manually classify web resources
by their location, or keep directory information that
lists where each company or web site is located. The North-ernLight
search engine
1
extracts addresses from web pages,
letting users narrow their searches to specific geographical
regions (e.g., to pages "originated" within a five-mile radius
of a given zip code). Users benefit from this information
because they can further filter their query results. McCurley
[14] presented a variety of approaches for recognizing
geographical references on web pages, together with a nav-igational
tool to browse pages by geographical proximity
and their spatial context. (Please refer to [16] for additional
references.) None of these techniques addresses our focus
problem in this paper: automatically determining the geographical
locality associated with a given, unmodified search
engine query.
DEFINING GEOGRAPHICAL LOCALITY
As discussed above, queries posed to a web search engine
can be regarded as local, if their best matches are likely to
be "local" pages, or as global, if their best matches are likely
to be "global" pages. In an attempt to make this distinction
more concrete, we now discuss several examples of local and
global queries.
Global queries often do not include a location name, as
is the case for query [Perl scripting]. A user issuing this
query is probably after tutorials about the Perl language,
and hence pages on the topic with a restricted geographi-1
http://www.northernlight.com/
326
cal scope are less desirable than global pages. Other global
queries do not mention a location explicitly either, but are
topically associated with one particular location. An example
of such a query is [Elgin marbles], which is topically associated
with the city of Athens. We consider these queries
as global, since their best matches are broad, global pages,
not localized pages with a limited geographical scope. In-terestingly
, global queries sometimes do include a location
name. For example, a query might be just a location name
(e.g., [Galapagos Islands]) or a request for concrete information
about a location (e.g., [Boston area codes]). General
resources about the location (e.g., tourist guides) are
arguably to be preferred for such queries, which are hence
regarded as global. Other global queries include locations
that are strongly associated topic-wise with the rest of the
query. Query [Woody Allen NYC] is an example of such a
query. The location mentioned in this query (i.e., "NYC,"
for "New York City") is not used to restrict query results to
pages of interest to New York residents, but rather expresses
a topic specification. Query [Ansel Adams Yosemite] is another
example: photographer Ansel Adams took a famous
series of photographs in Yosemite.
Local queries often include a location name, as is the case
for query [Wisconsin Christmas tree producers association].
The location mentioned in this query (i.e., "Wisconsin") is
used to "localize" the query results. Query [houses for sale
New York City] is a related example. Other local queries
do not include a location name, but still implicitly request
"localized" results. Query [houses for sale] is an example of
such a query. These queries tend to be underspecified, but
are still asked by (presumably na¨ive) search engine users.
We conducted a thorough examination of a large number
(over 1,200) of real search engine queries. Most queries
that we encountered can be cleanly categorized as being either
global or local. However, other queries are inherently
ambiguous, and their correct category is impossible to determine
without further information on the user intent behind
them. For example, query [New York pizza] could be con-strued
as a local query if it is, say, after pizza delivery web
sites for the New York area. In contrast, the same query
could be regarded as a global query if the user who issues it
wants to learn about the characteristics of New York-style
pizza.
USING CLASSIFIERS TO DETERMINE LOCALITY
We earlier established that queries are associated with local
or global status, which influences the kind of results that
are desirable. Since current search engines do not directly
take into account geographical information, for certain types
of queries they produce a large number of on-topic but un-wanted
results, as in the [houses for sale] example discussed
earlier. In this section, we discuss automatic methods that
can determine, given a query, whether the query is a local
or global one. To build the two-class classifier, we experimented
with several state-of-the-art classification techniques
, using widely available implementations for each. We
describe below the features used in the classification, how we
extract them from web pages, and the classifiers with which
we experimented.
4.1
Classification Features
Web queries, which we treat in this paper as ordered bags
of words with no other structure, are typically fairly short.
In the collection of 2,477,283 real queries that we used in
our experiments (Section 5.1), 84.9% were five words long
or shorter. Because few words are available per query, basing
the classification directly on the words in the query may
lead to severe sparse data problems. Even more importantly,
some of the characteristics that make a query local or global
are not directly observable in the query itself, but rather in
the results returned. For example, a query that returns results
that contain few references to geographical locations is
likely to be global, while a query that returns results spread
uniformly over many locations without including a significant
percentage of results with no locations is likely to be
local.
For these reasons, we base our classification on a sample
of results actually returned for a given query rather than the
words in the query itself. By observing distributional characteristics
in the unmodified results, the classifier can infer
the type of the query (global or local) so that the results can
be appropriately filtered or re-ordered, or the query modified
. In a way the approach is similar in spirit to query
expansion techniques that rely on pseudo-relevance feedback
[5].
In our experiments, we use Google (via the Google
API
2
) to obtain the top 50 web pages that match the query.
For simplicity, we limited our search to HTML pages, skipping
over non-HTML documents. We chose Google because
it represents state-of-the-art web search technology and offers
a published interface for submitting large numbers of
queries.
We represent the web pages returned by Google as text
documents. This conversion is achieved by using the
lynx
HTML browser with the -dump option. We base our classification
features on measures of frequency and dispersion of
location names in these text files. For this purpose, we have
constructed a database of 1,605 location names by concatenating
lists of all country names
3
, of the capitals of these
countries
4
, of the fifty U.S. states, and of all cities in the
United States with more than 25,000 people
5
. We then compare
the words in each text document with the database of
location names, and output any matching entries and their
count per document. This matching is case insensitive, because
we found capitalization of location names in web pages
to be erratic. Note that we do not attempt to disambiguate
words that match location names but also have other senses
(e.g., "China"), as this is a hard problem in natural language
analysis; instead, we count such words as locations. An alternative
approach that would detect and disambiguate location
names would be to use a named-entity tagger. We
experimented with a well-known third-party named-entity
tagger, but we encountered a very high error rate because
of the noise often introduced in web pages.
Our classification features combine these location counts
in various ways. For each query, we measure the average
2
http://www.google.com/apis
3
Obtained from the United Nations, http://www.un.org/
Overview/unmember.html.
4
Obtained from the CIA World Factbook, http://www.
capitals.com/.
5
Obtained from the U.S. Census Bureau (2000 census
figures), http://www.census.gov/prod/2002pubs/00ccdb/
cc00_tabC1.pdf.
327
(per returned web page) number of location words in the retrieved
results. We count the average frequency of location
words at different levels of detail (country, state, city), as
well as the average of the aggregate total for all locations.
We obtain these frequencies for both the total count (tokens)
and the unique location words in each page (types), as it is
possible that a few locations would be repeated many times
across the results, indicating a global query, or that many locations
would be repeated few times each, indicating a local
query. We also consider the total number of unique locations
across all the returned documents taken together, divided by
the number of retrieved documents. For the average token
frequencies of city, state, and country locations we also calculate
the minimum and maximum across the set of returned
web pages. To account for the hierarchical nature of location
information, we calculate an alternative frequency for states
where we include in the count for each state the counts for
all cities from that state that were found in that text; this
allows us to group together location information for cities
in the same state. We also include some distributional measures
, namely the fraction of the pages that include at least
one location of any kind, the percentage of words that match
locations across all pages, and the standard deviation of the
total per page location count. Finally, we add to our list
of features the total number of words in all of the returned
documents, to explore any effect the local/global distinction
may have on the size of the returned documents. These calculations
provide for 20 distinct features that are passed on
to the classifier.
6
The core data needed to produce these
20 query features (i.e., the locations mentioned in each web
page) could be efficiently computed by a search engine such
as Google at page-indexing time. Then, the final feature
computation could be quickly performed at query time using
this core data.
4.2
Classification Methods
We initially trained a classifier using
Ripper [8], which
constructs a rule-based classifier in an incremental manner.
The algorithm creates an initial set of very specific rules
based on the data, similar to the way in which decision trees
are generated. The rules are then pruned iteratively to eliminate
the ones that do not seem to be valid for a large enough
subset of the training data, so as to prevent overfitting.
Although
Ripper provides a robust classifier with high
accuracy and transparency (a human can easily examine
the produced rules), it outputs binary "local"­"global" decisions
. In many cases, it is preferable to obtain a measure
of confidence in the result or an estimate of the probability
that the assigned class is the correct one. To add this capability
to our classifier, we experimented with logistic regression
[19]. Logistic, or log-linear, regression models a binary
output variable (the class) as a function of a weighted sum
of several input variables (the classification features). Con-ceptually
, a linear predictor
is first fitted over the training
data in a manner identical to regular regression analysis,
i.e.,
= w
0
+
k
i=1
w
i
· F
i
where
F
i
is the
i-th feature and w
i
is the weight assigned to
6
Studying the effect on classification accuracy of a richer
feature set (e.g., including as well all words on the result
pages) is the subject of interesting future work.
that feature during training. Subsequently,
is transformed
to the final response,
C, via the logistic transformation
C =
e

1 +
e

which guarantees that
C is between 0 and 1. Each of the
endpoints of the interval (0
, 1) is associated with one of the
classes, and
C gives the probability that the correct class is
the one associated with "1". In practice, the calculations are
not performed as a separate regression and transformation,
but rather as a series of successive regressions of transformed
variables via the iterative reweighted least squares algorithm
[1].
7
We used the implementation of log-linear regression
provided in the R statistical package.
8
Another desideratum for our classifier is its ability to support
different costs for the two possible kinds of errors (misclassifying
local queries versus misclassifying global queries).
Which kind of error is the most important may vary for
different settings; for our search modification application,
we consider the misclassification of global queries as local
ones a more serious error. This is because during our subsequent
modification of the returned results (Section 6), we
reorder the results for some of the queries that we consider
global, but we modify the original queries for some of the
queries classified as local, returning potentially very different
results. Consequently, the results can change more significantly
for a query classified as local, and the potential for
error is higher when a global query is labeled local than the
other way around.
Both
Ripper and log-linear regression can incorporate different
costs for each type of error. We experimented with
a third classification approach that also supports this feature
, Support Vector Machines (SVMs) [2], which have been
found quite effective for text matching problems [11]. SVM
classifiers conceptually convert the original measurements of
the features in the data to points in a high-dimensional space
that facilitates the separation between the two classes more
than the original representation. While the transformation
between the original and the high-dimensional space may be
complex, it needs not to be carried out explicitly. Instead,
it is sufficient to calculate a kernel function that only involves
dot products between the transformed data points,
and can be calculated directly in the original feature space.
We report experiments with two of the most common kernel
functions: a linear kernel,
K(x, y) = x · y + 1
and a Gaussian (radial basis function) kernel,
K(x, y) = e
- x-y
2
/2
2
where
is a parameter (representing the standard deviation
of the underlying distribution). This latter kernel has been
recommended for text matching tasks [10]. Regardless of the
choice of kernel, determining the optimal classifier is equivalent
to determining the hyperplane that maximizes the total
distance between itself and representative transformed data
points (the support vectors).
Finding the optimal classifier
therefore becomes a constrained quadratic optimization
7
This is because the modeled distribution is binomial rather
than normal, and hence the variance depends on the mean-see
[19] for the technical details.
8
http://www.r-project.org/
328
Set
Original
number of
queries
Number of
appropriate
queries
Global
Local
Training
595
439
368
(83.8%)
71
(16.2%)
Development
199
148
125
(84.5%)
23
(15.5%)
Test
497
379
334
(88.1%)
45
(11.9%)
Table 1: Distribution of global and local queries in our training, development, and test sets.
problem. In our experiments, we use the SVM-Light implementation
9
[12].
In many binary classification tasks, one of the two classes
predominates, and thus trained classifiers tend to favor that
class in the absence of strong evidence to the contrary. This
certainly applies to our task; as we show in Section 5.1,
83­89% of web queries are global. Weiss and Provost [21]
showed that this imbalance can lead to inferior classifier
performance on the test set, and that the problem can be
addressed through oversampling of the rarer class in the
training data. Their method examines different oversampling
rates by constructing artificial training sets where the
smaller class is randomly oversampled to achieve a specific
ratio between samples from the two classes. For each such
sampling ratio, a classifier is trained, which assigns a score
to each object indicating strength of evidence for one of the
classes. By fixing a specific strength threshold, we divide
the classifier output into the two classes. Further, by varying
this threshold
10
we can obtain an error-rate curve for
each class as a function of the threshold. The entire process
results in a Receiver-Operator Characteristic (ROC) curve
[3] for each sampling ratio. Specific points on the curve that
optimize the desired combination of error rates can then be
selected, and the performance of the classification method
across the different thresholds can be measured from the
area between the curve and the x-axis. Weiss and Provost
use the C4.5 classifier [17], a decision tree classifier with
additional pruning of nodes to avoid overfitting. We use a
software package provided by them (and consequently also
the C4.5 algorithm) to explore the effect that different ratios
of global to local queries during training have on classifier
performance.
EXPERIMENTAL RESULTS
We now describe the data (Section 5.1) and metrics (Section
5.2) that we use for the experimental evaluation of the
query classifiers (Section 5.3).
5.1
Data
For the experiments reported in this paper, we used a sample
of real queries submitted to the Excite search engine.
11
We had access to a portion of the December 1999 query log of
Excite, containing 2,477,283 queries. We randomly selected
initial sets of queries for training, development (tuning the
parameters of the classifiers we train), and testing purposes
by selecting each of these queries for inclusion in each set
with a constant (very small) probability. These probabilities
were set to 400/2,477,283, 400/2,477,283, and 500/2,477,283
9
Available from http://svmlight.joachims.org/.
10
Setting the threshold to each extreme assigns all or none
of the data points to that category.
11
http://www.excite.com/
for the three sets, respectively. Subsequently we combined
the training and development set, and reassigned the queries
in the combined set so that three-fourths were placed in the
training set and one-fourth in the development; we kept the
test set separate. This process generated 595, 199, and 497
queries in the initial versions of the training, development,
and test sets. We further eliminated queries that passed any
of the following tests:
· Upon examination, they appeared likely to produce
results with explicit sexual content.
· When supplied to Google--and after filtering out any
non-HTML results and any broken links--the queries
produced fewer than 40 files. This constraint is meant
to ensure that we are not including in our experimental
data queries that contain misspellings or deal with
extremely esoteric subjects, for which not enough material
for determining locality would be available.
· They had already been included in an earlier set (we
constructed first the training set, then the development
set, and finally the test set). Since multiple people
may issue the same query, duplicates can be found
in the log. Although our algorithms take no special advantage
of duplicates, we eliminated them to avoid any
bias. Taking into account variations of upper/lower
case and spacing between queries (but not word order
), this constraint removed 6 queries from the test
set.
These filtering steps left us with 439 queries in the training
set, 148 queries in the development set, and 379 queries in
the test set.
We then classified the queries using the criteria laid out
in Section 3. Table 1 shows the size of the three sets before
and after filtering, and the distribution of local and global
queries in each set. We observe that, in general, most queries
(83­89%) tend to be global.
5.2
Evaluation Metrics
We consider a number of evaluation metrics to rate the
performance of the various classifiers and their configurations
. Since a large majority of the queries are global (85.6%
in the training, development, and test sets combined), overall
classification accuracy (i.e., the percentage of correct
classification decisions) may not be the most appropriate
measure. This is because a baseline method that always
suggests the most populous class ("global") will have an accuracy
equal to the proportion of global queries in the evaluated
set. Yet such a classifier will offer no improvement
during search since it provides no new information. The situation
is analogous to applications in information retrieval
or medicine where very few of the samples should be labeled
positive (e.g., in a test for a disease that affects only 0.1%
329
of patients). While we do not want overall accuracy to decrease
from the baseline (at least not significantly), we will
utilize measures that capture the classifier's improved ability
to detect the rarer class relative to the baseline method.
Two standard such metrics are precision and recall for
the local queries. Precision is the ratio of the number of
items correctly assigned to the class divided by the total
number of items assigned to the class. Recall is the ratio
of the number of items correctly assigned to a class as compared
with the total number of items in the class. Note that
the baseline method achieves precision of 100% but recall of
0%. For a given classifier with adjustable parameters, often
precision can be increased at the expense of recall, and vice
versa; therefore we also compute the F-measure [20] (with
equal weights) to combine precision and recall into a single
number,
F-measure = 2 × Precision × Recall
Precision + Recall
Finally, we argued earlier that one kind of misclassification
errors may be assigned a higher cost. We can then calculate
the average cost [15],
Average cost =
X{
Global
,
Local
}
Cost(
X) × Rate(X)
where Cost(
X) is the cost of wrong X classifications and
Rate(
X) is the rate of wrong X classifications. Average cost
is the measure to minimize from a decision theory perspective
. The rate of wrong classifications for a class is equal
to the number of data points that have been misclassified
into that class divided by the total number of classification
decisions, and the costs for each misclassification error are
predetermined parameters. If both costs are set to 1, then
the average cost becomes equal to the total error rate, i.e.,
one minus accuracy. In our experiments, we report the average
cost considering the mislabeling of global queries as
local twice as important as the mislabeling of local queries,
for the reasons explained in the previous section.
5.3
Results
We trained the classifiers of Section 4.2 on the 439 queries
in our training set.
Ripper and the regression model were
trained on that training set without modification. For C4.5
and SVMs, we explored the effect that different proportions
of local queries in the training set have on overall performance
. For that purpose, we used our development set to
evaluate the 
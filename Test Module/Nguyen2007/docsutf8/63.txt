Data Mining in Metric Space: An Empirical Analysis of Supervised Learning Performance Criteria
ABSTRACT
Many criteria can be used to evaluate the performance of
supervised learning. Different criteria are appropriate in
different settings, and it is not always clear which criteria
to use. A further complication is that learning methods
that perform well on one criterion may not perform well
on other criteria. For example, SVMs and boosting are designed
to optimize accuracy, whereas neural nets typically
optimize squared error or cross entropy. We conducted an
empirical study using a variety of learning methods (SVMs,
neural nets, k-nearest neighbor, bagged and boosted trees,
and boosted stumps) to compare nine boolean classification
performance metrics: Accuracy, Lift, F-Score, Area
under the ROC Curve, Average Precision, Precision/Recall
Break-Even Point, Squared Error, Cross Entropy, and Probability
Calibration. Multidimensional scaling (MDS) shows
that these metrics span a low dimensional manifold. The
three metrics that are appropriate when predictions are interpreted
as probabilities: squared error, cross entropy, and
calibration, lay in one part of metric space far away from
metrics that depend on the relative order of the predicted
values: ROC area, average precision, break-even point, and
lift. In between them fall two metrics that depend on comparing
predictions to a threshold: accuracy and F-score.
As expected, maximum margin methods such as SVMs and
boosted trees have excellent performance on metrics like accuracy
, but perform poorly on probability metrics such as
squared error. What was not expected was that the margin
methods have excellent performance on ordering metrics
such as ROC area and average precision. We introduce a
new metric, SAR, that combines squared error, accuracy,
and ROC area into one metric. MDS and correlation analysis
shows that SAR is centrally located and correlates well
with other metrics, suggesting that it is a good general purpose
metric to use when more specific criteria are not known.
Categories & Subject Descriptors: I.5.2 [Pattern Recognition
]: Design Methodology - classifier design & evaluation.
General Terms: Algorithms, Measurement, Performance,
Experimentation.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
KDD'04, August 22­25, 2004, Seattle, Washington, USA.
Copyright 2004 ACM 1-58113-888-1/04/0008 ...
$
5.00.
INTRODUCTION
In supervised learning, finding a model that could predict
the true underlying probability for each test case would be
optimal. We refer to such an ideal model as the One True
Model. Any reasonable performance metric should be optimized
(in expectation, at least) by the one true model, and
no other model should yield performance better than it.
Unfortunately, we usually do not know how to train models
to predict the true underlying probabilities. The one
true model is not easy to learn. Either the correct parametric
model type for the domain is not known, or the training
sample is too small for the model parameters to be esti-mated
accurately, or there is noise in the data. Typically,
all of these problems occur together to varying degrees.
Even if magically the one true model were given to us, we
would have difficulty selecting it from other less true models.
We do not have performance metrics that will reliably assign
best performance to the probabilistically true model given
finite validation data.
In practice, we train models to minimize loss measured via
a specific performance metric. Since we don't have metrics
that could reliably select the one true model, we must accept
the fact that the model(s) we select will necessarily be
suboptimal. There may be only one true model, but there
are many suboptimal models.
There are different ways that suboptimal models can differ
from the one true model ­ tradeoffs can be made between
different kinds of deviation from the one true model. Different
performance metrics reflect these different tradeoffs. For
example, ordering metrics such as area under the ROC curve
and average precision do not care if the predicted values are
near the true probabilities, but depend only on the relative
size of the values. Dividing all predictions by ten does
not change the ROC curve, and metrics based on the ROC
curve are insensitive to this kind of deviation from truth.
Metrics such as squared error and cross entropy, however,
are greatly affected by scaling the predicted values, but are
less affected by small changes in predicted values that might
alter the relative ordering but not significantly change the
deviation from the target values. Squared error and cross
entropy reflect very different tradeoffs than metrics based
on the ROC curve. Similarly, metrics such as accuracy depend
on how the predicted values fall relative to a threshold.
If predicted values are rescaled, accuracy will be unaffected
if the threshold also is rescaled. But if small changes to
69
Research Track Paper
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
W2
W1
Max ACC
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
W2
W1
Max AUC
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
W2
W1
Min RMS
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
W2
W1
Min MXE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
W2
W1
Min CAL
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
W2
W1
Max SAR
Figure 1: Level curves for six error metrics: ACC, AUC, RMS, MXE, CAL, SAR for a simple problem.
predicted values are made for cases near the threshold, this
can have large impact on accuracy. Accuracy reflects yet
another tradeoff in how deviation from truth is measured.
The one true model, if available, would have (in expectation
) the best accuracy, the best ROC curve, and the best
cross entropy, and the different tradeoffs made by these metrics
would not be important. But once we accept that we
will not be able to find the one true model, and must therefore
accept suboptimal models, the different tradeoffs made
by different performance metrics become interesting and important
. Unfortunately, little is known about how different
performance metrics compare to each other.
In this paper we present results from an empirical analysis
of nine widely used performance metrics. We perform
this empirical comparison using models trained with seven
learning algorithms: SVMs, neural nets, k-nearest neighbor
, bagged and boosted trees, and boosted stumps. We
use multidimensional scaling (MDS) and correlation analysis
to interpret the results. We also examine which learning
methods perform best on the different metrics. Finally, we
introduce a new metric, SAR, that combines squared error,
accuracy, and ROC area into a single, robust metric.
THE PERFORMANCE METRICS
We experiment with nine performance metrics for boolean
classification: Accuracy (ACC), Lift (LFT), F-Score (FSC),
Area under the ROC Curve (AUC), Average Precision (APR),
the Precision/Recall Break-Even Point (BEP), Root Mean
Squared Error (RMS), Mean Cross Entropy (MXE), and
Probability Calibration (CAL). Definitions for each of the
metrics can be found in Appendix A.
Figure 1 shows level curves for six of the ten performance
metrics for a model with only two parameters (W 1 and W 2)
trained on a simple synthetic binary problem. Peak performance
in the first two plots occurs along a ridge in weight
space. In the other four plots peak performance is indicated
by solid dots. Peak performance for some metrics nearly
coincide: RMS and MXE peak at nearly the same model
weights. But other metrics peak in different places: CAL
has a local optimum near the optima for RMS and MXE,
but its global optimum is in a different place. Also, the
ridges for optimal ACC and optimal AUC do not align, and
the ridges do not cross the optima for the other four metrics.
Optimizing to each of these metrics yields different models,
each representing different tradeoffs in the kinds of errors
the models make. Which of these tradeoffs is best depends
on the problem, the learning algorithm, and how the model
predictions ultimately will be used.
We originally divided the nine metrics into three groups:
threshold metrics, ordering/rank metrics, and probability
metrics. The three threshold metrics are accuracy (ACC),
F-score (FSC) and lift (LFT). F-score is the harmonic mean
of precision and recall at some threshold. Lift measures the
true positive rate in the fraction of cases that fall above
threshold. (See Appendix A for a definition of lift, and [3]
for a description of Lift Curves. Lift is the same as precision
at some threshold, but scaled so that it can be larger than
1.) Usually ACC and FSC use a fixed threshold. In this
paper we use 0.5. With lift, often the threshold is adjusted
so that a fixed percent, p, of cases are predicted as positive,
the rest falling below threshold. Usually p depends on the
problem. For example, in marketing one might want to send
fliers to 10% of customers. Here we somewhat arbitrarily set
p = 25% for all problems. Note that for all threshold metrics
it is not important how close a prediction is too a threshold,
only if the predicted value is above or below threshold.
The ordering/rank metrics look at predictions differently
from the threshold metrics. If cases are ordered by predicted
value, the ordering/rank metrics measure how well the ordering
ranks positive cases above negative cases. The rank
metrics can be viewed as a summary of the performance of
a model across all possible thresholds. The rank metrics we
use are area under the ROC curve (AUC), average precision
(APR), and precision/recall break even point (BEP). See
[10] for a discussion of ROC curves from a machine learning
perspective. Rank metrics depend only on the ordering
of the predictions, not the actual predicted values. If the
ordering is preserved it makes no difference if the predicted
values range between 0 and 1 or between 0.29 and 0.31.
Although we group Lift with the threshold metrics, and
BEP with the ordering metrics, BEP and Lift are similar to
each other in some respects. Lift is directly proportional to
BEP if Lift is calculated at p equal to the proportion of positives
in the data set. This threshold also is the break-even
point where precision equals recall. BEP and Lift are similar
to the ordering metrics because the threshold depends
implicitly on the ordering, but also are similar to the threshold
metrics because neither is sensitive to the orderings on
either side of the threshold once that threshold has been
defined. Results presented later suggest that both Lift and
BEP are more similar to the ordering metrics than to the
threshold metrics.
The three probability metrics depend on the predicted values
, not on how the values fall relative to a threshold or relative
to each other. The probability metrics are uniquely min-imized
(in expectation) when the predicted value for each
case coincides with the true probability of that case being
positive. The probability metrics we consider are squared
error (RMS), cross entropy (MXE) and calibration (CAL).
CAL measures the calibration of a model: if a model predicts
0.85 for a large number of cases, about 85% of those cases
should prove to be positive if the model is well calibrated.
See Appendix A for details of how CAL is calculated.
70
Research Track Paper
We also experiment with a new performance metric, SAR,
that combines squared error, accuracy, and ROC area into
one measure: SAR = (ACC + AU C + (1 - RM S))/3. SAR
behaves somewhat differently from ACC, AUC, and RMS
alone, and is a robust metric to use when the correct metric
is unknown. SAR is discussed further in Section 8.
NORMALIZING THE SCORES
Performance metrics such as accuracy or squared error
have range [0, 1], while others (lift, cross entropy) range from
0 to q where q depends on the data set. For some metrics
lower values indicate better performance. For others higher
values are better. Metrics such as ROC area have baseline
rates that are independent of the data, while others such as
accuracy have baseline rates that depend on the data. If
baseline accuracy is 0.98, an accuracy of 0.981 probably is
not good performance, yet on another problem, if the Bayes
optimal rate is 0.60, achieving an accuracy of 0.59 might be
excellent performance.
In order to compare performance metrics in a meaningful
way, all the metrics need to be placed on a similar scale. One
way to do this is to scale the performances for each problem
and metric from 0 to 1, where 0 is poor performance, and 1
is good performance. For example, we might place baseline
performance at 0, and the Bayes optimal performance at 1.
Unfortunately, we cannot estimate the Bayes optimal rate
on real problems. Instead, we can use the performance of
the best observed model as a proxy for the Bayes optimal
performance. We calculate baseline rate as follows: predict
p for every case, where p is the percent of positives in the test
set. We normalize performances to the range [0, 1], where
0 is baseline and 1 represents best performance. If a model
performs worse than baseline, its normalized score will be
negative. See Table 1 for an example of normalized scores.
The disadvantage of normalized scores is that recovering the
raw performances requires knowing the performances that
define the top and bottom of the scale, and as new best
models are found the top of the scale changes.
CAL, the metric we use to measure probability calibration
, is unusual in that the baseline model that predicts p
for all cases, where p is the percent of positives in the test
set, has excellent calibration. (Because of this, measures like
CAL typically are not used alone, but are used in conjunction
with other measures such as AUC to insure that only
models with good discrimination and good calibration are
selected. See Figure 1 for a picture of how unusual CAL's
error surface is compared with other metrics.) This creates a
problem when normalizing CAL scores because the baseline
model and Bayes optimal model have similar CAL scores.
This does not mean CAL is a poor metric ­ it is effective at
distinguishing poorly calibrated models from well calibrated
models. We address this problem later in the paper.
EXPERIMENTAL DESIGN
The goal of this work is to analyze how the ten metrics
compare to each other. To do this we train many different
kinds of models on seven test problems, and calculate for
each test problem the performance of every model on the
ten metrics.
We train models using seven learning algorithms: Neural
Nets (ANN), SVMs, Bagged Decision Trees (BAG-DT),
Boosted Decision Trees (BST-DT), Boosted Decision Stumps
Table 1: Accuracy on ADULT problem
model
acc
norm score
bst-stmp
0.8556
1.0000
bag-dt
0.8534
0.9795
dt
0.8503
0.9494
svm
0.8480
0.9267
bst-dt
0.8464
0.9113
ann
0.8449
0.8974
knn
0.8320
0.7731
baseline
0.7518
0.0000
(BST-STMP), single Decision Trees (DT) and Memory Based
Learning (KNN). For each algorithm we train many variants
and many parameter settings. For example, we train ten
styles of decision trees, neural nets of different sizes, SVMs
using many different kernels, etc. A total of 2000 models are
trained and tested on each problem. See Appendix B for a
description of the parameter settings we use for each learning
method. While this strategy won't create every possible
model, and won't create a uniform sample of the space of
possible models, we feel that this is an adequate sample of
the models that often will be trained in practice.
For each problem, the 2000 models are trained on the same
train set of 4000 points. The performance of each model
is measured on the same large test set for each of the ten
performance metrics. In order put the performances on the
same scale across different metrics and different problems,
we transform the raw performance to normalized scores as
explained in Section 3. In total, across the seven problems,
we have 2000  7 = 14, 000 models and for each model we
have it's score on each of the 10 performances metrics.
DATA SETS
We compare the algorithms on seven binary classification
problems. ADULT, COVER TYPE and LETTER are from
UCI Repository [1]. ADULT is the only problem that has
nominal attributes. For ANNs, SVMs and KNNs we transform
nominal attributes to boolean. Each DT, BAG-DT,
BST-DT and BST-STMP model is trained twice, once with
the transformed attributes and once with the original attributes
. COVER TYPE has been converted to a binary
problem by treating the largest class as the positive and the
rest as negative. We converted LETTER to boolean in two
ways. LETTER.p1 treats the letter "O" as positive and the
remaining 25 letters as negative, yielding a very unbalanced
binary problem. LETTER.p2 uses letters A-M as positives
and the rest as negatives, yielding a well balanced problem.
HYPER SPECT is the IndianPine92 data set [4] where the
difficult class Soybean-mintill is the positive class. SLAC is
a problem from collaborators at the Stanford Linear Accelerator
and MEDIS is a medical data set. The characteristics
of these data sets are summarized in Table 2.
Table 2: Description of problems
problem
#attr
train size
test size
% pos.
adult
14/104
4000
35222
25%
cover type
54
4000
25000
36%
letter.p1
16
4000
14000
3%
letter.p2
16
4000
14000
53%
medis
63
4000
8199
11%
slac
59
4000
25000
50%
hyper spect
200
4000
4366
24%
71
Research Track Paper
MDS IN METRIC SPACE
Training 2000 models on each problem using seven learning
algorithms gives us 14,000 models, each of which is eval-uated
on ten performance metrics. This gives us 14,000
sample points to compare for each performance metric. We
build a 10x14,000 table where lines represent the performance
metrics, columns represent the models, and each entry
in the table is the score of the model on that metric.
For MDS, we treat each row in the table as the coordinate
of a point in a 14,000 dimension space. The distance between
two metrics is calculated as the Euclidean distance
between the two corresponding points in this space. Because
the coordinates are strongly correlated, there is no
curse-of-dimensionality problem with Euclidean distance in
this 14,000 dimensional space.
We are more interested in how the metrics compare to
each other when models have good performance than when
models have poor performance. Because of this, we delete
columns representing poorer performing models in order to
focus on the "interesting" part of the space where models
that have good performance lie. For the analyses reported
in this paper we delete models that perform below baseline
on any metric (except CAL).
Ten metrics permits 10  9/2 = 45 pairwise comparisons.
We calculate Euclidean distance between each pair of metrics
in the sample space, and then perform multidimensional
scaling on these pairwise distances between metrics.
MDS is sensitive to how the performance metrics are scaled.
The normalized scores described in Section 3 yield well-scaled
performances suitable for MDS analysis for most metrics
. Unfortunately, as discussed in Section 3, normalized
scores do not work well with CAL. Because of this, we perform
MDS two ways. In the first, we use normalized scores,
but exclude the CAL metric. In the second, we include CAL,
but scale performances to mean 0.0 and standard deviation
1.0 instead of using normalized scores. Scaling by standard
deviation resolves the problem with CAL for MDS, but is
somewhat less intuitive because scores scaled by standard
deviation depend on the full distribution of models instead
of just the performances that fall at the top and bottom of
each scale.
Figure 2 shows the MDS stress as a function of the number
of dimensions in the MDS (when CAL is included). The
ten metrics appear to span an MDS space of about 3 to 5
dimensions. In this section we examine the 2-D MDS plots
in some detail.
Figure 3 shows two MDS plots for the metrics that result
when dimensionality is reduced to two dimensions. The plot
on the left is MDS using normalized scores when CAL is
excluded. The plot on the right is MDS using standard
deviation scaled scores when CAL is included.
Both MDS plots show a similar pattern. The metrics appear
to form 4-5 somewhat distinct groups. In the upper
right hand corner is a group that includes AUC, APR, BEP,
LFT, and SAR. The other groups are RMS and MXE, ACC
(by itself, or possibly with FSC), FSC (by itself, or possibly
with ACC), and CAL (by itself). It is not surprising that
squared error and cross entropy form a cluster. Also, presumably
because squared error tends to be better behaved
than cross entropy, RMS is closer to the other measures than
MXE. We are somewhat surprised that RMS is so centrally
located in the MDS plots. Perhaps this partially explains
why squared error has proved so useful in many applications.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
1
2
3
4
5
6
7
8
MDS Stress
Number of MDS Dimensions
Figure 2: MDS stress vs. number of dimensions
It is somewhat surprising that accuracy does not appear
to correlate strongly with any of the other metrics, except
possibly with FSC. ACC does not fall very close to other
metrics that use thresholds such as Lift and F-Score, even
though F-Score uses the same 0.5 threshold as accuracy in
our experiments. (The threshold for Lift is adjusted dynam-ically
so that 25% of the cases are predicted as positive.)
Accuracy is surprisingly close to RMS, and closer to RMS
than to MXE, again suggesting that part of the reason why
RMS has been so useful is because of its close relationship
to a metric such as ACC that has been so widely used.
The most surprising pattern in the MDS plot that includes
CAL is that CAL is distant from most other metrics
. There appears to be an axis running from CAL at
one end to the ordering metrics such as AUC and APR
at the other end that forms the largest dimension in the
space. This is surprising because one way to achieve excellent
ordering is to accurately predict true probabilities,
which is measured by the calibration metric. However, one
can achieve excellent AUC and APR using predicted values
that have extremely poor calibration, yet accurately predict
the relative ordering of the cases. The MDS plot suggests
that many models which achieve excellent ordering do so
without achieving good probabilistic calibration. Closer examination
shows that some models such as boosted decision
trees yield remarkably good ordering, yet have extremely
poor calibration.
We believe maximum margin methods
such as boosting tradeoff reduced calibration for better margin
. See Section 9 for further discussion of this issue. One
also can achieve good calibration, yet have poor AUC and
APR. For example, decision trees with few leaves may be
well calibrated, but the coarse set of values they predict do
not provide a basis for good ordering.
Figure 4 shows 2-D MDS plots for six of the seven test
problems. The seventh plot is similar and is omitted to
save space. (The omitted plot is one of the two LETTER
problems.) Although there are variations between the plots,
the 2-D MDS plots for the seven problems are remarkably
consistent given that these are different test problems. The
consistency between the seven MDS plots suggests that we
have an adequate sample size of models to reliably detect relationships
between the metrics. Metrics such as ACC, FSC,
and LFT seem to move around with respect to each other in
these plots. This may be because they have different sensi-72
Research Track Paper
acc
fsc
lft
auc
apr
bep
rms
mxe
sar
Dim 2
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Dim 1
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
Dim 2
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Dim 1
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Figure 3: 2D MDS plot using normalized scores (left) and standard deviation scaling (right).
tivities to the ratio of positives to negatives in the data sets.
For example, BEP is proprtional to LFT (and thus behaves
similarly) when the percentage of positives in the dataset
equals the fraction predicted above threshold (25% in this
paper). Other than this, we have not been able to correlate
differences we see in the individual plots with characteristics
of the problems that might explain those differences,
and currently believe that the MDS plots that combine all
seven problems in Figure 3 represents an accurate summary
of the relationships between metrics. Note that this does
not mean that the performance of the different learning algorithms
exhibits the same pattern on these test problems
(in fact they are very different), only that the relationships
between the ten metrics appear to be similar across the test
problems when all the learning algorithms are considered at
one time.
CORRELATION ANALYSIS
As with the MDS analysis in the previous section, we
used each of the ten performance metrics to measure the
performance of the 2000 models trained with the different
learning methods on each of the seven test problems. In
this section we use correlation analysis on these models to
compare metrics instead of MDS.
Again, to make the correlation analysis easier to interpret
, we first scale performances to the range [0, 1] so that
the best performance we observed with that metric on each
problem with any of the learning methods is performance
1, and baseline performance with that metric and data set
is performance 0. This eliminates the inverse correlation
between measures such as accuracy and squared error, and
normalizes the scale of each metric.
Ten metrics permits 10  9/2 = 45 pairwise correlations.
We do these comparisons using both linear correlation (excluding
CAL) and rank correlation. The results from the
linear and rank correlation analyses are qualitatively similar
. We present the results for non-parametric rank correlation
because rank correlation makes fewer assumptions
about the relationships between the metrics, and because
rank correlation is insensitive to how CAL is scaled.
Table 3 shows the rank correlation between all pairs of
metrics. Each entry in the table is the average rank correlation
across the seven test problems. The table is sym-metric
and contains only 45 unique pairwise comparisons.
We present the full matrix because this makes it easier to
scan some comparisons. The final column is the mean of
the rank correlations for each metric. This gives a rough
idea how correlated each metric is on average to all other
metrics.
Metrics with pairwise rank correlations near one behave
more similarly than those with smaller rank correlations. Ignoring
the SAR metric which is discussed in the next section,
seven metric pairs have rank correlations above 0.90:
0.96: Lift to ROC Area
0.95: ROC Area to Average Precision
0.93: Accuracy to Break-even Point
0.92: RMS to Cross-Entropy
0.92: Break-Even Point to ROC Area
0.92: Break-Even Point to Average Precision
0.91: Average Precision to Lift
We expected AUC and average precision to behave very
similarly and thus have high rank correlation. But we are
surprised to see that Lift has such high correlation to AUC.
Note that because Lift has high correlation to AUC, and
AUC has high correlation to average precision, it is not surprising
that Lift also has high correlation to average precision
. As expected, break-even point is highly correlated with
the other two ordering metrics, AUC and average precision.
But the high correlation between accuracy and break-even
73
Research Track Paper
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
COVER_TYPE
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
ADULT
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
LETTER.P1
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
HYPER_SPECT
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
MEDIS
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
SLAC
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Figure 4: 2-D MDS plots for six of the seven test problems. The seventh problem yields a similar plot and
is omitted only to save space. The missing plot is for one of the LETTER problems.
74
Research Track Paper
Table 3: Average rank correlations between metrics
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
mean
acc
1.00
0.87
0.85
0.88
0.89
0.93
0.87
0.75
0.56
0.92
0.852
fsc
0.87
1.00
0.77
0.81
0.82
0.87
0.79
0.69
0.50
0.84
0.796
lft
0.85
0.77
1.00
0.96
0.91
0.89
0.82
0.73
0.47
0.92
0.832
auc
0.88
0.81
0.96
1.00
0.95
0.92
0.85
0.77
0.51
0.96
0.861
apr
0.89
0.82
0.91
0.95
1.00
0.92
0.86
0.75
0.50
0.93
0.853
bep
0.93
0.87
0.89
0.92
0.92
1.00
0.87
0.75
0.52
0.93
0.860
rms
0.87
0.79
0.82
0.85
0.86
0.87
1.00
0.92
0.79
0.95
0.872
mxe
0.75
0.69
0.73
0.77
0.75
0.75
0.92
1.00
0.81
0.86
0.803
cal
0.56
0.50
0.47
0.51
0.50
0.52
0.79
0.81
1.00
0.65
0.631
sar
0.92
0.84
0.92
0.96
0.93
0.93
0.95
0.86
0.65
1.00
0.896
point is somewhat surprising and we currently do not know
how to explain this.
The weakest correlations are all between the calibration
metric (CAL) and the other metrics. On average, CAL correlates
with the other metrics only about 0.63. We are surprised
how low the correlation is between probability calibration
and other metrics, and are currently looking at other
measures of calibration to see if this is true for all of them.
acc
fsc
lft
auc
apr
bep
rms
mxe
cal
sar
Dim 2
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Dim 1
-2.0
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
Figure 5: MDS using rank correlation
Figure 5 shows an MDS plot for the metrics when distance
between metrics is calculated as 1 - rank correlation, making
MDS insensitive to how the metrics are scaled. (Distances
based on 1 - rank correlation do not respect the
triangle inequality so this is not a proper metric space.)
The overall pattern is similar to that observed in the MDS
plots in Figure 3. CAL is at one end of the space far from
the other metrics. Cross-entropy is closest to RMS, though
not as close as in the other plots. Cross-entropy and RMS
have high rank correlation, but because cross-entropy has
lower rank-correlation to other most metrics than RMS, it
is pushed far from RMS which is close to other metrics in
the MDS plot. APR and AUC are at the other end of the
space farthest from CAL. FSC is in the upper left side of
the space. ACC and RMS are near the center of the space.
SAR A GENERAL PURPOSE METRIC
When applying supervised learning to data, a decision
must be made about what metric to train to and what metric
to use for model selection. Often the learning algorithm
dictates what metrics can be used for training, e.g. it is difficult
to train a neural net for metrics other than RMS or
MXE. But there usually is much more freedom when selecting
the metric to use for model selection, i.e. the metric used
to pick the best learning algorithm and the best parameters
for that algorithm.
If the correct metric for the problem is known, model selection
probably should be done using that metric even if
the learning algorithm cannot be trained to it. What should
be done when the correct metric is not known? The MDS
plots and correlation analysis suggest that RMS is remarkably
well correlated with the other measures, and thus might
serve as a good general purpose metric to use when a more
specific optimization criterion is not known.
We wondered if we could devise a new metric more centrally
located than RMS and with better correlation to the
other metrics. Rather than devise a completely new metric
, we tried averaging several of the well behaved metrics
into a new metric that might be more robust than each one
individually. SAR combines Squared error, Accuracy, and
ROC area into one measure: SAR = (ACC + AU C + (1 RM
S))/3. We chose these metrics for SAR for three reasons
:
1. we wanted to select one metric from each metric group:
the threshold metrics, the ordering metrics, and the
probability metrics
2. ACC, AUC, and RMS seemed to be the most popular
metric from each of these groups, respectively
3. these three metrics are well correlated to the other
metrics in their groups, and in the MDS plots lie closest
to the other metrics in their groups
As can be seen from the MDS plots and in the tables,
SAR behaves differently from ACC, AUC, and RMS alone.
In Table 3 SAR has higher mean rank correlation to other
metrics t
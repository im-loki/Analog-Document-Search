Discovering and Ranking Web Services with BASIL: A Personalized Approach with Biased Focus
ABSTRACT
In this paper we present a personalized web service discovery
and ranking technique for discovering and ranking relevant
data-intensive web services. Our first prototype ­ called BASIL
­ supports a personalized view of data-intensive web services
through source-biased focus. BASIL provides service discovery
and ranking through source-biased probing and source-biased
relevance metrics. Concretely, the BASIL approach has three
unique features: (1) It is able to determine in very few interactions
whether a target service is relevant to the given source
service by probing the target with very precise probes; (2) It
can evaluate and rank the relevant services discovered based on
a set of source-biased relevance metrics; and (3) It can identify
interesting types of relationships for each source service
with respect to other discovered services, which can be used
as value-added metadata for each service. We also introduce a
performance optimization technique called source-biased probing
with focal terms to further improve the effectiveness of the
basic source-biased service discovery algorithm. The paper concludes
with a set of initial experiments showing the effectiveness
of the BASIL system.
Categories and Subject Descriptors
H.3.5 [Online Information
Services]: Web-based services
General Terms
Algorithms, Experimentation

INTRODUCTION
Most web services today are web-enabled applications that
can be accessed and invoked using a messaging system, typically
relying on standards such as XML, WSDL, and SOAP [29].
Many companies have latched onto the web services mantra, including
major software developers, business exchanges, eCom-merce
sites, and search engines [15, 9, 2, 1, 7]. A large and
growing portion of the web services today can be categorized
as data-intensive web services.

This research is partially supported by NSF CNS CCR, NSF ITR, DoE
SciDAC, DARPA, CERCS Research Grant, IBM Faculty Award, IBM
SUR grant, HP Equipment Grant, and LLNL LDRD.
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. To copy otherwise, to republish, to post
on servers or to redistribute to lists, requires prior specific permission and/or a
fee.
ICSOC'04, November 15­19, 2004, New York, New York, USA.
Copyright 2004 ACM 1-58113-871-7/04/0011 ...
$
5.00.
Data-intensive web services provide access to huge and growing
data stores and support tools for searching, manipulating,
and analyzing those data stores. For example, both Amazon [1]
and Google [7] now provide XML- and SOAP-based web service
interfaces to their underlying data repositories with support
for advanced search operators over, collectively, billions of
items. In the life sciences domain, many bioinformatics services
are transitioning from human-in-the-loop web interfaces to the
web services model [9], providing direct access to unprecedented
amounts of raw data and specialized research tools to provide
high-level analysis and search over these data services.
With the increasing visibility of web services and the Service-Oriented
Computing paradigm [18], there is a growing need
for efficient mechanisms for discovering and ranking services.
Effective mechanisms for web service discovery and ranking are
critical for organizations to take advantage of the tremendous
opportunities offered by web services, to engage in business
collaborations and service compositions, to identify potential
service partners, and to understand service competitors and
increase the competitive edge of their service offerings.
Current web service discovery techniques can be classified
into two types: categorization-based discovery and personalized
relevance-based discovery. The former discovers web services
by clustering and categorizing a collection of web services
into different groups based on certain common properties of the
services. Most of the existing UDDI [28] registry-based service
discovery methods are of this type. They typically discover relevant
services by querying metadata maintained in the common
registries (like the ones offered by Microsoft [16] and IBM [10]).
A typical question is "Which bioinformatics web services offer
BLAST capability" or "Which commercial services offer on-line
auctions". The second type of discovery mechanisms uses
personalized relevance reasoning and support questions such
as "Which services offer the same type of content as NCBI",
and "Find the top-ten web services that offer more coverage
than the BLAST services at NCBI". These two types of service
discovery techniques offer different focus and complementary
capabilities. Consider the following examples:
· A bioinformatics researcher may be interested in finding all
services similar to NCBI's BLAST service for searching DNA
and protein sequence libraries [17]. Current service registries
may provide pointers to other BLAST services, but they do
not describe how these other sites relate specifically to NCBI's
BLAST service. Which services provide the most similar coverage
with respect to NCBI (e.g.
of similar proteins or organisms
)? Which services are complementary in their coverage
(e.g. of other sequence libraries)? How best should the BLAST
services be ranked relative to the NCBI service?
· A health science researcher familiar with the PubMed med-153
ical literature service may be interested in discovering other
related medical digital library services. Given his prior knowledge
with PubMed, he may want to ask certain personalized
(source-biased) discovery requests, which are not supported by
the conventional service-registry-based discovery model. Examples
include: Find and rank all PubMed-related medical literature
sites. Which services have more general coverage than
PubMed? Which medical literature services are more specialized
than PubMed?
These examples highlight two fundamental differences between
the categorization-based and the personalization-based
discovery model: (1) Categorization of web services based on
general descriptions maintained at the service registries are insufficient
and inadequate when a user is interested in discovery
based on a particular service (or a set of services) with which
she has prior experience or knowledge (e.g. NCBI BLAST or
PubMed); and (2) There is a need for service ranking metrics
that capture the relative scope and coverage of the data offered
with respect to a previously known service. Although these two
types of discovery mechanisms are complementary, most existing
proposals on web service discovery fall into the first type.
Surprisingly, there are, to our knowledge, no effective means
to provide such personalized and biased discovery and ranking
support without relying on significant human intervention.
In this paper we present algorithms for discovering and ranking
relevant data-intensive web services. Our first prototype ­
called BASIL
1
-- supports a personalized view of web services
through source-biased probing and source-biased relevance detection
and ranking metrics. Concretely, our approach is capable
of discovering and ranking web services by focusing on the
nature and degree of the data relevance of the source service
to others. Given a service like NCBI's BLAST ­ called the
source - the BASIL source-biased probing technique leverages
the summary information of the source to generate a series of
biased probes to other services ­ called the targets. This source-biased
probing allows us to determine whether a target service
is relevant to the source by probing the target with very few focused
probes. We introduce the biased focus metric to discover
and rank highly relevant data services and measure relevance
between services. Our initial results on both simulation and
web experiments show that the BASIL system supports efficient
discovery and ranking of data-intensive web services.
MODEL AND PROBLEM STATEMENT
We consider a universe of discourse
W consisting of D data-intensive
web services:
W = {S
1
, S
2
, . . . , S
D
} where each service
produces one or more XML documents in response to a
particular service request. Hence, we describe each web service
S
i
as a set of M
i
documents: S
i
=
{doc
1
, doc
2
,
· · · , doc
M
i
}. For
example, the documents corresponding to the NCBI BLAST
service would consist of genetic sequences and documentation
generated in response to a service requests. Similarly, the documents
corresponding to PubMed would consist of the set of
medical journal articles in the PubMed data repository.
There are N terms (t
1
, t
2
, ..., t
N
) in the universe of discourse
W ­ including both the tags and content of the XML documents
­ where common stopwords (like `a', `the', and so on) have been
eliminated.
Optionally, the set of N terms may be further
refined by stemming [19] to remove prefixes and suffixes.
Adopting a vector-space model [22, 23] of the service data
repository, we describe each service S
i
as a vector consisting of
the terms in the service along with a corresponding weight:
Summary(S
i
) =
{(t
1
, w
i
1
), (t
2
, w
i
2
),
· · · , (t
N
, w
iN
)
}
1
BASIL: BiAsed Service dIscovery aLgorithm
A term that does not occur in any documents served by a
service S
i
will have weight 0.
Typically, for any particular
service S
i
, only a fraction of the N terms will have non-zero
weight. We refer to the number of non-zero weighted terms in
S
i
as N
i
.
We call the vector
Summary(S
i
) a service summary for the
data-intensive web service S
i
. A service summary is a single aggregate
vector that summarizes the overall distribution of terms
in the set of documents produced by the service. In this first
prototype of BASIL, we rely on a bag-of-words model that is
indifferent to the structure inherent in the XML documents. As
we demonstrate in the experiments section, this bag-of-words
approach is quite powerful without the added burden of structural
comparisons. We anticipate augmenting future versions
of BASIL to incorporate structural components (to support
schema matching, leverage existing ontologies, etc.).
To find
Summary(S
i
), we must first represent each document
doc
j
(1
j  M) as a vector of terms and the frequency of
each term in the document:
doc
j
=
{(t
1
, f req
j
1
), (t
2
, f req
j
2
),
· · · , (t
N
, f req
jN
)
}
where f req
jk
is the frequency of occurrence of term t
k
in
document j. The initial weight for each term may be based
on the raw frequency of the term in the document and it can
be refined using alternative occurrence-based metrics like the
normalized frequency of the term and the term-frequency inverse
document-frequency (TFIDF ) weight. TFIDF weights
the terms in each document vector based on the characteristics
of all documents in the set of documents.
Given a particular encoding for each document, we may generate
the overall service summary in a number of ways. Initially
, the weight for each term in the service summary may be
based on the overall frequency of the term across all the documents
in the service (called the service frequency, or servFreq ):
w
ik
= servF req
ik
=
M
j
=1
f req
jk
. Alternatively, we can also
define the weight for each term based on the number of documents
in which each term occurs (called the document count
frequency, or docCount).
Once we have chosen our service model, to effectively compare
two data-intensive web services and determine the relevance
of one service to another, we need two technical components
: (1) a technique for generating a service summary; and
(2) a metric for measuring the relevance between the two.
2.1
Estimating Service Summaries
Ideally, we would have access to the complete set of documents
belonging to a data-intensive web service. We call a
service summary for S
i
built on these documents an actual
service summary or
ASummary(S
i
). However, the enormous
size of the underlying repositories for many data-intensive web
services coupled with the non-trivial costs of collecting documents
(through repeated service requests and individual document
transfer) make it unreasonable to generate an actual
service summary for every service available. As a result, previous
researchers in the context of distributed databases have
introduced several probing techniques for generating representative
summaries based on small samples of a document-based
collections [3, 4]. We call such a representative summary an
estimated service summary, or
ESummary(S
i
):
ESummary(S
i
) =
{(t
1
, w
i
1
), (t
2
, w
i
2
),
· · · , (t
N
, w
iN
)
}
The number of occurring terms (i.e. those terms that have
non-zero weight) in the estimated summary is denoted by N
i
.
Typically, N
i
will be much less than the number of non-zero
weighted terms N
i
in the actual service summary since only
154
a fraction of the total documents in a service will be examined
. The goal of a prober is typically to find
ESummary(S
i
)
such that the relative distribution of terms closely matches the
distribution of terms in
ASummary(S
i
), even though only a
fraction of the total service documents will be examined.
Current probing techniques for estimating service summaries
aim at estimating the overall summary of the data served by
a web service. We classify them into two categories: random
sampling and query-based sampling.
Random Sampling
- N o Bias
If we had unfettered access to a data-intensive web service,
we could randomly select terms from the service to generate
the estimated service summary
ESummary(S
i
). Barring that,
we could randomly select documents with which to base the
estimated service summary. We will call such a random selection
mechanism an unbiased prober since all terms (or documents
) are equally likely to be selected. In practice, an unbiased
prober is unrealistic since most services only provide a
request-response mechanism for extracting documents.
Query-based Sampling
- Query Bias
As a good approximation to unbiased probing, Callan et al. [3,
4] have introduced a query-based sampling technique for generating
accurate estimates of document-based collections by examining
only a fraction of the total documents. The Callan
technique has been shown to provide accurate estimates using
very few documents (e.g. several hundred). Adapting the
Callan technique to the web services context requires repeat-edly
requesting documents from a service using a limited set of
service requests. Since the documents extracted are not chosen
randomly, but are biased by the service request mechanism
through the ranking of returned documents and by providing
incomplete access to the entire data service repository, we say
that the Callan technique displays query bias. There are several
ways to define the limited set of queries, including random
selection from a general dictionary and random selection augmented
by terms drawn from the extracted documents from the
service. In the rest of the paper, when we refer to an estimated
service summary
ESummary(S
i
), we mean one that has been
produced by a query-biased prober.
2.2
Comparing Service Summaries
In order to determine the relevance of one service S
i
to another
service S
j
and to assess the nature of their relationship,
we require an appropriate relevance metric. There are a number
of possible relevance metrics to compare two service summaries
. A fairly simple and straightforward approach is based
on a count of the number of common terms in the two services
S
i
and S
j
:
rel(S
i
, S
j
) =
|ESummary(S
i
)
ESummary(S
j
)
|
max(
|ESummary(S
j
)
|, |ESummary(S
i
)
|)
Two services with exactly the same terms represented in their
estimated summaries will have rel(S
i
, S
j
) = 1, indicating the
highest possible degree of relevance. Conversely, two services
with no terms in common will have rel(S
i
, S
j
) = 0, indicating
the lowest possible degree of relevance.
We now use an example to illustrate why the existing service
summary estimation techniques are inadequate for effectively
discovering relevant services, especially in terms of the data
coverage of one (target) in the context of the other (source).
Example:
We collected fifty documents from the Google web
service, the PubMed web service, and ESPN's search site, respectively
, using a query-based sampling technique for service
summary estimation. Using the service summaries constructed,
we find that rel(Google, P ubM ed) = 0.05 and rel(ESP N ,
P ubM ed) = 0.06. In both cases the service summaries share
very few terms in common and hence both Google and ESPN
appear to be irrelevant with respect to PubMed, even though
Google provides considerable health-related content. Based on
these figures, we could incorrectly conclude that: (1) Google
is irrelevant to PubMed; and (2) Relatively speaking, ESPN is
more relevant to PubMed than Google.
This example underlines two critical problems with current
techniques for probing and comparing service summaries:
First, current service summary estimation techniques are concerned
with generating overall (or global ) summaries of the underlying
data repositories. The goal is to generate essentially an
unbiased estimate of the actual service summary. Second, the
current relevance comparison metric fails to serve as a valuable
ranking metric or indicator of interesting relationships between
target services in terms of the data coverage of a target service
with respect to the source.
THE BASIL SYSTEM
Bearing these issues in mind, we now introduce BASIL ­ an
efficient web service discovery and ranking prototype that relies
on a biased perspective of services rather than on a single
global perspective. BASIL relies on three fundamental steps:
(1) source-biased probing for web service discovery; (2) evaluation
and ranking of discovered services with the biased focus
metric; and (3) leveraging the biased perspective of service
sources and targets to discover interesting relationships.
3.1
Source-Biased Probing
Given a data-intensive web service ­ the source ­ the source-biased
probing technique leverages the summary information
of the source to generate a series of biased probes for analyzing
another service ­ the target. This source-biased probing allows
us to determine in very few interactions whether a target service
is relevant to the source by probing the target with focused
probes.
To help differentiate the source-biased approach from others
discussed in Section 2, in this section we use  to denote the
source service and  to denote the target service instead of S
i
and S
j
. Given two services  and  , the output of the source-biased
probing is a subjective service summary for  that is
biased towards . We define the source-biased summary of the
target service, denoted by
ESummary

( ), as follows:
ESummary

( ) =
{(t
1
, w

1
), (t
2
, w

2
),
· · · , (t
N
, w
N
)
}
N is the total number of terms used in analyzing the set of
data-intensive web services. w
i
(1
i  N) is the weight of
term t
i
, defined using one of the weight function introduced in
Section 2. To distinguish the term weight w
j
from the corresponding
term weight in the biased target summary, we denote
the bias by w
j
. It is important to note that typically the inequality
w
j
= w
j
does hold.
Concretely, the source-biased probing algorithm generates a
source-biased summary for a target as follows: It uses the estimated
service summary of the source , denoted by
ESummary(),
as a dictionary of candidate probe terms and sends a series of
query requests parameterized by probe terms, selected from
ESummary (), to the target service ; for each probe term,
it retrieves the top m matched documents from  , generates
summary terms and updates
ESummary

( ).
This process
repeats until a stopping condition is met. Figure 1 illustrates
the source-biased probing process. Note that in this first prototype
of BASIL the service requests are constrained to keyword-based
probes. Note that the source-biased approach can also
be applied to UDDI-directory-based discovery by restricting
155
SourceBiasedProbing(Source , Target  )
For target service  , initialize
ESummary

( ) =
.
repeat
Invoke the probe term selection algorithm
to select a one-term query probe q from the
source of bias
ESummary().
Send the query q to the target service  .
Retrieve the top-m documents from  .
Update
ESummary

( ) with the terms and
frequencies from the top-m documents.
until Stop probing condition is met.
return
ESummary

( )
Figure 1: Source-Biased Probing Algorithm
the source summary to be generated from the meta-data description
maintained at the registries rather than performing
the source-biased probing directly. However, the quality of the
discovery results will be lower due to the lack of richness in the
metadata maintained at the service registries for many services.
Now we use a simple example to illustrate the power of
source-biased probing. For presentation brevity, we are considering
a simplistic world of only very few terms per service
summary. In reality, each service summary would consist of
orders of magnitude more terms:
Example:
Suppose that our goal is to understand the relevance
of Google to PubMed. Suppose,
ESummary(P ubMed)
=
{arthritis, bacteria, cancer} (where for simplicity we have
dropped the term weights from the summary). Again for simplicity
suppose that Google provides access to only three types of
information: health, animals, and cars:
ASummary(Google)
=
{arthritis, bacteria, cancer, dog, elephant, frog, garage,
helmet, indycar
}. An unbiased prober could result in ESummary
(Google) =
{arthritis, frog, helmet}, whereas a source-biased
prober could result in
ESummary
P ubM ed
(Google) =
{arthritis,
bacteria, cancer
}. This simple example illustrates the essence
of the source-biased probing and how it accentuates the commonality
between the two services.
The performance and effectiveness of the source-biased probing
algorithm depends upon a number of factors, including the
selection criterion used for choosing source-specific candidate
probe terms, and the type of stop condition used to terminate
the probing process.
Mechanisms to Select Probe Terms
There are several possible ways to select the probes based on
the statistics stored with each service summary, including uniform
random selection and selection based on top-weighted
terms.
In general, the selection criterion will recommend a
query term drawn from the set N

of all non-zero weighted
terms in the unbiased source summary
ESummary().
Uniform Random Selection: In this simplest of selection techniques
, each term that occurs in
ESummary() has an equal
probability of being selected, i.e. P rob(selecting term j) =
1
N

.
Weight-Based Selection: Rather than randomly selecting query
terms, we could instead rely on a ranking of the terms by one of
the statistics that are recorded with each service summary. For
example, all terms in
ESummary() could be ranked according
to the weight of each term. Terms would then be selected in
descending order of weight. Depending on the type of weight
cataloged (e.g. servF req, docCount, etc.), several flavors of
weight-based selection may be considered.
Different Types of Stop Probing Conditions
The stop probing condition is the second critical component in
the source-biased probing algorithm. We consider four different
types of conditions that might be used in practice:
Number of Queries: After some fixed number of query probes
(M axP robes), end the probing. This condition is agnostic to
the number of documents that are examined for each service.
Documents Returned: In contrast to the first technique, the
second condition considers not the number of queries, but the
total number of documents (M axDocs) returned by the service
. Since some queries may return no documents, this stopping
condition will require more query probes than the first
alternative when M axP robes = M axDocs.
Document Thresholding: Rather than treating each document
the same, this third alternative applies a threshold value to
each document to determine if it should be counted toward
M axDocs.
For each document, we may calculate the relevance
of the document to the source of bias
ESummary(). If
the document relevance is greater than some threshold value,
then the document is counted. Otherwise, the document is
discarded.
Steady-State: Rather than relying on a count of queries or documents
, this final stopping condition alternative instead relies
on the estimated summary reaching a steady-state. After each
probe, we calculate the difference between the new value of
ESummary

( ) and the old value.
If the difference (which
may be calculated in a number of ways) is less than some small
value
, then we consider the summary stable and stop the
probing.
Due to the space limitation, we refer readers to our technical
report [5] for detailed experiments on the impact of these two
parameters.
3.2
Evaluating and Ranking Services
Given a source and a target service, once we generate the
source-biased summary for the target service, we need an efficient
mechanism to evaluate the source-biased relevance of a
target service with respect to the source. Once a set of target
services have been evaluated with the source-biased relevance
metric, we can then rank the target services with respect to the
source of bias. We begin by discussing the necessary components
of the source-biased metric.
Let  denote a source service modeled by an estimated summary
and  denote a target service with a -biased summary,
and let f ocus

( ) denote the source-biased focus measure. We
define f ocus

( ) to be a measure of the topical focus of the
target service  with respect to the source of bias . The focus
metric ranges from 0 to 1, with lower values indicating less
focus and higher values indicating more focus.
In general, f ocus is not a symmetric relation. We may describe
any two data-intensive web services  and  with the
focus in terms of  by f ocus

( ) or in terms of  by f ocus

().
We propose to use the well-known cosine similarity (or normalized
inner product) to approximate the source-biased focus
measure. We define the cosine-based focus as follows:
Cosine f ocus

( ) =
N
k
=1
w
k
w
k
N
k
=1
(w
k
)
2
·
N
k
=1
(w
k
)
2
where w
k
is the weight for term k in
ESummary() and
w
k
is the -biased weight for term k in
ESummary

( ). The
cosine ranges from 0 to 1, with higher scores indicating a higher
degree of similarity. In contrast, the cosine between orthogonal
vectors is 0, indicating that they are completely dissimilar. The
cosine measures the angle between two vectors, regardless of the
length of each vector. Intuitively, the cosine-based biased focus
is appealing since it reasonably captures the relevance between
two data-intensive web services.
Ranking Relevant Services
Given the biased focus measure, we may probe a group of tar-156
get services to identify the most relevant services to the source
of bias. For a single source of bias S
1
from our universe of discourse
W, we may evaluate multiple target services S
2
, S
3
, ...,
S
d
. For each target service, we may evaluate the appropriate
focus measure for each source-target pair (i.e. f ocus
S
1
(S
2
),
f ocus
S
1
(S
3
), etc.). We may then rank the target services in
descending order in terms of their source-biased focus with respect
to S
1
.
As we will show in our experiments section, source-biased
probing results in the identification of relevant services that
existing approaches may overlook. We also show that source-biased
probing can generate source-biased summaries of good
quality using far fewer documents than existing approaches,
placing significantly less burden on the target services.
3.3
Identifying Interesting Relationships
The critical third component of the BASIL system consists of
the techniques for exploiting and understanding interesting relationships
between services using a source-biased lens. By analyzing
the nature of the relationships between data-intensive
web services, we will provide support for understanding the relative
scope and coverage of one service with respect to another.
The source-biased probing framework and biased focus measure
provide the flexible building blocks for automated identification
of interesting relationships between services, especially
since the framework promotes an asymmetric source-biased
view for any two services. Our relationship discovery module
creates a flexible organization of services, where each service
is annotated with a list of relationship sets. The two typical
relationship types we have identified are similarity-based and
hierarchical-based.
Similarity-Based Relationships
Given the universe of discourse
W = {S
1
, S
2
, . . . , S
D
}, we identify
three similarity-based relationship sets for a particular service
S
i
. These relationship sets are defined in terms of threshold
values
high
and
low
, where 0

low

high
&lt; 1.

- equivalent: The first relationship says that if both focus
S
i
(S
j
) &gt;
high
and f ocus
S
j
(S
i
) &gt;
high
hold, then we may conclude
that S
i
is sufficiently focused on S
j
and S
j
is sufficiently
focused on S
i
. Hence, the two services are approximately the
same in terms of their data coverage. We call this approximate
equality -equivalence. It indicates that the equivalence
is not absolute but is a function of the parameter
high
. Formally
, -equivalent(S
i
) =
{S
j
W | focus
S
i
(S
j
) &gt;
high

f ocus
S
j
(S
i
) &gt;
high
}.

- complement: If both focus
S
i
(S
j
) &lt;
low
and f ocus
S
j
(S
i
)
&lt;
low
hold, then we can conclude that S
i
and S
j
are sufficiently
concerned with different topics since neither one is very
focused on the other. We annotate this approximate complementary
nature with the  prefix. Formally, -complement(S
i
) =
{S
j
W | focus
S
i
(S
j
) &lt;
low
focus
S
j
(S
i
) &lt;
low
}.

- overlap: When two services S
i
and S
j
are neither equivalent
nor -complementary, we say that the two services
-overlap. Formally, -overlap(S
i
) =
{S
j
W | S
j
/
complement
(S
i
)
S
j
/
-equivalent(S
i
)
}.
Hierarchical Relationships
In addition to similarity-based relationship sets, we also define
hierarchical relationship sets by measuring the relative coverage
of target services in
W with respect to a particular text service
S
i
(source). These hierarchical relationship sets are defined in
terms of a parameter
dif f
, where 0

dif f
1.

- superset: If focus
S
i
(S
j
)
- focus
S
j
(S
i
) &gt;
dif f
, then a
relatively significant portion of S
i
is contained in S
j
, indicating
that S
j
has a -superset relationship with S
j
. We use the  prefix
to indicate that S
j
is not a strict superset of S
i
, but rather
that the relationship is parameterized by
dif f
. Formally, superset
(S
i
) =
{S
j
W | focus
S
i
(S
j
)
- focus
S
j
(S
i
) &gt;

dif f
}.

- subset: Conversely, If focus
S
j
(S
i
)
- focus
S
i
(S
j
) &gt;
dif f
,
then a relatively significant portion of S
j
is contained in S
i
,
indicating that S
j
has a -subset relationship with S
i
. Similarly
, S
j
is not a strict subset of S
i
, but rather the relationship
is parameterized by
dif f
. Formally, -subset(S
i
) =
{S
j

W | focus
S
j
(S
i
)
- focus
S
i
(S
j
) &gt;
dif f
}.
We note that the determination of the appropriate -values
is critical for the correct assignation of services to each relationship
set. In our experiments section, we illustrate how these
relationship sets may be created, but, for now, we leave the
optimization of -values as future work.
Using Relationship Sets Both similarity based and hierarchy-based
inter-service relationships can be generated automati-cally
, and used as metadata annotation to each of the services.
These source-biased relevance data provide a flexible foundation
for relationship analysis among services. For any service
S
i
, we need only consult the appropriate relationship set. The
three similarity-based relationship sets provide the basis for
answering queries of the form: "What other services are most
like X? Somewhat like X? Or complementary to X?". The two
hierarchical-based sets provide the basis for answering queries
of the form: "What other services are more general than X?
Or more specialized than X?".
In addition, these relationship sets are useful for routing
service requests to the appropriate services. For example, a
user interested in BLAST data may choose to use both NCBI's
BLAST service and all of the services that have a -equivalence
relationship with NCBI BLAST. Alternatively, a user interested
in maximizing coverage of multiple topically-distinct services
, may choose to query both the source servi
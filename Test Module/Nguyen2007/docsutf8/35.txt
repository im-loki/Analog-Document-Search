An Intensional Approach to the Specification of Test Cases for Database Applications
ABSTRACT
When testing database applications, in addition to creating
in-memory fixtures it is also necessary to create an initial
database state that is appropriate for each test case. Current
approaches either require exact database states to be
specified in advance, or else generate a single initial state
(under guidance from the user) that is intended to be suitable
for execution of all test cases. The first method allows
large test suites to be executed in batch, but requires considerable
programmer effort to create the test cases (and
to maintain them). The second method requires less programmer
effort, but increases the likelihood that test cases
will fail in non-fault situations, due to unexpected changes
to the content of the database. In this paper, we propose a
new approach in which the database states required for testing
are specified intensionally, as constrained queries, that
can be used to prepare the database for testing automatically
. This technique overcomes the limitations of the other
approaches, and does not appear to impose significant performance
overheads.
Categories and Subject Descriptors
D.2.5 [Software Engineering]: Testing and Debugging-Testing
tools
General Terms
Experimentation, Verification

INTRODUCTION
Modern information systems are typically organised as
collections of independent application programs that communicate
with one another by means of a central database.
The database records the state of the organisation that the
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
ICSE'06,
May 20Â­28, 2006, Shanghai, China.
Copyright 2006 ACM 1-59593-085-X/06/0005 ...
$
5.00.
information system supports, while the application programs
implement the business processes that manipulate the state.
To take a simple but ubiquitous example, a database system
might record details of customers, products and sales,
while the application programs associated with it handle operations
such as new product purchases and update of the
product catalogue, as well as supporting decision making
by generating reports regarding the most profitable product
lines, names and addresses of loss-making customers, etc.
In order to test such application programs, it is necessary
to create test fixtures that simulate the presence of the rest
of the information system. Fixtures for traditional test cases
typically consist of in-memory objects and data structures
that provide the inputs to the program being tested. This
kind of fixture is also needed when testing database applications
(especially when performing unit testing); however,
since it is unrealistic (and often incorrect) to execute test
cases against an empty database, we need to create additional
fixture elements within the database itself.
Current practice in the software industry is to maintain
one or more test databases that can be used for testing individual
programs. These databases can be artificially generated
(e.g., using tools such as DBMonster
1
and DataFac-tory
2
) or they may be subsets of the live database, taken
as a snapshot at some recent point in time. Copies of the
live data sets have the advantage that they are more likely
to be representative of the patterns of data encountered in
practice, while artificial data sets have the advantage that
they can be made to embody specific characteristics (such
as particular data skew patterns or volumes), which may be
useful for load and stress testing.
Both approaches, however, suffer from several disadvantages
. The most significant problem occurs when none of
the available test databases are suitable starting points for a
particular test case. For example, suppose a particular test
case executes a program which purges inactive customers,
with the aim of verifying that the business rule forbidding
deletion of customers with negative balances is correctly en-forced
. If none of the test databases contains any inactive
customers with negative balances, then the test case cannot
be executed successfully. For a one-off test run, testing
personnel can choose a database that is close to what is required
, and manually update it so that it is suitable for use
with the test case. But if a complete test suite is to be executed
(possibly including test cases which themselves make
modifications to the database state) then in the worst case
1
http://DBMonster.kernelpanic.pl
2
http://www.quest.com/datafactory
102
this manual intervention will be required in between every
test case execution. This is clearly undesirable if test suites
are large or time-consuming to execute, or if the test suite
is to be run in batch (as in the case of overnight regression
testing, for example).
Current research in testing for database systems proposes
two approaches to this problem. One of these is to include
within the test case description a full (extensional) specification
of the database state against which it is to be run (and
of the database state that should be produced if the test has
executed successfully) [13, 14]. This solution is exemplified
by DBUnit
3
, an extension of the JUnit testing framework
4
that is designed for testing database applications written in
Java. Each DBUnit test case is accompanied by an XML
file describing the data set required for the test. Before each
test run, DBUnit clears the database state and inserts the
data described by the XML file.
This approach has the advantage of simplicity, but it places
a considerable burden on testing personnel, especially when
complex database states are required. It is also inefficient,
since the database must be continually destroyed and recre-ated
between tests, even when significant parts of the database
might have been reused by the succeeding tests. Moreover,
maintenance of a large suite of such tests is extremely challenging
, since any small change to the database schema may
require corresponding changes to many test cases.
The second approach that has been explored in the literature
is more efficient in that it requires the creation of only
one database state per test suite (rather than one per test
case). It is exemplified by the AGENDA database testing
toolkit [6, 7], which can automatically generate a database
state given information about the schema, some data generation
functions for individual attributes and some user-selected
heuristics describing the kind of database state required
. The AGENDA tool also generates test cases from a
simple analysis of the program being verified. The user must
then add preconditions to each test case that are checked
just before it is executed and that will prevent a case from
being executed against an inappropriate database state. This
approach successfully relieves the user of the need to specify
complete database states in full detail, but at a cost. The
user must accept that some of the test cases may not be
executed because the database state fails the precondition,
even when it would require only a small change to bring the
database into a suitable state for the test. Since only one
database state is created per test suite, this problem of failed
tests is likely to become more severe as the size of the test
suite grows. There is also a potential inefficiency involved
in generating test descriptions and inputs, and in creating
the additional log tables and constraints/triggers needed by
the AGENDA tool, for test cases that are not in fact going
to be executed.
Ideally, we would prefer to be able to combine the advantages
of both these approaches, to give a form of database
test case that is quick and natural to specify, and which
maximises the number of cases within the suite that can be
executed while minimising the number of full test databases
that need to be maintained.
Our thesis is that this can
be achieved by allowing testing personnel to describe the
database states involved in their test cases intensionally, in
3
http://www.dbunit.org
4
http://www.junit.org
the form of declarative conditions that the input database
must satisfy, and by providing a testing harness that can
automatically adjust the input database so that the test
conditions are satisfied [19].
In this paper, we present a language for specifying such
intensional database tests, and describe its semantics and
operational behaviour (Section 2). We present an algorithm
for automatically modifying database states so that test preconditions
are satisfied (Section 3), thus ensuring that all
test cases can be executed without requiring any human
intervention. We further describe how we have extended the
JUnit testing framework to allow intensional database tests
to be specified and executed in practice (Section 4). Finally,
we present the results of an evaluation of the performance
of the techniques (Section 5) and conclude (Section 6).
SPECIFYING INTENSIONAL TESTS
A conventional test case is typically modelled as a triple
&lt; p, i, o &gt;, which denotes a test that executes program p
with inputs (e.g., parameters) denoted by i. If no faults are
encountered during the test execution, the output that will
be produced is o. In the case of test cases for database applications
, we must add two further elements--the specification
of the database state against which p is to be executed,
and some statement of the database state that should result
from the execution of p if it is operating correctly according
to its specification.
For example, consider the example program mentioned
in Section 1 that prunes inactive customer details from the
database. For this test case, we require a database state that
contains at least one inactive customer. This could easily
be stated as a predicate logic condition over the database,
assuming the obvious mapping between stored relations and
predicates, e.g.:
(custNo, lastOrderOn, a, b, c)
customer (custNo, a, b, c, lastOrderOn)
lastOrderOn &lt; today - 90
The program in question does not access any parts of the
database other than the customer table. Therefore, we do
not care what values the other tables contain and need not
mention them in the intensional specification of the test.
This approach works equally well for observing the results
of the test. For example, when testing the customer pruning
behaviour, we might require that no inactive customer with
a non-negative balance should exist in the database after
the test:
Â¬((custNum, lastOrderDate, a, b, c)
customer (custNum, a, bal , c, lastOrderDate)
lastOrderDate &lt; today - 90  bal &gt; 0)
Effectively, the test case describes a set of valid (i.e., fault-free
) state transition for the database, as a classic pre/post-condition
pair.
This first-order-logic style of database specification does
not work so well when we consider the testing problem in
more depth, however. The problem is that we need to do
more than test the input database for compliance with the
requirements of the test case; we also need to extract information
from it to be used to instantiate other elements
103
of the test case. For example, suppose we wish to test a
program that deletes details of individual customers. Such
programs typically require some input from the user, identifying
the specific customer record that is to be deleted (e.g.,
by supplying the relevant customer code as a parameter).
This could be achieved by requiring the tester to embed the
customer code into the test case elements, as literal values.
Alternatively, we could search for a suitable customer that
already exists in the database, using a standard database
query, and use the values from that in specifying the inputs
for the test case. This would minimise the amount of work
required to prepare the database for test execution (since we
would be using data already present in the database), and it
would also mean that test cases can be written very quickly,
since the user does not need to specify every last detail of
the data to be used.
Under this approach, the specification of the input database
state now has a dual role: it must state the condition that
determines whether the database state is suitable for execution
of the test case and it must also return bindings for the
free variables that appear in the remaining components of
the test case. For the latter purpose, we would prefer to use
a straightforward query language, while for the former we
require the ability to place conditions on the data. With a
simple extension of a standard query language such as SQL,
we can combine both these purposes in a single statement.
For example, the following statement:
ANY :cn GENERATED BY
SELECT custNo FROM customer
WHERE lastOrderDate &lt; today() - 90
AND balance &lt; 0
retrieves the customer code of some record that meets the
given conditions (an inactive customer with negative balance
) from the database, and binds it to the variable :cn.
It also places a cardinality constraint on the result of the
query, that at least one such binding must exist (implied by
the use of the keyword ANY).
The variable :cn can then be used to specify other elements
of the test case. The obvious usage in this example is
in specifying the inputs to the program being tested, but it
can also be used in describing the expected outputs of the
program. In this example test case, the correct behaviour
of the DeleteCustomer program is to reject the deletion
of :cn, since customers with a negative balance cannot be
purged from the database. We might therefore give the following
specification of the desired output database state:
AT LEAST 1 :cn2 GENERATED BY
SELECT custNo FROM customer
WHERE custNo = :cn
Of course, not all test cases are best specified in terms of
values retrieved from the database. For example, suppose
that we wish to write test cases for a program that adds new
customers to the database. The inputs to this program are
the details of the new customer, and the precondition for one
particular test case states that no customer should exist that
has the same customer code as that of the customer being
created. We cannot retrieve the customer details from the
database in this case, as they have not yet been stored in it.
Again, we could force the user to include the required values
as literals in the test case, but ideally we would like to give
&lt;CONDITION&gt; ::= &lt;TYPE&gt; &lt;BINDINGLIST&gt;
GENERATED BY &lt;SELECT&gt;
&lt;TYPE&gt; ::= ANY | NO | AT LEAST &lt;i&gt; |
AT MOST &lt;i&gt; | EXACTLY &lt;i&gt; |
ALL | FIRST
&lt;i&gt;
::= {0-9}
&lt;BINDINGLIST&gt;
::=
&lt;BINDING&gt; { `,' &lt;BINDINGLIST&gt; }
&lt;BINDING&gt; ::= {A-Z | a-z}
&lt;SELECT&gt;
::= ...
Figure 1: Simplified BNF Grammar for SQL Extensions
more support to the process of test case generation. One
way to achieve this is to allow user-defined data generator
functions to be incorporated within queries as though they
were relations. For example, the following expression states
our requirements for this test case, while also binding the
variables needed for input to the program:
ANY :cn, :name, :addr, :bal GENERATED BY
SELECT gc.custno, gc.name, gc.addr, 0
FROM genCustomerDetails() AS gc
WHERE gc.custno NOT IN (
SELECT custno
FROM customer
WHERE balance &gt; 0)
Here, the data generator function getCustomerDetails()
is used as if it were a normal relation, whereas in fact the
results it returns are computed on the fly. In fact, several
of the main commercial database management systems already
allow user-defined functions to be embedded in queries
in this way, so this does not require a further extension of
SQL. Figure 1 shows the minimal extensions that are needed
to support all the kinds of constrained query shown above
using the SQL99 standard [17].
2.1
Test Case Semantics
Clearly, the semantics of these intensional database test
cases is more complex than for traditional extensional tests.
However, we can define their semantics formally in terms
of a mapping from intensional tests to sets of equivalent
extensional database test cases. We first present a formal
definition of the structure of our intensional test cases:
Definition 1. An intensional database test case is a quintuple
&lt; p, i, DB
i
, o, DB
o
&gt;, where:
Â· p is the program to be executed in the test,
Â· i is a tuple of n variables and literals that describes the
inputs to be given to program p, where n is the number
of parameters expected by p,
Â· DB
i
is a set of constrained queries that together specify
the initial database state.
Â· o is a tuple of m variables and literal that describes the
expected outputs from the program p.
Â· DB
o
is a set of constrained queries that together specify
the conditions that must hold in the database state after
execution of p if no fault has been encountered.
104
A constrained query has the form &lt; Q, min, max , vars &gt;,
where Q is a standard relational algebra query, min and
max describe the constraints on the cardinality of the query
result set, and vars is the list of variables bound by the
query result.
A database test case is well-formed for use with a particular
database schema  iff:
Â· for every variable v that occurs free in i, DB
i
, o and
DB
o
, there exists a query in DB
i
that provides a binding
for v,
Â· for every query &lt; q, n, m, vs &gt; in DB
i
DB
o
, q is a
well-formed query over  that returns k-tuples, where
|vs| = k, and
Â· there are no circular variable dependencies amongst
the queries in DB
i
.
We can now define a semantics for the intensional database
test cases as follows. Every intensional test case is equivalent
to a set of extensional test cases. An extensional test case
defines a specific test run, in terms of actual inputs and
outputs, rather than expressions denoting sets of inputs and
outputs.
The set of all possible extensional test cases is
given by:
P Ã L
n
Ã DB Ã L Ã DB
where P is the set of all programs, L is the set of all literals
, L
n
is the set of all n-tuples formed from L and DB
is the set of all database states (relative to all schemas)
5
.
The components of each extensional test are the program
to be tested, the input values, the initial database state,
the expected output and the expected final database state,
respectively.
An intensional test case is effectively a shorthand expression
for a set of extensional test cases that are all derived
from the same equivalence partition of the test case inputs.
An intensional database test &lt; p, i, DB
i
, o, DB
o
&gt;, where
DB
i
= {&lt; q
i
, n
i
, m
i
, v
i
&gt;} and DB
o
= {&lt; q
o
, n
o
, m
o
, v
o
&gt;},
is equivalent to the following set of extensional tests:
{&lt; p, i[v
i
/v], db
i
, o[v
i
/v], db
o
&gt; |
db
i
DB
(n
i
|q
i
(db
i
)|  m
i
)
v  q
i
(db
i
)
db
o
DB
(n
o
|(q
o
[v
i
/v])(db
o
)|  m
o
)}
We use the notation exp[
1
/
2
] to express the substitution of
the values in
1
by the corresponding values in
2
whereever
they occur in exp. Therefore, this expression denotes the set
of extensional tests where the input database satisfies the
constraints imposed by the initial constrained query, and
where the bindings from execution of that query (here expressed
as the tuple of variables v) are substituted into the
5
For simplicity of presentation, we assume that all programs
require the same number of inputs (n). In practice, n can
be the largest number of inputs required by any program,
and the unused values can be filled with nulls.
expressions defining the inputs, expected output and expected
final database state before they too are evaluated
6
.
The idea underlying this notion of an intensional test is
that when any of its corresponding extensional sets are executed
, the intensional test is itself deemed to have been
executed. Thus, the use of intensional tests allows much
greater freedom at test execution time, since we may choose
any of the possible extensional tests, depending on which is
closest to our starting environment. In the next section, we
will consider the practical ramifications of this approach to
testing, and describe how the semantics just described can
be implemented in practice.
DATABASE PREPARATION
The execution of an intensional database test case consists
of three distinct phases: 1) preparation of the environment
for test execution; 2) execution of the test with the
prepared inputs; and 3) capture and storage of the results,
for later analysis.
Since all the work of finding bindings
for the variables in the test case specification is done in the
preparation phase, the final two phases are straightforward
and differ little from standard testing procedures. When
program execution is complete, the constrained query that
determines whether the test has been successful or not is
evaluated against the database, and the output from the
program is checked against what is expected. In the case
of test failure, the details of the actual extensional test that
was executed are recorded, for diagnosis purposes.
The first phase, however, is more complex. If we were
content to execute only those test cases which happen to
be suitable for use with the initial database state, then the
preparation phase would simply be a matter of executing
the input constrained queries against the database and, if
they are all successful, using the bindings thus produced
to instantiate the remaining components of the test case.
However, thanks to the declarative nature of our test case
specifications, the testing framework can be pro-active in
cases where the given database is not suitable for use by
the test case, and can automatically generate a sequence of
updates that will cause the constrained queries to produce
the required number of bindings.
In fact, this problem is similar (though not identical) to
one that has been studied by the database and artificial intelligence
communities for many years. It is known variously
as the view update problem [9], the knowledge base update
problem [12], and the transaction repair problem [10]. Many
database systems have the capability to define views on top
of the basic database. A view is a kind of virtual relation.
To the user, it appears to be a normal relation, but it contains
no stored data. Instead, the contents of the view are
defined by a expression over other relations, and attempts
to retrieve data from the view are converted into queries
over these relations. To take a simple example for illustration
, we might create a view called Debtors which appears
to be a relation of the same name containing all customers
with a negative balance. Attempts to retrieve Debtors is
6
For simplicity of presentation, we assume here that there
is only one query in each of DB
i
and DB
o
. In practice,
it may be necessary to include several queries, each producing
different bindings and imposing different cardinality
constraints. In this case, the constraints must be conjoined,
and the full set of bindings can be retrieved by performing
a natural join of all the queries, with join condition true.
105
converted into a query against the customer table with an
added constraint on the balance.
If views are truly to act as normal relations then it should
be possible to update them as well query them. But what
does it mean to update a virtual relation? In this case, the
view update must be converted into a sequence of updates
on the stored relations that will cause the desired change in
the contents of the view itself. This is a non-trivial problem
for realistic view languages, and becomes even more difficult
when we move into the context of knowledge bases, where
virtual relations can be defined using rules over other relations
, and when we add integrity constraints that must be
maintained by all updates [1, 2, 3, 4, 5, 8, 11].
Only in very narrow circumstances does a view update
have a single translation into real updates [15, 18]. Various
heuristics for selecting from amongst the possible translations
have been proposed (of which the most common is to
choose the update that results in the smallest change to the
existing data set [2]), but in real applications user input is
needed in order to identify the translation that corresponds
most closely to the real world state that the database should
reflect [10].
In the case of intensional database tests, we have a query
(the constrained query that describes our requirements for
the test) that does not produce the correct number of answers
when executed against the test database. We need to
find a sequence of updates to the base data that will cause
our query to produce the number of answers we need. However
, in this case, there is no requirement to find the set of
updates that matches the state of reality -- any sensible update
that satisfies the query conditions will be acceptable.
This simplifies the problem considerably, removing the need
for complex search procedures and for any user input.
3.1
The Preparation Algorithm
One of the advantages of using a query-based language
for test specification (as opposed to a predicate calculus-based
language) is that we can make use of a very common
and easy-to-analyse internal form for (relational) database
queries, called relational algebra. This form provides a small
number of operations on relations that can be combined to
form complex queries. For example, the three most basic
(and useful) relational algebra operators are:
Â· The projection operator,
Atts
R, which creates a relation
from R by deleting all attributes not in Atts.
For example,
[Country]
Customer produces a relation
that contains just the countries that appear in the
Customer
relation.
Â· The selection operator,
c
R, which creates a relation
that contains all the rows from relation R that satisfy
the condition c. For example,
bal &lt;0
Customer returns
a relation containing details of all customers with negative
balances.
Â· The join operator, R
1
c
S, which creates a relation
containing rows from the cross product of R and S that
satisfy the join condition c. The query Debtor
1
dNo=iNo
Inactive returns details of all debtors who are also inactive
.
Since the result of each relational algebra operator is itself
a relation, together they form a closed algebra. This means
that we can form arbitrarily complex queries by applying
operators to the results of other operators. For example, a
query which retrieves the customer number of all customers
with a negative balance would be written as:

[custNo]
(
balance&lt;0
Customer )
A common way to visualise such expressions is as a tree of
operators. The tree for the above query is shown in Figure 2.
Figure 2: Relational Algebra Tree for Negative Balance
Query.
Our algorithm for preparing a database for testing is based
around this notion of a relational algebra tree. We take the
cardinality constraints from the test specification, and push
them down through the nodes of the input database query
tree, collecting up additional conditions as we go. When we
reach a leaf node (i.e. a base relation), we make updates
to the database so that the pushed-down constraints are
satisfied for that relation.
At each stage, we collect up the different kinds of constraint
and push them further down into the tree. These
constraint types are:
Â· Min and Max, the upper and lower bounds on the desired
cardinality of the result set.
Â· SelC, the selection conditions on the relations that we
are interested in.
Â· UAtts, the collection of attributes that are used in the
constrained query, and that must be populated in any
new data that we insert.
We also build up a collection of queries that describe the
data that has been prepared for testing so far, as we progress
through the tree. We call these queries "bindings" (Bgs),
since they give us values for the variables that occur within
the selection and join conditions. At each stage, the bindings
should contain one query for each leaf node that has so far
been prepared.
It is easiest to see how this works by considering a simple
example, such as that shown in Figure 2. Let us assume we
have a constrained query that requires at least one customer
with negative balance to exist, and that our database does
not currently contain any such customers. We begin at the
root node of the tree, with only the cardinality constraints
extracted from the test specification:
Min = 1, Max = null, SelC = true,
UAtts = , Bgs =
The top node is a projection operator. Projection does not
affect the cardinality of the result set, nor impose any conditions
, but it does tell us something about the attributes used
106
Figure 3: Relational Algebra Tree Showing Multiple
Joins
by the query. We therefore add the projection attributes to
UAtts and push the constraints down to the next node:
Min = 1, Max = null, SelC = true,
UAtts = {custNo}, Bgs =
Next we must deal with the selection node. Selection nodes
reduce the cardinality of their input, so we need to push
down the selection conditions to ensure that any updates
we may make affect the correct tuples. We also need to add
any attributes appearing in the selection condition to UAtts:
Min = 1, Max = null, SelC = balance &lt; 0,
UAtts = {custNo, balance}, Bgs =
The final node is the leaf node, representing the Customer
relation. We construct a query from the conditions on that
relation and execute it, to find out how many answers are
currently in the database. In this case, there are none, so
we need to insert a new Customer record with at least
the custNo and balance attributes populated, and with
a negative balance. If there are any integrity constraints
on this relation, then we need to make sure they are also
satisfied by the new data.
We use the DBMonster data generator mentioned earlier
to create the new data. It allows generation functions to
be specified for attributes, and additional constraints to be
placed on them. It will also maintain primary key, foreign
key, non-null and domain constraints if configured appro-priately
using the information present in the pushed-down
constraints.
Of course, this is a very simple example. In general, we
can expect to have to deal with more complicated queries
involving several joins, such as that shown in Figure 3. This
relational algebra tree is equivalent to the following constrained
query:
ANY :orderNo, :productNo GENERATED BY
SELECT o.orderno, p.productno
FROM Order o, Orderdetail d, Product p
WHERE o.orderno = d.orderno AND
d.productno = p.productno AND
p.price &gt; 50
which requires that at least one order must exist that involves
the purchase of at least one product that costs more
than Â£50. Joins complicate the process of preparing the
database, because they introduce dependencies between the
updates that take place at different leaf nodes. For example,
imagine that we have processed the tree shown in Figure 3 as
far as the leaf node representing the OrderDetail relation.
Join operators further constrain the selection condition (by
conjoining in their join condition), but add no other constraints
. So, by the time we reach this leaf node, SelC will
have been set to:
o.orderno = d.orderno  d.productno = p.productno
We need to find out whether a suitable OrderDetail record
exists within the database. However, in order to do this,
we need to know something about what preparation actions
were performed when the Product leaf node was processed.
Maybe there were already plenty of Â£50-plus products in
the catalogue, or maybe there were none and one had to
be created. How is this information passed through to the
OrderDetail
node so that the correct tuple can be identified
or created?
In the current version of our algorithm, we have chosen
to use the database itself to communicate these values. If
there are many suitable Product records, then we can find
one by querying the database directly once again. If a new
product had to be created, then it will now be present in
the database, so we can still retrieve it by querying. The
information needed to construct these queries is present in
the selection conditions that have been considered during
the processing of the relational algebra tree up to this point.
For example, in order to search for an OrderDetail tuple
that is connected to a suitable Product, we need to issue
the following query:
SELECT d.* FROM OrderDetail d, Product p
WHERE d.productno = p.productno AND
p.price &gt; 50
This query cannot be constructed from only the constraints
pushed-down from the parent nodes of the leaf node; instead,
we need to collect up the constraints imposed by all nodes
visited before 
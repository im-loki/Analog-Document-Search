A Computational Approach to Reflective Meta-Reasoning about Languages with Bindings
Abstract
We present a foundation for a computational meta-theory of languages
with bindings implemented in a computer-aided formal reasoning
environment. Our theory provides the ability to reason abstractly
about operators, languages, open-ended languages, classes
of languages, etc. The theory is based on the ideas of higher-order
abstract syntax, with an appropriate induction principle parameterized
over the language (i.e. a set of operators) being used. In our approach
, both the bound and free variables are treated uniformly and
this uniform treatment extends naturally to variable-length bindings
. The implementation is reflective, namely there is a natural
mapping between the meta-language of the theorem-prover and the
object language of our theory. The object language substitution operation
is mapped to the meta-language substitution and does not
need to be defined recursively. Our approach does not require designing
a custom type theory; in this paper we describe the implementation
of this foundational theory within a general-purpose
type theory. This work is fully implemented in the MetaPRL theorem
prover, using the pre-existing NuPRL-like Martin-L¨of-style
computational type theory. Based on this implementation, we lay
out an outline for a framework for programming language experimentation
and exploration as well as a general reflective reasoning
framework. This paper also includes a short survey of the existing
approaches to syntactic reflection.
Categories and Subject Descriptors
D.3.1 [Programming Languages
]: Formal Definitions and Theory--Syntax; F.4.3 [Mathematical
Logic and Formal Languages]: Formal Languages-Operations
on languages
General Terms
Languages, Theory, Verification

An extended version of this paper is available as Caltech Technical Report
CaltechCSTR:2005.003 <A href="3.html#11">[NKYH05]
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. To copy otherwise, to republish, to post on servers or to redistribute
to lists, requires prior specific permission and/or a fee.
MERLIN'05
September 30, 2005, Tallinn, Estonia.
Copyright c 2005 ACM 1-59593-072-8/05/0009. . . $5.00.

Introduction
1.1
Reflection
Very generally, reflection is the ability of a system to be "self-aware"
in some way. More specifically, by reflection we mean the
property of a computational or formal system to be able to access
and internalize some of its own properties.
There are many areas of computer science where reflection
plays or should play a major role. When exploring properties of
programming languages (and other languages) one often realizes
that languages have at least two kinds of properties -- semantic
properties that have to do with the meaning of what the language's
constructs express and syntactic properties of the language itself.
Suppose for example that we are exploring some language that
contains arithmetic operations. And in particular, in this language
one can write polynomials like x
2
+ 2x + 1. In this case the number
of roots of a polynomial is a semantic property since it has to do
with the valuation of the polynomial. On the other hand, the degree
of a polynomial could be considered an example of a syntactic
property since the most natural way to define it is as a property of
the expression that represents that polynomial. Of course, syntactic
properties often have semantic consequences, which is what makes
them especially important. In this example, the number of roots of
a polynomial is bounded by its degree.
Another area where reflection plays an important role is run-time
code generation -- in most cases, a language that supports
run-time code generation is essentially reflective, as it is capable
of manipulating its own syntax. In order to reason about run-time
code generation and to express its semantics and properties, it is
natural to use a reasoning system that is reflective as well.
There are many different flavors of reflection. The syntactic
reflection we have seen in the examples above, which is the ability
of a system to internalize its own syntax, is just one of these
many flavors. Another very important kind of reflection is logical
reflection, which is the ability of a reasoning system or logic to
internalize and reason about its own logical properties. A good
example of a logical reflection is reasoning about knowledge -since
the result of reasoning about knowledge is knowledge itself,
the logic of knowledge is naturally reflective <A href="3.html#10">[Art04].
In most cases it is natural for reflection to be iterated. In the
case of syntactic reflection we might care not only about the syntax
of our language, but also about the syntax used for expressing the
syntax, the syntax for expressing the syntax for expressing the
syntax and so forth. In the case of the logic of knowledge it is
natural to have iterations of the form "I know that he knows that
I know . . .".
When a formal system is used to reason about properties of programming
languages, iterated reflection magnifies the power of the
2
system, making it more natural to reason not just about individual
languages, but also about classes of languages, language schemas,
and so on. More generally, reflection adds a lot of additional power
to a formal reasoning system <A href="3.html#10">[GS89, Art99]. In particular, it is
well-known <A href="3.html#10">[G¨od36, <A href="3.html#11">Mos52, <A href="3.html#10">EM71, <A href="3.html#11">Par71] that reflection allows
a super-exponential reduction in the size of certain proofs. In addition
, reflection could be a very useful mechanism for implementing
proof search algorithms <A href="3.html#9">[ACU93, <A href="3.html#10">GWZ00, CFW04]. See also
<A href="3.html#10">[Har95] for a survey of reflection in theorem proving.
1.2
Uniform Reflection Framework
For each of the examples in the previous section there are many
ad-hoc ways of achieving the specific benefits of a specific flavor
of reflection. This work aims at creating a unifying reflective
framework that would allow achieving most of these benefits in a
uniform manner, without having to reinvent and re-implement the
basic reflective methodology every time. We believe that such a
framework will increase the power of the formal reasoning tools,
and it may also become an invaluable tool for exploring the properties
of novel programming languages, for analyzing run-time code
generation, and for formalizing logics of knowledge.
This paper establishes a foundation for the development of this
framework -- a new approach to reflective meta-reasoning about
languages with bindings. We present a theory of syntax that:
·
in a natural way provides both a higher-order abstract syntax
(HOAS) approach to bindings and a de Bruijn-style approach
to bindings, with easy and natural translation between the two;
·
provides a uniform HOAS-style approach to both bound and
free variables that extends naturally to variable-length "vectors"
of binders;
·
permits meta-reasoning about languages -- in particular, the
operators, languages, open-ended languages, classes of languages
etc. are all first-class objects that can be reasoned about
both abstractly and concretely;
·
comes with a natural induction principle for syntax that can be
parameterized by the language being used;
·
provides a natural mapping between the object syntax and meta-syntax
that is free of exotic terms, and allows mapping the
object-level substitution operation directly to the meta-level one
(i.e. -reduction);
·
is fully derived in a pre-existing type theory in a theorem
prover;
·
is designed to serve as a foundation for a general reflective
reasoning framework in a theorem prover;
·
is designed to serve as a foundation for a programming language
experimentation framework.
The paper is structured as follows. Our work inherits a large
number of ideas from previous efforts and we start in Section <A href="3.html#2">2
with a brief survey of existing techniques for formal reasoning
about syntax. Next in Section <A href="3.html#4">3 we outline our approach to reasoning
about syntax and in Section <A href="3.html#5">4 we present a formal account
of our theory based on a Martin-L¨of style computational type theory
<A href="3.html#10">[CAB
<A href="3.html#10">+
<A href="3.html#10">86, HAB
<A href="3.html#10">+
<A href="3.html#10">] and the implementation of that account in
the MetaPRL theorem prover <A href="3.html#11">[Hic97, Hic99, Hic01, HNC
<A href="3.html#11">+
<A href="3.html#11">03,
<A href="3.html#11">HNK
<A href="3.html#11">+
<A href="3.html#11">, <A href="3.html#10">HAB
<A href="3.html#10">+
<A href="3.html#10">]. Then in Section <A href="3.html#8">5 we outline our plan for building
a uniform reflection framework based on the syntactic reflection.
Finally, in Section <A href="3.html#9">6 we resume the discussion of related work that
was started in Section <A href="3.html#2">2.
1.3
Notation and Terminology
We believe that our approach to reasoning about syntax is fairly
general and does not rely on any special features of the theorem
prover we use. However, since we implement this theory in
MetaPRL, we introduce some basic knowledge about MetaPRL
terms.
A MetaPRL term consists of:
1. An operator name (like "sum"), which is a unique name indicating
the logic and component of a term;
2. A list of parameters representing constant values; and
3. A set of subterms with possible variable bindings.
We use the following syntax to describe terms, based on the NuPRL
definition <A href="3.html#9">[ACHA90]:
opname
operator name
[ p
1
; · · · ; p
n
]
parameters
{v
1
.t
1
; · · · ; v
m
.t
m
}
subt er ms
In addition, MetaPRL has a meta-syntax somewhat similar to
the higher-order abstract syntax presented in Pfenning and Elliott
<A href="3.html#11">[PE88]. MetaPRL uses the second-order variables in the style of
Huet and Lang <A href="3.html#11">[HL78] to describe term schemas. For example,
x.V [x], where V is a second-order variable of arity 1, is a schema
that stands for an arbitrary term whose top-level operator is .
This meta-syntax requires that every time a binding occurrence
is explicitly specified in a schema, all corresponding bound occurrences
have to be specified as well. This requirement makes it very
easy to specify free variable restrictions -- for example, x.V ,
where V is a second-order meta-variable of arity 0, is a schema
that stands for an arbitrary term whose top-level operator is  and
whose body does not have any free occurrences of the variable
bound by that . In particular, the schema x.V matches the term
y.1, but not the term x.x.
In addition, this meta-language allows specifying certain term
transformations, including implicit substitution specifications. For
example, a beta reduction transformation may be specified using
the following schema:
(x.V
1
[x]) V
2
V
1
[V
2
]
Here the substitution of V
2
for x in V
1
is specified implicitly.
Throughout this paper we will use this second-order notation to
denote arbitrary terms -- namely, unless stated otherwise, when we
write "x.t [x]" we mean an arbitrary term of this form, not a term
containing a concrete second-order variable named "t".
As in LF <A href="3.html#11">[HHP93] we assume that object level variables (i.e.
the variables of the language whose syntax we are expressing)
are directly mapped to meta-theory variables (i.e. the variable of
the language that we use to express the syntax). Similarly, we
assume that the object-level binding structure is mapped to the
meta-level binding structure. In other words, the object-level notion
of the "binding/bound occurrence" is a subset of that in the metalanguage
. We also consider -equal terms -- both on the object
level and on the meta-level -- to be identical and we assume that
substitution avoids capture by renaming.
The sequent schema language we use <A href="3.html#11">[NH02] contains a number
of more advanced features in addition to those outlined here.
However, for the purposes of this presentation, the basic features
outlined above are sufficient.
Previous Models of Reflection
In 1931 G¨odel used reflection to prove his famous incompleteness
theorem <A href="3.html#10">[G¨od31]. To express arithmetic in arithmetic itself, he
assigned a unique number (a G¨odel number) to each arithmetic
3
formula. A G¨odel number of a formula is essentially a numeric
code of a string of symbols used to represent that formula.
A modern version of the G¨odel's approach was used by Aitken
et al. <A href="3.html#9">[ACHA90, AC92, ACU93, <A href="3.html#10">Con94] to implement reflection
in the NuPRL theorem prover <A href="3.html#10">[CAB
<A href="3.html#10">+
<A href="3.html#10">86, <A href="3.html#9">ACE
<A href="3.html#9">+
<A href="3.html#9">00]. A large part
of this effort was essentially a reimplementation of the core of the
NuPRL prover inside NuPRL's logical theory.
In G¨odel's approach and its variations (including Aitken's one),
a general mechanism that could be used for formalizing one logical
theory in another is applied to formalizing a logical theory in itself.
This can be very convenient for reasoning about reflection, but for
our purposes it turns out to be extremely impractical. First, when
formalizing a theory in itself using generic means, the identity
between the theory being formalized and the one in which the
formalization happens becomes very obfuscated, which makes it
almost impossible to relate the reflected theory back to the original
one. Second, when one has a theorem proving system that already
implements the logical theory in question, creating a completely
new implementation of this logical theory inside itself is a very
tedious redundant effort. Another practical disadvantage of the
G¨odel numbers approach is that it tends to blow up the size of
the formulas; and iterated reflection would cause the blow-up to
be iterated as well, making it exponential or worse.
A much more practical approach is being used in some programming
languages, such as Lisp and Scheme. There, the common
solution is for the implementation to expose its internal syntax
representation to user-level code by the quote constructor (where
quote (t) prevents the evaluation of the expression t). The problems
outlined above are solved instantly by this approach: there is
no blow-up, there is no repetition of structure definitions, there is
even no need for verifying that the reflected part is equivalent to the
original implementation since they are identical. Most Scheme implementations
take this even further: the eval function is the internal
function for evaluating a Scheme expression, which is exposed
to the user-level; Smith <A href="3.html#11">[Smi84] showed how this approach can
achieve an infinite tower of processors. A similar language with the
quotation and antiquotation operators was introduced in <A href="3.html#10">[GMO03].
This approach, however, violates the congruence property with
respect to computation: if two terms are computationally equal then
one can be substituted for the other in any context. For instance,
although 2  2 is equal to 4, the expressions "2*2" and "4" are
syntactically different, thus we can not substitute 2*2 by 4 in
the expression quote(2*2). The congruence property is essential
in many logical reasoning systems, including the NuPRL system
mentioned above and the MetaPRL system <A href="3.html#11">[HNC
<A href="3.html#11">+
<A href="3.html#11">03, HNK
<A href="3.html#11">+
<A href="3.html#11">,
<A href="3.html#10">HAB
<A href="3.html#10">+
<A href="3.html#10">] that our group uses.
A possible way to expose the internal syntax without violating
the congruence property is to use the so-called "quoted" or
"shifted" operators <A href="3.html#9">[AA99, <A href="3.html#10">Bar01, Bar05] rather than quoting the
whole expression at once. For any operator op in the original language
, we add the quoted operator (denoted as op) to represent a
term built with the operator op. For example, if the original language
contains the constant "0" (which, presumably, represents the
number 0), then in the reflected language, 0 would stand for the
term that denotes the expression "0". Generally, the quoted operator
has the same arity as the original operator, but it is defined on
syntactic terms rather than on semantic objects. For instance, while
 is a binary operator on numbers,  is a binary operator on terms.
Namely, if t
1
and t
2
are syntactic terms that stand for expressions
e
1
and e
2
respectively, then t
1
t
2
is a new syntactic term that stands
for the expression e
1
e
2
. Thus, the quotation of the expression 12
would be 1  2.
In general, the well-formedness (typing) rule for a quoted operator
is the following:
t
1
Term
. . .
t
n
Term
op{t
1
; . . . ; t
n
}  Term
(1)
where Term is a type of terms.
Note that quotations can be iterated arbitrarily many times,
allowing us to quote quoted terms. For instance, 1 stands for the
term that denotes the term that denotes the numeral 1.
Problems arise when quoting expressions that contain binding
variables. For example, what is the quotation of x.x? There are
several possible ways of answering this question. A commonly
used approach <A href="3.html#11">[PE88, <A href="3.html#10">DH94, DFH95, <A href="3.html#9">ACM02, ACM03] in logical
frameworks such as Elf <A href="3.html#11">[Pfe89], LF <A href="3.html#11">[HHP93], and Isabelle <A href="3.html#11">[PN90,
Pau94] is to construct an object logic with a concrete  operator
that has a type like
(Term  Term)  Term or (Var  Term)  Term.
In this approach, the quoted x.x might look like (x.x) and the
quoted x.1 might look like (x.1). Note that in these examples
the quoted terms have to make use of both the syntactic (i.e. quoted)
operator  and the semantic operator .
Exotic Terms.
Na¨ive implementations of the above approach
suffer from the well-known problem of exotic terms <A href="3.html#10">[DH95,
DFH95]. The issue is that in general we can not allow applying
the  operator to an arbitrary function that maps terms to terms (or
variables to terms) and expect the result of such an application to
be a "proper" reflected term.
Consider for example the following term:
(x. if x = 1 then 1 else 2)
It is relatively easy to see that it is not a real syntactic term and
can not be obtained by quoting an actual term. (For comparison,
consider (x. if x = 1 then 1 else 2), which is a quotation of
x. if x = 1 then 1 else 2).
How can one ensure that e denotes a "real" term and not an
"exotic" one? That is, is it equal to a result of quoting an actual
term of the object language? One possibility is to require e to be
a substitution function; in other words it has to be equal to an
expression of the form x.t [x] where t is composed entirely of term
constructors (i.e. quoted operators) and x, while using destructors
(such as case analysis, the if operator used in the example above,
etc) is prohibited.
There are a number of approaches to enforcing the above restriction
. One of them is the usage of logical frameworks with restricted
function spaces <A href="3.html#11">[PE88, HHP93], where -terms may only contain
constructors. Another is to first formalize the larger type that
does include exotic terms and then to define recursively a predicate
describing the "validity" or "well-formedness" of a term <A href="3.html#10">[DH94,
DFH95] thus removing the exotic terms from consideration. Yet
another approach is to create a specialized type theory that combines
the idea of restricted function spaces with a modal type operator
<A href="3.html#10">[DPS97, DL99, DL01]. There the case analysis is disallowed
on objects of "pure" type T , but is allowed on objects of a special
type
T . This allows expressing both the restricted function space
"T
1
T
2
" and the unrestricted one "( T
1
)  T
2
" within a single
type theory.
Another way of regarding the problem of exotic terms is that it
is caused by the attempt to give a semantic definition to a primarily
syntactic property. A more syntax-oriented approach was used by
Barzilay et al. <A href="3.html#10">[BA02, BAC03, Bar05]. In Barzilay's approach, the
quoted version of an operator that introduces a binding has the
same shape (i.e. the number of subterms and the binding structure)
as the original one and the variables (both the binding and the
4
bound occurrences) are unaffected by the quotation. For instance,
the quotation of x.x is just x.x.
The advantages of this approach include:
·
This approach is simple and clear.
·
Quoted terms have the same structure as original ones, inheriting
a lot of properties of the object syntax.
·
In all the above approaches, the -equivalence relation for
quoted terms is inherited "for free". For example, x.x and
y.y are automatically considered to be the same term.
·
Substitution is also easy: we do not need to re-implement the
substitution that renames binding variables to avoid the capture
of free variables; we can use the substitution of the original
language instead.
To prune exotic terms, Barzilay says that x.t [x] is a valid term
when x.t [x] is a substitution function. He demonstrates that it is
possible to formalize this notion in a purely syntactical fashion. In
this setting, the general well-formedness rule for quoted terms with
bindings is the following:
is subst
k
{x
1
, · · · , x
k
.t[x]}
· · ·
is subst
l
{z
1
, · · · , z
l
.s[z]}
op{x
1
, · · · , x
k
.t[x];
· · · ;
z
1
, · · · , z
l
.s[z]}  Term
(2)
where is subst
n
{x
1
, · · · , x
n
.t[x]} is the proposition that t is a substitution
function over variables x
1
, · · · , x
n
(in other words, it is a
syntactic version of the Valid predicate of <A href="3.html#10">[DH94, DFH95]). This
proposition is defined syntactically by the following two rules:
is subst
n
{x
1
, · · · , x
n
. x
i
}
and
is subst
n+k
{x
1
, · · · , x
n
, y
1
, · · · , y
k
.t[x; y]}
.
.
.
is subst
n+l
{x
1
, · · · , x
n
, z
1
, · · · , z
l
.s[x; z]}}
is subst
n
{x
1
· · · x
n
.op{y
1
· · · y
k
.t[x; y]; · · · ; z
1
· · · z
l
.s[x; z]}}
In this approach the is subst
n
{} and  operators are essentially
untyped (in NuPRL type theory, the computational properties of
untyped terms are at the core of the semantics; types are added on
top of the untyped computational system).
Recursive Definition and Structural Induction Principle.
A
difficulty shared by both the straightforward implementations of
the (Term  Term)  Term approach and by the Barzilay's one
is the problem of recursively defining the Term type. We want to
define the Term type as the smallest set satisfying rules <A href="3.html#3">(1) and <A href="3.html#4">(2).
Note, however, that unlike rule <A href="3.html#3">(1), rule <A href="3.html#4">(2) is not monotonic in the
sense that is subst
k
{x
1
, · · · , x
k
.t[x]} depends non-monotonically
on the Term type. For example, to say whether x.t [x] is a term, we
should check whether t is a substitution function over x. It means at
least that for every x in Term, t [x] should be in Term as well. Thus
we need to define the whole type Term before using <A href="3.html#4">(2), which
produces a logical circle. Moreover, since  has type (Term 
Term)  Term, it is hard to formulate the structural induction
principle for terms built with the  term constructor.
Variable-Length Lists of Binders.
In Barzilay's approach, for
each number n, is subst
n
{} is considered to be a separate operator
-- there is no way to quantify over n, and there is no way to
express variable-length lists of binders. This issue of expressing the
unbounded-length lists of binders is common to some of the other
approaches as well.
Meta-Reasoning.
Another difficulty that is especially apparent
in Barzilay's approach is that it only allows reasoning about concrete
operators in concrete languages. This approach does not provide
the ability to reason about operators abstractly; in particular,
there is no way to state and prove meta-theorems that quantify over
operators or languages, much less classes of languages.
Higher-Order Abstract Syntax with Inductive Definitions
Although it is possible to solve the problems outlined in the previous
Section (and we will return to the discussion of some of those
solutions in Section <A href="3.html#9">6), our desire is to avoid these difficulties from
the start. We propose a natural model of reflection that manages to
work around those difficulties. We will show how to give a simple
recursive definition of terms with binding variables, which does
not allow the construction of exotic terms and does allow structural
induction on terms.
In this Section we provide a conceptual overview of our approach
; details are given in Section <A href="3.html#5">4.
3.1
Bound Terms
One of the key ideas of our approach is how we deal with terms
containing free variables. We extend to free variables the principle
that variable names do not really matter. In fact, we model free
variables as bindings that can be arbitrarily -renamed. Namely,
we will write bterm{x
1
, · · · , x
n
.t[x]} for a term t over variables
x
1
, · · · , x
n
. For example, instead of term xy we will use the
term bterm{x, y.xy} when it is considered over variables x and
y and bterm{x, y, z.xy} when it is considered over variables x,
y and z. Free occurrences of x
i
in t [x] are considered bound
in bterm{x
1
, · · · , x
n
.t[x]} and two -equal bterm{} expressions
("bterms") are considered to be identical.
Not every bterm is necessarily well-formed. We will define the
type of terms in such a way as to eliminate exotic terms. Consider
for example a definition of lambda-terms.
E
XAMPLE
1. We can define a set of reflected lambda-terms as the
smallest set such that
·
bterm{x
1
, · · · , x
n
.x
i
}, where 1  i  n, is a lambda-term (a
variable);
·
if bterm x
1
, · · · , x
n
, x
n+1
.t[x] is a lambda-term, then
bterm x
1
, · · · , x
n
.x
n+1
.t[x]
is also a lambda-term (an abstraction);
·
if bterm{x
1
, · · · , x
n
.t
1
[x]} and bterm{x
1
, · · · , x
n
.t
2
[x]} are
lambda-terms, then
bterm{x
1
; · · · ; x
n
.apply{t
1
[x]; t
2
[x]}}
is also a lambda-term (an application).
In a way, bterms could be understood as an explicit coding for
Barzilay's substitution functions. And indeed, some of the basic
definitions are quite similar. The notion of bterms is also very
similar to that of local variable contexts <A href="3.html#10">[FPT99].
3.2
Terminology
Before we proceed further, we need to define some terminology.
D
EFINITION
1. We change the notion of subterm so that the subterms
of a bterm are also bterms. For example, the immediate subterms
of bterm{x , y.x y} are bterm{x , y.x } and bterm{x , y.y}; the
immediate subterm of bterm{x .y.x } is bterm{x, y.x }.
D
EFINITION
2. We call the number of outer binders in a bterm
expression its binding depth. Namely, the binding depth of the
bterm bterm{x
1
, · · · , x
n
.t[x]} is n.
D
EFINITION
3. Throughout the rest of the paper we use the notion
of operator shape. The shape of an operator is a list of natural numbers
each stating how many new binders the operator introduces on
5
the corresponding subterm. The length of the shape list is therefore
the arity of the operator. For example, the shape of the + operator
is [0; 0] and the shape of the  operator is [1].
The mapping from operators to shapes is also sometimes called
a binding signature of a language <A href="3.html#10">[FPT99, <A href="3.html#11">Plo90].
D
EFINITION
4. Let op be an operator with shape [d
1
; · · · ; d
N
],
and let btl be a list of bterms [b
1
; · · · ; b
M
]. We say that btl is
compatible with op at depth n when,
1. N = M;
2. the binding depth of bterm b
j
is n + d
j
for each 1  j  N .
3.3
Abstract Operators
Expressions of the form bterm{x.op{· · · }} can only be used to express
syntax with concrete operators. In other words, each expression
of this form contains a specific constant operator op. However,
we would like to reason about operators abstractly; in particular,
we want to make it possible to have variables of the type "Op" that
can be quantified over and used in the same manner as operator
constants. In order to address this we use explicit term constructors
in addition to bterm{x.op{· · · }} constants.
The expression mk bterm{n; "op"; btl}, where "op" is some encoding
of the quoted operator op, stands for a bterm with binding
depth n, operator op and subterms btl. Namely,
mk bterm{n; op; bterm{x
1
, · · · , x
n
, y
1
.t
1
[x; y
1
]} :: · · · ::
bterm{x
1
, · · · , x
n
, y
k
.t
k
[x; y
k
]} :: nil}
is bterm{x
1
, · · · , x
n
.op {y
1
.t
1
[x; y
1
]; · · · ; y
k
.t
k
[x; y
k
]}}. Here,
nil is the empty list and :: is the list cons operator and therefore
the expression b
1
:: · · · :: b
n
:: nil represents the concrete list
[b
1
; · · · ; b
n
].
Note that if we know the shape of the operator op and we know
that the mk bterm expression is well-formed (or, more specifically,
if we know that btl is compatible with op at depth n), then it
would normally be possible to deduce the value of n (since n is
the difference between the binding depth of any element of the list
btl and the corresponding element of the shape(op) list). There are
two reasons, however, for supplying n explicitly:
·
When btl is empty (in other words, when the arity of op is 0),
the value of n can not be deduced this way and still needs to be
supplied somehow. One could consider 0-arity operators to be a
special case, but this results in a significant loss of uniformity.
·
When we do not know whether an mk bterm expression is
necessarily well-formed (and as we will see it is often useful
to allow this to happen), then a lot of definitions and proofs
are greatly simplified when the binding depth of mk bterm
expressions is explicitly specified.
Using the mk bterm constructor and a few other similar constructors
that will be introduced later, it becomes easy to reason abstractly
about operators. Indeed, the second argument to mk bterm
can now be an arbitrary expression, not just a constant. This has a
cost of making certain definitions slightly more complicated. For
example, the notion of "compatible with op at depth n" now becomes
an important part of the theory and will need to be explicitly
formalized. However, this is a small price to pay for the ability to
reason abstractly about operators, which easily extends to reasoning
abstractly about languages, classes of languages and so forth.
3.4
Inductively Defining the Type of Well-Formed Bterms
There are two equivalent approaches to inductively defining the
general type (set) of all well-formed bterms. The first one follows
the same idea as in Example <A href="3.html#4">1:
·
bterm{x
1
, · · · , x
n
.x
i
} is a well-formed bterm for 1  i  n;
·
mk bterm{n; op; btl} is a well-formed bterm when op is a well-formed
quoted operator and btl is a list of well-formed bterms
that is compatible with op at some depth n.
If we denote bterm{x
1
, · · · , x
l
, y, z
1
, · · · , z
r
.y} as var{l; r},
we can restate the base case of the above definition as "var{l; r },
where l and r are arbitrary natural numbers, is a well-formed
bterm". Once we do this it becomes apparent that the above definition
has a lot of similarities with de Bruijn-style indexing of
variables <A href="3.html#10">[dB72]. Indeed, one might call the numbers l and r the
left and right indices of the variable var{l; r }.
It is possible to provide an alternate definition that is closer to
pure HOAS:
·
bnd{x .t [x]}, where t is a well-formed substitution function, is
a well-formed bterm (the bnd operation increases the binding
depth of t by one by adding x to the beginning of the list of t's
outer binders).
·
mk term{op; btl}, where op is a well-formed quoted operator,
and btl is a list of well-formed bterms that is compatible with
op at depth 0, is a well-formed bterm (of binding depth 0).
Other than better capturing the idea of HOAS, the latter definition
also makes it easier to express the reflective correspondence
between the meta-syntax (the syntax used to express the theory of
syntax, namely the one that includes the operators mk bterm, bnd,
etc.) and the meta-meta-syntax (the syntax that is used to express
the theory of syntax and the underlying theory, in other words, the
syntax that includes the second-order notations.) Namely, provided
that we define the subst{bt; t } operation to compute the result of
substituting a closed term
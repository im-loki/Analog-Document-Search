Accelerated Focused Crawling through Online Relevance Feedback
Abstract
The organization of HTML into a tag tree structure, which
is rendered by browsers as roughly rectangular regions with
embedded text and HREF links, greatly helps surfers locate
and click on links that best satisfy their information need.
Can an automatic program emulate this human behavior
and thereby learn to predict the relevance of an unseen
HREF target page w.r.t. an information need, based on
information limited to the HREF source page?
Such a
capability would be of great interest in focused crawling and
resource discovery, because it can fine-tune the priority of
unvisited URLs in the crawl frontier, and reduce the number
of irrelevant pages which are fetched and discarded.
We show that there is indeed a great deal of usable
information on a HREF source page about the relevance
of the target page. This information, encoded suitably, can
be exploited by a supervised apprentice which takes online
lessons from a traditional focused crawler by observing
a carefully designed set of features and events associated
with the crawler.
Once the apprentice gets a sufficient
number of examples, the crawler starts consulting it to
better prioritize URLs in the crawl frontier. Experiments on
a dozen topics using a 482-topic taxonomy from the Open
Directory (Dmoz) show that online relevance feedback can
reduce false positives by 30% to 90%.
Categories and subject descriptors:
H.5.4 [Information interfaces and presentation]:
Hypertext/hypermedia; I.5.4 [Pattern recognition]:
Applications, Text processing; I.2.6 [Artificial
intelligence]: Learning; I.2.8 [Artificial intelligence]:
Problem Solving, Control Methods, and Search.
General terms:
Algorithms, performance,
measurements, experimentation.

Introduction
Keyword search and clicking on links are the dominant
modes of accessing hypertext on the Web.
Support for
keyword search through crawlers and search engines is very
mature, but the surfing paradigm is not modeled or assisted

(Note: The HTML version of this paper is best viewed using
Microsoft Internet Explorer. To view the HTML version using
Netscape, add the following line to your ~/.Xdefaults or
~/.Xresources file:
Netscape*documentFonts.charset*adobe-fontspecific: iso-8859-1
For printing use the PDF version, as browsers may not print the
mathematics properly.)

Contact author, email soumen@cse.iitb.ac.in
Copyright is held by the author/owner(s).
WWW2002, May 7­11, 2002, Honolulu, Hawaii, USA.
ACM 1-58113-449-5/02/0005
Baseline learner
Dmoz
topic
taxonomy
Class models
consisting of
term stats
Frontier URLS
priority queue
Crawler
Pick
best
Newly fetched
page u
Submit page for classification
If Pr(c*|u) is large enough
then enqueue all outlinks v of u
with priority Pr(c*|u)
Crawl
database
Seed
URLs
Figure 1: A basic focused crawler controlled by one topic
classifier/learner.
as well. Support for surfing is limited to the basic interface
provided by Web browsers, except for a few notable research
prototypes.
While surfing, the user typically has a topic-specific
information need, and explores out from a few known
relevant starting points in the Web graph (which may be
query responses) to seek new pages relevant to the chosen
topic/s. While deciding for or against clicking on a specific
link (u, v), humans use a variety of clues on the source
page u to estimate the worth of the (unseen) target page
v, including the tag tree structure of u, text embedded in
various regions of that tag tree, and whether the link is
relative or remote. "Every click on a link is a leap of faith"
<A href="25.html#12">[19], but humans are very good at discriminating between
links based on these clues.
Making an educated guess about the worth of clicking
on a link (u, v) without knowledge of the target v is
central to the surfing activity. Automatic programs which
can learn this capability would be valuable for a number
of applications which can be broadly characterized as
personalized, topic-specific information foragers.
Large-scale, topic-specific information gatherers are
called focused crawlers <A href="25.html#12">[1, 9, 14, 28, 30]. In contrast to giant,
all-purpose crawlers which must process large portions of
the Web in a centralized manner, a distributed federation of
focused crawlers can cover specialized topics in more depth
and keep the crawl more fresh, because there is less to cover
for each crawler.
In its simplest form, a focused crawler consists of a
supervised topic classifier (also called a `learner') controlling
the priority of the unvisited frontier of a crawler (see
Figure <A href="25.html#1">1). The classifier is trained a priori on document
samples embedded in a topic taxonomy such as Yahoo!
or Dmoz.
It thereby learns to label new documents as
belonging to topics in the given taxonomy <A href="25.html#12">[2, 5, 21]. The
goal of the focused crawler is to start from nodes relevant
to a focus topic c

in the Web graph and explore links to
selectively collect pages about c

, while avoiding fetching
pages not about c

.
Suppose the crawler has collected a page u and
148
encountered in u an unvisited link to v. A simple crawler
(which we call the baseline) will use the relevance of u
to topic c

(which, in a Bayesian setting, we can denote
Pr(c

|u)) as the estimated relevance of the unvisited page
v.
This reflects our belief that pages across a hyperlink
are more similar than two randomly chosen pages on the
Web, or, in other words, topics appear clustered in the
Web graph <A href="25.html#12">[11, 23]. Node v will be added to the crawler's
priority queue with priority Pr(c

|u). This is essentially a
"best-first" crawling strategy. When v comes to the head
of the queue and is actually fetched, we can verify if the
gamble paid off, by evaluating Pr(c

|v). The fraction of
relevant pages collected is called the harvest rate.
If V
is the set of nodes collected, the harvest rate is defined
as (1/
|V |)
v
V
Pr(c

|v). Alternatively, we can measure
the loss rate, which is one minus the harvest rate, i.e., the
(expected) fraction of fetched pages that must be thrown
away.
Since the effort on relevant pages is well-spent,
reduction in loss rate is the primary goal and the most
appropriate figure of merit.
For focused crawling applications to succeed, the "leap
of faith" from u to v must pay off frequently. In other words,
if Pr(c

|v) is often much less than the preliminary estimate
Pr(c

|u), a great deal of network traffic and CPU cycles
are being wasted eliminating bad pages. Experience with
random walks on the Web show that as one walks away
from a fixed page u
0
relevant to topic c
0
, the relevance of
successive nodes u
1
, u
2
, . . . to c
0
drops dramatically within
a few hops <A href="25.html#12">[9, 23]. This means that only a fraction of outlinks
from a page is typically worth following. The average
out-degree of the Web graph is about 7 <A href="25.html#12">[29]. Therefore, a
large number of page fetches may result in disappointment,
especially if we wish to push the utility of focused crawling
to topic communities which are not very densely linked.
Even w.r.t. topics that are not very narrow, the
number of distracting outlinks emerging from even fairly
relevant pages has grown substantially since the early
days of Web authoring <A href="25.html#12">[4].
Template-based authoring,
dynamic page generation from semi-structured databases,
ad links, navigation panels, and Web rings contribute many
irrelevant links which reduce the harvest rate of focused
crawlers. Topic-based link discrimination will also reduce
these problems.
1.1
Our contribution: Leaping with more faith
In this paper we address the following questions:
How much information about the topic of the HREF
target is available and/or latent in the HREF source page,
its tag-tree structure, and its text? Can these sources be
exploited for accelerating a focused crawler?
Our basic idea is to use two classifiers. Earlier, the regular
baseline classifier was used to assign priorities to unvisited
frontier nodes. This no longer remains its function. The role
of assigning priorities to unvisited URLs in the crawl frontier
is now assigned to a new learner called the apprentice, and
the priority of v is specific to the features associated with
the (u, v) link which leads to it
<A href="25.html#2">1
<A href="25.html#2">. The features used by the
apprentice are derived from the Document Object Model or
1
If many u's link to a single v, it is easiest to freeze the priority of
v when the first-visited u linking to v is assessed, but combinations
of scores are also possible.
Baseline learner (Critic)
Dmoz
topic
taxonomy
Class models
consisting of
term stats
Frontier URLS
priority queue
Crawler
Pick
best
Newly fetched
page u
Submit page for classification
If Pr(c*|u) is
large enough...
An instance (u,v)
for the apprentice
u
v
Pr(c*|v)
Pr(c|u) for
all classes c
Crawl
database
Apprentice learner
Class
models
+
Online
training
... submit (u,v)
to the apprentice
Apprentice
assigns more
accurate priority
to node v
Figure 2:
The apprentice is continually presented with
training cases (u, v) with suitable features. The apprentice
is interposed where new outlinks (u, v) are registered with
the priority queue, and helps assign the unvisited node v a
better estimate of its relevance.
DOM (<A href="http://www.w3.org/DOM/">http://www.w3.org/DOM/) of u. Meanwhile, the role
of the baseline classifier becomes one of generating training
instances for the apprentice, as shown in Figure <A href="25.html#2">2. We may
therefore regard the baseline learner as a critic or a trainer,
which provides feedback to the apprentice so that it can
improve "on the job."
The critic-apprentice paradigm is related to reinforcement
learning and AI programs that learn to play games
<A href="25.html#12">[26,
§1.2]. We argue that this division of labor is natural
and effective.
The baseline learner can be regarded as
a user specification for what kind of content is desired.
Although we limit ourselves to a generative statistical model
for this specification, this can be an arbitrary black-box
predicate.
For rich and meaningful distinction between
Web communities and topics, the baseline learner needs
to be fairly sophisticated, perhaps leveraging off human
annotations on the Web (such as topic directories).
In
contrast, the apprentice specializes in how to locate pages
to satisfy the baseline learner.
Its feature space is more
limited, so that it can train fast and adapt nimbly to
changing fortunes at following links during a crawl.
In
Mitchell's words <A href="25.html#12">[27], the baseline learner recognizes "global
regularity" while the apprentice helps the crawler adapt
to "local regularity."
This marked asymmetry between
the classifiers distinguishes our approach from Blum and
Mitchell's co-training technique <A href="25.html#12">[3], in which two learners
train each other by selecting unlabeled instances.
Using a dozen topics from a topic taxonomy derived
from the Open Directory, we compare our enhanced crawler
with the baseline crawler. The number of pages that are
thrown away (because they are irrelevant), called the loss
rate, is cut down by 30­90%. We also demonstrate that
the fine-grained tag-tree model, together with our synthesis
and encoding of features for the apprentice, are superior to
simpler alternatives.
1.2
Related work
Optimizing the priority of unvisited URLs on the crawl
frontier for specific crawling goals is not new. FishSearch
by De Bra et al. <A href="25.html#12">[12, 13] and SharkSearch by Hersovici
et al. <A href="25.html#12">[16] were some of the earliest systems for localized
searches in the Web graph for pages with specified keywords.
149
In another early paper, Cho et al. <A href="25.html#12">[10] experimented with a
variety of strategies for prioritizing how to fetch unvisited
URLs.
They used the anchor text as a bag of words to
guide link expansion to crawl for pages matching a specified
keyword query, which led to some extent of differentiation
among out-links, but no trainer-apprentice combination was
involved. No notion of supervised topics had emerged at
that point, and simple properties like the in-degree or the
presence of specified keywords in pages were used to guide
the crawler.
Topical locality on the Web has been studied for a few
years.
Davison made early measurements on a 100000-node
Web subgraph <A href="25.html#12">[11] collected by the DiscoWeb system.
Using the standard notion of vector space TFIDF similarity
<A href="25.html#12">[31], he found that the endpoints of a hyperlink are much
more similar to each other than two random pages, and that
HREFs close together on a page link to documents which are
more similar than targets which are far apart. Menczer has
made similar observations <A href="25.html#12">[23]. The HyperClass hypertext
classifier also uses such locality patterns for better semi-supervised
learning of topics <A href="25.html#12">[7], as does IBM's Automatic
Resource Compilation (ARC) and Clever topic distillation
systems <A href="25.html#12">[6, 8].
Two important advances have been made beyond the
baseline best-first focused crawler: the use of context graphs
by Diligenti et al. <A href="25.html#12">[14] and the use of reinforcement learning
by Rennie and McCallum <A href="25.html#12">[30].
Both techniques trained
a learner with features collected from paths leading up to
relevant nodes rather than relevant nodes alone. Such paths
may be collected by following backlinks.
Diligenti et al. used a classifier (learner) that regressed
from the text of u to the estimated link distance from u to
some relevant page w, rather than the relevance of u or an
outlink (u, v), as was the case with the baseline crawler.
This lets their system continue expanding u even if the
reward for following a link is not immediate, but several
links away.
However, they do favor links whose payoffs
are closest. Our work is specifically useful in conjunction
with the use of context graphs: when the context graph
learner predicts that a goal is several links away, it is crucial
to offer additional guidance to the crawler based on local
structure in pages, because the fan-out at that radius could
be enormous.
Rennie and McCallum <A href="25.html#12">[30] also collected paths leading
to relevant nodes, but they trained a slightly different
classifier, for which:
· An instance was a single HREF link like (u, v).
· The features were terms from the title and headers
(&lt;h1&gt;...&lt;/h1&gt; etc.)
of u, together with the text
in and `near' the anchor (u, v).
Directories and
pathnames were also used.
(We do not know the
precise definition of `near', or how these features were
encoded and combined.)
· The prediction was a discretized estimate of the
number of relevant nodes reachable by following (u, v),
where the reward from goals distant from v was
geometrically discounted by some factor  &lt; 1/2 per
hop.
Rennie and McCallum obtained impressive harvests of
research papers from four Computer Science department
sites, and of pages about officers and directors from 26
company Websites.
Lexical proximity and contextual features have been
used extensively in natural language processing for disambiguating
word sense <A href="25.html#12">[15]. Compared to plain text, DOM
trees and hyperlinks give us a richer set of potential features.
Aggarwal et al. have proposed an "intelligent crawling"
framework <A href="25.html#12">[1] in which only one classifier is used, but similar
to our system, that classifier trains as the crawl progresses.
They do not use our apprentice-critic approach, and do not
exploit features derived from tag-trees to guide the crawler.
The "intelligent agents" literature has brought forth
several systems for resource discovery and assistance to
browsing <A href="25.html#12">[19].
They range between client- and site-level
tools. Letizia <A href="25.html#12">[18], Powerscout, and WebWatcher <A href="25.html#12">[17] are
such systems.
Menczer and Belew proposed InfoSpiders
<A href="25.html#12">[24], a collection of autonomous goal-driven crawlers without
global control or state, in the style of genetic algorithms. A
recent extensive study <A href="25.html#12">[25] comparing several topic-driven
crawlers including the best-first crawler and InfoSpiders
found the best-first approach to show the highest harvest
rate (which our new system outperforms).
In all the systems mentioned above, improving the
chances of a successful "leap of faith" will clearly reduce
the overheads of fetching, filtering, and analyzing pages.
Furthermore, whereas we use an automatic first-generation
focused crawler to generate the input to train the apprentice,
one can envisage specially instrumented browsers being used
to monitor users as they seek out information.
We distinguish our work from prior art in the following
important ways:
Two classifiers:
We use two classifiers. The first one is
used to obtain `enriched' training data for the second one.
(A breadth-first or random crawl would have a negligible
fraction of positive instances.) The apprentice is a simplified
reinforcement learner. It improves the harvest rate, thereby
`enriching' the data collected and labeled by the first learner
in turn.
No manual path collection:
Our two-classifier framework
essentially eliminates the manual effort needed to
create reinforcement paths or context graphs. The input
needed to start off a focused crawl is just a pre-trained topic
taxonomy (easily available from the Web) and a few focus
topics.
Online training:
Our apprentice trains continually, acquiring
ever-larger vocabularies and improving its accuracy
as the crawl progresses. This property holds also for the
"intelligent crawler" proposed by Aggarwal et al., but they
have a single learner, whose drift is controlled by precise
relevance predicates provided by the user.
No manual feature tuning:
Rather than tune ad-hoc
notions of proximity between text and hyperlinks, we encode
the features of link (u, v) using the DOM-tree of u, and
automatically learn a robust definition of `nearness' of a
textual feature to (u, v).
In contrast, Aggarwal et al
use many tuned constants combining the strength of text-and
link-based predictors, and Rennie et al. use domain
knowledge to select the paths to goal nodes and the word
bags that are submitted to their learner.
150
Methodology and algorithms
We first review the baseline focused crawler and then
describe how the enhanced crawler is set up using the
apprentice-critic mechanism.
2.1
The baseline focused crawler
The baseline focused crawler has been described in detail
elsewhere <A href="25.html#12">[9, 14], and has been sketched in Figure <A href="25.html#1">1. Here
we review its design and operation briefly.
There are two inputs to the baseline crawler.
· A topic taxonomy or hierarchy with example URLs
for each topic.
· One or a few topics in the taxonomy marked as the
topic(s) of focus.
Although we will generally use the terms `taxonomy' and
`hierarchy', a topic tree is not essential; all we really need is
a two-way classifier where the classes have the connotations
of being `relevant' or `irrelevant' to the topic(s) of focus.
A topic hierarchy is proposed purely to reduce the tedium
of defining new focused crawls. With a two-class classifier,
the crawl administrator has to seed positive and negative
examples for each crawl. Using a taxonomy, she composes
the `irrelevant' class as the union of all classes that are not
relevant. Thanks to extensive hierarchies like Dmoz in the
public domain, it should be quite easy to seed topic-based
crawls in this way.
The baseline crawler maintains a priority queue on the
estimated relevance of nodes v which have not been visited,
and keeps removing the highest priority node and visiting it,
expanding its outlinks and checking them into the priority
queue with the relevance score of v in turn.
Despite its
extreme simplicity, the best-first crawler has been found to
have very high harvest rates in extensive evaluations <A href="25.html#12">[25].
Why do we need negative examples and negative classes
at all? Instead of using class probabilities, we could maintain
a priority queue on, say, the TFIDF cosine similarity
between u and the centroid of the seed pages (acting as an
estimate for the corresponding similarity between v and the
centroid, until v has been fetched). Experience has shown
<A href="25.html#12">[32] that characterizing a negative class is quite important to
prevent the centroid of the crawled documents from drifting
away indefinitely from the desired topic profile.
In this paper, the baseline crawler also has the implicit
job of gathering instances of successful and unsuccessful
"leaps of faith" to submit to the apprentice, discussed next.
2.2
The basic structure of the apprentice
learner
In estimating the worth of traversing the HREF (u, v), we
will limit our attention to u alone. The page u is modeled
as a tag tree (also called the Document Object Model or
DOM). In principle, any feature from u, even font color and
site membership may be perfect predictors of the relevance
of v. The total number of potentially predictive features will
be quite staggering, so we need to simplify the feature space
and massage it into a form suited to conventional learning
algorithms. Also note that we specifically study properties
of u and not larger contexts such as paths leading to u,
meaning that our method may become even more robust and
useful in conjunction with context graphs or reinforcement
along paths.
Initially, the apprentice has no training data, and passes
judgment on (u, v) links according to some fixed prior
obtained from a baseline crawl run ahead of time (e.g., see
the statistics in
<A href="25.html#7">§3.3). Ideally, we would like to train the
apprentice continuously, but to reduce overheads, we declare
a batch size between a few hundred and a few thousand
pages. After every batch of pages is collected, we check if any
page u fetched before the current batch links to some page
v in the batch. If such a (u, v) is found, we extract suitable
features for (u, v) as described later in this section, and add
(u, v), Pr(c

|v) as another instance of the training data for
the apprentice. Many apprentices, certainly the simple naive
Bayes and linear perceptrons that we have studied, need not
start learning from scratch; they can accept the additional
training data with a small additional computational cost.
2.2.1
Preprocessing the DOM tree
First, we parse u and form the DOM tree for u.
Sadly,
much of the HTML available on the Web violates any
HTML standards that permit context-free parsing, but
a variety of repair heuristics (see, e.g., HTML Tidy,
available at <A href="http://www.w3.org/People/Raggett/tidy/">http://www.w3.org/People/Raggett/tidy/)
let us generate reasonable DOM trees from bad HTML.
a
HREF
TEXT
font
TEXT
li
li
li
ul
li
TEXT
TEXT
em
TEXT
tt
TEXT
TEXT
@0
@0
@1
@2
@3
@-1
@-2
Figure 3: Numbering of DOM leaves used to derive offset
attributes for textual tokens. `@' means "is at offset".
Second, we number all leaf nodes consecutively from left
to right. For uniformity, we assign numbers even to those
DOM leaves which have no text associated with them. The
specific &lt;a href...&gt; which links to v is actually an internal
node a
v
, which is the root of the subtree containing the
anchor text of the link (u, v). There may be other element
tags such as &lt;em&gt; or &lt;b&gt; in the subtree rooted at a
v
. Let
the leaf or leaves in this subtree be numbered (a
v
) through
r(a
v
)
(a
v
). We regard the textual tokens available from
any of these leaves as being at DOM offset zero w.r.t. the
(u, v) link. Text tokens from a leaf numbered µ, to the left of
(a
v
), are at negative DOM offset µ
- (a
v
). Likewise, text
from a leaf numbered µ to the right of r(a
v
) are at positive
DOM offset µ
- r(a
v
). See Figure <A href="25.html#4">3 for an example.
2.2.2
Features derived from the DOM and text
tokens
Many related projects mentioned in
§<A href="25.html#2">1.2 use a linear notion
of proximity between a HREF and textual tokens. In the
ARC system, there is a crude cut-off distance measured
151
in bytes to the left and right of the anchor.
In the
Clever system, distance is measured in tokens, and the
importance attached to a token decays with the distance.
In reinforcement learning and intelligent predicate-based
crawling, the exact specification of neighborhood text is not
known to us. In all cases, some ad-hoc tuning appears to be
involved.
We claim (and show in
§<A href="25.html#7">3.4) that the relation between
the relevance of the target v of a HREF (u, v) and the
proximity of terms to (u, v) can be learnt automatically. The
results are better than ad-hoc tuning of cut-off distances,
provided the DOM offset information is encoded as features
suitable for the apprentice.
One obvious idea is to extend the Clever model: a page
is a linear sequence of tokens. If a token t is distant x from
the HREF (u, v) in question, we encode it as a feature t, x .
Such features will not be useful because there are too many
possible values of x, making the t, x space too sparse to
learn well. (How many HREFS will be exactly five tokens
from the term `basketball' ?)
Clearly, we need to bucket x into a small number of
ranges. Rather than tune arbitrary bucket boundaries by
hand, we argue that DOM offsets are a natural bucketing
scheme provided by the page author.
Using the node
numbering scheme described above, each token t on page u
can be annotated w.r.t. the link (u, v) (for simplicity assume
there is only one such link) as t, d , where d is the DOM
offset calculated above.
This is the main set of features
used by the apprentice. We shall see that the apprentice
can learn to limit
|d| to less than d
max
= 5 in most cases,
which reduces its vocabulary and saves time.
A variety of other feature encodings suggest themselves.
We are experimenting with some in ongoing work (
<A href="25.html#11">§4),
but decided against some others. For example, we do not
expect gains from encoding specific HTML tag names owing
to the diversity of authoring styles.
Authors use &lt;div&gt;,
&lt;span&gt;, &lt;layer&gt; and nested tables for layout control in
non-standard ways; these are best deflated to a nameless
DOM node representation.
Similar comments apply to
HREF collections embedded in &lt;ul&gt;, &lt;ol&gt;, &lt;td&gt; and
&lt;dd&gt;.
Font and lower/upper case information is useful
for search engines, but would make features even sparser
for the apprentice.
Our representation also flattens two-dimensional
tables to their "row-major" representation.
The features we ignore are definitely crucial for other
applications, such as information extraction. We did not
see any cases where this sloppiness led to a large loss rate.
We would be surprised to see tables where relevant links
occurred in the third column and irrelevant links in the fifth,
or pages where they are rendered systematically in different
fonts and colors, but are not otherwise demarcated by the
DOM structure.
2.2.3
Non-textual features
Limiting d may lead us to miss features of u that may be
useful at the whole-page level. One approach would be to use
"d =
" for all d larger in magnitude than some threshold.
But this would make our apprentice as bulky and slow to
train as the baseline learner.
Instead, we use the baseline learner to abstract u for
the apprentice. Specifically, we use a naive Bayes baseline
learner to classify u, and use the vector of class probabilities
returned as features for the apprentice. These features can
help the apprentice discover patterns such as
"Pages about /Recreation/Boating/Sailing often
link to pages about /Sports/Canoe_and_Kayaking."
This also covers for the baseline classifier confusing between
classes with related vocabulary, achieving an effect similar
to context graphs.
Another kind of feature can be derived from co-citation.
If v
1
has been fetched and found to be relevant and HREFS
(u, v
1
) and (u, v
2
) are close to each other, v
2
is likely to
be relevant. Just like textual tokens were encoded as t, d
pairs, we can represent co-citation features as , d , where
 is a suitable representation of relevance.
Many other features can be derived from the DOM tree
and added to our feature pool. We discuss some options
in
<A href="25.html#11">§4. In our experience so far, we have found the t, d
features to be most useful. For simplicity, we will limit our
subsequent discussion to t, d features only.
2.3
Choices of learning algorithms for the
apprentice
Our feature set is thus an interesting mix of categorical,
ordered and continuous features:
· Term tokens t, d have a categorical component t and
a discrete ordered component d (which we may like to
smooth somewhat). Term counts are discrete but can
be normalized to constant document length, resulting
in continuous attribute values.
· Class names are discrete and may be regarded as
synthetic terms. The probabilities are continuous.
The output we desire is an estimate of Pr(c

|v), given all the
observations about u and the neighborhood of (u, v) that
we have discussed. Neural networks are a natural choice
to accommodate these requirements. We first experimented
with a simple linear perceptron, training it with the delta
rule (gradient descent) <A href="25.html#12">[26]. Even for a linear perceptron,
convergence was surprisingly slow, and after convergence,
the error rate was rather high.
It is likely that local
optima were responsible, because stability was generally
poor, and got worse if we tried to add hidden layers or
sigmoids.
In any case, convergence was too slow for use
as an online learner. All this was unfortunate, because the
direct regression output from a neural network would be
convenient, and we were hoping to implement a Kohonen
layer for smoothing d.
In contrast, a naive Bayes (NB) classifier worked very
well. A NB learner is given a set of training documents,
each labeled with one of a finite set of classes/topic.
A
document or Web page u is modeled as a multiset or bag
of words,
{ , n(u,  ) } where  is a feature which occurs
n(u,  ) times in u. In ordinary text classification (such as
our baseline learner) the features  are usually single words.
For our apprentice learner, a feature  is a t, d pair.
NB classifiers can predict from a discrete set of classes,
but our prediction is a continuous (probability) score. To
bridge this gap, We used a simple two-bucket (low/high
relevance) special case of Torgo and Gama's technique of
using classifiers for discrete labels for continuous regression
<A href="25.html#12">[33], using "equally probable intervals" as far as possible.
152
Torgo and Gama recommend using a measure of centrality,
such as the median, of each interval as the predicted value of
that class. Rennie and McCallum <A href="25.html#12">[30] corroborate that 2­3
bins are adequate. As will be clear from our experiments, the
medians of our `low' and `high' classes are very close to zero
and one respectively (see Figure <A href="25.html#7">5). Therefore, we simply
take the probability of the `high' class as the prediction from
our naive Bayes apprentice.
The prior probability of class c, denoted Pr(c) is the
fraction of training documents labeled with class c. The NB
model is parameterized by a set of numbers
c,
which is
roughly the rate of occurrence of feature  in class c, more
exactly,

c,
=
1 +
u
V
c
n(u,  )
|T | +
u,
n(u,  ) ,
(1)
where V
c
is the set of Web pages labeled with c and T is the
entire vocabulary. The NB learner assumes independence
between features, and estimates
Pr(c
|u)

Pr(c) Pr(u
|c)

Pr(c)

u

n(u, )
c,
. (2)
Nigam et al. provide further details <A href="25.html#12">[22].
Experimental study
Our experiments were guided by the following requirements.
We wanted to cover a broad variety of topics, some `easy' and
some `difficult', in terms of the harvest rate of the baseline
crawler. Here is a quick preview of our results.
· The apprentice classifier achieves high accuracy in
predicting the relevance of unseen pages given t, d
features. It can determine the best value of d
max
to
use, typically, 4­6.
· Encoding DOM offsets in features improves the
accuracy of the apprentice substantially, compared
to a bag of ordinary words collected from within the
same DOM offset window.
· Compared to a baseline crawler, a crawler that is
guided by an apprentice (trained offline) has a 30%
to 90% lower loss rate.
It finds crawl paths never
expanded by the baseline crawler.
· Even if the apprentice-guided crawler is forced to
stay within the (inferior) Web 
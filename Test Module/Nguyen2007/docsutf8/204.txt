Unified Utility Maximization Framework for Resource Selection
ABSTRACT
This  paper  presents  a  unified  utility  framework  for  resource 
selection  of  distributed  text  information  retrieval.  This  new 
framework  shows  an  efficient  and  effective  way  to  infer  the 
probabilities  of  relevance  of  all  the  documents  across  the  text 
databases.  With  the  estimated  relevance  information,  resource 
selection  can  be  made  by  explicitly  optimizing  the  goals  of 
different  applications.  Specifically,  when  used  for  database 
recommendation, the selection is optimized for the goal of high-recall
 (include  as  many  relevant  documents  as  possible  in  the 
selected  databases);  when  used  for  distributed  document 
retrieval,  the  selection  targets  the  high-precision  goal  (high 
precision in the final merged list of documents). This new model 
provides  a  more  solid  framework  for  distributed  information 
retrieval. Empirical studies show that it is at least as effective as 
other state-of-the-art algorithms.
Categories and Subject Descriptors
H.3.3  [Information Search and Retrieval]:

General Terms
Algorithms

INTRODUCTION
Conventional  search  engines  such  as  Google  or  AltaVista  use 
ad-hoc  information  retrieval  solution  by  assuming  all  the 
searchable  documents  can  be  copied  into  a  single  centralized 
database  for  the  purpose  of  indexing.  Distributed  information 
retrieval,  also  known  as  federated  search  [1,4,7,11,14,22]  is 
different  from  ad-hoc  information  retrieval  as  it  addresses  the 
cases when documents cannot be acquired and stored in a single 
database.  For  example,  "Hidden  Web"  contents  (also  called 
"invisible" or "deep" Web contents) are information on the Web
that  cannot  be  accessed  by  the  conventional  search  engines. 
Hidden web contents have been estimated to be 2-50 [19] times 
larger  than  the  contents  that  can  be  searched  by  conventional 
search engines. Therefore, it is very important to search this type 
of valuable information. 
 
The  architecture  of  distributed  search  solution  is  highly 
influenced by different environmental characteristics. In a small 
local  area  network  such  as  small  company  environments,  the 
information providers may cooperate to provide corpus statistics 
or  use  the  same  type  of  search  engines.  Early  distributed 
information  retrieval  research  focused  on  this  type  of 
cooperative  environments  [1,8].  On  the  other  side,  in  a  wide 
area  network  such  as  very  large  corporate  environments  or  on 
the Web there are many types of search engines and it is difficult 
to  assume  that  all  the  information  providers  can  cooperate  as 
they are required. Even if they are willing to cooperate in these 
environments, it may be hard to enforce a single solution for all 
the  information  providers  or  to  detect  whether  information 
sources  provide  the  correct  information  as  they  are  required. 
Many  applications  fall  into  the  latter  type  of  uncooperative 
environments  such  as  the  Mind  project  [16]  which  integrates 
non-cooperating  digital  libraries  or  the  QProber  system  [9] 
which supports browsing and searching of uncooperative hidden 
Web databases. In this paper, we focus mainly on uncooperative 
environments  that  contain  multiple  types  of  independent  search 
engines. 
 
There  are  three  important  sub-problems  in  distributed 
information  retrieval.  First,  information  about  the  contents  of 
each  individual  database  must  be  acquired  (resource 
representation)  [1,8,21].  Second,  given  a  query,  a  set  of 
resources must be selected to do the search (resource selection) 
[5,7,21].  Third,  the  results  retrieved  from  all  the  selected 
resources have to be merged into a single final list before it can 
be  presented  to  the  end  user  (retrieval  and  results  merging) 
[1,5,20,22].  
 
Many  types  of  solutions  exist  for  distributed  information 
retrieval. Invisible-web.net
1
provides guided browsing of hidden
Web  databases  by  collecting  the  resource  descriptions  of  these 
databases and building hierarchies of classes that group them by 
similar  topics.  A  database  recommendation  system  goes  a  step 
further  than  a  browsing  system  like  Invisible-web.net  by 
recommending  most  relevant  information  sources  to  users' 
queries.  It  is  composed  of  the  resource  description  and  the

1
http://www.invisible-web.net
Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
CIKM '04, November 8--13, 2004, Washington, DC, USA. 
Copyright 2004 ACM 1-58113-874-1//04/0011...$5.00.

32
resource selection components. This solution is useful when the 
users  want  to  browse  the  selected  databases  by  themselves 
instead  of  asking  the  system  to  retrieve  relevant  documents 
automatically.  Distributed  document  retrieval  is  a  more 
sophisticated  task.  It  selects  relevant  information  sources  for 
users'  queries  as  the  database  recommendation  system  does. 
Furthermore,  users'  queries  are  forwarded  to  the  corresponding 
selected  databases  and  the  returned  individual  ranked  lists  are 
merged into a single list to present to the users.
The  goal  of  a  database  recommendation  system  is  to  select  a 
small  set  of  resources  that  contain  as  many  relevant  documents 
as possible, which we call a high-recall goal. On the other side, 
the  effectiveness  of  distributed  document  retrieval  is  often 
measured  by  the  Precision  of  the  final  merged  document  result 
list,  which  we  call  a  high-precision  goal.  Prior  research 
indicated that these two goals are related but not identical [4,21]. 
However, most previous solutions simply use effective resource 
selection  algorithm  of  database  recommendation  system  for 
distributed document retrieval system or solve the inconsistency 
with heuristic methods [1,4,21].  
 
This paper presents a unified utility maximization framework to 
integrate  the  resource  selection  problem  of  both  database 
recommendation and distributed document retrieval together by 
treating them as different optimization goals. 
 
First,  a  centralized  sample  database  is  built  by  randomly 
sampling a small amount of documents from each database with 
query-based  sampling  [1];  database  size  statistics  are  also 
estimated  [21].  A  logistic  transformation  model  is  learned  off 
line  with  a  small  amount  of  training  queries  to  map  the 
centralized  document  scores  in  the  centralized  sample  database 
to the corresponding probabilities of relevance.  
 
Second, after a new query is submitted, the query can be used to 
search  the  centralized  sample  database  which  produces  a  score 
for  each  sampled  document.  The  probability  of  relevance  for 
each  document  in  the  centralized  sample  database  can  be 
estimated  by  applying  the  logistic  model  to  each  document's 
score.  Then,  the  probabilities  of  relevance  of  all  the  (mostly 
unseen)  documents  among  the  available  databases  can  be 
estimated  using  the  probabilities of  relevance  of  the  documents 
in  the  centralized  sample  database  and  the  database  size 
estimates.  
 
For  the  task  of  resource  selection  for  a  database 
recommendation  system,  the  databases  can  be  ranked  by  the 
expected  number  of  relevant  documents  to  meet  the  high-recall 
goal. For resource selection for a distributed document retrieval 
system, databases containing a small number of documents with 
large  probabilities  of  relevance  are  favored  over  databases 
containing  many  documents  with  small  probabilities  of 
relevance. This selection criterion meets the high-precision goal 
of  distributed  document  retrieval  application.  Furthermore,  the 
Semi-supervised  learning  (SSL)  [20,22]  algorithm  is  applied  to 
merge the returned documents into a final ranked list.  
 
The unified utility framework  makes very  few assumptions and 
works in uncooperative environments. Two key features make it 
a  more  solid  model  for  distributed  information  retrieval:  i)  It 
formalizes  the  resource  selection  problems  of  different 
applications as various utility functions, and optimizes the utility 
functions  to  achieve  the  optimal  results  accordingly;  and  ii)  It
shows an effective and efficient way to estimate the probabilities 
of relevance of all documents across databases. Specifically, the 
framework  builds  logistic  models  on  the  centralized  sample 
database  to  transform  centralized  retrieval  scores  to  the 
corresponding probabilities of relevance and uses the centralized 
sample database as the bridge between individual databases and 
the  logistic  model.  The  human  effort  (relevance  judgment) 
required  to  train  the  single  centralized  logistic  model  does  not 
scale  with  the  number  of  databases.  This  is  a  large  advantage 
over  previous  research,  which  required  the  amount  of  human 
effort to be linear with the number of databases [7,15].  
 
The  unified  utility  framework  is  not  only  more  theoretically 
solid  but  also  very  effective.  Empirical  studies  show  the  new 
model to be at least as accurate as the state-of-the-art algorithms 
in a variety of configurations.  
 
The next section discusses related work. Section 3 describes the 
new  unified  utility  maximization  model.  Section  4  explains  our 
experimental  methodology.  Sections  5  and  6  present  our 
experimental  results  for  resource  selection  and  document 
retrieval. Section 7 concludes.

PRIOR RESEARCH
There has been considerable research on all the sub-problems of 
distributed  information  retrieval.  We  survey  the  most  related 
work in this section. 
 
The first problem of distributed information retrieval is resource 
representation.  The  STARTS  protocol  is  one  solution  for 
acquiring resource descriptions in cooperative environments [8]. 
However, in uncooperative environments, even the databases are 
willing to share their information, it is not easy to judge whether 
the  information  they  provide  is  accurate  or  not. Furthermore,  it 
is  not  easy  to  coordinate  the  databases  to  provide  resource 
representations  that  are  compatible  with  each  other.  Thus,  in 
uncooperative environments, one common choice is query-based 
sampling,  which  randomly  generates  and  sends  queries  to 
individual search engines and retrieves some documents to build 
the  descriptions.  As  the  sampled  documents  are  selected  by 
random  queries,  query-based  sampling  is  not  easily  fooled  by 
any adversarial spammer that is interested to attract more traffic. 
Experiments  have  shown  that  rather  accurate  resource 
descriptions  can  be  built  by  sending  about  80  queries  and 
downloading about 300 documents [1].
Many  resource  selection  algorithms  such  as  gGlOSS/vGlOSS 
[8]  and  CORI  [1]  have  been  proposed  in  the  last  decade.  The 
CORI  algorithm  represents  each  database  by  its  terms,  the 
document  frequencies  and  a  small  number  of  corpus  statistics 
(details in [1]). As prior research on different datasets has shown 
the  CORI  algorithm  to  be  the  most  stable  and  effective  of  the 
three  algorithms  [1,17,18],  we  use  it  as  a  baseline  algorithm  in 
this  work.  The  relevant  document  distribution  estimation 
(ReDDE [21]) resource selection algorithm is a recent algorithm 
that  tries  to  estimate  the  distribution  of  relevant  documents 
across  the  available  databases  and  ranks  the  databases 
accordingly. Although the ReDDE algorithm has been shown to 
be  effective,  it  relies  on  heuristic  constants  that  are  set 
empirically [21]. 
 
The  last  step  of  the  document  retrieval  sub-problem  is  results 
merging, which is the process of transforming database-specific
33
document
scores
into
comparable
database-independent
document  scores.  The  semi  supervised  learning  (SSL)  [20,22] 
result merging algorithm uses the documents acquired by query-based
sampling as training data and linear regression to learn the 
database-specific,  query-specific  merging  models.  These  linear 
models  are  used  to  convert  the  database-specific  document 
scores  into  the  approximated  centralized  document  scores.  The 
SSL algorithm has been shown to be effective [22]. It serves as 
an  important  component  of  our  unified  utility  maximization 
framework (Section 3).

In  order  to  achieve  accurate  document  retrieval  results,  many 
previous  methods  simply  use  resource  selection  algorithms  that 
are  effective  of  database  recommendation  system.  But  as 
pointed  out  above,  a  good  resource  selection  algorithm 
optimized  for  high-recall  may  not  work  well  for  document 
retrieval,  which  targets  the  high-precision  goal.  This  type  of 
inconsistency  has  been  observed  in  previous  research  [4,21]. 
The  research  in [21]  tried  to  solve  the problem  with  a  heuristic 
method.  
 
The  research  most  similar  to  what  we  propose  here  is  the 
decision-theoretic  framework  (DTF)  [7,15].  This  framework 
computes  a  selection  that  minimizes  the  overall  costs  (e.g., 
retrieval quality, time) of document retrieval system and several 
methods  [15]  have  been  proposed  to  estimate  the  retrieval 
quality.  However,  two  points  distinguish  our  research  from  the 
DTF model. First, the DTF is a framework designed specifically 
for  document  retrieval,  but  our  new  model  integrates  two 
distinct  applications  with  different  requirements  (database 
recommendation  and  distributed  document  retrieval)  into  the 
same  unified  framework.  Second,  the  DTF  builds  a  model  for 
each  database  to  calculate  the  probabilities  of  relevance.  This 
requires  human  relevance  judgments  for  the  results  retrieved 
from  each  database.  In  contrast,  our  approach  only  builds  one 
logistic  model  for  the  centralized  sample  database.  The 
centralized sample database can serve as a bridge to connect the 
individual databases with the centralized logistic model, thus the 
probabilities  of  relevance  of  documents  in  different  databases 
can be estimated. This strategy can save large amount of human 
judgment  effort  and  is  a  big  advantage  of  the  unified  utility 
maximization  framework  over  the  DTF  especially  when  there 
are a large number of databases.
UNIFIED UTILITY MAXIMIZATION FRAMEWORK
The  Unified  Utility  Maximization  (UUM)  framework  is  based 
on  estimating  the  probabilities  of  relevance  of  the  (mostly 
unseen)  documents  available  in  the  distributed  search 
environment. In this section we describe how the probabilities of 
relevance  are  estimated  and  how  they  are  used  by  the  Unified 
Utility  Maximization  model.  We  also  describe  how  the  model 
can  be  optimized  for  the  high-recall  goal  of  a  database 
recommendation  system  and  the  high-precision  goal  of  a 
distributed document retrieval system.
3.1 Estimating Probabilities of Relevance

As pointed out above, the purpose of resource selection is high-recall
and the purpose of document retrieval is high-precision. In 
order to meet these diverse goals, the key issue is to estimate the 
probabilities of relevance of the documents in various databases. 
This  is  a  difficult  problem  because  we  can  only  observe  a 
sample  of  the  contents  of  each  database  using  query-based
sampling.  Our  strategy  is  to  make  full  use  of  all  the  available 
information to calculate the probability estimates.
3.1.1 Learning Probabilities of Relevance
In the resource description step, the centralized sample database 
is  built  by  query-based  sampling  and  the  database  sizes  are 
estimated  using  the  sample-resample  method  [21].  At  the  same 
time, an effective retrieval algorithm (Inquery [2]) is applied on 
the  centralized  sample  database  with  a  small  number  (e.g.,  50) 
of  training queries. For  each  training query,  the  CORI  resource 
selection  algorithm  [1]  is  applied  to  select  some  number       
(e.g.,  10)  of  databases  and  retrieve  50  document  ids  from  each 
database. The SSL results  merging algorithm [20,22] is used to 
merge the results. Then, we can download the top 50 documents 
in  the  final  merged  list  and  calculate  their  corresponding 
centralized scores using Inquery and the corpus statistics of the 
centralized  sample  database.  The  centralized  scores  are  further 
normalized (divided by the maximum centralized score for each 
query), as this method has been suggested to improve estimation 
accuracy in previous research [15]. Human judgment is acquired 
for  those  documents  and  a  logistic  model  is  built  to  transform 
the  normalized  centralized  document  scores  to  probabilities  of 
relevance as follows:
(
)
))
(
exp(
1
))
(
exp(
|
)
(
_
_
d
S
b
a
d
S
b
a
d
rel
P
d
R
c
c
c
c
c
c
+
+
+
=
=

(1)
where
)
(
_
d
S
c
is  the  normalized  centralized  document  score  and
a
c
and b
c
are the two parameters of the logistic model. These two
parameters  are  estimated  by  maximizing  the  probabilities  of 
relevance of the training queries. The logistic model provides us 
the  tool  to  calculate  the  probabilities  of  relevance  from 
centralized document scores.
3.1.2 Estimating Centralized Document Scores
When  the  user  submits  a  new  query,  the  centralized  document 
scores  of  the  documents  in  the  centralized  sample  database  are 
calculated.  However,  in  order  to  calculate  the  probabilities  of 
relevance,  we  need  to  estimate  centralized  document  scores  for 
all documents  across  the  databases  instead of  only  the  sampled 
documents.  This  goal  is  accomplished  using:  the  centralized 
scores of the documents in the centralized sample database, and 
the database size statistics. 
 
We  define  the  database  scale  factor  for  the  i
th
database  as  the
ratio  of  the  estimated  database  size  and  the  number  of 
documents sampled from this database as follows:
^
_
i
i
i
db
db
db
samp
N
SF
N
=

(2)
where
^
i
db
N
is  the  estimated  database  size  and
_
i
db
samp
N
is  the
number  of  documents  from  the  i
th
database  in  the  centralized
sample  database.  The  intuition  behind  the  database  scale  factor 
is that, for a database whose scale factor is 50, if one document 
from  this  database  in  the  centralized  sample  database  has  a 
centralized  document  score  of  0.5,  we  may  guess  that  there  are 
about 50 documents in that database which have scores of about 
0.5.  Actually,  we  can  apply  a  finer  non-parametric  linear 
interpolation method to estimate the centralized document score 
curve  for  each  database.  Formally,  we  rank  all  the  sampled 
documents  from  the  i
th
database  by  their  centralized  document
34
scores  to  get  the  sampled  centralized  document  score  list 
{S
c
(ds
i1
),  S
c
(ds
i2
),  S
c
(ds
i3
),.....}  for  the  i
th
database;  we  assume
that if we could calculate the centralized document scores for all 
the documents in this database and get the complete centralized 
document score list, the top document in the sampled list would 
have  rank  SF
dbi
/2,  the  second  document  in  the  sampled  list
would  rank  SF
dbi
3/2,  and  so  on.  Therefore,  the  data  points  of
sampled documents in the complete list are: {(SF
dbi
/2, S
c
(ds
i1
)),
(SF
dbi
3/2,  S
c
(ds
i2
)),  (SF
dbi
5/2,  S
c
(ds
i3
)),.....}.  Piecewise  linear
interpolation  is  applied  to  estimate  the  centralized  document 
score curve, as illustrated in Figure 1. The complete centralized 
document score list can be estimated by calculating the values of 
different  ranks  on  the  centralized  document  curve  as:
]
,
1
[
,
)
(
S
^
^
c
i
db
ij
N
j
d

.
It  can  be  seen  from  Figure  1  that  more  sample  data  points 
produce  more  accurate  estimates  of  the  centralized  document 
score  curves.  However,  for  databases  with  large  database  scale 
ratios, this kind of linear interpolation may be rather inaccurate, 
especially  for  the  top  ranked  (e.g.,  [1,  SF
dbi
/2])  documents.
Therefore,  an  alternative  solution  is  proposed  to  estimate  the 
centralized  document  scores  of  the  top  ranked  documents  for 
databases  with  large  scale  ratios  (e.g.,  larger  than  100). 
Specifically, a logistic model is built for each of these databases. 
The logistic model is used to estimate the centralized document 
score  of  the  top  1  document  in  the  corresponding  database  by 
using  the  two  sampled  documents  from  that  database  with 
highest centralized scores.
))
(
)
(
exp(
1
))
(
)
(
exp(
)
(
2
2
1
1
0
2
2
1
1
0
^
1
i
c
i
i
c
i
i
i
c
i
i
c
i
i
i
c
ds
S
ds
S
ds
S
ds
S
d
S






+
+
+
+
+
=

(3)
0
i

,
1
i

and
2
i

are  the  parameters  of  the  logistic  model.  For
each training query, the top retrieved document of each database 
is  downloaded  and  the  corresponding  centralized  document 
score  is  calculated.  Together  with  the  scores  of  the  top  two 
sampled documents, these parameters can be estimated.  
 
After the centralized score of the top document is estimated, an 
exponential function is fitted for the top part ([1, SF
dbi
/2]) of the
centralized document score curve as:
]
2
/
,
1
[
)
*
exp(
)
(
1
0
^
i
db
i
i
ij
c
SF
j
j
d
S

+
=



(4)
^
0
1
1
log(
(
))
i
c
i
i
S d


=

(5)
)
1
2
/
(
))
(
log(
)
(
(log(
^
1
1
1

=
i
db
i
c
i
c
i
SF
d
S
ds
S


(6)
The  two  parameters
0
i

and
1
i

are  fitted  to  make  sure  the
exponential  function  passes  through  the  two  points  (1,
^
1
)
(
i
c
d
S
)
and  (SF
dbi
/2,  S
c
(ds
i1
)).  The  exponential  function  is only  used  to
adjust the  top  part  of  the  centralized  document  score  curve  and 
the  lower  part  of  the  curve  is  still  fitted  with  the  linear 
interpolation method described above. The adjustment by fitting 
exponential  function  of  the  top  ranked  documents  has  been 
shown empirically to produce more accurate results.
From  the  centralized  document  score  curves,  we  can  estimate 
the complete centralized document score lists accordingly for all 
the  available  databases.  After  the  estimated  centralized 
document  scores  are  normalized,  the  complete  lists  of 
probabilities of relevance can be constructed out of the complete 
centralized document score lists by Equation 1. Formally for the 
i
th
database,  the  complete  list  of  probabilities  of  relevance  is:
]
,
1
[
,
)
(
R
^
^
i
db
ij
N
j
d

.


3.2 The Unified Utility Maximization Model

In  this  section,  we  formally  define  the  new  unified  utility 
maximization  model,  which  optimizes  the  resource  selection 
problems
for
two
goals
of
high-recall
(database
recommendation)  and  high-precision  (distributed  document 
retrieval) in the same framework. 
 
In  the  task  of  database  recommendation,  the  system  needs  to 
decide how to rank databases. In the task of document retrieval, 
the system not only needs to select the databases but also needs 
to  decide  how  many  documents  to  retrieve  from  each  selected 
database. We generalize the database recommendation selection 
process,  which  implicitly  recommends  all  documents  in  every 
selected database, as a special case of the selection decision for 
the  document  retrieval  task.  Formally,  we  denote  d
i
as  the
number  of  documents  we  would  like  to  retrieve  from  the  i
th

database  and
,.....}
,
{
2
1
d
d
d
=
as  a  selection  action  for  all  the
databases.  
 
The  database  selection  decision  is  made  based  on  the  complete 
lists  of  probabilities  of  relevance  for  all  the  databases.  The 
complete lists of probabilities of relevance are inferred from all
the  available  information  specifically
s
R
,  which  stands  for  the
resource descriptions acquired by query-based sampling and the
database size estimates acquired by sample-resample;
c
S
stands
for  the  centralized  document  scores  of  the  documents  in  the 
centralized sample database. 
 
If  the  method  of  estimating  centralized  document  scores  and 
probabilities  of  relevance  in  Section  3.1  is  acceptable,  then  the 
most probable complete lists of probabilities of relevance can be
derived  and  we  denote  them  as
1
^
^
*
1
{(R(
),
[1,
]),
db
j
d
j
N

=


2
^
^
2
(R(
),
[1,
]),.......}
db
j
d
j
N

.
Random  vector

denotes  an
arbitrary  set  of  complete  lists  of  probabilities  of  relevance  and
)
,
|
(
c
s
S
R
P

as  the  probability  of  generating  this  set  of  lists.
Finally, to each selection action
d

and a set of complete lists of

Figure 1.
Linear interpolation construction of the complete
centralized document score list (database scale factor is 50).

35
probabilities  of  relevance

,  we  associate  a  utility  function
)
,
(
d
U


which  indicates  the  benefit  from  making  the
d

selection  when  the  true  complete  lists  of  probabilities  of 
relevance are

.
Therefore,  the  selection  decision  defined  by  the  Bayesian 
framework is:




d
S
R
P
d
U
d
c
s
d
)
.
|
(
)
,
(
max
arg
*
=

(7)
One  common  approach  to  simplify  the  computation  in  the 
Bayesian  framework  is  to  only  calculate  the  utility  function  at 
the  most  probable  parameter  values  instead  of  calculating  the 
whole  expectation.  In  other  words,  we  only  need  to  calculate
)
,
(
*
d
U

and Equation 7 is simplified as follows:
)
,
(
max
arg
*
*

d
U
d
d
=

(8)
This  equation  serves  as  the  basic  model  for  both  the  database 
recommendation system and the document retrieval system.
3.3  Resource Selection for High-Recall
High-recall  is  the  goal  of  the  resource  selection  algorithm  in 
federated  search  tasks  such  as  database  recommendation.  The 
goal  is  to  select  a  small  set  of  resources  (e.g.,  less  than  N
sdb
databases) that contain as many relevant documents as possible, 
which can be formally defined as:
=
=
i
N
j
ij
i
i
db
d
d
I
d
U
^
1
^
*
)
(
R
)
(
)
,
(


(9)
I(d
i
) is the indicator function, which is 1 when the i
th
database is
selected and 0 otherwise. Plug this equation into the basic model 
in  Equation  8  and  associate  the  selected  database  number 
constraint to obtain the following:
sdb
i
i
i
N
j
ij
i
d
N
d
I
to
Subject
d
d
I
d
i
db
=
=
=
)
(
:
)
(
R
)
(
max
arg
^
1
^
*

(10)
The  solution  of  this  optimization  problem  is  very  simple.  We 
can  calculate  the  expected  number  of  relevant  documents  for 
each database as follows:
=
=
i
db
i
N
j
ij
Rd
d
N
^
1
^
^
)
(
R

(11)
The N
sdb
databases with the largest expected number of relevant
documents can be selected to meet the high-recall goal. We call 
this  the  UUM/HR  algorithm  (Unified  Utility  Maximization  for 
High-Recall).
3.4  Resource Selection for High-Precision
High-Precision  is  the  goal  of  resource  selection  algorithm  in 
federated search tasks such as distributed document retrieval. It 
is measured by the Precision at the top part of the final merged 
document  list.  This  high-precision  criterion  is  realized  by  the 
following  utility  function,  which  measures  the  Precision  of 
retrieved documents from the selected databases.
=
=
i
d
j
ij
i
i
d
d
I
d
U
1
^
*
)
(
R
)
(
)
,
(


(12)
Note that the key difference between Equation 12 and Equation 
9 is that Equation 9 sums up the probabilities of relevance of all 
the documents in a database, while Equation 12 only considers a 
much smaller part of the ranking. Specifically, we can calculate 
the optimal selection decision by:
=
=
i
d
j
ij
i
d
i
d
d
I
d
1
^
*
)
(
R
)
(
max
arg

(13)
Different kinds of constraints caused by different characteristics 
of the document retrieval tasks can be associated with the above 
optimization problem. The most common one is to select a fixed 
number (N
sdb
) of databases and retrieve a fixed number (N
rdoc
) of
documents from each selected database, formally defined as:
0
,
)
(
:
)
(
R
)
(
max
arg
1
^
*

=
=
=
=
i
rdoc
i
sdb
i
i
i
d
j
ij
i
d
d
if
N
d
N
d
I
to
Subject
d
d
I
d
i

(14)
This  optimization  problem  can  be  solved  easily  by  calculating 
the number of expected relevant documents in the top part of the 
each database's complete list of probabilities of relevance:
=
=
rdoc
i
N
j
ij
Rd
Top
d
N
1
^
^
_
)
(
R

(15)
Then the databases can be ranked by these values and selected. 
We  call  this  the  UUM/HP-FL  algorithm  (Unified  Utility 
Maximization  for  High-Precision  with  Fixed  Length  document 
rankings from each selected database). 
 
A  more  complex  situation  is  to  vary  the  number  of  retrieved 
documents  from  each  selected  database.  More  specifically,  we 
allow different selected databases to return different numbers of 
documents. For simplification, the result list lengths are required 
to be multiples of a baseline number 10. (This value can also be 
varied,  but  for  simplification  it  is  set  to  10  in  this  paper.)  This 
restriction  is  set  to  simulate  the  behavior  of  commercial  search 
engines  on  the  Web.  (Search  engines  such  as  Google  and 
AltaVista  return  only  10  or  20  document  ids  for  every  result 
page.) This procedure saves the computation time of calculating 
optimal  database  selection  by  allowing  the  step  of  dynamic 
programming  to  be  10  instead  of  1  (more  detail  is  discussed 
latterly). For further simplification, we restrict to select  at  most 
100  documents  from  each  database  (d
i
&lt;=100)  Then,  the
selection optimization problem is formalized as follows:
]
10
..,
,
2
,
1
,
0
[
,
*
10
)
(
:
)
(
R
)
(
max
arg
_
1
^
*

=
=
=
=
=
k
k
d
N
d
N
d
I
to
Subject
d
d
I
d
i
rdoc
Total
i
i
sdb
i
i
i
d
j
ij
i
d
i

(16)
N
Total_rdoc
is the total number of documents to be retrieved.
Unfortunately,  there  is  no  simple  solution  for  this  optimization 
problem  as  there  are  for  Equations  10  and  14.  However,  a
36
dynamic programming algorithm can be applied to calculate the 
optimal  solution.  The  basic  steps  of  this dynamic  programming 
method  are  described  in  Figure  2.  As  this  algorithm  allows 
retrieving  result  lists  of  varying  lengths  from  each  selected 
database, it is called UUM/HP-VL algorithm. 
 
After the selection decisions are made, the selected databases are 
searched and the corresponding document ids are retrieved from 
each  database.  The  final  step  of  document  retrieval  is  to  merge 
the  returned  results  into  a  single  ranked  list  with  the  semi-supervised
learning algorithm. It was pointed out before that the 
SSL  algorithm  maps  the  database-specific  scores  into  the 
centralized  document  scores  and  builds  the  final  ranked  list 
accordingly,  which  is  consistent  with  all  our  selection 
procedures  where  documents  with  higher  probabilities  of 
relevance (thus higher centralized document scores) are selected.
EXPERIMENTAL METHODOLOGY
It  is  desirable  to  evaluate  distributed  information  retrieval 
algorithms  with  testbeds  that  closely  simulate  the  real  world 
applications.  
 
The  TREC  Web  collections  WT2g  or  WT10g  [4,13]  provide  a  
way  to  partition  documents  by  different  Web  servers.  In  this 
way, a large number (O(1000)) of databases with rather diverse
contents could be created, which may  make this testbed a good 
candidate to simulate the operational environments such as open 
domain hidden Web. However, two weakness of this testbed are: 
i) Each database contains only a small amount of document (259 
documents  by  average  for  WT2g)  [4];  and  ii)  The  contents  of 
WT2g or WT10g are arbitrarily crawled from the Web. It is not 
likely for a hidden Web database to provide personal homepages 
or  web  pages  indicating  that  the  pages  are  under  construction 
and  ther
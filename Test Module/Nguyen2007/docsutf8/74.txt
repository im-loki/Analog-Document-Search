Downloading Textual Hidden Web Content Through Keyword Queries
ABSTRACT
An ever-increasing amount of information on the Web today is
available only through search interfaces: the users have to type in a
set of keywords in a search form in order to access the pages from
certain Web sites. These pages are often referred to as the Hidden
Web or the Deep Web. Since there are no static links to the Hidden
Web pages, search engines cannot discover and index such pages
and thus do not return them in the results. However, according to
recent studies, the content provided by many Hidden Web sites is
often of very high quality and can be extremely valuable to many
users.
In this paper, we study how we can build an effective Hidden Web
crawler that can autonomously discover and download pages from
the Hidden Web. Since the only "entry point" to a Hidden Web site
is a query interface, the main challenge that a Hidden Web crawler
has to face is how to automatically generate meaningful queries to
issue to the site. Here, we provide a theoretical framework to investigate
the query generation problem for the Hidden Web and we
propose effective policies for generating queries automatically. Our
policies proceed iteratively, issuing a different query in every iteration
. We experimentally evaluate the effectiveness of these policies
on
4 real Hidden Web sites and our results are very promising. For
instance, in one experiment, one of our policies downloaded more
than
90% of a Hidden Web site (that contains 14 million documents
) after issuing fewer than 100 queries.
Categories and Subject Descriptors
H.3.7 [Information Systems]:
Digital Libraries; H.3.1 [Information Systems]: Content Analysis
and Indexing; H.3.3 [Information Systems]: Information Search
and Retrieval.

General Terms
Algorithms, Performance, Design.

INTRODUCTION
Recent studies show that a significant fraction of Web content
cannot be reached by following links [7, 12]. In particular, a large
part of the Web is "hidden" behind search forms and is reachable
only when users type in a set of keywords, or queries, to the forms.
These pages are often referred to as the Hidden Web [17] or the
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
JCDL'05, June 7­11, 2005, Denver, Colorado, USA.
Copyright 2005 ACM 1-58113-876-8/05/0006 ...
$
5.00.
Deep Web [7], because search engines typically cannot index the
pages and do not return them in their results (thus, the pages are
essentially "hidden" from a typical Web user).
According to many studies, the size of the Hidden Web increases
rapidly as more organizations put their valuable content online
through an easy-to-use Web interface [7]. In [12], Chang et al.
estimate that well over 100,000 Hidden-Web sites currently exist
on the Web. Moreover, the content provided by many Hidden-Web
sites is often of very high quality and can be extremely valuable
to many users [7]. For example, PubMed hosts many high-quality
papers on medical research that were selected from careful peer-review
processes, while the site of the US Patent and Trademarks
Office
1
makes existing patent documents available, helping potential
inventors examine "prior art."
In this paper, we study how we can build a Hidden-Web crawler
2
that can automatically download pages from the Hidden Web, so
that search engines can index them. Conventional crawlers rely
on the hyperlinks on the Web to discover pages, so current search
engines cannot index the Hidden-Web pages (due to the lack of
links). We believe that an effective Hidden-Web crawler can have
a tremendous impact on how users search information on the Web:
· Tapping into unexplored information:
The Hidden-Web
crawler will allow an average Web user to easily explore the
vast amount of information that is mostly "hidden" at present.
Since a majority of Web users rely on search engines to discover
pages, when pages are not indexed by search engines, they are
unlikely to be viewed by many Web users. Unless users go directly
to Hidden-Web sites and issue queries there, they cannot
access the pages at the sites.
· Improving user experience: Even if a user is aware of a number
of Hidden-Web sites, the user still has to waste a significant
amount of time and effort, visiting all of the potentially relevant
sites, querying each of them and exploring the result. By making
the Hidden-Web pages searchable at a central location, we can
significantly reduce the user's wasted time and effort in searching
the Hidden Web.
· Reducing potential bias: Due to the heavy reliance of many Web
users on search engines for locating information, search engines
influence how the users perceive the Web [28]. Users do not
necessarily perceive what actually exists on the Web, but what
is indexed by search engines [28]. According to a recent article
[5], several organizations have recognized the importance of
bringing information of their Hidden Web sites onto the surface,
and committed considerable resources towards this effort. Our
1
US Patent Office: http://www.uspto.gov
2
Crawlers are the programs that traverse the Web automatically and
download pages for search engines.
100
Figure 1: A single-attribute search interface
Hidden-Web crawler attempts to automate this process for Hidden
Web sites with textual content, thus minimizing the associated
costs and effort required.
Given that the only "entry" to Hidden Web pages is through
querying a search form, there are two core challenges to implementing
an effective Hidden Web crawler: (a) The crawler has to
be able to understand and model a query interface, and (b) The
crawler has to come up with meaningful queries to issue to the
query interface. The first challenge was addressed by Raghavan
and Garcia-Molina in [29], where a method for learning search interfaces
was presented. Here, we present a solution to the second
challenge, i.e. how a crawler can automatically generate queries so
that it can discover and download the Hidden Web pages.
Clearly, when the search forms list all possible values for a query
(e.g., through a drop-down list), the solution is straightforward. We
exhaustively issue all possible queries, one query at a time. When
the query forms have a "free text" input, however, an infinite number
of queries are possible, so we cannot exhaustively issue all possible
queries. In this case, what queries should we pick? Can the
crawler automatically come up with meaningful queries without
understanding the semantics of the search form?
In this paper, we provide a theoretical framework to investigate
the Hidden-Web crawling problem and propose effective ways of
generating queries automatically. We also evaluate our proposed
solutions through experiments conducted on real Hidden-Web sites.
In summary, this paper makes the following contributions:
· We present a formal framework to study the problem of Hidden-Web
crawling. (Section 2).
· We investigate a number of crawling policies for the Hidden
Web, including the optimal policy that can potentially download
the maximum number of pages through the minimum number of
interactions. Unfortunately, we show that the optimal policy is
NP-hard and cannot be implemented in practice (Section 2.2).
· We propose a new adaptive policy that approximates the optimal
policy. Our adaptive policy examines the pages returned from
previous queries and adapts its query-selection policy automatically
based on them (Section 3).
· We evaluate various crawling policies through experiments on
real Web sites. Our experiments will show the relative advantages
of various crawling policies and demonstrate their potential
. The results from our experiments are very promising. In
one experiment, for example, our adaptive policy downloaded
more than 90% of the pages within PubMed (that contains 14
million documents) after it issued fewer than 100 queries.
FRAMEWORK
In this section, we present a formal framework for the study of
the Hidden-Web crawling problem. In Section 2.1, we describe our
assumptions on Hidden-Web sites and explain how users interact
with the sites. Based on this interaction model, we present a high-level
algorithm for a Hidden-Web crawler in Section 2.2. Finally in
Section 2.3, we formalize the Hidden-Web crawling problem.
2.1
Hidden-Web database model
There exists a variety of Hidden Web sources that provide information
on a multitude of topics. Depending on the type of information
, we may categorize a Hidden-Web site either as a textual
database or a structured database. A textual database is a site that
Figure 2: A multi-attribute search interface
mainly contains plain-text documents, such as PubMed and Lexis-Nexis
(an online database of legal documents [1]). Since plain-text
documents do not usually have well-defined structure, most
textual databases provide a simple search interface where users
type a list of keywords in a single search box (Figure 1). In contrast
, a structured database often contains multi-attribute relational
data (e.g., a book on the Amazon Web site may have the fields
title=`Harry Potter'
, author=`J.K. Rowling' and
isbn=`0590353403'
) and supports multi-attribute search interfaces
(Figure 2). In this paper, we will mainly focus on textual
databases that support single-attribute keyword queries. We
discuss how we can extend our ideas for the textual databases to
multi-attribute structured databases in Section 6.1.
Typically, the users need to take the following steps in order to
access pages in a Hidden-Web database:
1. Step 1. First, the user issues a query, say "liver," through the
search interface provided by the Web site (such as the one shown
in Figure 1).
2. Step 2. Shortly after the user issues the query, she is presented
with a result index page. That is, the Web site returns a list of
links to potentially relevant Web pages, as shown in Figure 3(a).
3. Step 3. From the list in the result index page, the user identifies
the pages that look "interesting" and follows the links. Clicking
on a link leads the user to the actual Web page, such as the one
shown in Figure 3(b), that the user wants to look at.
2.2
A generic Hidden Web crawling algorithm
Given that the only "entry" to the pages in a Hidden-Web site
is its search from, a Hidden-Web crawler should follow the three
steps described in the previous section. That is, the crawler has
to generate a query, issue it to the Web site, download the result
index page, and follow the links to download the actual pages. In
most cases, a crawler has limited time and network resources, so
the crawler repeats these steps until it uses up its resources.
In Figure 4 we show the generic algorithm for a Hidden-Web
crawler. For simplicity, we assume that the Hidden-Web crawler
issues single-term queries only.
3
The crawler first decides which
query term it is going to use (Step (2)), issues the query, and retrieves
the result index page (Step (3)). Finally, based on the links
found on the result index page, it downloads the Hidden Web pages
from the site (Step (4)). This same process is repeated until all the
available resources are used up (Step (1)).
Given this algorithm, we can see that the most critical decision
that a crawler has to make is what query to issue next. If the
crawler can issue successful queries that will return many matching
pages, the crawler can finish its crawling early on using minimum
resources. In contrast, if the crawler issues completely irrelevant
queries that do not return any matching pages, it may waste all
of its resources simply issuing queries without ever retrieving actual
pages. Therefore, how the crawler selects the next query can
greatly affect its effectiveness. In the next section, we formalize
this query selection problem.
3
For most Web sites that assume "AND" for multi-keyword
queries, single-term queries return the maximum number of results.
Extending our work to multi-keyword queries is straightforward.
101
(a) List of matching pages for query "liver".
(b) The first matching page for "liver".
Figure 3: Pages from the PubMed Web site.
A
LGORITHM
2.1.
Crawling a Hidden Web site
Procedure
(1)
while ( there are available resources ) do
// select a term to send to the site
(2)
q
i
= SelectTerm()
// send query and acquire result index page
(3)
R(q
i
) = QueryWebSite( q
i
)
// download the pages of interest
(4)
Download(
R(q
i
) )
(5)
done
Figure 4: Algorithm for crawling a Hidden Web site.
S
q
1
q
q
q
2
3
4
Figure 5: A set-formalization of the optimal query selection
problem.
2.3
Problem formalization
Theoretically, the problem of query selection can be formalized
as follows: We assume that the crawler downloads pages from a
Web site that has a set of pages
S (the rectangle in Figure 5). We
represent each Web page in
S as a point (dots in Figure 5). Every
potential query
q
i
that we may issue can be viewed as a subset of
S,
containing all the points (pages) that are returned when we issue
q
i
to the site. Each subset is associated with a weight that represents
the cost of issuing the query. Under this formalization, our goal is to
find which subsets (queries) cover the maximum number of points
(Web pages) with the minimum total weight (cost). This problem
is equivalent to the set-covering problem in graph theory [16].
There are two main difficulties that we need to address in this
formalization. First, in a practical situation, the crawler does not
know which Web pages will be returned by which queries, so the
subsets of
S are not known in advance. Without knowing these
subsets the crawler cannot decide which queries to pick to maximize
the coverage. Second, the set-covering problem is known to
be NP-Hard [16], so an efficient algorithm to solve this problem
optimally in polynomial time has yet to be found.
In this paper, we will present an approximation algorithm that
can find a near-optimal solution at a reasonable computational cost.
Our algorithm leverages the observation that although we do not
know which pages will be returned by each query
q
i
that we issue,
we can predict how many pages will be returned. Based on this information
our query selection algorithm can then select the "best"
queries that cover the content of the Web site. We present our prediction
method and our query selection algorithm in Section 3.
2.3.1
Performance Metric
Before we present our ideas for the query selection problem, we
briefly discuss some of our notation and the cost/performance metrics
.
Given a query
q
i
, we use
P (q
i
) to denote the fraction of pages
that we will get back if we issue query
q
i
to the site. For example, if
a Web site has 10,000 pages in total, and if 3,000 pages are returned
for the query
q
i
= "medicine", then P (q
i
) = 0.3. We use P (q
1

q
2
) to represent the fraction of pages that are returned from both
q
1
and
q
2
(i.e., the intersection of
P (q
1
) and P (q
2
)). Similarly, we
use
P (q
1
q
2
) to represent the fraction of pages that are returned
from either
q
1
or
q
2
(i.e., the union of
P (q
1
) and P (q
2
)).
We also use Cost
(q
i
) to represent the cost of issuing the query
q
i
. Depending on the scenario, the cost can be measured either in
time, network bandwidth, the number of interactions with the site,
or it can be a function of all of these. As we will see later, our
proposed algorithms are independent of the exact cost function.
In the most common case, the query cost consists of a number
of factors, including the cost for submitting the query to the site,
retrieving the result index page (Figure 3(a)) and downloading the
actual pages (Figure 3(b)). We assume that submitting a query incurs
a fixed cost of
c
q
. The cost for downloading the result index
page is proportional to the number of matching documents to the
query, while the cost
c
d
for downloading a matching document is
also fixed. Then the overall cost of query
q
i
is
Cost
(q
i
) = c
q
+ c
r
P (q
i
) + c
d
P (q
i
).
(1)
In certain cases, some of the documents from
q
i
may have already
been downloaded from previous queries. In this case, the crawler
may skip downloading these documents and the cost of
q
i
can be
Cost
(q
i
) = c
q
+ c
r
P (q
i
) + c
d
P
new
(q
i
).
(2)
Here, we use
P
new
(q
i
) to represent the fraction of the new documents
from
q
i
that have not been retrieved from previous queries.
Later in Section 3.1 we will study how we can estimate
P (q
i
) and
P
new
(q
i
) to estimate the cost of q
i
.
Since our algorithms are independent of the exact cost function,
we will assume a generic cost function Cost
(q
i
) in this paper. When
we need a concrete cost function, however, we will use Equation 2.
Given the notation, we can formalize the goal of a Hidden-Web
crawler as follows:
102
P
ROBLEM
1. Find the set of queries
q
1
, . . . , q
n
that maximizes
P (q
1
· · ·  q
n
)
under the constraint
n
i=1
Cost
(q
i
)  t.
Here,
t is the maximum download resource that the crawler has.
KEYWORD SELECTION
How should a crawler select the queries to issue? Given that the
goal is to download the maximum number of unique documents
from a textual database, we may consider one of the following options
:
· Random: We select random keywords from, say, an English dictionary
and issue them to the database. The hope is that a random
query will return a reasonable number of matching documents.
· Generic-frequency: We analyze a generic document corpus collected
elsewhere (say, from the Web) and obtain the generic frequency
distribution of each keyword. Based on this generic distribution
, we start with the most frequent keyword, issue it to the
Hidden-Web database and retrieve the result. We then continue
to the second-most frequent keyword and repeat this process until
we exhaust all download resources. The hope is that the frequent
keywords in a generic corpus will also be frequent in the
Hidden-Web database, returning many matching documents.
· Adaptive: We analyze the documents returned from the previous
queries issued to the Hidden-Web database and estimate which
keyword is most likely to return the most documents. Based on
this analysis, we issue the most "promising" query, and repeat
the process.
Among these three general policies, we may consider the random
policy as the base comparison point since it is expected to
perform the worst. Between the generic-frequency and the adaptive
policies, both policies may show similar performance if the
crawled database has a generic document collection without a specialized
topic. The adaptive policy, however, may perform significantly
better than the generic-frequency policy if the database has a
very specialized collection that is different from the generic corpus.
We will experimentally compare these three policies in Section 4.
While the first two policies (random and generic-frequency policies
) are easy to implement, we need to understand how we can analyze
the downloaded pages to identify the most "promising" query
in order to implement the adaptive policy. We address this issue in
the rest of this section.
3.1
Estimating the number of matching pages
In order to identify the most promising query, we need to estimate
how many new documents we will download if we issue the
query
q
i
as the next query. That is, assuming that we have issued
queries
q
1
, . . . , q
i-1
we need to estimate
P (q
1
· · ·q
i-1
q
i
), for
every potential next query
q
i
and compare this value. In estimating
this number, we note that we can rewrite
P (q
1
· · ·  q
i-1
q
i
)
as:
P ((q
1
· · ·  q
i-1
)  q
i
)
= P (q
1
· · ·  q
i-1
) + P (q
i
) - P ((q
1
· · ·  q
i-1
)  q
i
)
= P (q
1
· · ·  q
i-1
) + P (q
i
)
- P (q
1
· · ·  q
i-1
)P (q
i
|q
1
· · ·  q
i-1
)
(3)
In the above formula, note that we can precisely measure
P (q
1

· · ·  q
i-1
) and P (q
i
| q
1
· · ·  q
i-1
) by analyzing previously-downloaded
pages: We know
P (q
1
· · ·  q
i-1
), the union of
all pages downloaded from
q
1
, . . . , q
i-1
, since we have already issued
q
1
, . . . , q
i-1
and downloaded the matching pages.
4
We can
also measure
P (q
i
| q
1
· · ·  q
i-1
), the probability that q
i
appears
in the pages from
q
1
, . . . , q
i-1
, by counting how many times
q
i
appears in the pages from
q
1
, . . . , q
i-1
. Therefore, we only need
to estimate
P (q
i
) to evaluate P (q
1
· · ·  q
i
). We may consider a
number of different ways to estimate
P (q
i
), including the following
:
1. Independence estimator: We assume that the appearance of the
term
q
i
is independent of the terms
q
1
, . . . , q
i-1
. That is, we
assume that
P (q
i
) = P (q
i
|q
1
· · ·  q
i-1
).
2. Zipf estimator: In [19], Ipeirotis et al. proposed a method to
estimate how many times a particular term occurs in the entire
corpus based on a subset of documents from the corpus. Their
method exploits the fact that the frequency of terms inside text
collections follows a power law distribution [30, 25]. That is,
if we rank all terms based on their occurrence frequency (with
the most frequent term having a rank of 1, second most frequent
a rank of 2 etc.), then the frequency
f of a term inside the text
collection is given by:
f = (r + )

(4)
where
r is the rank of the term and , , and  are constants that
depend on the text collection.
Their main idea is (1) to estimate the three parameters,
,  and
, based on the subset of documents that we have downloaded
from previous queries, and (2) use the estimated parameters to
predict
f given the ranking r of a term within the subset. For
a more detailed description on how we can use this method to
estimate
P (q
i
), we refer the reader to the extended version of
this paper [27].
After we estimate
P (q
i
) and P (q
i
|q
1
· · ·  q
i-1
) values, we
can calculate
P (q
1
· · ·  q
i
). In Section 3.3, we explain how
we can efficiently compute
P (q
i
|q
1
· · ·  q
i-1
) by maintaining a
succinct summary table. In the next section, we first examine how
we can use this value to decide which query we should issue next
to the Hidden Web site.
3.2
Query selection algorithm
The goal of the Hidden-Web crawler is to download the maximum
number of unique documents from a database using its limited
download resources. Given this goal, the Hidden-Web crawler
has to take two factors into account. (1) the number of new documents
that can be obtained from the query
q
i
and (2) the cost of
issuing the query
q
i
. For example, if two queries,
q
i
and
q
j
, incur
the same cost, but
q
i
returns more new pages than
q
j
,
q
i
is more
desirable than
q
j
. Similarly, if
q
i
and
q
j
return the same number
of new documents, but
q
i
incurs less cost then
q
j
,
q
i
is more desirable
. Based on this observation, the Hidden-Web crawler may
use the following efficiency metric to quantify the desirability of
the query
q
i
:
Efficiency
(q
i
) = P
new
(q
i
)
Cost
(q
i
)
Here,
P
new
(q
i
) represents the amount of new documents returned
for
q
i
(the pages that have not been returned for previous queries).
Cost
(q
i
) represents the cost of issuing the query q
i
.
Intuitively, the efficiency of
q
i
measures how many new documents
are retrieved per unit cost, and can be used as an indicator of
4
For exact estimation, we need to know the total number of pages in
the site. However, in order to compare only relative values among
queries, this information is not actually needed.
103
A
LGORITHM
3.1.
Greedy SelectTerm()
Parameters:
T : The list of potential query keywords
Procedure
(1)
Foreach
t
k
in
T do
(2)
Estimate Efficiency
(t
k
) =
P
new
(t
k
)
Cost(t
k
)
(3)
done
(4)
return
t
k
with maximum Efficiency
(t
k
)
Figure 6: Algorithm for selecting the next query term.
how well our resources are spent when issuing
q
i
. Thus, the Hidden
Web crawler can estimate the efficiency of every candidate
q
i
,
and select the one with the highest value. By using its resources
more efficiently, the crawler may eventually download the maximum
number of unique documents. In Figure 6, we show the query
selection function that uses the concept of efficiency. In principle,
this algorithm takes a greedy approach and tries to maximize the
"potential gain" in every step.
We can estimate the efficiency of every query using the estimation
method described in Section 3.1. That is, the size of the new
documents from the query
q
i
,
P
new
(q
i
), is
P
new
(q
i
)
= P (q
1
· · ·  q
i-1
q
i
) - P (q
1
· · ·  q
i-1
)
= P (q
i
) - P (q
1
· · ·  q
i-1
)P (q
i
|q
1
· · ·  q
i-1
)
from Equation 3, where
P (q
i
) can be estimated using one of the
methods described in section 3. We can also estimate Cost
(q
i
) sim-ilarly
. For example, if Cost
(q
i
) is
Cost
(q
i
) = c
q
+ c
r
P (q
i
) + c
d
P
new
(q
i
)
(Equation 2), we can estimate Cost
(q
i
) by estimating P (q
i
) and
P
new
(q
i
).
3.3
Efficient calculation of query statistics
In estimating the efficiency of queries, we found that we need to
measure
P (q
i
|q
1
· · ·q
i-1
) for every potential query q
i
. This calculation
can be very time-consuming if we repeat it from scratch for
every query
q
i
in every iteration of our algorithm. In this section,
we explain how we can compute
P (q
i
|q
1
· · ·  q
i-1
) efficiently
by maintaining a small table that we call a query statistics table.
The main idea for the query statistics table is that
P (q
i
|q
1
· · ·
q
i-1
) can be measured by counting how many times the keyword
q
i
appears within the documents downloaded from
q
1
, . . . , q
i-1
.
We record these counts in a table, as shown in Figure 7(a). The
left column of the table contains all potential query terms and the
right column contains the number of previously-downloaded documents
containing the respective term. For example, the table in Figure
7(a) shows that we have downloaded 50 documents so far, and
the term model appears in 10 of these documents. Given this number
, we can compute that
P (model|q
1
· · ·  q
i-1
) =
10
50
= 0.2.
We note that the query statistics table needs to be updated whenever
we issue a new query
q
i
and download more documents. This
update can be done efficiently as we illustrate in the following example
.
E
XAMPLE
1. After examining the query statistics table of Figure
7(a), we have decided to use the term "computer" as our next
query
q
i
. From the new query
q
i
= "computer," we downloaded
20 more new pages. Out of these, 12 contain the keyword "model"
Term
t
k
N (t
k
)
model
10
computer
38
digital
50
Term
t
k
N (t
k
)
model
12
computer
20
disk
18
Total pages:
50
New pages:
20
(a) After
q
1
, . . . , q
i-1
(b) New from
q
i
= computer
Term
t
k
N (t
k
)
model
10+12 = 22
computer
38+20 = 58
disk
0+18 = 18
digital
50+0 = 50
Total pages:
50 + 20 = 70
(c) After
q
1
, . . . , q
i
Figure 7: Updating the query statistics table.
q
i
1
i-1
q
\/ ... \/
q
q
i
/
S
Figure 8: A Web site that does not return all the results.
and
18 the keyword "disk." The table in Figure 7(b) shows the
frequency of each term in the newly-downloaded pages.
We can update the old table (Figure 7(a)) to include this new
information by simply adding corresponding entries in Figures 7(a)
and (b). The result is shown on Figure 7(c). For example, keyword
"model" exists in
10 + 12 = 22 pages within the pages retrieved
from
q
1
, . . . , q
i
. According to this new table,
P (model|q
1
· · ·q
i
)
is now
22
70
= 0.3.
3.4
Crawling sites that limit the number of
results
In certain cases, when a query matches a large number of pages,
the Hidden Web site returns only a portion of those pages. For example
, the Open Directory Project [2] allows the users to see only
up to
10, 000 results after they issue a query. Obviously, this kind
of limitation has an immediate effect on our Hidden Web crawler.
First, since we can only retrieve up to a specific number of pages
per query, our crawler will need to issue more queries (and potentially
will use up more resources) in order to download all the
pages. Second, the query selection method that we presented in
Section 3.2 assumes that for every potential query
q
i
, we can find
P (q
i
|q
1
· · ·  q
i-1
). That is, for every query q
i
we can find the
fraction of documents in the whole text database that contains
q
i
with at least one of
q
1
, . . . , q
i-1
. However, if the text database returned
only a portion of the results for any of the
q
1
, . . . , q
i-1
then
the value
P (q
i
|q
1
· · ·  q
i-1
) is not accurate and may affect our
decision for the next query
q
i
, and potentially the performance of
our crawler. Since we cannot retrieve more results per query than
the maximum number the Web site allows, our crawler has no other
choice besides submitting more queries. However, there is a way
to estimate the correct value for
P (q
i
|q
1
· · ·  q
i-1
) in the case
where the Web site returns only a portion of the results.
104
Again, assume that the Hidden Web site we are currently crawling
is represented as the rectangle on Figure 8 and its pages as
points in the figure. Assume that we have already issued queries
q
1
, . . . , q
i-1
which returned a number of results less than the maximum
number than the site allows, and therefore we have downloaded
all the pages for these queries (big circle in Figure 8). That
is, at this point, our estimation for
P (q
i
|q
1
· · ·q
i-1
) is accurate.
Now assume that we submit query
q
i
to the Web site, but due to a
limitation in the number of results that we get back, we retrieve the
set
q
i
(small circle in Figure 8) instead of the set
q
i
(dashed circle
in Figure 8). Now we need to update our query statistics table so
that it has accurate information for the next step. That is, although
we got the set
q
i
back, for every potential query
q
i+1
we need to
find
P (q
i+1
|q
1
· · ·  q
i
):
P (q
i+1
|q
1
· · ·  q
i
)
=
1
P (q
1
· · ·  q
i
) · [P (q
i+1
(q
1
· · ·  q
i-1
))+
P (q
i+1
q
i
) - P (q
i+1
q
i
(q
1
· · ·  q
i-1
))]
(5)
In the previous equation, we can find
P (q
1
· · ·q
i
) by estimating
P (q
i
) with the method shown in Section 3. Additionally, we
can calculate
P (q
i+1
(q
1
· · ·  q
i-1
)) and P (q
i+1
q
i
(q
1

· · ·  q
i-1
)) by directly examining the documents that we have
downloaded from queries
q
1
, . . . , q
i-1
. The term
P (q
i+1
q
i
)
however is unknown and we need to estimate it. Assuming that
q
i
is a random sample of
q
i
, then:
P (q
i+1
q
i
)
P (q
i+1
q
i
) =
P (q
i
)
P (q
i
)
(6)
From Equation 6 we can calculate
P (q
i+1
q
i
) and after we
replace this value to Equation 5 we can find
P (q
i+1
|q
1
· · ·  q
i
).
EXPERIMENTAL EVALUATION
In this section we experimentally evaluate the performance of
the various algorithms for Hidden Web crawling presented in this
paper. Our goal is to validate our theoretical analysis through real-world
experiments, by crawling popular Hidden Web sites of textual
databases. Since the number of documents that are discovered
and downloaded from a textual database depends on the selection
of the words that will be issued as queries
5
to the search interface
of each site, we compare the various selection policies that were
described in section 3, namely the random, generic-frequency, and
adaptive algorithms.
The adaptive algorithm learns new keywords and terms from the
documents that it downloads, and its selection process is driven by
a cost model as described in Section 3.2. To keep our experiment
and its analysis simple at this point, we will assume that the cost for
every query is constant. That is, our goal is to maximize the number
of downloaded pages by issuing the least number of queries. Later,
in Section 4.4 we will present a comparison of our policies based
on a more elaborate cost model. In addition, we use the independence
estimator (Section 3.1) to estimate
P (q
i
) from downloaded
pages. Although the independence estimator is a simple estimator,
our experiments will show that it can work very well in practice.
6
For the generic-frequency policy, we compute the frequency distribution
of words that appear in a 5.5-million-Web-page corpus
5
Throughout our experiments, once an algorithm has submitted a
query to a database, we exclude the query from subsequent submissions
to the same database from the same algorithm.
6
We defer the reporting of results based on the Zipf estimation to a
future work.
downloaded from 154 Web sites of various topics [26]. Keywords
are selected based on their decreasing frequency with which they
appear in this document set, with the most frequent one being selected
first, followed by the secon
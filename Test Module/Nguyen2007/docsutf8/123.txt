Learning Concepts from Large Scale Imbalanced Data Sets Using Support Cluster Machines
ABSTRACT
This paper considers the problem of using Support Vector
Machines (SVMs) to learn concepts from large scale imbalanced
data sets.
The objective of this paper is twofold.
Firstly, we investigate the effects of large scale and imbalance
on SVMs.
We highlight the role of linear non-separability
in this problem. Secondly, we develop a both
practical and theoretical guaranteed meta-algorithm to handle
the trouble of scale and imbalance.
The approach is
named Support Cluster Machines (SCMs). It incorporates
the informative and the representative under-sampling mechanisms
to speedup the training procedure. The SCMs differs
from the previous similar ideas in two ways, (a) the theoretical
foundation has been provided, and (b) the clustering
is performed in the feature space rather than in the input
space. The theoretical analysis not only provides justification
, but also guides the technical choices of the proposed
approach. Finally, experiments on both the synthetic and
the TRECVID data are carried out. The results support
the previous analysis and show that the SCMs are efficient
and effective while dealing with large scale imbalanced data
sets.
Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing--Abstracting methods,Indexing methods
; I.5.2 [Pattern Recognition]: Design Methodology-Classifier
design and evaluation
General Terms
Algorithms, Theory, Experimentation
Supported by National Natural Science Foundation of
China (60135010, 60321002) and Chinese National Key
Foundation Research & Development Plan (2004CB318108).
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
MM'06, October 23­27, 2006, Santa Barbara, California, USA.
Copyright 2006 ACM 1-59593-447-2/06/0010 ...
$
5.00.

INTRODUCTION
In the context of concept modelling, this paper considers
the problem of how to make full use of the large scale annotated
data sets. In particular, we study the behaviors of
Support Vector Machines (SVMs) on large scale imbalanced
data sets, not only because its solid theoretical foundations
but also for its empirical success in various applications.
1.1
Motivation
Bridging the semantic gap has been becoming the most
challenging problem of Multimedia Information Retrieval
(MIR). Currently, there are mainly two types of methods
to bridge the gap [8]. The first one is relevance feedback
which attempts to capture the user's precise needs through
iterative feedback and query refinement. Another promising
direction is concept modelling. As noted by Hauptmann
[14], this splits the semantic gap between low level features
and user information needs into two, hopefully smaller gaps:
(a) mapping the low-level features into the intermediate semantic
concepts and (b) mapping these concepts into user
needs. The automated image annotation methods for CBIR
and the high level feature extraction methods in CBVR are
all the efforts to model the first mapping. Of these methods,
supervised learning is one of the most successful ones. An
early difficulty of supervised learning is the lack of annotated
training data. Currently, however, it seems no longer
a problem. This is due to both the techniques developed to
leverage surrounding texts of web images and the large scale
collaborative annotation. Actually, there is an underway effort
named Large Scale Concept Ontology for Multimedia
Understanding (LSCOM), which intends to annotate 1000
concepts in broadcast news video [13]. The initial fruits of
this effort have been harvested in the practice of TRECVID
hosted by National Institute of Standards and Technology
(NIST) [1]. In TRECVID 2005, 39 concepts are annotated
by multiple participants through web collaboration, and ten
of them are used in the evaluation.
The available large amount of annotated data is undoubt-edly
beneficial to supervised learning. However, it also brings
out a novel challenge, that is, how to make full use of the
data while training the classifiers. On the one hand, the annotated
data sets are usually in rather large scale. The de-441
velopment set of TRECVID 2005 includes 74523 keyframes.
The data set of LSCOM with over 1000 annotated concepts
might be even larger. With all the data, the training of
SVMs will be rather slow. On the other hand, each concept
will be the minority class under one-against-all strategy
. Only a small portion of the data belong to the concept,
while all the others are not (In our case, the minority class
always refers to the positive class). The ratio of the positive
examples and the negative ones is typically below 1 : 100
in TRECVID data. These novel challenges have spurred
great interest in the communities of data mining and machine
learning[2, 6, 21, 22, 29]. Our first motivation is to
investigate the effects of large scale and imbalance on SVMs.
This is critical for correct technical choices and development.
The second objective of this paper is to provide a practical
as well as theoretical guaranteed approach to addressing the
problem.
1.2
Our Results
The major contribution of this paper can be summarized
as follows:
1. We investigate the effects of large scale and imbalance
on SVMs and highlight the role of linear non-separability
of the data sets. We find that SVMs has
no difficulties with linear separable large scale imbalanced
data.
2. We establish the relations between the SVMs trained
on the centroids of the clusters and the SVMs obtained
on the original data set. We show that the difference
between their optimal solutions are bounded by the
perturbation of the kernel matrix. We also prove the
optimal criteria for approximating the original optimal
solutions.
3. We develop a meta-algorithm named Support Cluster
Machines (SCMs).
A fast kernel k-means approach
has been employed to partition the data in the feature
space rather than in the input space.
Experiments on both the synthetic data and the TRECVID
data are carried out. The results support the previous analysis
and show that the SCMs are efficient and effective while
dealing with large scale imbalanced data sets.
1.3
Organization
The structure of this paper is as follows. In Section 2 we
give a brief review of SVMs and kernel k-means. We discuss
the effects of the large scale imbalanced data on SVMs
in Section 3. We develop the theoretical foundations and
present the detailed SCMs approach in Section 4. In Section
5 we carry out experiments on both the synthetic and
the TRECVID data sets. Finally, we conclude the paper in
Section 6.
PRELIMINARIES
Here, we present a sketch introduction to the soft-margin
SVMs for the convenience of the deduction in Section 4. For
a binary classification problem, given a training data set
D
of size n
D = {(x
i
, y
i
)
|x
i
R
N
, y
i
{1, -1}},
where x
i
indicates the training vector of the ith sample and
y
i
indicates its target value, and i = 1, . . . , n. The classification
hyperplane is defined as
w, (x) + b = 0,
where (
·) is a mapping from R
N
to a (usually) higher dimension
Hilbert space
H, and ·, · denotes the dot product
in
H. Thus, the decision function f(x) is
f (x) = sign( w, (x) + b).
The SVMs aims to find the hyperplane with the maximum
margin between the two classes, i.e., the optimal hyperplane.
This can be obtained by solving the following quadratic optimization
problem
min
w,b,
1
2 w
2
+ C
n
i=1

i
subject to
y
i
( w, (x
i
) + b)
1 i
(1)

i
0, i = 1, . . . , n.
With the help of Lagrange multipliers, the dual of the above
problem is
min

G() = 1
2
T
Q
- e
T

subject to
0

i
C, i = 1, . . . , n
(2)

T
y = 0,
where  is a vector with components
i
that are the Lagrange
multipliers, C is the upper bound, e is a vector of
all ones, and Q is an n
× n positive semi-definite matrix,
Q
ij
= y
i
y
j
(x
i
), (x
j
) . Since the mapping (
·) only appears
in the dot product, therefore, we need not know its
explicit form. Instead, we define a kernel K(
·, ·) to calculate
the dot product, i.e., K(x
i
, x
j
) = (x
i
), (x
j
) . The matrix
K with components K(x
i
, x
j
) is named Gram Matrix
(or kernel matrix). With kernel K
·, · , we can implicitly
map the training data from input space to a feature space
H.
2.2
Kernel
k
-means and Graph Partitioning
Given a set of vectors x
1
, . . . , x
n
, the standard k-means
algorithm aims to find clusters
1
, . . . ,
k
that minimize the
objective function
J(
{
c
}
k
c=1
) =
k
c=1 x
i

c
x
i
- m
c 2
,
(3)
where
{
c
}
k
c=1
denotes the partitioning of the data set and
m
c
=
È
xic
x
i
|
c
|
is the centroid of the cluster
c
. Similar
to the idea of nonlinear SVMs, the k-means can also be
performed in the feature space with the help of a nonlinear
mapping (
·), which results in the so-called kernel k-means
J(
{
c
}
k
c=1
) =
k
c=1 x
i

c
(x
i
)
- m
c 2
,
(4)
where m
c
=
È
xic
(x
i
)
|
c
|
. If we expand the Euclidean distance
(x
i
)
- m
c 2
in the objective function, we can find
that the image of x
i
only appears in the form of dot product
. Thus, given a kernel matrix K with the same meaning
442
in SVMs, we can compute the distance between points and
centroids without knowing explicit representation of (x
i
).
Recently, an appealing alternative, i.e., the graph clustering
has attracted great interest. It treats clustering as
a graph partition problem. Given a graph G = (
V, E, A),
which consists of a set of vertices
V and a set of edges E such
that an edge between two vertices represents their similarity.
The affinity matrix A is
|V|×|V| whose entries represent the
weights of the edges. Let links(
V
1
,
V
2
) be the sum of the
edge weights between the nodes in
V
1
and
V
2
, that is
links(
V
1
,
V
2
) =
iV
1
,jV
2
A
ij
.
Ratio association is a type of graph partitioning objective
which aims to maximize within-cluster association relative
to the size of the cluster
RAssoc(G) =
max
V
1
,...,V
k
k
c=1
links(
V
c
,
V
c
)
|V
c
|
.
(5)
The following theorem establishes the relation between kernel
k-means and graph clustering [10].
With this result,
we can develop some techniques to handle the difficulty of
storing the large kernel matrix for kernel k-means.
Theorem 1. Given a data set, we can construct a weighted
graph G = (
V, E, A), by treating each sample as a node and
linking an edge between each other. If we define the edge
weight A
ij
= K(x
i
, x
j
), that is, A = K, the minimization
of (4) is equivalent to the maximization of (5).
THE EFFECTS OF LARGE SCALE IM-BALANCED DATA ON SVMS
There are two obstacles yielded by large scale. The first
one is the kernel evaluation, which has been intensively discussed
in the previous work. The computational cost scales
quadratically with the data size. Furthermore, it is impossible
to store the whole kernel matrix in the memory for
common computers. The decomposition algorithms (e.g.,
SMO) have been developed to solve the problem [20, 22].
The SMO-like algorithms actually transform the space load
to the time cost, i.e., numerous iterations until convergence.
To reduce or avoid the kernel reevaluations, various efficient
caching techniques are also proposed [16]. Another obstacle
caused by large scale is the increased classification difficulty
, that is, the more probable data overlapping. We can
not prove it is inevitable but it typically happens. Assume
we will draw n randomly chosen numbers between 1 to 100
from a uniform distribution, our chances of drawing a number
close to 100 would improve with increasing values of n,
even though the expected mean of the draws is invariant [2].
The checkerboard experiment in [29] is an intuitive example
. This is true especially for the real world data, either
because of the weak features (we mean features that are
less discriminative) or because of the noises. With the large
scale data, the samples in the overlapping area might be
so many that the samples violating KKT conditions become
abundant. This means the SMO algorithm might need more
iterations to converge.
Generally, the existing algorithmic approaches have not
been able to tackle the very large data set. Whereas, the
under-sampling method, e.g., active learning, is possible.
With unlabelled data, active learning selects a well-chosen
subset of data to label so that reduce the labor of manual annotations
[24]. With large scale labelled data, active learning
can also be used to reduce the scale of training data [21].
The key issue of active learning is how to choose the most
"valuable" samples. The informative sampling is a popular
criterion. That is, the samples closest to the boundary or
maximally violating the KKT conditions (the misclassified
samples) are preferred [24, 26]. Active learning is usually
in an iterative style. It requires an initial (usually random
selected) data set to obtain the estimation of the boundary.
The samples selected in the following iterations depend on
this initial boundary. In addition, active learning can not
work like the decomposition approach which stops until all
the samples satisfy the KKT conditions. This imply a potential
danger, that is, if the initial data are selected improperly,
the algorithm might not be able to find the suitable hyperplane
. Thus, another criterion, i.e., representative, must be
considered. Here, "representative" refers to the ability to
characterize the data distribution. Nguyen et al. [19] show
that the active learning method considering the representative
criterion will achieve better results. Specifically for
SVMs, pre-clustering is proposed to estimate the data distribution
before the under-sampling [31, 3, 30]. Similar ideas
of representative sampling appear in [5, 12].
3.2
The Imbalanced Data
The reason why general machine learning systems suffer
performance loss with imbalanced data is not yet clear [23,
28], but the analysis on SVMs seems relatively straightforward
. Akbani et al. have summarized three possible causes
for SVMs [2].
They are, (a) positive samples lie further
from the ideal boundary, (b) the weakness of the soft-margin
SVMs, and (c) the imbalanced support vector ratio. Of these
causes, in our opinion, what really matters is the second one.
The first cause is pointed out by Wu et al. [29]. This situation
occurs when the data are linearly separable and the
imbalance is caused by the insufficient sampling of the minority
class. Only in this case does the "ideal" boundary
make sense.
As for the third cause, Akbani et al.
have
pointed out that it plays a minor role because of the constraint
T
y = 0 on Lagrange multipliers [2].
The second cause states that the soft-margin SVMs has inherent
weakness for handling imbalanced data. We find that
it depends on the linear separability of the data whether the
imbalance has negative effects on SVMs. For linearly separable
data, the imbalance will have tiny effects on SVMs,
since all the slack variables  of (1) tend to be zeros (, unless
the C is so small that the maximization of the margin dominates
the objective). In the result, there is no contradiction
between the capacity of the SVMs and the empirical error
. Unfortunately, linear non-separable data often occurs.
The SVMs has to achieve a tradeoff between maximizing
the margin and minimizing the empirical error. For imbalanced
data, the majority class outnumbers the minority one
in the overlapping area. To reduce the overwhelming errors
of misclassifying the majority class, the optimal hyperplane
will inevitably be skew to the minority. In the extreme, if C
is not very large, SVMs simply learns to classify everything
as negative because that makes the "margin" the largest,
with zero cumulative error on the abundant negative examples
. The only tradeoff is the small amount of cumulative
443
error on the few positive examples, which does not count for
much.
Several variants of SVMs have been adopted to solve the
problem of imbalance. One choice is the so-called one-class
SVMs, which uses only positive examples for training. Without
using the information of the negative samples, it is usually
difficult to achieve as good result as that of binary SVMs
classifier [18]. Using different penalty constants C
+
and C
for
the positive and negative examples have been reported to
be effective [27, 17]. However, Wu et al. point out that the
effectiveness of this method is limited [29]. The explanation
of Wu is based on the KKT condition
T
y = 0, which imposes
an equal total influence from the positive and negative
support vectors. We evaluate this method and the result
shows that tuning
C
+
C
does
work (details refer to Section
5). We find this also depends on the linear separability of
the data whether this method works. For linearly separable
data, tuning
C
+
C
has
little effects, since the penalty constants
are useless with the zero-valued slack variables. However, if
the data are linearly non-separable, tuning
C
+
C
does
change
the position of separating hyperplane. The method to modify
the kernel matrix is also proposed to improve SVMs for
imbalanced data [29]. A possible drawback of this type approach
is its high computational costs.
OVERALL APPROACH
The proposed approach is named Support Cluster Machines
(SCMs). We first partition the negative samples into
disjoint clusters, then train an initial SVMs model using
the positive samples and the representatives of the negative
clusters. With the global picture of the initial SVMs, we can
approximately identify the support vectors and non-support
vectors. A shrinking technique is then used to remove the
samples which are most probably not support vectors. This
procedure of clustering and shrinking are performed itera-tively
several times until some stop criteria satisfied. With
such a from coarse-to-fine procedure, the representative and
informative mechanisms are incorporated. There are four
key issues in the meta-algorithm of SCMs: (a) How to get
the partition of the training data, (b) How to get the representative
for each cluster, (c) How to safely remove the
non-support vector samples, (d) When to stop the iteration
procedure. Though similar ideas have been proposed
to speed-up SVMs in [30, 3, 31], no theoretical analysis of
this idea has been provided. In the following, we present an
in-depth analysis for this type of approaches and attempt to
improve the algorithm under the theoretical guide.
4.1
Theoretical Analysis
Suppose
{
c
}
k
c=1
is a partition of the training set that the
samples within the same cluster have the same class label.
If we construct a representative u
c
for each cluster
c
, we
can obtain two novel models of SVMs.
The first one is named Support Cluster Machines (SCMs).
It treats each representative as a sample, thus the data size
is reduced from n to k. This equals to the classification of
the clusters. That is where the name SCMs come from. The
new training set is
D

=
{(u
c
, y
c
)
|u
c
R
N
, y
c
{1, -1}, c = 1, . . . , k},
in which y
c
equals the labels of the samples within
c
. We
define the dual problem of support cluster machines as
min


G

(

) = 1
2
T

Q



- e
T



subjectto
0

i
|
i
|C, i = 1, . . . , k
(6)

T

y

= 0,
where

is a vector of size k with components
i
corresponding
to u
i
,
|
i
|C is the upper bound for
i
, e

is a
k dimension vector of all ones, and Q

is an k
× k positive
semi-definite matrix, Q
ij
= y
i
y
j
(u
i
), (u
j
) .
Another one is named Duplicate Support Vector Machines
(DSVMs). Different from SCMs, it does not reduce the size
of training set. Instead, it replace each sample x
i
with the
representative of the cluster that x
i
belongs to. Thus, the
samples within the same cluster are duplicate. That is why
it is named DSVMs. The training set is
~
D = {(~x
i
, ~
y
i
)
|x
i
D, if x
i

c
, ~
x
i
= u
c
and ~
y
i
= y
i
},
and the corresponding dual problem is defined as
min

~
G() = 1
2
T
~
Q
- e
T

subjectto
0

i
C, i = 1, . . . , n
(7)

T
y = 0,
where ~
Q is is an n
× n positive semi-definite matrix, ~
Q
ij
=
~
y
i
~
y
j
(~
x
i
), (~
x
j
) .
We have the following theorem that
states (6) is somehow equivalent to (7):
Theorem 2. With the above definitions of the SCMs and
the DSVMs, if

and

are their optimal solutions respectively
, the relation G

(

) = ~
G(

) holds. Furthermore,
any

R
k
satisfying
{
c
=
È
x
i

c

i
,
c = 1, . . . , k}
is the optimal solution of SCMs. Inversely, any
R
n
satisfying
{
È
x
i

c

i
=
c
,
c = 1, . . . , k} and the constraints
of (7) is the optimal solution of DSVMs.
The proof is in Appendix A. Theorem 2 shows that solving
the SCMs is equivalent to solving a quadratic programming
problem of the same scale as that of the SVMs in (2).
Comparing (2) and (7), we can find that only the Hessian
matrix is different. Thus, to estimate the approximation
from SCMs of (6) to SVMs of (2), we only need to analyze
the stability of the quadratic programming model in
(2) when the Hessian matrix varies from Q to ~
Q. Daniel
has presented a study on the stability of the solution of definite
quadratic programming, which requires that both Q
and ~
Q are positive definite [7]. However, in our situation,
Q is usually positive definite and ~
Q is not (because of the
duplications). We develop a novel theorem for this case.
If define  = Q
- ~
Q , where
· denotes the Frobenius
norm of a matrix, the value of  measure the size of the
perturbations between Q and ~
Q.
We have the following
theorem:
Theorem 3. If Q is positive definite and  = Q - ~
Q
,
let

and ~


be the optimal solutions to (2) and (7) respectively
, we have
~



~
mC

G( ~


)
- G(

)
(m
2
+ ~
m
2
)C
2

2
444
where  is the minimum eigenvalue of Q, m and ~
m indicate
the numbers of the support vectors for (2) and (7) respectively
.
The proof is in Appendix B. This theorem shows that the
approximation from (2) to (7) is bounded by . Note that
this does not mean that with minimal  we are sure to get
the best approximate solution. For example, adopting the
support vectors of (1) to construct ~
Q will yield the exact
optimal solution of (2) but the corresponding  are not necessarily
minimum. However, we do not know which samples
are support vectors beforehand. What we can do is to minimize
the potential maximal distortion between the solutions
between (2) and (7).
Now we consider the next problem, that is, given the partition
{
c
}
k
c=1
, what are the best representatives
{u
c
}
k
c=1
for the clusters in the sense of approximating Q? In fact,
we have the following theorem:
Theorem 4. Given the partition {
c
}
k
c=1
, the
{u
c
}
k
c=1
satisfying
(u
c
) =
È
x
i

c
(x
i
)
|
c
|
, c = 1, . . . , k
(8)
will make  = Q
- ~
Q
minimum.
The proof is in Appendix C. This theorem shows that, given
the partition, (u
c
) = m
c
yields the best approximation
between ~
Q and Q.
Here we come to the last question, i.e., what partition
{
c
}
k
c=1
will make  =
Q
- ~
Q
minimum. To make the
problem more clearly, we expand
2
as
Q
- ~
Q
2
=
k
h=1
k
l=1 x
i

h
x
j

l
( (x
i
), (x
j
)
- m
h
, m
l
)
2
.
(9)
There are approximately k
n
/k! types of such partitions of
the data set. An exhaustive search for the best partition is
impossible. Recalling that (9) is similar to (4), we have the
following theorem which states their relaxed equivalence.
Theorem 5. The relaxed optimal solution of minimizing
(9) and the relaxed optimal solution of minimizing (4) are
equivalent.
The proof can be found in Appendix D. Minimizing  amounts
to find a low-rank matrix approximating Q. Ding et al. have
pointed out the relaxed equivalence between kernel PCA and
kernel k-means in [11]. Note that minimizing (9) is different
from kernel PCA in that it is with an additional block-wise
constant constraint. That is, the value of ~
Q
ij
must be invariant
with respect to the cluster
h
containing ~
x
i
and the
cluster
l
containing ~
x
j
. With Theorem 5 we know that
kernel k-means is a suitable method to obtain the partition
of data.
According to the above results, the SCMs essentially finds
an approximate solution to the original SVMs by smoothing
the kernel matrix K (or Hessian matrix Q). Fig.1 illustrates
the procedure of smoothing the kernel matrix via clustering.
Hence, by solving a smaller quadratic programming problem
, the position of separating hyperplane can be roughly
determined.
-5
0
5
10
15
-4
-2
0
2
4
6
8
10
12
14
16
50
100
150
200
20
40
60
80
100
120
140
160
180
200
50
100
150
200
20
40
60
80
100
120
140
160
180
200
50
100
150
200
20
40
60
80
100
120
140
160
180
200
(a)
(b)
(c )
(d)
Figure 1: (a) 2D data distribution, (b) the visualization
of the kernel matrix Q, (c) the kernel matrix
Q by re-ordering the entries so that the samples belonging
to the same cluster come together, (d) the
approximate kernel matrix ~
Q obtained by replacing
each sample with the corresponding centroid.
4.2
Kernel-based Graph Clustering
In the previous work, k-means [30], BIRCH [31] and PDDP
[3] have been used to obtain the partition of the data. None
of them performs clustering in the feature space, though the
SVMs works in the feature space. This is somewhat unnatural
. Firstly, recalling that the kernel K(
·, ·) usually implies
an implicitly nonlinear mapping from the input space to the
feature space, the optimal partition of input space is not necessarily
the optimal one of feature space. Take k-means as
an example, due to the fact that the squared Euclidean distance
is used as the distortion measure, the clusters must be
separated by piece-wise hyperplanes (i.e., voronoi diagram).
However, these separating hyperplanes are no longer hyperplanes
in the feature space with nonlinear mapping (
·).
Secondly, the k-means approach can not capture the complex
structure of data. As shown in Fig.2, the negative class
is in a ring-shape in the input space. If the k-means is used,
the centroids of positive and negative class might overlap.
Whereas in the feature space, the kernel k-means might get
separable centroids.
Several factors limit the application of kernel k-means to
large scale data. Firstly, it is almost impossible to store the
whole kernel matrix K in the memory, e.g., for n = 100 000,
we still need 20 gigabytes memory taking the symmetry into
account. Secondly, the kernel k-means relies heavily on an
effective initialization to achieve good results, and we do not
have such a sound method yet. Finally, the computational
cost of the kernel k-means might exceeds that of SVMs, and
therefore, we lose the benefits of under-sampling. Dhillon et
al. recently propose a multilevel kernel k-means method [9],
which seems to cater to our requirements. The approach
is based on the equivalence between graph clustering and
kernel k-means. It incorporates the coarsening and initial
partitioning phases to obtain a good initial clustering. Most
importantly, the approach is extremely efficient. It can handle
a graph with 28,294 nodes and 1,007,284 edges in several
seconds. Therefore, here we adopt this approach. The detailed
description can be found in [9]. In the following, we
focus on how to address the difficulty of storing large scale
kernel matrix.
Theorem 1 states that kernel k-means is equivalent to a
type of graph clustering. Kernel k-means focuses on grouping
data so that their average distance from the centroid is
minimum ,while graph clustering aims to minimizing the average
pair-wise distance among the data. Central grouping
and pair-wise grouping are two different views of the same
approach. From the perspective of pair-wise grouping, we
can expect that two samples with large distance will not
belong to the same cluster in the optimal solution. Thus,
445
1
x
2
x
1
z
2
z
3
z
Positive Class
Negative Class
Figure 2: The left and right figures show the data
distribution of input space and feature space respectively
. The two classes are indicated by squares and
circles. Each class is grouped into one cluster, and
the solid mark indicates the centroid of the class.
we add the constraint that two samples with distance large
enough are not linked by an edge, that is, transforming the
dense graph to a sparse graph. This procedure is the common
practice in spectral clustering or manifold embedding.
Usually, two methods have been widely used for this purpose
, i.e., k-nearest neighbor and -ball. Here, we adopt the
-ball approach. Concretely, the edges with weight A
ij
&lt;
is removed from the original graph, in which the parameter
is pre-determined. By transforming a dense graph into
a sparse graph, we only need store the sparse affinity matrix
instead of the original kernel matrix. Nevertheless, we
have to point out that the time complexity of constructing
sparse graph is O(n
2
) for data set with n examples, which
is the efficiency bottleneck of the current implementation.
With the sparse graph, each iteration of the multilevel kernel
k-means costs O(ln

) time, where ln
is
the number of
nonzero entries in the kernel matrix.
4.3
Support Cluster Machines
According to Theorem 4, choosing the centroid of each
cluster as representative will yield the best approximation.
However, the explicit form of (
·) is unknown. We don't
know the exact pre-images of
{m
c
}
k
c=1
, what we can get are
the dot products between the centroids by
m
h
, m
l
=
1
|
h
||
l
|
x
i

h
x
j

l
(x
i
), (x
j
) ,
which requires O(n
2
) costs. Then the pre-computed kernel
SVMs can be used. The pre-computed kernel SVMs takes
the kernel matrix K

as input, and save the indices of support
vectors in the model [15].
To classify the incoming
sample x, we have to calculate the dot product between x
and all the samples in the support clusters, e.g.,
c
(If m
c
is
a support vector, we define the cluster
c
as support cluster.)
x, m
c
=
1
|
c
|
x
i

c
x, x
i
.
We need another O(nm) costs to predict all the training
samples if there are m samples in support clusters. This
is unacceptable for large scale data. To reduce the kernel
reevaluation, we adopt the same method as [3], i.e., selecting
a pseudo-center for each cluster as the representative
u
c
= arg min
x
i

c
(x
i
)
- 1
|
c
|
x
j

c
(x
j
)
2
,
1
x
Positive class
Negative class
2
x
1
x
Positive class
Negative class
2
x
(a)
(b)
Figure 3: (a) Each class is grouped into one cluster,
(b) each class is grouped into two clusters. The solid
mark represents the centroid of the corresponding
class.
The solid lines indicate the support hyperplanes
yielded by SCMs and the dot lines indicate
the true support hyperplanes.
which can be directly obtained by
u
c
= arg max
x
i

c
x
j

c
(x
i
), (x
j
) .
(10)
Thus, the kernel evaluation within training procedure requires
O(
È
k
c=1
|
c
|
2
+ k
2
) time, which be further reduced
by probabilistic speedup proposed by Smola [25]. The kernel
evaluation of predicting the training samples is reduced
from O(nm) to O(ns), where s indicates the number of support
clusters.
4.4
Shrinking Techniques
With the initial SCMs, we can remove the samples that
are not likely support vectors. However, there is no theoretical
guarantee for the security of the shrinking. In Fig. 3,
we give a simple example to show that the shrinking might
not be safe. In the example, if the samples outside the margin
between support hyperplanes are to be removed, the
case (a) will remove the true support vectors while the case
(b) will not. The example shows that the security depends
on whether the hyperplane of SCMs is parallel to the true
separating hyperplane. However, we do not know the direction
of true separating hyperplane before the classification.
Therefore, what we can do is to adopt sufficient initial cluster
numbers so that the solution of SCMs can approximate
the original optimal solution enough. Specifically for large
scale imbalanced data, the samples satisfying the following
condition will be removed from the training set:
| w, (x) + b| &gt; ,
(11)
where  is a predefined parameter.
4.5
The Algorithm
Yu [31] and Boley [3] have adopted different stop criteria
. In Yu et al.'s approach, the algorithm stops when each
cluster has only one sample. Whereas, Boley et al. limit
the maximum iterations by a fixed parameter.
Here, we
propose two novel criteria especially suitable for imbalanced
data. The first one is to stop whenever the ratio of positive
and negative samples is relatively imbalanced. Another
choice is the Neyman-Pearson criterion, that is, minimizing
the total error rate subject to a constraint that the miss rate
of positive class is less than some threshold. Thus, once the
446
miss rate of positive class exceeds some threshold, we stop
the al
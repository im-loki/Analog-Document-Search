Tactons: Structured Tactile Messages for Non-Visual Information Display
Abstract
Tactile displays are now becoming available in a form 
that can be easily used in a user interface. This paper describes
a new form of tactile output. Tactons, or tactile 
icons, are structured, abstract messages that can be used 
to communicate messages non-visually. A range of different
parameters can be used for Tacton construction including
: frequency, amplitude and duration of a tactile 
pulse, plus other parameters such as rhythm and location. 
Tactons have the potential to improve interaction in a 
range of different areas, particularly where the visual display
is overloaded, limited in size or not available, such as 
interfaces for blind people or in mobile and wearable devices
.
.
This paper describes Tactons, the parameters used
to construct them and some possible ways to design them. 
Examples of where Tactons might prove useful in user 
interfaces are given.
Introduction
The area of haptic (touch-based) human computer interaction
(HCI) has grown rapidly over the last few years. A 
range of new applications has become possible now that 
touch can be used as an interaction technique (Wall et al., 
2002). However, most current haptic devices have scant 
provision for tactile stimulation, being primarily pro-grammable
, constrained motion force-feedback devices 
for kinaesthetic display. The cutaneous (skin-based) 
component is ignored even though it is a key part of our 
experience of touch (van Erp, 2002). It is, for example, 
important for recognising texture, and detecting slip, 
compliance and direction of edges. As Tan (1997) says 
"In the general area of human-computer interfaces ... the 
tactual sense is still underutilised compared with vision 
and audition". One reason for this is that, until recently, 
the technology for tactile displays was limited.
Tactile displays are not new but they have not received 
much attention from HCI researchers as they are often 
engineering prototypes or designed for very specific ap
Copyright © 2004, Australian Computer Society, Inc.  This 
paper appeared at the 5th Australasian User Interface Conference
(AUIC2004), Dunedin.  Conferences in Research and Practice
in Information Technology, Vol. 28. A. Cockburn, Ed. Reproduction
for academic, not-for profit purposes permitted provided
this text is included.
plications (Kaczmarek et al., 1991). They have been used 
in areas such as tele-operation or displays for blind people
to provide sensory substitution ­ where one sense is 
used to receive information normally received by another 
(Kaczmarek et al.). Most of the development of these 
devices has taken place in robotics or engineering labs 
and has focused on the challenges inherent in building 
low cost, high-resolution devices with realistic size, 
power and safety performance. Little research has gone 
into how they might actually be used at the user interface. 
Devices are now available that allow the use of tactile 
displays so the time is right to think about how they 
might be used to improve interaction.
In this paper the concept of Tactons, or tactile icons, is 
introduced as a new communication method to complement
graphical and auditory feedback at the user interface
. Tactons are structured, abstract messages that can be 
used to communicate messages non-visually. Conveying 
structured messages through touch will be very useful in 
areas such as wearable computing where screens are limited
. The paper gives some background to the perception  
and use of tactile stimuli and then describes the design of 
Tactons. It finishes with examples of potential uses for 
Tactons.
Background and previous work
The skin is the largest organ in the body, about 2 m
2

in
the average male (Montagu, 1971). Little direct use is 
made of it for displaying information in human-computer 
interfaces (Tan and Pentland, 1997, van Erp, 2002), yet a 
touch on the hand or other parts of the body is a very rich 
experience. The skin can therefore potentially be used as 
a medium to communicate information. As a receiving 
instrument the skin combines important aspects of the eye 
and the ear, with high acuity in both space and time 
(Gunther, 2001) giving it good potential as a communication
medium.
The human sense of touch can be roughly split in to two 
parts: kinaesthetic and cutaneous. "Kinaesthetic" is often 
used as catch-all term to describe the information arising 
from forces and positions sensed by the muscles and 
joints. Force-feedback haptic devices (such as the 
PHANToM from SensAble) are used to present information
to the kinaesthetic sense. Cutaneous perception refers 
to the mechanoreceptors contained within the skin, and 
includes the sensations of vibration, temperature, pain 
and indentation. Tactile devices are used to present feedback
to the cutaneous sense.
15
Current haptic devices use force-feedback to present kinaesthetic
stimuli. This works well for some aspects of 
touch (e.g. identifying the geometric properties of objects
) but is poor for features such as texture (normally 
perceived cutaneously). Oakley et al. (2000) found that 
trying to use texture in a user interface with a force-feedback
device actually reduced user performance. One 
reason for this is that the textures had to be made large so 
that they could be perceived kinaesthetically, but they 
then perturbed users' movements. The use of a tactile 
haptic device to present texture would not have this problem
as small indentations in the fingertip would not affect 
hand movements. At present, however, there are no haptic
devices that do a good job of presenting both tactile 
and force-feedback cues to users.
Current force-feedback devices use a point interaction 
model; the user is represented by a single point of contact 
corresponding to the tip of a stylus. This is analogous to 
exploring the world by remote contact through a stick 
thus depriving the user of the rich, spatially varying cutaneous
cues that arise on the finger pad when contacting a 
real object (Wall and Harwin, 2001). Users must integrate 
temporally varying cues as they traverse the structure of 
virtual objects with the single point of contact, which 
places considerable demands on short-term memory 
(Jansson and Larsson, 2002). Even when exploring simple
geometric primitives, performance is greatly reduced 
compared to natural touch. Lederman and Klatzky (1999) 
have shown that such removal of cutaneous input to the 
fingertip impedes perception of edge direction, which is 
an essential component of understanding haptic objects. It 
can therefore be seen that tactile feedback and cutaneous 
perception are key parts of touch that must be incorporated
into haptic displays if they are to be effective and 
usable.
2.1  Vibrotactile actuators
There are two basic types of vibrotactile display device. 
These evoke tactile sensations using mechanical vibration 
of the skin (usually in the range 10-500Hz) (Kaczmarek 
et al., 1991). This is commonly done by vibrating a small 
plate pressed against the skin or via a pin or array of pins 
on the fingertip. These are very easy to control from standard
PC hardware. Other types of actuator technology  
are available, including pneumatic and electrotactile 
(Stone, 2000), but these tend to be bulkier and harder to 
control so are less useful in many situations.

Figure 1: The pins arrays on the VirTouch tactile
mouse (www.virtouch.com).

The first type of vibrotactile display uses a pin or array of 
small pins (e.g. the VirTouch mouse in Figure 1 or those 
produced by Summers et al. (2001)) to stimulate the fingertip
. Such devices can present very fine cues for surface 
texture, edges, lines, etc. The second type uses larger 
point-contact stimulators (e.g. Figure 2 or alternatively 
small loudspeaker cones playing tones, or other simple 
vibrating actuators placed against the skin as used by Tan 
(1997) and in devices such as the CyberTouch glove ­ 
www.immersion.com). The cues here are much lower 
resolution but can exert more force; they can also be distributed
over the body to allow multiple simultaneous 
cues (often mounted in a vest on the user's back or in a 
belt around the waist). These devices are both easy to 
control and use. For a full review see Kaczmarek et al. 
(1991).

Figure 2: Audiological Engineering Corp. VBW32
transducers (www.tactaid.com).
2.2    Previous work on tactile display
One common form of tactile output is Braille, and dynamic
Braille cells are available. A display is made up of 
a line of `soft' cells (often 40 or 80), each with 6 or 8 pins 
that move up and down to represent the dots of a Braille 
cell. The user can read a line of Braille cells by touching 
the pins of each cell as they pop up (for more information 
see www.tiresias.org). The focus of the work reported 
here is not on Braille as it tends to be used mainly for 
representing text (although other notations are used, e.g. 
music) and the cells are very low resolution (8 pins 
maximum). These displays are also very expensive with 
an 80 cell display costing around £4000. There have been 
many other tactile devices for blind people, such as the 
Optacon (TeleSensory Inc.), which used an array of 144 
pins to display the input from a camera to the fingertip, 
but again these are mainly used for reading text. Pin arrays
produce Braille but can do much more, especially the 
higher resolution displays such as shown in Figure 1.
Our research also builds on the work that has been done 
on tactile graphics for blind people (this mainly takes the 
form of raised lines and dots on special `swell' paper). 
Kurze (1997, 1998) and Challis (2001) have developed 
guidelines which allow images and objects to be presented
that are understandable through touch by blind 
users.
Two other examples show that the cutaneous sense is 
very effective for communication. Firstly, Tadoma is a 
tactile language used by deaf/blind people. The transmitter
speaks normally and the receiver puts a hand on the 
face of the speaker, covering the mouth and neck (Tan 
and Pentland, 2001). Tadoma users can listen at very high
16
speeds (normal speaking speed for experts) and pick up 
subtleties of the speech such as accent. In the second example
, Geldard (1957) taught participants a simple tactile 
language of 45 symbols, using three intensities, three 
durations and five locations on the chest. Participants 
were able to learn the alphabet quickly and could recognise
up to 38 words per minute in some cases. Other sensory
substitution systems convert sound into vibration for 
hearing-impaired people (e.g. the TactAid system from 
Audiological Engineering). Again this shows that cutaneous
perception is very powerful and if we can make use 
of it at the user interfaces we will have a rich new way to 
present information to users.
Research and existing applications have shown that the 
cutaneous sense is a very powerful method of receiving 
information. Other work has shown that it can be used in 
user interfaces and wearable computers (Gemperle et al., 
1998). Tan has begun to investigate the use of tactile displays
on wearable computers (Tan and Pentland, 1997). 
She used a 3x3 grid of stimulators on a user's back to 
provide navigation information. Informal results suggested
it was useful but no formal evaluation has taken 
place. Other relevant work has taken place in aircraft 
cockpits to provide pilots with navigation information 
(van Veen and van Erp, 2001, Rupert, 2000). In these 
examples only simple tactile cues for direction have been 
provided. For example, an actuator maybe vibrated on 
one side of the body to indicate the direction to turn. 
More sophisticated cues could be used to provide much 
more information to users without them needing to use 
their eyes.
Gunther et al. have used tactile cues to present `musical' 
compositions to users (Gunther, 2001, Gunther et al., 
2002). They say: "The approach taken ... views haptic 
technologies ­ in particular the vibrotactile stimulator ­ 
as independent output devices to be used in conjunction 
with the composition and perception of music. Vibrotactile
stimuli are viewed not as signals carrying information 
per se, but as aesthetic artifacts themselves".  He used an 
array of 13 transducers across the body of a `listener' so 
that he/she could experience the combined sonic/tactile 
presentation. Gunther created a series of compositions 
played to listeners who appeared to enjoy them. This 
work was artistic in nature so no formal usability assessments
were made but the listeners all liked the experience
.
In order to create a tactile composition (the same is true 
for the Tactons described below) a good understanding of 
the experience of touch is needed. However, as Gunther 
et al. suggest: "It is indeed premature to hammer out the 
details of a language for tactile composition. It seems 
more productive at this point in time to identify the underpinnings
of such a language, specifically those dimensions
of tactile stimuli that can be manipulated to form 
the basic vocabulary elements of a compositional lan-guage"
. Research is needed to gain a more systematic 
understanding of cutaneous perception for use in the 
presentation of such messages.
Enriquez and MacLean (2003) recently proposed `haptic 
icons', which they define as "brief programmed forces 
applied to a user through a haptic interface, with the role
of communicating a simple idea in a manner similar to 
visual or auditory icons". The problem they are trying to 
address is different to that of Tactons, as they say "With 
the introduction of "active" haptic interfaces, a single 
handle ­ e.g. a knob or a joystick ­ can control several 
different and perhaps unrelated functions. These multi-function
controllers can no longer be differentiated from 
one another by position, shape or texture... Active haptic 
icons, or "hapticons", may be able to solve this problem 
by rendering haptically distinct and meaningful sensations
for the different functions". These use one degree-of
-freedom force-feedback devices, rather than tactile 
displays, so encode information very differently to Tactons
. They report the construction of a tool to allow a user 
to create and edit haptic icons. This is early work and 
they do not report results from the use of hapticons in any 
interfaces.  Their results, however, will be directly relevant
to Tactons.
Tactons
Given that the cutaneous sense is rich and a powerful 
communication medium currently little utilised in HCI, 
how can we make effective use of it? One approach is to 
use it to render objects from the real world more realisti-cally
in virtual environments, for example in improving 
the presentation of texture in haptic devices. It could also 
be used to improve targeting in desktop interactions along 
the lines suggested by Oakley et al. (2000). In this paper 
it is suggested that it can additionally be used to present 
structured informational messages to users.
Tactons are structured, abstract messages that can be used 
to communicate complex concepts to users non-visually. 
Shneiderman (1998) defines an icon as "an image, picture 
or symbol representing a concept". Tactons can represent 
complex interface concepts, objects and actions very con-cisely
.  Visual icons and their auditory equivalent earcons 
(Blattner et al., 1989, Brewster et al., 1994) are very 
powerful ways of displaying information but there is currently
no tactile equivalent. In the visual domain there is 
text and its counterpart the icon, the same is true in sound 
with synthetic speech and the earcon. In the tactile domain
there is Braille but it has no `iconic' counterpart. 
Tactons fill this gap. Icons/Earcons/Tactons form a simple
, efficient language to represent concepts at the user 
interface.
Tactons are similar to Braille in the same way that visual 
icons are similar to text, or earcons are similar to synthetic
speech. For example, visual icons can convey complex
information in a very small amount of screen space, 
much smaller than for a textual description. Earcons convey
information in a small amount of time as compared to 
synthetic speech. Tactons can convey information in a 
smaller amount of space and time than Braille. Research 
will also show which form of iconic display is most suitable
for which type of information. Visual icons are good 
for spatial information, earcons for temporal. One property
of Tactons is that they operate both spatially and 
temporally so they can complement both icons and earcons
. Further research is needed to understand how these 
different types of feedback work together.
17
Using speech as an example from the auditory domain: 
presenting information in speech is slow because of its 
serial nature; to assimilate information the user must hear 
a spoken message from beginning to end and many words 
may have to be comprehended before the message can be 
understood. With earcons the messages are shorter and 
therefore more rapidly heard, speeding up interactions. 
The same is true of Tactons when compared to Braille. 
Speech suffers from many of the same problems as 
graphical text in text-based computer systems, as this is 
also a serial medium. Barker & Manji (1989) claim that 
an important limitation of text is its lack of expressive 
capability: It may take many words to describe a fairly 
simple concept. Graphical iconic displays were introduced
that speeded up interactions as users could see a 
picture of the thing they wanted instead of having to read 
its name from a list (Barker and Manji, 1989). In the 
same way, an encoded tactile message may be able to 
communicate its information in fewer symbols. The user 
feels the Tacton then recalls its meaning rather than having
the meaning described in Braille (or speech or text). 
The icon is also (in principle) universal: it means the 
same thing in different languages and the Tacton would 
have similar universality.
Designing with Tactons
Tactons are created by encoding information using the 
parameters of cutaneous perception. The encoding is 
similar to that of earcons in sound (Blattner et al., 1989, 
Brewster et al., 1994) where each of the musical parameters
(e.g. timbre, frequency, amplitude) is varied to encode
information. Similar parameters can be used for 
Tactons (although their relative importance is different). 
As suggested by Blattner, short motifs could be used to 
represent simple objects or actions and these can then be 
combined in different ways to represent more complex 
messages and concepts. As Tactons are abstract the mapping
between the Tacton and what it represents must be 
learned, but work on earcons has shown that learning can 
take place quickly (Brewster, 1998b).
The properties that can be manipulated for Tactons are 
similar to those used in the creation of earcons. The parameters
for manipulation also vary depending on the 
type of transducer used; not all transducers allow all types 
of parameters. The general basic parameters are:
Frequency: A range of frequencies can be used to differentiate
Tactons. The range of 20 ­ 1000 Hz is perceivable 
but maximum sensitivity occurs around 250 Hz (Gunther 
et al., 2002). The number of discrete values that can be 
differentiated is not well understood, but Gill (2003) suggests
that a maximum of nine different levels can be used. 
As in audition, a change in amplitude leads to a change in 
the perception of frequency so this has an impact on the 
use of frequency as a cue. The number of levels of frequency
that can be discriminated also depends on whether 
the cues are presented in a relative or absolute way. Making
relative comparisons between stimuli is much easier 
than absolute identification, which will lead to much 
fewer discriminable values, as shown in the work on earcon
design (Brewster et al., 1994).
Amplitude: Intensity of stimulation can be used to encode 
values to present information to the user. Gunther (2002) 
reports that the intensity range extends to 55 dB above the 
threshold of detection; above this pain occurs. Craig and 
Sherrick (1982) indicate that perception deteriorates 
above 28 dB so this would seem to be a useful maximum. 
Gunther (2001) reports that various values, ranging from 
0.4dB to 3.2dB, have been reported for the just noticeable 
difference (JND) value for intensity. Gill states that that 
no more than four different intensities should be used 
(Gill, 2003). Again the number of useful discriminable 
values will depend on absolute or relative presentation of 
stimuli. Due to the interactions between this and frequency
several researchers have suggested that they be 
combined into a single parameter to simplify design
Waveform: The perception of wave shape is much more 
limited than with the perception of timbre in sound. Users 
can differentiate sine waves and square waves but more 
subtle differences are more difficult (Gunther, 2001). 
This limits the number of different values that can be encoded
and makes this a much less important variable than 
it is in earcon design (where it is one of the key variables
).
Duration: Pulses of different durations can encode information
. Gunther (2001) investigated a range of subjective 
responses to pulses of different durations. He found that 
stimuli lasting less than 0.1 seconds were perceived as 
taps or jabs whereas stimuli of longer duration, when 
combined with gradual attacks and decays, may be perceived
as smoothly flowing tactile phrases. He suggests 
combining duration with alterations in the envelope of a 
vibration, e.g. an abrupt attack feels like a tap against the 
skin, a gradual attack feels like something rising up out of 
the skin.
Rhythm: Building on from duration, groups of pulses of 
different durations can be composed into rhythmic units. 
This is a very powerful cue in both sound and touch. 
Gunther (2001) suggests that differences in duration can 
be used to group events when multiple events occur on 
the same area of skin.
Specific transducer types allow other parameters to be 
used:
Body location: Spatially distributed transducers can encode
information in the position of stimulation across the 
body. The choice of body location for vibrotactile display 
is important, as different locations have different levels of 
sensitivity and spatial acuity. A display may make use of 
several body locations, so that the location can be used as 
another parameter, or can be used to group tactile stimuli.
The fingers are often used for vibrotactile displays because
of their high sensitivity to small amplitudes and 
their high spatial acuity (Craig and Sherrick, 1982). However
, the fingers are often required for other tasks, so 
other body locations may be more suitable. Craig and 
Sherrick suggest the back, thigh and abdomen as other 
suitable body locations. They report that, once subjects 
have been trained in vibrotactile pattern recognition on 
the back, they can almost immediately recognise the same 
patterns when they are presented to the thigh or abdomen. 
This transfer also occurs to some extent when patterns are
18
presented to different fingers after training on one finger, 
but is not so immediate.
Certain body locations are particularly suitable, or particularly
unsuitable, for certain types of vibrotactile displays
. For example, transducers should not be placed on 
or near the head, as this can cause leakage of vibrations 
into the ears, resulting in unwanted sounds (Gunther et 
al., 2002). An example of a suitable body location is in 
Gunther's Skinscape display, where he positions low frequency
transducers on the torso as this is where low frequencies
are felt when loud music is heard.
The method of attaching the transducers to a user's body 
is also important. The pressure of the transducer against 
the body has a significant effect on the user's perception 
of the vibrations. Transducers should rest lightly on the 
skin, allowing the user to feel the vibration against the 
skin, and to isolate the location of the vibration with ease. 
Exerting too much pressure with the transducer against 
the user's body will cause the vibrations to be felt in the 
bone structure, making them less isolated due to skeletal 
conduction. In addition, tightening the straps holding the 
transducer to achieve this level of pressure may impede 
circulation (Gunther, 2001).
Rupert (2000) suggests using the full torso for displaying 
3D information, with 128 transducers distributed over the 
body. His system displays information to pilots about the 
location of objects around them in 3D space, by stimulating
the transducers at the part of their body corresponding 
to the location of the object in 3D space around them. 
This could be used to indicate horizons, borders, targets, 
or other aircraft.
Spatiotemporal patterns: Related to position and rhythm, 
spatial patterns can also be "drawn" on the user's body. 
For example, if a user has a 3x3 array of stimulators lo-cated
on his/her back, lines and geometric shapes can be 
"drawn" on the back, by stimulating, in turn, the stimulators
that make up that shape. In Figure 3, an `L' shaped 
gesture can be drawn by activating the stimulators: 1-4-78
-9 in turn. Patterns can move about the body, varying in 
time and location to encode information. Cholewiak 
(1996) and Sherrick (1985) have also looked at low-level 
perception of distributed tactile cues.
.

Figure 3: "Drawing" an L-shaped gesture.
Now that the basic parameters for Tactons have been described
, we will give some examples of how they might 
be designed to convey information. The fundamental design
of Tactons is similar to that of earcons.
4.1  Compound Tactons
A simple set of Tactons could be created as in Figure 4. A 
high-frequency pulse that increases in intensity could 
represent `Create', a lower frequency pulse that decreases 
in intensity could represent `Delete'. A two note falling 
Tacton could represent a file and a two rising notes a 
folder. The mapping is abstract; there is no intuitive link 
between what the user feels and what it represents.


Create
Delete
File
Folder
Create File
Delete Folder

Figure 4: Compound Tactons (after Blattner et al.,
1989).
These Tactons can then be combined to create compound 
messages. For example, `create file' or `delete folder'. 
The set of basic elements could be extended and a simple 
language of tactile elements created to provide feedback 
in a user interface.
4.2  Hierarchical Tactons
Tactons could also be combined in a hierarchical way, as 
shown in Figure 5. Each Tacton is a node in a tree and 
inherits properties from the levels above it. Figure 5 
shows a hierarchy of Tactons representing a hypothetical 
family of errors. The top of the tree is a family Tacton 
which has a basic rhythm played using a sinewave (a different
family of errors would use a different rhythm so 
that they are not confused). The rhythmic structure of 
Level 2 inherits the Tacton from Level 1 and adds to it. In 
this case a second, higher frequency Tacton played with a 
squarewave. At Level 3 the tempo of the two Tactons is 
changed. In this way a hierarchical structure can be presented
. The other parameters discussed above could be 
used to add further levels.
4.3  Transformational Tactons
A third type of Tacton is the Transformational Tacton. 
These have several properties, each represented by a different
tactile parameter. For example, if Transformational 
Tactons were used to represent files in a computer interface
, the file type could be represented by rhythm, size by 
frequency, and creation date by body location. Each file 
type would be mapped to a unique rhythm. Therefore, 
two files of the same type, and same size, but different 
creation date would share the same rhythm and frequency
, but would be presented to a different body location
. If two files were of different types but the same size 
they would be represented by different rhythms with the 
same frequency.
19
Uses for Tactons
We are interested in three areas of use for Tactons, although
there are many others where they have potential to 
improve usability.
5.1  Enhancements of desktop interfaces
The first, and simplest, area of interest is in the addition 
of Tactons to desktop graphical interfaces. The addition 
of earcons to desktops has shown many advantages in 
terms of reduced errors, reduced times to complete tasks 
and lowered workload (Brewster, 1998a). One problem 
with audio is that users believe that it may be annoying to 
use (although no research has actually shown this to be 
the case) and it has the potential to annoy others nearby 
(for a discussion see (Brewster, 2002)). The addition of 
Tactons to widgets has the same potential to indicate usability
problems but without the potential to annoy.
One reason for enhancing standard desktop interfaces is 
that users can become overloaded with visual information 
on large, high-resolution displays. In highly complex 
graphical displays users must concentrate on one part of 
the display to perceive the visual feedback, so that feedback
from another part may be missed. This becomes 
very important in situations where users must notice and 
deal with large amounts of dynamic data or output from 
multiple applications or tasks. If information about secondary
tasks was presented through touch then users 
could concentrate their visual attention on the primary 
one but feel information about the others.
As a simple example, the display of a progress bar widget 
could be presented tactually. Two sets of tactile pulses 
could be used to indicate the current and end points of a 
download. The time between the two pulses would indicate
the amount of time remaining, the closer the two 
pulses the nearer the download is to finishing. The two 
pulses could use different waveforms to ensure they were 
not confused. Different rhythms for each pulse could be 
used to indicate different types of downloads. If a more 
sophisticated set of transducers on a belt around the waist 
was available then the position of a pulse moving around 
the body in a clockwise direction (starting from the front) 
would give information about progress: when the pulse 
was at the right side of the body the download would be 
25% of the way through, when it was on the left hand 
side 75%, and when it got back around to the front it 
would be finished. There would be no need for any visual 
presentation of the progress bar, allowing users to focus 
their visual attention on the main task they are involved 
with.
Tactons could also be used to enhance interactions with 
buttons, scrollbars, menus, etc. to indicate when users are 
on targets and when certain types of errors occur. Others 
have shown that basic tactile feedback can improve  
pointing and steering type interactions (Akamatsu et al., 
1995, Campbell et al., 1999). There are some commercial 
systems that give simple tactile feedback in desktop user 
interfaces, e.g. the software that comes with the Logitech 
iFeel mouse (www.logitech.com). This provides basic 
targeting: a brief pulse is played, for example, when a 
user moves over a target. We believe there is much more 
that can be presented with tactile feedback.
5.2  Visually impaired users
Tactons will be able to work alongside Braille in tactile 
displays for blind and visually impaired users, in the same 
way as earcons work alongside synthetic speech. They 
will allow information to be delivered more efficiently. In 
addition, hierarchical Tactons could help users navigate
Sine
Sine
Square
Error
Operating system error
Execution error
Sine
Square
Overflow
Sine
Square
Underflow
Sine
Square
Fast tempo
Slow tempo
Figure 5: Hierarchical Tacton composition.
Level 1
Level 2
Level 3
20
around Braille media by providing navigation information 
(Brewster, 1998b).
One of our main interests is in using Tactons to improve 
access to graphical information non-visually. Text can be 
rendered in a relatively straightforward manner by speech 
or Braille, but graphics are more problematic. One area 
that we and others have focused on is visualisation for 
blind people. Understanding and manipulating information
using visualisations such as graphs, tables, bar charts 
and 3D plots is very common for sighted people. The 
skills needed are learned early in school and then used 
throughout life, for example, in analysing information or 
managing home finances. The basic skills needed for creating
and manipulating graphs are necessary for all parts 
of education and employment. Blind people have very 
restricted access to information pre
Energy Management Schemes for Memory-Resident Database Systems
ABSTRACT
With the tremendous growth of system memories, memory-resident
databases are increasingly becoming important in various
domains. Newer memories provide a structured way of storing
data in multiple chips, with each chip having a bank of memory
modules. Current memory-resident databases are yet to take full
advantage of the banked storage system, which offers a lot of
room for performance and energy optimizations. In this paper,
we identify the implications of a banked memory environment
in supporting memory-resident databases, and propose hardware
(memory-directed) and software (query-directed) schemes
to reduce the energy consumption of queries executed on these
databases. Our results show that high-level query-directed schemes
(hosted in the query optimizer) better utilize the low-power modes
in reducing the energy consumption than the respective hardware
schemes (hosted in the memory controller), due to their complete
knowledge of query access patterns. We extend this further and
propose a query restructuring scheme and a multi-query optimization
. Queries are restructured and regrouped based on their
table access patterns to maximize the likelihood that data accesses
are clustered. This helps increase the inter-access idle times of
memory modules, which in turn enables a more effective control of
their energy behavior. This heuristic is eventually integrated with
our hardware optimizations to achieve maximum savings. Our
experimental results show that the memory energy reduces by 90%
if query restructuring method is applied along with basic energy
optimizations over the unoptimized version.
The system-wide
performance impact of each scheme is also studied simultaneously.
Categories and Subject Descriptors
H.2.2 [Database Management]:Physical Design ­ Access Methods
H.3.2 [Information Storage and Retrieval]: Information Storage
B.3.1 [Memory Structures]: Semiconductor Memories ­ DRAM
General Terms:
Design, Performance

INTRODUCTION
Memory-resident databases (also called in-memory databases
<A href="81.html#10">[6]) are emerging to be more significant due to the current era of
memory-intensive computing. These databases are used in a wide
range of systems ranging from real-time trading applications to IP
routing. With the growing complexities of embedded systems (like
real-time constraints), use of a commercially developed structured
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKM'04, November 8­13, 2004, Washington, DC, USA.
Copyright 2004 ACM 1-58113-874-1/04/0011 ...
$
5.00.
memory database is becoming very critical <A href="81.html#10">[5].
Consequently,
device developers are turning to commercial databases, but existing
embedded DBMS software has not provided the ideal fit.
Embedded databases emerged well over a decade ago to support
business systems, with features including complex caching logic
and abnormal termination recovery. But on a device, within a
set-top box or next-generation fax machine, for example, these
abilities are often unnecessary and cause the application to exceed
available memory and CPU resources. In addition, current
in-memory database support does not consider embedded system
specific issues such as energy consumption.
Memory technology has grown tremendously over the years,
providing larger data storage space at a cheaper cost.
Recent
memory designs have more structured and partitioned layouts
in the form of multiple chips, each having memory banks <A href="81.html#10">[30].
Banked memories are energy efficient by design, as per-access
energy consumption decreases with decreasing memory size (and
a memory bank is typically much smaller compared to a large
monolithic memory). In addition, these memory systems provide
low-power operating modes, which can be used for reducing the
energy consumption of a bank when it is not being used.
An
important question regarding the use of these low-power modes
is when to transition to one once an idleness is detected. Another
important question is whether the application can be modified to
take better advantage of these low-power modes.
While these
questions are slowly being addressed in architecture, compiler, and
OS communities, to our knowledge, there has been no prior work
that examines the energy and performance behavior of databases
under a banked memory architecture. Considering increasingly
widespread use of banked memories, such a study can provide
us with valuable information regarding the behavior of databases
under these memories and potential modifications to DBMSs
for energy efficiency. Since such banked systems are also being
employed in high-end server systems, banked memory friendly
database strategies can also be useful in high-end environments to
help reduce energy consumption.
Our detailed energy characterization of a banked memory architecture
that runs a memory-resident DBMS showed that nearly
59% of overall energy (excluding input/output devices) in a typical
query execution is spent in the memory, making this component
an important target for optimization (see Figure <A href="81.html#2">1). Moreover, for
any system, memory power and energy consumption have become
critical design parameters besides cost and performance. Based on
these observations, this paper evaluates the potential energy benefits
that memory-resident database queries can achieve by making
use of banked memory architectures supported with low-power operating
modes. Since each memory bank is capable of operating
independently, this opens up abundant avenues for energy and performance
optimizations.
In this paper, we focus on a banked memory architecture and
study potential energy benefits when database queries are executed.
Specifically, we focus on two important aspects of the problem:
· Characterizing energy benefits of banked memories using hardware
and software techniques: To see whether query execution can
make use of available low-power modes, we study both hardware
and software techniques. The hardware techniques detect the idleness
of memory banks and switch the inactive (idle) banks (during
218
Memory
59%
Cache
16%
ALU
14%
Bus
1%
Others
10%
Figure 1: Breakup of the energy consumption for various system
components. The results are based on the average energy
consumption of TPC-H benchmarks <A href="81.html#10">[35] executed on a
memory-resident DBMS.
query execution) to low-power operating modes. We also present
a query-based memory energy optimization strategy, wherein the
query plan is augmented by explicit bank turn-off/on instructions
that transition memory banks into appropriate operating modes during
the course of execution based on the query access pattern. We
experimentally evaluate all the proposed schemes and obtain energy
consumptions using an energy simulator. Our experiments
using TPC-H queries <A href="81.html#10">[35] and a set of queries suitable for handheld
devices clearly indicate that both hardware-based and query-directed
strategies save significant memory energy.
· Query restructuring for memory energy savings: We propose a
query restructuring scheme and a multi-query optimization strategy
to further increase energy benefits coming from using low-power
operating modes. The idea behind these schemes is to increase
bank inter-access times so that more aggressive low-power modes
can be employed and a memory bank can stay in a low-power mode
longer once it is transitioned. Our experimental evaluation indicates
that this query restructuring strategy does not only reduce
energy consumption, but also helps improve overall performance
(execution cycles).
Apart from providing useful input for database designers, our results
can also be used by hardware designers to tune the behavior
of low-power modes so that they handle query access patterns better
. Similar to the observation that creating a lightweight version
of a disk-based database will not serve as a suitable in-memory
database, our belief is that taking an in-memory database system
and using it on a banked architecture without any modification may
not generate the desired results. Therefore, the results presented in
this work also shed light on how database design and memory architecture
design interact with each other.
The remainder of this paper is organized as follows. Section <A href="81.html#2">2
presents related work. Section <A href="81.html#2">3 elaborates on the memory database
that we built and also on the memory banking scheme that we employ
for our experiments. Section <A href="81.html#3">4 presents in detail the proposed
hardware and query-directed energy optimization techniques. The
results of our energy evaluation of these schemes are discussed in
Section <A href="81.html#5">5. Our experiments also account for the performance overhead
incurred in supporting our schemes. Section <A href="81.html#7">6 presents our
query restructuring and regrouping scheme, and Section <A href="81.html#8">7 discusses
its energy/performance benefits within the context of our banked
memory architecture. Finally, Section <A href="81.html#10">8 summarizes the results.
RELATED WORK
In the past, memory has been redesigned, tuned or optimized
to suit emerging fields. Need for customized memory structures
and allocation strategies form the foundation for such studies.
Copeland et al proposed SafeRAM <A href="81.html#10">[11], a modified DRAM model
for safely supporting memory-resident databases alike disk-based
systems, and for achieving good performance. In PicoDBMS <A href="81.html#10">[27],
Pucheral et al present techniques for scaling down a database to
a smart card. This work also investigates some of the constraints
involved in mapping a database to an embedded system, especially
memory constraints and the need for a structured data layout.
Anciaux et al <A href="81.html#10">[3] explicitly model the lower bound of the memory
space that is needed for query execution. Their work focuses on
light weight devices like personal organizers, sensor networks, and
mobile computers. Boncz et al show how memory accesses form a
major bottleneck during database accesses <A href="81.html#10">[7]. In their work, they
also suggest a few remedies to alleviate the memory bottleneck.
An et al analyze the energy behavior of mobile devices when spatial
access methods are used for retrieving memory-resident data
<A href="81.html#10">[2]. They use a cycle accurate simulator to identify the pros and
text
Query Optimizer
Query Execution
Engine
Memory
Database
Queries
Data
Results
Energy & Performance
Optimizations
(Using Cost Plan)
Hardware
Optimizations
Targeting DBMS
Rewrite System
Parser
System Catalog
Figure 2: DBMS architecture.
cons of various indexing schemes. In <A href="81.html#10">[1], Alonso et al investigate
the possibility of increasing the effective battery life of mobile
computers by selecting energy efficient query plans through
the optimizer. Although the ultimate goal seems the same, their
cost plan and the optimization criterion are entirely different from
our scheme. Specifically, their emphasis is on a client-server model
optimizing the network throughput and overall energy consumption
. Gruenwald et al propose an energy-efficient transaction management
system for real-time mobile databases in ad-hoc networks
<A href="81.html#10">[16]. They consider an environment of mobile hosts. In <A href="81.html#10">[22],
Madden et al propose TinyDB, an acquisitional query processor
for sensor networks. They provide SQL-like extensions to sensor
networks, and also propose acquisitional techniques that reduce the
power consumption of these networks. It should be noted that the
queries in such a mobile ad-hoc network or a sensor environment
is different from those in a typical DBMS. This has been shown
by Imielinksi et al in <A href="81.html#10">[19]. In our model, we base our techniques
on a generic banked memory environment and support complex,
memory-intensive typical database operations. There are more opportunities
for energy optimizations in generic memory databases,
which have not yet been studied completely. The approach proposed
in this paper is different from prior energy-aware database
related studies, as we focus on a banked memory architecture, and
use low-power operating modes to save energy.
Gassner et al review some of the key query optimization techniques
required by industrial-strength commercial query optimizers
, using the DB2 family of relational database products as examples
<A href="81.html#10">[15]. This paper provides insight into design of query cost
plans and optimization using various approaches. In <A href="81.html#10">[23], Manegold
studies the performance bottlenecks at the memory hierarchy
level and proposes a detailed cost plan for memory-resident
databases. Our cost plan and optimizer mimics the PostgreSQL
model <A href="81.html#10">[12, 14]. We chose it due to its simple cost models and open
source availability.
A query restructuring algorithm is proposed by Hellerstein
in <A href="81.html#10">[18].
This algorithm uses predicate migration to optimize
expensive data retrievals.
In <A href="81.html#10">[10], Chaudhuri et al extend this
approach to study user-defined predicates and also guarantee an
optimal solution for the migration process. Sarawagi et al present
a query restructuring algorithm that reduces the access times of
data retrieval from tertiary databases <A href="81.html#10">[32]. Monma et al develop
the series-parallel algorithm for reordering primitive database operations
<A href="81.html#10">[24]. This algorithm optimizes an arbitrarily constrained
stream of primitive operations by isolating independent modules.
This work forms the basic motivation for our query restructuring
algorithm. However, our paper is different from all of the above
work in the sense that we reorder queries for reducing energy
consumption. Moreover, our database is memory-resident, with
the presence of banked memory that gives more freedom for
optimizations.
SYSTEM ARCHITECTURE
For our work, we modified the PostgreSQL DMBS to work with
memory-resident data sets as its workload. The block diagram for
our setup is shown in Figure <A href="81.html#2">2. The core components are derived
from PostgreSQL. The flow of our model is similar to PostgreSQL
except that the database is memory resident. A query is parsed for
syntax and then sent to the rewrite system. The rewrite system uses
the system catalog to generate the query tree, which is then sent to
the optimizer. The query optimizer derives the cost of the query in
219
Configuration
Registers
Self-Monitoring/
Prediction
Hardware
Memory
Controller
Bank
To/From
CPU
Module
Memory Bus
Figure 3: Banked memory architecture.
multiple ways using the query tree and issues the best suited plan
to the query execution engine. We incorporate our software-based
techniques at the optimizer stage of the DBMS. These optimizations
are based on the cost that is derived for each of the query plans
(the discussion pertaining to the modified cost model is deferred till
Section <A href="81.html#3">4). Based on the final query execution plan, the execution
engine executes the query by using the database. The database is
entirely memory resident and the memory is organized in a banked
format (elaborated in the following section). The executor recur-sively
iterates the query plan and uses a per-tuple based strategy
(pipelined execution, and not bulk processing) to project the output
results. The proposed hardware optimizations are at the computer
architecture level of the system. Since the base DBMS model is
similar to PostgreSQL, we do not elaborate each component in detail
( <A href="81.html#10">[26] provides an elaborate discussion). Instead, we highlight
our contributions, and modifications to DBMS (shown in blue in
Figure <A href="81.html#2">2) in the following sections. Overall, our strategies require
modification to the query optimizer, memory hardware, and system
software components.
3.2
Memory Model
We use a memory system that contains a memory array organized
as banks (rows) and modules (columns), as is shown picto-rially
in Figure <A href="81.html#3">3 for a 4 × 4 memory module array. Such banked
systems are already being used in high-end server systems <A href="81.html#10">[30] as
well as low-end embedded systems <A href="81.html#10">[31]. The proposed optimizations
will, however, apply to most bank-organized memory systems
. Accessing a word of data would require activating the corresponding
modules of the shown architecture. Such an organization
allows one to put the unused banks into a low-power operating
mode. To keep the issue tractable, this paper bases the experimental
results on a sequential database environment and does not consider
a multiprocessing environment (like transaction processing which
requires highly complex properties to be satisfied). We assume in
our experiments that there is just one module in a bank; hence, in
the rest of our discussion, we use the terms "bank" and "module"
interchangeably.
3.3
Operating Modes
We assume the existence of five operating modes for a memory
module: active, standby, nap, power-down, and disabled
1
.
Each mode is characterized by its energy consumption and the
time that it takes to transition back to the active mode (termed
resynchronization time or resynchronization cost). Typically, the
lower the energy consumption, the higher the resynchronization
time <A href="81.html#10">[30]. Figure <A href="81.html#3">4 shows possible transitions between the various
low-power modes (the dynamic energy
2
consumed in a cycle is
given for each node) in our model. The resynchronization times
in cycles (based on a cycle time of 3.3ns) are shown along the
arrows (we assume a negligible cost

for transitioning to a lower
power mode). The energy and resynchronization values shown
in this figure have been obtained from the RDRAM memory data
sheet (512MB, 2.5V, 3.3ns cycle time, 8MB modules) <A href="81.html#10">[30]. When
a module in standby, nap, or power-down mode is requested to
perform a memory transaction, it first goes to the active mode, and
1
Current DRAMs <A href="81.html#10">[30] support up to six energy modes of operation
with a few of them supporting only two modes. One may choose to
vary the number of modes based on the target memory.
2
We exclusively concentrate on dynamic power consumption that
arises due to bit switching, and do not consider the static (leakage)
power consumption <A href="81.html#10">[28] in this paper.
Full Power
(2.063 nJ)
Standby
(0.743 nJ)
Nap
(0.035 nJ)
Power
Down
(0.025 nJ)
Disabled
(0 nJ)
1
16
9000







Figure 4: Available operating modes and their resynchronization
costs.
then performs the requested transaction. While one could employ
all possible transitions given in Figure <A href="81.html#3">4 (and maybe more), our
query-directed approach only utilizes the transitions shown by
solid arrows. The runtime (hardware-based) approaches, on the
other hand, can exploit two additional transitions: from standby to
nap, and from nap to power-down.
3.4
System Support for Power Mode Setting
Typically, several of the memory modules (that are shown in Figure
<A href="81.html#3">3) are controlled by a memory controller which interfaces with
the memory bus. For example, the operating mode setting could
be done by programming a specific control register in each memory
module (as in RDRAM <A href="81.html#10">[30]). Next is the issue of how the
memory controller can be told to transition the operating modes
of the individual modules. This is explored in two ways in this
paper: hardware-directed approach and software-directed (query-directed
) approach.
In the hardware-directed approach, there is a Self-Monitoring
and Prediction Hardware block (as shown in Figure <A href="81.html#3">3), which
monitors all ongoing memory transactions. It contains some prediction
hardware (based on the hardware scheme) to estimate the
time until the next access to a memory bank and circuitry to ask the
memory controller to initiate mode transitions (limited amount of
such self-monitored power down is already present in current memory
controllers, for example: Intel 82443BX and Intel 820 Chip
Sets).
In the query-directed approach, the DBMS explicitly requests
the memory controller to issue the control signals for a specific
module's mode transitions. We assume the availability of a set of
configuration registers in the memory controller (see Figure
<A href="81.html#3">3) that are mapped into the address space of the CPU (similar
to the registers in the memory controller in <A href="81.html#10">[20]). These registers
are then made available to the user space (so that the DBMS application
can have a control) through operating system calls.
Regardless of which strategy is used, the main objective of employing
such strategies is to reduce the energy consumption of a
query when some memory banks are idle during the query's execution
. That is, a typical query only accesses a small set of tables
, which corresponds to a small number of banks. The remaining
memory banks can be placed into a low-power operating mode
to save memory energy. However, it is also important to select
the low-power mode to use carefully (when a bank idleness is detected
), as switching to a wrong mode either incurs significant performance
penalties (due to large resynchronization costs) or prevents
us from obtaining maximum potential energy benefits.
Note that energy optimization is our context can be performed
from two angles. First, suitable use of low-power operating modes
can reduce energy consumption of a given query execution. Second
, the query plan can be changed (if it is possible to do so) to further
increase energy benefits. In this work, we explore both these
angles.
POWER MANAGEMENT SCHEMES
In a banked architecture, the memory can be managed through
either of the following two approaches: (1) a runtime approach
wherein the hardware is in full control of operating mode transitions
; and (2) a query-directed scheme wherein explicit bank turn-on/off
instructions are inserted in the query execution plan to invoke
mode transitions. One also has the option of using both the
approaches simultaneously (which we illustrate in later sections).
220
Full Power
Standby
Nap
Power
Down
idle
stanby
idle
nap
idle
down
resynch
stanby
resynch
nap
resynch
down
Figure 5: Dynamic threshold scheme.
4.1
Hardware-Directed Schemes
We explore two hardware-directed approaches that allow the
memory system to automatically transition the idle banks to an
energy conserving state. The problem then is to detect/predict
bank idleness and transition idle banks into appropriate low-power
modes.
4.1.1
Static Standby Scheme
The first approach is a per-access optimization. Most of the recent
DRAMs allow the chips to be put to standby mode immediately
after each reference <A href="81.html#10">[30]. After a read/write access, the memory
module that gets accessed can be placed into the standby mode
in the following cycle. We refer to this scheme as the static standby
mode in the rest of our discussion. Note that, while this scheme is
not very difficult to implement, it may lead to frequent resynchro-nizations
, which can be very harmful as far as execution cycles are
concerned.
4.1.2
Dynamic Threshold Scheme
Our second hardware-guided approach is based on runtime dynamics
of the memory subsystem. The rationale behind this approach
is that if a memory module has not been accessed in a while,
then it is not likely to be needed in the near future (that is, inter-access
times are predicted to be long). A threshold is used to determine
the idleness of a module after which it is transitioned to a
low-power mode. More specifically, we propose a scheme where
each memory module is put into a low-power state with its idle
cycles as the threshold for transition.
The schematic of our dynamic threshold scheme is depicted in
Figure <A href="81.html#4">5. After idle
stndby
cycles of idleness, the corresponding
module is put in the standby mode. Subsequently, if the module
is not referenced for another idle
nap
cycles, it is transitioned to the
nap mode. Finally, if the module is not referenced for a further
idle
down
cycles, it is placed into the power-down mode. Whenever
the module is referenced, it is brought back into the active mode incurring
the corresponding resynchronization costs (based on what
low-power mode it was in). It should be noted that even if a single
bank experiences a resynchronization cost, the other banks will also
incur the corresponding delay (to ensure correct execution). Implementing
the dynamic mechanism requires a set of counters (one for
each bank) that are decremented at each cycle, and set to a threshold
value whenever they expire or the module is accessed. A zero
detector for a counter initiates the memory controller to transmit
the instructions for mode transition to the memory modules.
4.2
Software-Directed Scheme
It is to be noted that a hardware-directed scheme works well
independent of the DBMS and the query optimizer used. This is
because the idleness predictors are attached to the memory banks
and monitor idleness from the perspective of banks. In contrast,
a query-directed scheme gives the task of enforcing mode transitions
to the query. This is possible because the query optimizer,
once it generates the execution plan, has a complete information
about the query access patterns (i.e., which tables will be accessed
and in what order, etc). Consequently, if the optimizer also knows
the table-to-bank mappings, it can have a very good idea about the
bank access patterns. Then, using this information, it can proac-tively
transition memory banks to different modes. In this section,
we elaborate on each step in the particular query-directed approach
that we implemented, which includes customized bank allocation,
query analysis, and insertion of bank turn-on/off (for explicit power
mode control) instructions.
4.2.1
Bank Allocation
In the case of software-directed scheme, the table allocation is
handled by the DBMS. Specifically, the DBMS allocates the newly-created
tables to the banks, and keeps track of the table-to-bank
mappings. When a "create table" operation is issued, the DBMS
first checks for free space. If there is sufficient free space available
in a single bank, the table is allocated from that bank. If a bank is
not able to accommodate the entire table, the table is split across
multiple banks. Also, while creating a new table, the DBMS tries
to reuse the already occupied banks to the highest extent possible;
that is, it does not activate a new bank unless it is necessary. Note
that the unactivated (unused) banks ­ i.e., the banks that do not hold
any data ­ can remain in the disabled mode throughout the execution
. However, it also tries not to split tables excessively. In more
detail, when it considers an already occupied bank for a new table
allocation, the table boundaries are checked first using the available
space in that bank. If a bank is more than two-thirds full with the
table data, the rest of the bank is padded with empty bits and the
new table is created using pages from a new bank. Otherwise, the
table is created beginning in the same bank. Irrespective of whether
the table is created on a new bank or not, the DBMS creates a new
table-to-bank mapping entry after each table creation.
In hardware-directed schemes, we avoid these complexities involved
in bank allocation as we assume that there is absolutely no
software control. Consequently, in the hardware-directed schemes,
we use the sequential first touch placement policy. This policy allocates
new pages sequentially in a single bank until it gets completely
filled, before moving on to the next bank. Also, the table-to
-bank mapping is not stored within the DBMS since the mode
control mechanism is handled by the hardware.
4.2.2
Estimating Idleness and Selecting the
Appropriate Low-Power Mode
It should be emphasized that the main objective of our query-directed
scheme is to identify bank idleness. As explained above,
in order to achieve this, it needs table-to-bank mapping. However
, this is not sufficient as it also needs to know when each table
will be accessed and how long an access will take (i.e., the
query access pattern). To estimate this, we need to estimate the duration
of accesses to each table, which means estimating the time
taken by the database operations. Fortunately, the current DBMSs
already maintain such estimates for query optimization purposes
<A href="81.html#10">[12, 15, 29, 33, 34]. More specifically, given a query, the optimizer
looks at the query access pattern using the generated query plan.
The inter-access times are calculated using the query plan. A query
plan elucidates the operations within a query and also the order in
which these operations access the various tables in the database.
Even in current databases, the query plan generator estimates access
costs using query plans <A href="81.html#10">[12]. We use the same access cost estimation
methodology. These access costs are measured in terms of
page (block) fetches. In our memory-resident database case, a page
is basically the block that is brought from memory to the cache. For
instance, the cost of sequential scan is defined as follows (taken
from <A href="81.html#10">[12]):
Cost
seq scan
= N
blocks
+ CPU  N
tuples
Here, N
blocks
is the number of data blocks retrieved, N
tuples
is the
number of output tuples, and CPU is the fudge factor that adjusts
the system tuple-read speed with the actual memory hierarchy data-retrieval
speed. Usually, optimizers use the above cost metric to
choose between multiple query plan options before issuing a query.
We attach a cost to each page (block) read/write operation to obtain
an estimate of the access cost (time) in terms of execution cycles.
For instance, the above scan operation is modified as follows:
Cost
block f etch
= T cycles
Cost
seq scan
= N
blocks
T + CPU  N
tuples

block
tuples
T
In these expressions, T is the delay in cycles to fetch a block from
the memory. Thus, our cost plan is projected in terms of access
cycles. We extend this to other database operations like JOIN and
AGGREGATE based on the cost models defined in <A href="81.html#10">[14, 12].
Given a query, we break down each operation within the plan (including
sub-plans) and estimate the access cost (in cycles) for each
221
- &gt; scan  A (9000  cycles)
- &gt; aggregate (20 cycles)
        - &gt; scan  B (9000  cycles)
        - &gt; scan  A (9000  cycles)
- &gt; scan  A 
- &gt; Put  A=ON
- &gt; aggregate 
        - &gt; Put  B=OFF
        - &gt; scan  B
        - &gt; Put  B=ON
        - &gt; Put  A=OFF
        - &gt; scan  A
        - &gt; Put  A=ON
        (B is already OFF)
P2
P1
(i)
(ii)
Figure 6: Example application of the query-directed scheme.
(i) The original execution plan. (b) The augmented execution
plan.
primitive operation. Our objective in estimating the per-operation
time in cycles is to eventually identify the inter-access times of operations
in the query (and hence, to put the banks that hold unused
tables to low-power modes). There are table accesses associated
with each operation, and bank inter-access times depend on the table
inter-access times. A query has information of the tables that
it accesses. Thus, knowing the inter-access time for each operation
leads to the inter-access times for each table as well. A table is
mapped to certain banks, and the table-to-bank mapping is available
in the query optimizer.
Consequently, if the table inter-access time is T , and the resynchronization
time is T
p
(assuming less than T ), then the optimizer
can transition the associated modules into a low-power mode (with
a unit time energy of E
p
) for the initial T - T
p
period (which
would consume a total [T - T
p
]E
p
energy), activate the module to
bring it back to the active mode at the end of this period following
which the module will resynchronize before it is accessed again
(consuming T
p
E
a
energy during the transition assuming that E
a
is
the unit time energy for active mode as well as during the transition
period). As a result, the total energy consumption with this transitioning
is [T - T
p
]E
p
+ T
p
E
a
without any resynchronization overheads
, w
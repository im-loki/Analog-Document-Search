The Potential of the Cell Processor for Scientific Computing
ABSTRACT
The slowing pace of commodity microprocessor performance
improvements combined with ever-increasing chip power demands
has become of utmost concern to computational scientists
. As a result, the high performance computing community
is examining alternative architectures that address
the limitations of modern cache-based designs. In this work,
we examine the potential of using the forthcoming STI Cell
processor as a building block for future high-end computing
systems. Our work contains several novel contributions.
First, we introduce a performance model for Cell and apply
it to several key scientific computing kernels: dense matrix
multiply, sparse matrix vector multiply, stencil computations
, and 1D/2D FFTs.
The difficulty of programming
Cell, which requires assembly level intrinsics for the
best performance, makes this model useful as an initial step
in algorithm design and evaluation. Next, we validate the
accuracy of our model by comparing results against published
hardware results, as well as our own implementations
on the Cell full system simulator. Additionally, we compare
Cell performance to benchmarks run on leading superscalar
(AMD Opteron), VLIW (Intel Itanium2), and vector
(Cray X1E) architectures. Our work also explores several
different mappings of the kernels and demonstrates a simple
and effective programming model for Cell's unique architecture
. Finally, we propose modest microarchitectural modifications
that could significantly increase the efficiency of
double-precision calculations. Overall results demonstrate
the tremendous potential of the Cell architecture for scientific
computations in terms of both raw performance and
power efficiency.
Categories and Subject Descriptors
C.1.2 [Processor Architectures] : Multiple Data Stream Architectures
-- Single-instruction- stream, multiple-data-stream
processors (SIMD)
Copyright 2005 Association for Computing Machinery. ACM acknowledges
that this contribution was authored or co-authored by a contractor or
affiliate of the U.S. Government. As such, the Government retains a nonex-clusive
, royalty-free right to publish or reproduce this article, or to allow
others to do so, for Government purposes only.
CF'06, May 3Â­5, 2006, Ischia, Italy.
Copyright 2006 ACM 1-59593-302-6/06/0005 ...
$
5.00.
C.1.3 [Processor Architectures] : Other Architecture Styles
-- Heterogeneous (hybrid) systems
C.1.4 [Processor Architectures] : Parallel Architectures
C.4 [Performance of Systems] : Design studies, modeling
techniques, performance attributes
D.1.3 [Programming Techniques] : Concurrent Programming
-- Parallel Programming
General Terms
Performance, Design

INTRODUCTION
Over the last decade the HPC community has moved towards
machines composed of commodity microprocessors as
a strategy for tracking the tremendous growth in processor
performance in that market. As frequency scaling slows,
and the power requirements of these mainstream processors
continues to grow, the HPC community is looking for alternative
architectures that provide high performance on scientific
applications, yet have a healthy market outside the
scientific community. In this work, we examine the potential
of the forthcoming STI Cell processor as a building block for
future high-end computing systems, by investigating performance
across several key scientific computing kernels: dense
matrix multiply, sparse matrix vector multiply, stencil computations
on regular grids, as well as 1D and 2D FFTs.
Cell combines the considerable floating point resources required
for demanding numerical algorithms with a power-efficient
software-controlled memory hierarchy. Despite its
radical departure from previous mainstream/commodity processor
designs, Cell is particularly compelling because it
will be produced at such high volumes that it will be cost-competitive
with commodity CPUs. The current implementation
of Cell is most often noted for its extremely high performance
single-precision (SP) arithmetic, which is widely
considered insufficient for the majority of scientific applications
. Although Cell's peak double precision performance
is still impressive relative to its commodity peers (~14.6
Gflop/s@3.2GHz), we explore how modest hardware changes
could significantly improve performance for computationally
intensive DP applications.
9
This paper presents several novel results.
We present
quantitative performance data for scientific kernels that compares
Cell performance to leading superscalar (AMD Opteron),
VLIW (Intel Itanium2), and vector (Cray X1E) architectures
. We believe this study examines the broadest array
of scientific algorithms to date on Cell. We developed both
analytical models and lightweight simulators to predict kernel
performance that we demonstrated to be accurate when
compared against published Cell hardware result, as well as
our own implementations on the Cell full system simulator.
Our work also explores the complexity of mapping several
important scientific algorithms onto the Cell's unique architecture
in order to leverage the large number of available
functional units and the software-controlled memory. Additionally
, we propose modest microarchitectural modifications
that could increase the efficiency of double-precision
arithmetic calculations, and demonstrate significant performance
improvements compared with the current Cell implementation
.
Overall results demonstrate the tremendous potential of
the Cell architecture for scientific computations in terms of
both raw performance and power efficiency. We also conclude
that Cell's heterogeneous multi-core implementation
is inherently better suited to the HPC environment than
homogeneous commodity multicore processors.
RELATED WORK
One of the key limiting factors for computational performance
is off-chip memory bandwidth. Since increasing
the off-chip bandwidth is prohibitively expensive, many architects
are considering ways of using available bandwidth
more efficiently. Examples include hardware multithreading
or more efficient alternatives to conventional cache-based architectures
such as software-controlled memories. Software-controlled
memories can potentially improve memory subsystem
performance by supporting finely controlled prefetch-ing
and more efficient cache-utilization policies that take advantage
of application-level information -- but do so with far
less architectural complexity than conventional cache architectures
. While placing data movement under explicit software
control increases the complexity of the programming
model, prior research has demonstrated that this approach
can be more effective for hiding memory latencies (including
cache misses and TLB misses) -- requiring far smaller cache
sizes to match the performance of conventional cache implementations
[17, 19]. The performance of software-controlled
memory is more predictable, thereby making it popular for
real-time embedded applications where guaranteed response
rates are essential.
Over the last five years, a plethora of alternatives to conventional
cache-based architectures have been suggested including
scratchpad memories [9,16,30], paged on-chip memories
[12, 17], and explicit three-level memory architectures
[18, 19]. Until recently, few of these architectural concepts
made it into mainstream processor designs, but the increasingly
stringent power/performance requirements for embedded
systems have resulted in a number of recent implementations
that have adopted these concepts. Chips like the
Sony Emotion Engine [20, 23, 29] and Intel's MXP5800 both
achieved high performance at low power by adopting three
levels (registers, local memory, external DRAM) of software-managed
memory. More recently, the STI Cell processor has
adopted a similar approach where data movement between
SPE
256 KB
PPC
512 KB
memo ry
con troller
I/O

I/O
EIB
4 rings, 8bytes/ core cycle

25.6 GB/s
SPE
256 KB
SPE
256 KB
SPE
256 KB
SPE
256 KB
SPE
256 KB
SPE
256 KB
SPE
256 KB
Figure 1: Overview of the Cell processor
these three address spaces is explicitly controlled by the application
.
For predictable data access patterns the local
store approach is highly advantageous as it can be very efficiently
utilized through explicit software-controlled scheduling
. Improved bandwidth utilization through deep pipelining
of memory requests requires less power, and has a faster
access time, than a large cache due in part to its lower complexity
. If however, the data access pattern lacks predictabil-ity
, then the advantages of software-managed memory are
lost. This more aggressive approach to memory architecture
was adopted to meet the demanding cost/performance
and real-time responsiveness requirements of Sony's upcoming
video game console. However, to date, an in-depth study
to evaluate the potential of utilizing the Cell architecture in
the context of scientific computations does not appear in the
literature.
CELL BACKGROUND
Cell [8,27] was designed by a partnership of Sony, Toshiba,
and IBM (STI) to be the heart of Sony's forthcoming PlayStation3
gaming system. Cell takes a radical departure from
conventional multiprocessor or multi-core architectures. Instead
of using identical cooperating commodity processors,
it uses a conventional high performance PowerPC core that
controls eight simple SIMD cores, called synergistic processing
elements (SPEs), where each SPE contains a synergistic
processing unit (SPU), a local memory, and a memory flow
controller. An overview of Cell is provided in Figure 1.
Access to external memory is handled via a 25.6GB/s
XDR memory controller. The cache coherent PowerPC core,
the eight SPEs, the DRAM controller, and I/O controllers
are all connected via 4 data rings, collectively known as the
EIB. The ring interface within each unit allows 8 bytes/cycle
to be read or written. Simultaneous transfers on the same
ring are possible. All transfers are orchestrated by the PowerPC
core.
Each SPE includes four single precision (SP) 6-cycle pipelined
FMA datapaths and one double precision (DP) half-pumped
(the double precision operations within a SIMD
operation must be serialized) 9-cycle pipelined FMA datapath
with 4 cycles of overhead for data movement [22]. Cell
has a 7 cycle in-order execution pipeline and forwarding network
[8]. IBM appears to have solved the problem of inserting
a 13 (9+4) cycle DP pipeline into a 7 stage in-order machine
by choosing the minimum effort/performance/power
solution of simply stalling for 6 cycles after issuing a DP
10
instruction. The SPE's DP throughput [14] of one DP instruction
every 7 (1 issue + 6 stall) cycles coincides perfectly
with this reasoning.
Thus for computationally intense algorithms like dense
matrix multiply (GEMM), we expect SP implementations to
run near peak whereas DP versions would drop to approximately
one fourteenth the peak SP flop rate [10]. Similarly,
for bandwidth intensive applications such as sparse matrix
vector multiplication (SpMV) we expect SP versions to be
between 1.5x and 4x as fast as DP, depending on density
and uniformity.
Unlike a typical coprocessor, each SPE has its own local
memory from which it fetches code and reads and writes
data. All loads and stores issued from the SPE can only
access the SPE's local memory. The Cell processor depends
on explicit DMA operations to move data from main memory
to the local store of the SPE. The limited scope of loads
and stores allows one to view the SPE as having a two-level
register file. The first level is a 128 x 128b single cycle register
file, where the second is a 16K x 128b six cycle register
file. Data must be moved into the first level before it can be
operated on by instructions. Dedicated DMA engines allow
multiple concurrent DMA loads to run concurrently with the
SIMD execution unit, thereby mitigating memory latency
overhead via double-buffered DMA loads and stores. The
selectable length DMA operations supported by the SPE
are much like a traditional unit stride vector load. We exploit
these similarities to existing HPC platforms to select
programming models that are both familiar and tractable
for scientific application developers.
PROGRAMMING MODELS
The Cell architecture poses several challenges to programming
: an explicitly controlled memory hierarchy, explicit
parallelism between the 8 SPEs and the PowerPC, and a
quadword based ISA. Our goal is to select the programming
paradigm that offers the simplest possible expression of an
algorithm while being capable of fully utilizing the hardware
resources of the Cell processor.
The memory hierarchy is programmed using explicit DMA
intrinsics with the option of user programmed double buffering
to overlap data movement with computation on the
SPEs. Moving from a hardware managed memory hierarchy
to one controlled explicitly by the application significantly
complicates the programming model, and pushes it towards
a one sided communication model. Unlike MPI, the intrinsics
are very low level and map to half a dozen instructions.
This allows for very low software overhead and good performance
, but requires the user to be capable and either ensure
correct usage or provide an interface or abstraction.
For programming the parallelism on Cell, we considered
three possible programming models: task parallelism with
independent tasks scheduled on each SPE; pipelined parallelism
where large data blocks are passed from one SPE to
the next; and data parallelism, where the processors perform
identical computations on distinct data. For simplicity, we
do not consider parallelism between the PowerPC and the
SPEs, so we can treat this as a homogeneous parallel machine
. Data pipelining may be suitable for certain classes
of algorithms and will be the focus of future investigation.
We adopt the data-parallel programming model, which is a
good match to many scientific applications and offers the
simplest and most direct method of decomposing the problem
. Data-parallel programming is quite similar to loop-level
parallelization afforded by OpenMP or the vector-like
multistreaming on the Cray X1E and the Hitachi SR-8000.
The focus of this paper is Cell architecture and performance
; we do not explore the efficacy of the IBM SPE XLC
compiler. Thus, we heavily rely on SIMD intrinsics and do
not investigate if appropriate SIMD instructions are gener-ated
by the compiler. Although the produced Cell code may
appear verbose -- due to the use of intrinsics instead of C
operators -- it delivers readily understandable performance.
Our first Cell implementation, SpMV, required about a
month of learning the programming model, the architecture,
the compiler, the tools, and deciding on a final algorithmic
strategy. The final implementation required about 600 lines
of code. The next code development examined two flavors
of double precision stencil-based algorithms. These implementations
required one week of work and are each about
250 lines, with an additional 200 lines of common code. The
programming overhead of these kernels on Cell required significantly
more effort than the scalar version's 15 lines, due
mainly to loop unrolling and intrinsics use. Although the
stencils are a simpler kernel, the SpMV learning experience
accelerated the coding process.
Having become experienced Cell programmers, the single
precision time skewed stencil -- although virtually a complete
rewrite from the double precision single step version
-- required only a single day to code, debug, benchmark,
and attain spectacular results of over 65 Gflop/s. This implementation
consists of about 450 lines, due once again to
unrolling and the heavy use of intrinsics.
SIMULATION METHODOLOGY
The simplicity of the SPEs and the deterministic behavior
of the explicitly controlled memory hierarchy make Cell
amenable to performance prediction using a simple analytic
model. Using this approach, one can easily explore multiple
variations of an algorithm without the effort of programming
each variation and running on either a fully cycle-accurate
simulator or hardware. With the newly released cycle accurate
simulator (Mambo), we have succesfully validated our
performance model for SGEMM, SpMV, and Stencil Computations
, as will be shown in the subsequent sections.
Our modeling approach is broken into two steps commensurate
with the two phase double buffered computational
model. The kernels were first segmented into code-snippets
that operate only on data present in the local store of the
SPE. We sketched the code snippets in SPE assembly and
performed static timing analysis. The latency of each operation
, issue width limitations, and the operand alignment requirements
of the SIMD/quadword SPE execution pipeline
determined the number of cycles required. The in-order nature
and fixed local store memory latency of the SPEs makes
the analysis deterministic and thus more tractable than on
cache-based, out-of-order microprocessors.
In the second step, we construct a model that tabulates
the time required for DMA loads and stores of the operands
required by the code snippets. The model accurately reflects
the constraints imposed by resource conflicts in the
memory subsystem. For instance, concurrent DMAs issued
by multiple SPEs must be serialized, as there is only a single
DRAM controller. The model also presumes a conservative
fixed DMA initiation latency of 1000 cycles.
The model computes the total time by adding all the per-11
Cell
X1E
AMD64
IA64
SPE
Chip
(MSP)
SIMD
Multi-Multi
Super
VLIW
Architecture
core
chip
scalar
SIMD Vector
Clock (GHz)
3.2
3.2
1.13
2.2
1.4
DRAM (GB/s)
25.6
25.6
34
6.4
6.4
SP Gflop/s
25.6
204.8
36
8.8
5.6
DP Gflop/s
1.83
14.63
18
4.4
5.6
Local Store
256KB
2MB


-L2
Cache
-512KB
2MB
1MB
256KB
L3 Cache



-3MB
Power (W)
3
~40
120
89
130
Year
-2006
2005
2004
2003
Table 1: Architectural overview of STI Cell, Cray
X1E MSP, AMD Opteron, and Intel Itanium2. Es-timated
total Cell power and peak Gflop/s are
based on the active SPEs/idle PowerPC programming
model.
iteration (outer loop) times, which are themselves computed
by taking the maximum of the snippet and DMA transfer
times. In some cases, the per-iteration times are constant
across iterations, but in others it varies between iterations
and is input-dependent. For example, in a sparse matrix, the
memory access pattern depends on the nonzero structure of
the matrix, which varies across iterations. Some algorithms
may also require separate stages which have different execution
times; e.g., the FFT has stages for loading data, loading
constants, local computation, transpose, local computation,
bit reversal, and storing the results.
For simplicity we chose to model a 3.2GHz, 8 SPE version
of Cell with 25.6GB/s of memory bandwidth. This version
of Cell is likely to be used in the first release of the Sony
PlayStation3 [28]. The lower frequency had the simplifying
benefit that both the EIB and DRAM controller could deliver
two SP words per cycle. The maximum flop rate of
such a machine would be 204.8 Gflop/s, with a computational
intensity of 32 FLOPs/word. For comparison, we ran
these kernels on actual hardware of several leading processor
designs: the vector Cray X1E MSP, superscalar AMD
Opteron 248 and VLIW Intel Itanium2. The key architectural
characteristics are detailed in Table 1.
5.1
Cell+ Architectural Exploration
The Double Precision (DP) pipeline in Cell is obviously
an afterthought as video games have limited need for DP
arithmetic.
Certainly a redesigned pipeline would rectify
the performance limitations, but would do so at a cost of
additional design complexity and power consumption. We
offer a more modest alternative that can reuse most of the
existing circuitry. Based on our experience designing the VI-RAM
vector processor-in-memory chip [12], we believe these
"Cell+" design modifications are considerably less complex
than a redesigned pipeline, consume very little additional
surface area on the chip, but show significant DP performance
improvements for scientific kernels.
In order to explore the limitations of Cell's DP issue bandwidth
, we propose an alternate design with a longer forwarding
network to eliminate the all but one of the stall cycles
-- recall the factors that limit DP throughput as described
in Section 3. In this hypothetical implementation, called
Cell+, each SPE would still have the single DP datapath,
but would be able to dispatch one DP SIMD instruction
every other cycle instead of one every 7 cycles. The Cell+
design would not stall issuing other instructions and would
achieve 3.5x the DP throughput of the Cell (51.2 Gflop/s) by
fully utilizing the existing DP datapath; however, it would
maintain the same SP throughput, frequency, bandwidth,
and power as the Cell.
DENSE MATRIX-MATRIX MULTIPLY
We begin by examining the performance of dense matrix-matrix
multiplication, or GEMM. This kernel is character-ized
by high computational intensity and regular memory
access patterns, making it an extremely well suited for the
Cell architecture. We explored two storage formats: column
major and block data layout [26] (BDL). BDL is a two-stage
addressing scheme (block row/column, element sub
row/column).
6.1
Algorithm Considerations
For GEMM, we adopt what is in essence an outer loop
parallelization approach. Each matrix is broken into 8n x
n element tiles designed to fit into the memory available on
the Cell chip, which are in turn split into eight n x n element
tiles that can fit into the 8 SPE local stores. For the column
layout, the matrix will be accessed via a number of short
DMAs equal to the dimension of the tile -- e.g. 64 DMAs
of length 64. BDL, on the other hand, will require a single
long DMA of length 16KB.
Since the local store is only 256KB, and must contain
both the program and stack, program data in the local
store is limited to about 56K words. The tiles, when double
buffered, require 6n
2
words of local store (one from each
matrix) -- thus making 96
2
the maximum square tiles in
SP. Additionally, in column layout, there is added pressure
on the maximum tile size for large matrices, as each column
within a tile will be on a different page resulting in TLB
misses. The minimum size of a tile is determined by the
FLOPs to word ratio of the processor. In the middle, there
is a tile-size "sweet spot" that delivers peak performance.
The loop order was therefore chosen to minimize the average
number of pages touched per phase for a column major
storage format. The BDL approach, as TLB misses are of
little concern, allows us to structure the loop order to minimize
memory bandwidth requirements.
A possible alternate approach is to adapt Cannon's algorithm
[3] for parallel machines. Although this strategy could
reduce the DRAM bandwidth requirements by transferring
blocks via the EIB, for a column major layout, it could significantly
increase the number of pages touched. This will
be the subject of future work. Note that for small matrix
sizes, it is most likely advantageous to choose an algorithm
that minimizes the number of DMAs. One such solution
would be to broadcast a copy of the first matrix to all SPEs.
6.2
Single Precision GEMM Results
The Cell performance of GEMM based on our performance
model (referred to as Cell
pm
) for large matrices is
presented in Table 2. SGEMM simulation data show that
32
2
blocks do not achieve sufficient computational intensity
to fully utilize the processor. The choice of loop order
12
Cell
pm
+
Cell
pm
X1E
AMD64
IA64
DP (Gflop/s)
51.1
14.6
16.9
4.0
5.4
SP (Gflop/s)
-204
.7
29.5
7.8
3.0
Table 2: GEMM performance (in Gflop/s) for large
square matrices on Cell, X1E, Opteron, and Itanium2
.
Only the best performing numbers are
shown. Cell data based on our performance model
is referred to as Cell
pm
.
and the resulting increase in memory traffic prevents column
major 64
2
blocks from achieving a large fraction of peak
(over 90%) for large matrices. Only 96
2
block sizes provide
enough computational intensity to overcome the additional
block loads and stores, and thus achieving near-peak performance
-- over 200Gflop/s. For BDL, however, 64
2
blocks
effectively achieve peak performance. Whereas we assume a
1000 cycle DMA startup latency in our simulations, if the
DMA latency were only 100 cycles, then the 64
2
column
major performance would reach parity with BDL.
At 3.2GHz, each SPE requires about 3W [8]. Thus with
a nearly idle PPC and L2, Cell
pm
achieves over 200 Gflop/s
for approximately 40W of power -- nearly 5 Gflop/s/Watt.
Clearly, for well-suited applications, Cell is extremely power
efficient.
6.3
Double Precision GEMM Results
A similar set of strategies and simulations were performed
for DGEMM. Although the time to load a DP 64
2
block is
twice that of the SP version, the time required to compute
on a 64
2
DP block is about 14x as long as the SP counterpart
(due to the limitations of the DP issue logic). Thus it is far
easier for DP to reach its peak performance. -- a mere 14.6
Gflop/s. However, when using our proposed Cell+ hardware
variant, DGEMM performance jumps to an impressive 51
Gflop/s.
6.4
Performance Comparison
Table 2 shows a performance comparison of GEMM between
Cell
pm
and the set of modern processors evaluated in
our study. Note the impressive performance characteristics
of the Cell processors, achieving 69x, 26x, and 7x speed
up for SGEMM compared with the Itanium2, Opteron, and
X1E respectively. For DGEMM, the default Cell processor
is 2.7x and 3.7x faster than the Itanium2 and Opteron. In
terms of power, the Cell performance is even more impressive
, achieving over 200x the efficiency of the Itanium2 for
SGEMM!
Our Cell
pm
+
exploration architecture is capable, for large
tiles, of fully exploiting the DP pipeline and achieving over
50 Gflop/s. In DP, the Cell+ architecture would be nearly
10 times faster than the Itanium2 and nearly 30 times more
power efficient. Additionally, traditional micros (Itanium2,
Opteron, etc) in multi-core configurations would require either
enormous power saving innovations or dramatic reductions
in performance, and thus would show even poorer performance/power
compared with the Cell technology. Com-pared
to the X1E, Cell+ would be 3 times as fast and 9
times more power efficient.
The decoupling of main memory data access from the
computational kernel guarantees constant memory access
latency since there will be no cache misses, and all TLB accesses
are resolved in the communication phase. Matrix multiplication
is perhaps the best benchmark to demonstrate
Cell's computational capabilities, as it achieves high performance
by buffering large blocks on chip before computing
on them.
6.5
Model Validation
IBM recently released their in-house performance evaluation
of their prototype hardware [4]. On SGEMM, they
achieve about 201 Gflop/s, which is within 2% of our pred-icated
performance.
SPARSE MATRIX VECTOR MULTIPLY
At first glance, SpMV would seem to be a poor application
choice for the Cell since the SPEs have neither caches
nor word-granularity gather/scatter support. Furthermore,
SpMV has a relatively low O(1) computational intensity.
However, these considerations are perhaps less important
than the Cell's low functional unit and local store latency
(&lt;2ns), the task parallelism afforded by the SPEs, the eight
independent load store units, and the ability to stream nonzeros
via DMAs.
7.1
Algorithmic Considerations
Two storage formats are presented in this paper: Compressed
Sparse Row (CSR) and Blocked Compressed Sparse
Row (BCSR). Only square BCSR was explored, and only
2x2 BCSR numbers will be presented here.
Future Cell
SpMV work will examine the entire BCSR space. Because
of the quadword nature of the SPEs, all rows within a CSR
tile are padded to a multiple of 4. This greatly simplifies
the programming model at the expense of increasing memory
traffic. Note that this is very different than 1x4 BCSR..
To perform a stanza gather operation the Cell utilizes the
MFC "get list" command, where a list of addresses/lengths
is created in local store. The MFC then gathers these stanzas
from the global store and packs them into the local store.
It is possible to make every stanza a single quadword, however
, without an accurate performance model of the MFC
"get list" command, one must resort to tiling to provide
a reasonable estimate for performance. For simplicity all
benchmarks were run using square tiles. The data structure
required to store the entire matrix is a 2D array of tiles,
where each block stores its nonzeros and row pointers as if
it were an entire matrix. We chose not to buffer the source
and destination vector tiles as this would result in a smaller
block size. These tradeoffs will be examined in future work.
Collectively the blocks are chosen to be no larger than ~36K
words in SP (half that in DP).
The inner loop of CSR SpMV either requires significant
software pipelining, hefty loop unrolling, or an approach al-gorithmically
analogous to a segmented scan [1]. As there
are no conditional stores in the SPE assembly language, we
chose to partially implement a segmented scan, where the
gather operations are decoupled from the dot products. This
decoupled gather operation can be unrolled and software
pipelined, thereby completing in close to three cycles per
element (the ISA is not particularly gather friendly). It is
important to note that since the local store is not a write
back cache, it is possible to overwrite its contents without
fear of consuming DRAM bandwidth or corrupting the actual
arrays.
As the nonzeros are stored contiguously in arrays, it is
13
#
Name
N
NNZ
Comments
15
Vavasis
40K 1.6M
2D PDE Problem
17
FEM
22K
1M
Fluid Mechanics Problem
18
Memory
17K 125K
MotorolaMemory Circuit
36
CFD
75K 325K Navier-Stokes, viscous flow
06 FEM Crystal
14K 490K
FEM stiffness matrix
09
3D Tube
45K 1.6M
3D pressure Tube
25
Portfolio
74K 335K
Financial Portfolio
27
NASA
36K 180K
PWT NASA Matrix
28 Vibroacoustic 12K 177K
Flexible box structure
40 Linear Prog.
31K
1M
AA
T
-7pt
64
256K 1.8M
64
3
7pt stencil
Table 3: Suite of matrices used to evaluate SpMV
performance.
Matrix numbers as defined in the
SPARSITY suite are shown in the first column.
straightforward to stream them in via DMA. Here, unlike
the source and destination vectors, it is essential to double
buffer in order to maximize the SPEs computational
throughput. Using buffers of 16KB for SP allows for 2K
values and 2K indices for CSR, and 1K tiles for 2x2 BCSR.
Note that for each phase -- loading nonzeros and indices
-- there is the omnipresent 1000 cycle DMA latency overhead
in addition to the startup and finalize penalties (as in
traditional pipelining).
To partition the work among the SPEs, we implemented
a cooperative blocking model. By forcing all SPEs to work
on the same block, it is possible to broadcast the blocked
source vector and row pointers to minimize memory traffic.
One approach, referred to as PrivateY, was to divide work
among SPEs within a block by distributing the nonzeros
as evenly as possible. This strategy necessitates that each
SPE contains a private copy of the destination vector, and
requires an inter-SPE reduction at the end of each blocked
row.
The alternate method, referred to as PartitionedY,
partitions the destination vector evenly among the SPEs.
However there is no longer any guarantee that the SPEs'
computations will remain balanced, causing the execution
time of the entire tile to be limited by the most heavily
loaded SPE. Thus for load balanced blocks, the PartitionedY
approach is generally advantageous; however, for matrices
exhibiting irregular (uneven) nonzero patterns, we expect
higher performance using PrivateY.
Note that there is a potential performance benefit by writing
a kernel specifically optimized for symmetric matrices.
For these types of matrices, the number of operations can
effectively double relative to the memory traffic. However,
the algorithm must block two tiles at a time -- thus the symmetric
matrix kernel divides memory allocated for blocking
the vector evenly among the two submatrices, and performs
a dot product and SAXPY for each row in the lower triangle.
7.2
Evaluation Matrices
In order to effectively evaluate SpMV performance, we examine
a synthetic stencil matrix, as well as ten real matrices
used in numerical calculations from the BeBop SPARSITY
suite [11, 31] (four nonsymmetric and six symmetric). Table
3 presents an overview of the evaluated matrices.
7.3
Single Precision SpMV Results
Single and double precision tuned SpMV results for the
SPARSITY matrices are show in Tables 4 and 5. Surpris-ingly
, given Cell's inherent SpMV limitations, the SPARSITY
nonsymmetric matrices average over 4 Gflop/s, while
the s
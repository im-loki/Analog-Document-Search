Automated Rich Presentation of a Semantic Topic
ABSTRACT
To have a rich presentation of a topic, it is not only expected that 
many relevant multimodal information, including images, text, 
audio and video, could be extracted; it is also important to 
organize and summarize the related information, and provide 
users a concise and informative storyboard about the target topic.  
It facilitates users to quickly grasp and better understand the 
content of a topic.  In this paper, we present a novel approach to 
automatically generating a rich presentation of a given semantic 
topic. In our proposed approach, the related multimodal information
of a given topic is first extracted from available multimedia 
databases or websites.  Since each topic usually contains multiple 
events, a text-based event clustering algorithm is then performed 
with a generative model.  Other media information, such as the 
representative images, possibly available video clips and flashes 
(interactive animates), are associated with each related event. A 
storyboard of the target topic is thus generated by integrating each 
event and its corresponding multimodal information.  Finally, to 
make the storyboard more expressive and attractive, an incidental 
music is chosen as background and is aligned with the storyboard.  
A user study indicates that the presented system works quite well 
on our testing examples.
Categories and Subject Descriptors
H.5.3 [Information Interfaces and Presentation]: Group and 
Organization Interfaces - Organizational design; H.3.1 [Information
Storage and Retrieval]: Content Analysis and Indexing - 
abstracting methods.
General Terms
Algorithms, Design, Management, Experimentation, Theory

INTRODUCTION
In the multimedia field, a major objective of content analysis is to 
discover the high-level semantics and structures from the low-level
features, and thus to facilitate indexing, browsing, searching, 
and managing the multimedia database.  In recent years, a lot of
technologies have been developed for various media types, 
including images, video, audio and etc.  For example, various 
approaches and systems have been proposed in image content 
analysis, such as semantic classification [1], content-based image 
retrieval [2] and photo album management [3]. There are also a lot 
of research focuses on video analysis, such as video segmentation 
[4], highlight detection [5], video summarization [6][7], and video 
structure analysis [8], applied in various data including news 
video, movie and sports video.  Since audio information is very 
helpful for video analysis, many research works on audio are also 
developed to enhance multimedia analysis, such as audio 
classification [9], and audio effect detection in different audio 
streams [10].  Most recently, there are more and more approaches 
and systems integrating multimodal information in order to 
improve analysis performance [11][12].
The main efforts of the above mentioned research have focused on 
understanding the semantics (including a topic, an event or the 
similarity) from the multimodal information.  That is, after the 
multimedia data is given, we want to detect the semantics implied 
in these data.  In this paper, we propose a new task, Rich 
Presentation, which is an inverse problem of the traditional 
multimedia content analysis.  That is, if we have a semantic topic, 
how can we integrate its relevant multimodal information, 
including image, text, audio and video, to richly present the target 
topic and to provide users a concise and informative storyboard?  
In this paper, the so-called "semantic topic" is a generic concept.  
It could be any keyword representing an event or events, a 
person's name, or anything else. For example, "World Cup 2002" 
and "US election" could be topics, as well as "Halloween" and 
"Harry Potter".  In this paper, our task is to find sufficient 
information on these topics, extract the key points, fuse the 
information from different modalities, and then generate an 
expressive storyboard.
Rich presentation can be very helpful to facilitate quickly 
grasping and better understanding the corresponding topic.  
People usually search information from (multimedia) database or 
the Internet.  However, what they get is usually a bulk of 
unorganized information, with many duplicates and noise.  It is 
tedious and costs a long time to get what they want by browsing 
the search results. If there is a tool to help summarize and 
integrate the multimodal information, and then produce a concise 
and informative storyboard, it will enable users to quickly figure 
out the overview contents of a topic that they want to understand. 
Rich presentation provides such a tool, and thus it could have 
many potential applications, such as education and learning, 
multimedia authoring, multimedia retrieval, documentary movie 
production, and information personalization.
In this paper, we will present the approach to rich presentation. In 
order to produce a concise and informative storyboard to richly 
present a target topic, we need to answer the following questions. 
1) How to extract the relevant information regarding the target
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
MM'05, November 6­11, 2005, Singapore. 
Copyright 2005 ACM 1-59593-044-2/05/0011...$5.00.
745
topic? 2) How to extract the key points from the relevant 
information and build a concise and informative storyboard? 3) 
How to fuse all the information from different modality? and 4) 
how to design the corresponding rendering interface?
Storyboard
Relevant Media
Music
Multiple Events Clustering
· Event summary (4w + time)
· Geographic information
Relevant multimodal information Retrieval
Text
A Target Topic
Rhythm Analysis
· Onset/Beat Sequence
· Strength confidence
Media Association
· Representative images
· Relevant video clips
Storyboard Generation
Event presentation, multimodal information fusion, layout design
Music and storyboard synchronization
Rich Presentation
User
Interaction

Fig. 1  The system framework of rich presentation of a target 
semantic topic. It is mainly composed of three steps, relevant 
multimodal information extraction, media analysis, and rich 
presentation generation.

In this paper, we propose a number of novel approaches to deal 
with the above issues and also present an example system.  Fig. 1 
illustrates the proposed system framework of rich presentation.  It 
is mainly composed of three steps, relevant multimodal information
extraction, media analysis including multiple events clustering
, representative media detection and music rhythm analysis; 
and the final storyboard generation and music synchronization.
In the proposed system, given the semantic topic, the relevant 
information, including text, image, video and music, is first 
extracted from the available multimedia database or the web database
.  User interaction is also allowed to provide extra relevant 
material or give relevant feedback.  Then, the information is 
summarized, with an event clustering algorithm, to give a concise 
representation of the topic and figure out the overview of the 
contents. Other multimedia materials, such as representative 
images (or image sequences) and geographic information, are 
subsequently associated with each event.  In the next step, all the 
above information is integrated to generate a storyboard, in which 
each event is presented as one or multiple slides.  An incidental 
music, which is also possibly relevant to the topic, is finally 
synchronized with the storyboard to improve its expressiveness 
and attractiveness. Thus, with these steps, a concise and 
informative rich presentation regarding the target topic is generated
.
The rest of the paper is organized as follows. Section 2 discusses 
the relevant information extraction corresponding to the target 
topic. Section 3 presents our approach to the topic representation, 
including multiple events clustering, event description, and 
representative media selection. Section 4 describes the approach 
to rich presentation generation, including storyboard generation, 
incidental music analysis and synchronization. Experiments and 
evaluations are presented in the Section 5.  Conclusions are given 
in the Section 6.
OBTAINING RELEVANT INFORMATION
To obtain the multimodal information which is relevant to the 
input topic (keyword), generally, we could search them from 
various databases which have been indexed with the "state-of-the-art"
multimedia analysis techniques.  However, in current stage, 
there is lack of such publicly available multimedia databases.  The 
public search engine like MSN or Google indexes all the Internet 
web-pages and can return a lot of relevant information, but the 
search results usually contain much noise.  We could also build a 
private database for this system to provide more relevant and 
clean results, but it will be too much expensive to collect and 
annotate sufficient multimedia data for various topics.  In order to 
obtain relatively accurate and sufficient data for an arbitrary topic, 
in our system, we chose to collect the relevant multimodal 
information of the given topic from the news websites such as 
MSNBC, BBC and CNN, instead of building an available 
database from the scratch.  These news websites are usually well 
organized and managed; and contain various kinds of high quality 
information including text, image and news video clips.  Although 
the news websites are used as the information sources in our 
system, other various multimedia databases can be also easily 
incorporated into the system if they are available.
Instead of directly submitting the topic as a query and getting the 
returned results by using the search function provided by the 
websites, in our system, we crawled the news documents from 
these websites in advance and then build a full-text index.  It 
enables us to quickly obtain the relevant documents, and also enable
us to use some traditional information retrieval technologies, 
such as query expansion [13], to remove the query ambiguousness 
and get more relevant documents.
In our approach, user interaction is also allowed to provide more 
materials relevant to the topic, or give relevant feedback on the 
returned results.  For example, from the above websites, we can 
seldom find a music clip relevant to the target topic.  In this case, 
users could provide the system a preferred music, which will be 
further used as incidental music to accompany with the storyboard 
presentation.  Users could also give some feedbacks on the 
obtained documents.  For example, if he gives a thumb-up to a 
document, the relevant information of the document needs to be 
presented in the final storyboard.  On the other side, users could 
also thumb-down a document to remove the related information.
TOPIC REPRESENTATION
A semantic topic is usually a quite broad concept and it usually 
contains multiple events. For example, in the topic "Harry Potter", 
the publication of each book and the release of each movie could 
be considered as an event; while in the topic "World Cup 2002", 
each match could also be taken as an event.  For each event, there 
are usually many documents reporting it.  Therefore, in order to 
generate an informative and expressive storyboard to present the 
topic, it would be better to decompose the obtained information 
and cluster the documents into different events.
However, event definition is usually subjective, different 
individuals may have different opinions. It is also confusing in 
which scale an event should be defined.  Also take "World Cup" 
as an example, in a larger scale, "World Cup 2002" and "World 
Cup 2006" could also be considered as a big event.  Therefore, 
due to the above vagueness, in this paper, we do not strictly define
746
each event of the target topic.  Following our previous works on 
news event detection [14], an event is assumed as some similar 
information describing similar persons, similar keywords, similar 
places, and similar time duration.  Therefore, in our system, an 
event is represented by four primary elements: who (persons), 
when (time), where (locations) and what (keywords); and event 
clustering is to group the documents reporting similar primary 
elements.  As for the scale of event, in the paper, it could be 
adaptively determined by the time range of the obtained 
documents or the required event number.
In this section, we present a novel clustering approach based on a 
generative model proposed in [14], instead of using traditional 
clustering methods such as K-means. After event clusters are 
obtained, the corresponding event summary is then extracted and 
other representative media is associated with each event.
3.1  Multiple Event Clustering
To group the documents into different events, essentially, we need 
to calculate p(e
j
|

x
i
), which represents the probability that a document
 x
i
belongs to an event  e
j
.  Here, as mentioned above, an
event  e
j
(and thus the document x
i
describing the event) is
represented by four primary elements: who (persons), when (time), 
where (locations) and what (keywords).  That is,
}
,
,
,
{
/
time
keywords
locations
persons
Docment
Event
=

Assuming that a document is always caused by an event [14] and 
the four primary elements are independent, to calculate the 
probability p(e
j
|

x
i
), in our approach, we first determine the likelihood
that the document x
i
is generated from event e
j
,  p(x
i
|

e
j
)
which could be further represented by the following generative 
model,
)
|
(
)
|
(
)
|
(
)
|
(
)
|
(
j
i
j
i
j
i
j
i
j
i
e
time
p
e
key
p
e
loc
p
e
name
p
e
x
p
=
(1)
where  name
i
,  loc
i
,  key
i
, and time
i
are the feature vectors
representing persons, locations, keywords and time in the 
document x
i
, respectively.  In our approach, the above entities are
extracted by the BBN NLP tools [15].  The tool can extract seven 
types of entities, including persons, organizations, locations, date, 
time, money and percent. In our approach, the obtained organization
entity is also considered as a person entity; and all the words 
except of persons, locations, and other stop-words are taken as 
keywords.
In more detail, name
i
(similarly, loc
i
and key
i
) is a  vector &lt;c
i1
,
c
i2
, ..., c
iNp
&gt;, where c
in
is the occurrence frequency of the person
n

appears in the document x
i
, and person
n
is the nth person in the
person vocabulary, which is composed of all the persons appeared 
in all the obtained documents (similarly, we can define keyword 
vocabulary and location vocabulary). Assuming N
p
is the size of
person vocabulary, p(name
i
|e
j
) could be further expressed by

=
=
p
in
N
n
c
j
n
j
i
e
person
p
e
name
p
1
)
|
(
)
|
(
(2)
Since the person, location and keyword are discrete variables 
represented by words, and the probability of the location and 
keyword can be also defined similarly as that of the person in (2), 
in the flowing sections, we will not discriminate them and 
uniformly represent the probability p(person
n
|

e
j
) (correspond-ingly
, the p(location
n
|

e
j
) and p(keyword
n
|

e
j
)) as p(w
n
|

e
j
), which
denotes the probability that the word w
n
appears in the event e
j

On the other hand, the time of an event usually lasts a continuous 
duration. It is also observed, especially in the news domain, that 
the documents about an event usually increases at the beginning 
stage of the event and then decreases at the end.   Therefore, in 
our approach, a Gaussian model N(u
j
,
j
) is utilized to roughly
represent the probability p(time
i
|

e
j
), where u
j
and
j
is the mean
and standard deviation, respectively.
To this end, in order to estimate the probability p(e
j
|

x
i
), we need
to estimate the parameters  = {p(w
n
|

e
j
), u
j
,
j
, 1jK}, assuming
K is the number of events (the selection of K is discussed in 
section 3.2). In our approach, the Maximum Likelihood is used to 
estimate the model parameters, as,


=

=
=
=
=
=
M
i
K
j
j
i
j
M
i
i
e
x
p
e
p
x
p
X
p
1
1
1
*
))
,
|
(
)
(
log(
max
arg
))
|
(
log(
max
arg
))
|
(
log(
max
arg







(3)
where X represents the corpus of the obtained documents; M and 
K are number of documents and events, respectively.
Since it is difficult to derive a close formula to estimate the 
parameters, in our approach, an Expectation Maximization (EM) 
algorithm is applied to maximize the likelihood, by running E-step 
and  M-step iteratively. A brief summary of these two steps is 
listed as follows, and more details can be found in [14].
In E-step, the posterior probability p(e
j
| x
i
) is estimated as:
)
(
)
(
)
|
(
)
|
(
)
(
)
(
)
1
(
i
t
j
t
j
i
t
i
j
x
p
e
p
e
x
p
x
e
p
=
+

(4)
where the upper script (t) indicate the tth iteration.
In M-step, the model parameters are updated, as,



+


+
=
=
=
+
=
+
+
M
i
N
s
t
i
j
M
i
t
i
j
t
j
n
s
i
tf
x
e
p
N
n
i
tf
x
e
p
e
w
p
1
1
)
1
(
1
)
1
(
)
1
(
)
)
,
(
)
|
(
(
)
,
(
)
|
(
1
)
|
(
(5)



=
=
+
=
+
+
M
i
t
i
j
M
i
i
t
i
j
t
j
x
e
p
time
x
e
p
u
1
)
1
(
1
)
1
(
)
1
(
)
|
(
)
|
(

(6)



=
=
+
=
+
+
+
M
i
t
i
j
M
i
tj
i
t
i
j
t
j
x
e
p
u
time
x
e
p
1
)
1
(
1
2
)
1
(
)
1
(
)
1
(
2
)
|
(
)
(
)
|
(


(7)
where  tf(i,n) is the term frequency of the word w
n
in the
document x
i
and N is the corresponding vocabulary size.  It
is noted that, in (5), the Laplace smoothing [16] is applied to 
prevent zero probability for the infrequently occurring word.
At last, the prior of each event is updated as:
M
x
e
p
e
p
M
i
t
i
j
t
j

=
=
+
+
1
)
1
(
)
1
(
)
|
(
)
(

(8)
The algorithm can increase the log-likelihood consistently with 
the iterations; and then converge to a local maximum. Once the 
parameters are estimated, we can simply assign each document to 
an event, as following
))
|
(
(
max
arg
i
j
j
i
x
e
p
y
=

(9)
where y
i
is the event label of the document x
i
.
747
The advantage of this generative approach is that it not only 
considers the temporal continuity of an event, it also can deal with 
the issue that some events overlap in some time durations.  In this 
case, the Gaussian model of the event time can also be overlapped 
through this data-driven parameter estimation.  From this view, 
the event clustering is also like a Gaussian mixture model (GMM) 
estimation in the timeline.
3.2  Determining the Number of Events
In the above approach to event clustering, the event number K is 
assumed known (as shown in (3)-(8)). However, the event number 
is usually very difficult to be determined a priori. In our approach, 
an intuitive way is adopted to roughly estimate the event number 
based on the document distribution along with the timeline.
As mentioned above, it is assumed that each document is caused 
by an event, and the document number of an event changes with 
the development of the event.  According to this property, each 
peak (or the corresponding contour) of the document distribution 
curve might indicate one event [14], as the Fig. 2 shows. Thus, we 
can roughly estimate the event number by simply counting the 
peak number. However, the curve is quite noisy and there 
inevitably exist some noisy peaks in the curve.  In order to avoid 
the noisy peaks, in our approach, only the salient peaks are 
assumed to be relevant to the event number.
To detect the salient peaks, we first smooth the document curve 
with a half-Hamming (raised-cosine) window, and then remove 
the very small peaks with a threshold.  Fig.2 illustrates a 
smoothed document distribution with the corresponding threshold, 
collected on the topic "US Election" in four months.   In 
experiments, the threshold is adaptively set as µ
d
d
/2, where µ
d

and
d
are the mean and standard deviation of the curve,
respectively.
After the smoothing and tiny peaks removal, we further detect the 
valleys between every two contingent peaks.  Thus, the range of 
an event (which is correlated to the corresponding peak) can be 
considered as the envelope in the two valleys. As shown in Fig2, 
the duration denoted by  L
i
+R
i
is a rough range of the event
correlated to the peak  P
i
.  Assuming an important event usually
has more documents and has effects in a longer duration, the 
saliency of each peak is defined as,
)
)(
(
avr
i
i
avr
i
i
D
R
L
P
P
S
+
=
(10)
where P
i
is the ith peak, L
i
and R
i
is the duration from the ith peak
to the previous and next valley; P
avr
is the average peak value and
D
avr
is average duration between two valleys in the curve. S
i
is the
saliency value of the peak P
i
. It could also be considered as the
normalized area under peak P
i
, and thus, it roughly represents the
document number of the corresponding event.
In our approach, the top K salient peaks are selected to determine 
the event number:
}
/
{
max
arg
'
1
1
'




=
=
=
i
N
i
k
i
i
k
S
S
K

(11)

where
'
i
S is the sorted saliency value from large to small, N is
total number of detected peaks and   is  a threshold.  In our 
experiments,    is set as 0.9,  which roughly means that at least 
90% documents will be kept in the further initialization of event
clustering. This selection scheme is designed to guarantee there is 
no important information is missed in presentation. After the 
event number and initial clusters (the most salient peaks with their 
corresponding range) are selected, the event parameters could be 
initialized and then updated iteratively.
0
5
10
15
20
0
20
40
60
80
100
120
#Doc
Threshold
P
i
P
i+1
P
i-1
L
i
R
i
Peaks relevant to event

Fig.2 Peak saliency definition. It also illustrates the smoothed 
document distribution (document number per day) with the 
corresponding threshold for tiny peak removal. Each peak P
i
is
assumed to be correlated with each event.
It is noted that some technology such as Bayesian Information 
Criteria (BIC) or minimum description length (MDL) [17] could 
be used to estimate the optimal event number, by searching 
through a reasonable range of the event number to find the one 
which maximizes the likelihood in (3).  However, these algorithms
take long time, and it is usually not necessary to estimate 
the exact event number in our scenario of rich presentation. 
Actually, in our system, the most important point of event clustering
is that the clustered documents `really' represent the same 
event, rather than the event number, as observed in the experiments
.  Moreover, in the step of synchronization between the 
music and storyboard (in the section 4.2), the number of presented 
events may be further refined, based on the user's preference, in 
order to match the presentation duration with the music duration.
3.3 Event Description
After obtaining the events and the corresponding documents, we 
not only need a concise event summary, but also need to extract 
some representative media to describe each event.
3.3.1 Event Summary
A simple way to summarize an event is to choose some 
representative words on the persons, locations and keywords of 
the event.  For example, for the event e
j
, the `leading actor' could
be chosen as the person with the maximum p(person
n
|

e
j
), while
the major location could be selected based on p(location
n
|

e
j
).
However, such brief description might have a bad readability.  
Therefore, in order to increase the readability of the summary, in 
our system, we also provide an alterative way.  That is, we choose 
a candidate document to represent an event. For example, the 
document with the highest p(x
i
|e
j
) is a good candidate representative
of the event e
j
.  However, a document might be too long to
be shown on the storyboard.  Therefore, in our system, only the 
"title-brow" (the text between the news title and news body) of 
the document, which usually exists and is usually a good 
overview (summary) of the document based on our observation 
(especially true in our case of news document), is selected to 
describe the event.
748
I
III
II
IV

Fig. 3 The event template of the Storyboard, which illustrates (I) the representative media, (II)geographic information, (III) event summary, 
and (IV) a film strip  giving an overview of the events in the temporal order.
3.3.2  Extracting Representative Media
In the obtained documents describing an event, there are usually 
many illustrational images, with possible flashes and video clips. 
These media information is also a good representative of the 
corresponding event.  However, since the obtained documents are 
directly crawled from the news websites, they usually contain 
many noisy multimedia resources, such as the advertisements. 
Moreover, there also possible exist some duplicate images in 
different documents describing the same event. Therefore, to 
extract the representative media from the documents, we need to 
remove noisy media and possible duplicate images.  Before this, 
we also performed a pre-filtering to remove all the images smaller 
than 50 pixels in height or width.
Noisy Media Detection.  In our approach, a simple but
efficient rule is used to remove the noisy media resources. 
We find almost all advertisements are provided by other 
agencies rather than these news websites themselves. That is, 
the hosts of advertisement resources are from different 
websites.  Thus, in our approach, we extract the host names 
from the URLs of all multimedia resources, and remove 
those resources with different host name.
Duplicate Detection.  A number of image signature schemes
can be adopted here to accomplish duplicate detection.  In 
our implementation, each image is converted into grayscale, 
and down-sampled to 8×8.  That is, a 64-byte signature for 
each image is obtained.  Then the Euclidean distance of the 
64-byte signature are taken as the dissimilarity measure.  
Images have sufficiently small distance are considered as 
duplicates.
Once removing the noisy resources and duplicate images, we 
simply select the 1-4 large images from the top representative 
documents (with the top largest p(x
i
|e
j
)), and take them as
representative media of the corresponding event. The exact 
number of the selected images is dependent on the document 
number (i.e., the importance) of the event and the total image
number the event has.  It is noted that, in our current system, we 
only associates images with each event.  However, other media 
like video and flashes can be chosen in a similar way.
RICH PRESENTATION GENERATION
In the proposed system, the above obtained information, including 
event summary and representative media, are fused to generate a 
concise and informative storyboard, in order to richly present the 
target topic.  In this section, we will first describe the storyboard 
generation for the target topic, by presenting each event with the 
multimodal information. Then, we present the approach to 
synchronizing the storyboard with an incidental music.
4.1 Storyboard Generation
In our approach, a storyboard of a target topic is generated by 
presenting each event of the topic slide by slide.  To describe an 
event, we have obtained the corresponding information including 
the person, time, location, event summary and other relevant 
images.  Therefore, to informatively present each event, we need 
first to design an event template (i.e., an interface) to integrate all 
the information.
Fig. 3 illustrates the event template used in our proposed system, 
with an example event in the topic `US Election".  First, the 
template presents the representative images in the largest area 
(part I), since the pictures are more vivid than the words.  As for 
each representative picture, the title and date of the document from 
which it is extracted is also illustrated.  In the Fig.3, there are 4 
pictures extracted from 3 documents. Then, the corresponding 
event summaries of these three documents are presented (part III), 
where each paragraph refers to the summary of one document. If a 
user is interested in one document, he can click on the corresponding
title to read more details.  Moreover, the geographic information
of the event is shown with a map in the top-left corner (part 
II), to give users a view of the event location.  The map is obtained 
from "MapPoint Location" service [18], which can return a
749
corresponding map based on user's location query. However, the 
mapping is usually difficult, especially when the event location is 
confusing so that the representative location is not accurately 
detected.  For example, the event shown in the Fig 1 is mapped to 
Washington D.C. rather than New York where the republic 
convention is held, since Washington is the most frequently 
mentioned places in the documents.  Finally, a film strip (part IV) 
is also presented, arranging each event in the temporal order, 
where each event is simply represented by a cluster of images, 
with the current event highlighted.  It enables users to have a quick 
overview of the past and the future in the event sequence.
By connecting various events slide by slide, we could get an 
informative storyboard regarding the target topic.  In order to 
catch the development process of a topic, the events are ordered 
by their timestamps in the generated storyboard.
4.2 Synchronizing with Music
To make the storyboard more expressive and attractive, and to 
provide a more relaxing way to read information, in the proposed 
system, we will accompany the storyboard with an incidental 
music and align the transitions between event slides with the 
music beats, following the idea in music video generation [19][20].  
Sometimes, music could also provide extra information about the 
target topic. For example, when the target topic is a movie, the 
corresponding theme song could be chosen for the rich presentation
.  In this sub-section, we will present our approach to music 
analysis and synchronization with the storyboard.
4.2.1  Music Rhythm Analysis
In the proposed system, we detect the onset sequences instead of 
the exact beat series to represent music rhythm.  This is because 
the beat information is sometimes not obvious, especially in light 
music which is usually selected as incidental music. The strongest 
onset in a time window could be assumed as a "beat".  This is 
reasonable since there are some beat positions in a time window 
(for example, 5 seconds); thus, the most possible position of a beat 
is the position of the strongest onset.
The process of onset estimation is illustrated in Fig. 4.  After FFT 
is performed on each frame of 16ms-length, an octave-scale filter-bank
is used to divide the frequency domain into six sub-bands, 
including [0,
0
/2
6
), [
0
/2
6
,
0
/2
5
), ..., [
0
/2
2
,
0
/2], where
0

refers to the sampling rate.
Acoustic Music Data
FFT
Sub-Band 1
Envelope
Extractor
Difference curve
Onset Curve
Sub-Band N
Envelope
Extractor
Difference curve
...       ...      ...
.
.
.
.
.
.

Fig. 4  The process of onset sequence estimation
After the amplitude envelope of each sub-band is extracted by 
using a half-Hamming window, a Canny operator is used for onset 
sequence detection by estimating its difference function,
)
(
)
(
)
(
n
C
n
A
n
D
i
i

=
(12)
where D
i
(n) is the difference function in the ith sub-band, A
i
(n) is
the amplitude envelope of the ith sub-band, and C(n) is the Canny 
operator with a Gaussian kernel,
]
,
[
)
(
2
2
/
2
2
c
c
i
L
L
n
e
i
n
C

=


(13)
where  L
c
is the length of the Canny operator and   is  used  to
control the operator's shape, which are set as 12 and 4 in our 
implementation, respectively.
Finally, the sum of the difference curves of these six sub-bands is 
used to extract onset sequence.  Each peak is considered as an 
onset, and the peak value is considered as the onset strength.
Based on the obtained onsets, an incidental music is further 
segmented into music sub-clips, where a strong onset is taken as 
the boundary of a music sub-clip.  These music sub-clips are then 
used as the basic timeline for the synchronization in the next step.  
Thus, to satisfy the requirement that the event slide transitions of 
the storyboard should occur at the music beats, we just need to 
align the event slide boundaries and music sub-clip boundaries.
To give a more pleasant perception, the music sub-clip should not 
be too short or too long, also it had better not always keep the 
same length.  In our implementation, the length of music sub-clips 
is ra
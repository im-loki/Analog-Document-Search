Estimating the Global PageRank of Web Communities
ABSTRACT
Localized search engines are small-scale systems that index
a particular community on the web. They offer several benefits
over their large-scale counterparts in that they are relatively
inexpensive to build, and can provide more precise
and complete search capability over their relevant domains.
One disadvantage such systems have over large-scale search
engines is the lack of global PageRank values. Such information
is needed to assess the value of pages in the localized
search domain within the context of the web as a whole.
In this paper, we present well-motivated algorithms to estimate
the global PageRank values of a local domain. The
algorithms are all highly scalable in that, given a local domain
of size n, they use O(n) resources that include computation
time, bandwidth, and storage. We test our methods
across a variety of localized domains, including site-specific
domains and topic-specific domains. We demonstrate that
by crawling as few as n or 2n additional pages, our methods
can give excellent global PageRank estimates.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval; G.1.3 [Numerical Analysis]:
Numerical Linear Algebra; G.3 [Probability and Statistics
]: Markov Processes
General Terms
PageRank, Markov Chain, Stochastic Complementation

INTRODUCTION
Localized search engines are small-scale search engines
that index only a single community of the web. Such communities
can be site-specific domains, such as pages within
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for pro£t or commercial advantage and that copies
bear this notice and the full citation on the £rst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speci£c
permission and/or a fee.
KDD'06, August 20­23, 2006, Philadelphia, Pennsylvania, USA.
Copyright 2006 ACM 1-59593-339-5/06/0008 ...
$
5.00.
the cs.utexas.edu domain, or topic-related communities-for
example, political websites. Compared to the web graph
crawled and indexed by large-scale search engines, the size
of such local communities is typically orders of magnitude
smaller. Consequently, the computational resources needed
to build such a search engine are also similarly lighter. By
restricting themselves to smaller, more manageable sections
of the web, localized search engines can also provide more
precise and complete search capabilities over their respective
domains.
One drawback of localized indexes is the lack of global
information needed to compute link-based rankings. The
PageRank algorithm [3], has proven to be an effective such
measure. In general, the PageRank of a given page is dependent
on pages throughout the entire web graph. In the
context of a localized search engine, if the PageRanks are
computed using only the local subgraph, then we would expect
the resulting PageRanks to reflect the perceived popularity
within the local community and not of the web as a
whole. For example, consider a localized search engine that
indexes political pages with conservative views. A person
wishing to research the opinions on global warming within
the conservative political community may encounter numerous
such opinions across various websites. If only local PageRank
values are available, then the search results will reflect
only strongly held beliefs within the community. However, if
global PageRanks are also available, then the results can ad-ditionally
reflect outsiders' views of the conservative community
(those documents that liberals most often access within
the conservative community).
Thus, for many localized search engines, incorporating
global PageRanks can improve the quality of search results.
However, the number of pages a local search engine indexes
is typically orders of magnitude smaller than the number of
pages indexed by their large-scale counterparts. Localized
search engines do not have the bandwidth, storage capacity,
or computational power to crawl, download, and compute
the global PageRanks of the entire web. In this work, we
present a method of approximating the global PageRanks of
a local domain while only using resources of the same order
as those needed to compute the PageRanks of the local
subgraph.
Our proposed method looks for a supergraph of our local
subgraph such that the local PageRanks within this supergraph
are close to the true global PageRanks. We construct
this supergraph by iteratively crawling global pages on the
current web frontier--i.e., global pages with inlinks from
pages that have already been crawled. In order to provide
116
Research Track Paper
a good approximation to the global PageRanks, care must
be taken when choosing which pages to crawl next; in this
paper, we present a well-motivated page selection algorithm
that also performs well empirically. This algorithm is derived
from a well-defined problem objective and has a running
time linear in the number of local nodes.
We experiment across several types of local subgraphs,
including four topic related communities and several site-specific
domains. To evaluate performance, we measure the
difference between the current global PageRank estimate
and the global PageRank, as a function of the number of
pages crawled. We compare our algorithm against several
heuristics and also against a baseline algorithm that chooses
pages at random, and we show that our method outperforms
these other methods. Finally, we empirically demonstrate
that, given a local domain of size n, we can provide good
approximations to the global PageRank values by crawling
at most n or 2n additional pages.
The paper is organized as follows.
Section 2 gives an
overview of localized search engines and outlines their advantages
over global search. Section 3 provides background
on the PageRank algorithm. Section 4 formally defines our
problem, and section 5 presents our page selection criteria
and derives our algorithms. Section 6 provides experimental
results, section 7 gives an overview of related work, and,
finally, conclusions are given in section 8.
LOCALIZED SEARCH ENGINES
Localized search engines index a single community of the
web, typically either a site-specific community, or a topic-specific
community. Localized search engines enjoy three
major advantages over their large-scale counterparts: they
are relatively inexpensive to build, they can offer more precise
search capability over their local domain, and they can
provide a more complete index.
The resources needed to build a global search engine are
enormous. A 2003 study by Lyman et al. [13] found that
the `surface web' (publicly available static sites) consists of
8.9 billion pages, and that the average size of these pages is
approximately 18.7 kilobytes. To download a crawl of this
size, approximately 167 terabytes of space is needed. For a
researcher who wishes to build a search engine with access
to a couple of workstations or a small server, storage of this
magnitude is simply not available. However, building a localized
search engine over a web community of a hundred
thousand pages would only require a few gigabytes of storage
. The computational burden required to support search
queries over a database this size is more manageable as well.
We note that, for topic-specific search engines, the relevant
community can be efficiently identified and downloaded by
using a focused crawler [21, 4].
For site-specific domains, the local domain is readily available
on their own web server. This obviates the need for
crawling or spidering, and a complete and up-to-date index
of the domain can thus be guaranteed. This is in contrast
to their large-scale counterparts, which suffer from several
shortcomings. First, crawling dynamically generated
pages--pages in the `hidden web'--has been the subject of
research [20] and is a non-trivial task for an external crawler.
Second, site-specific domains can enable the robots exclusion
policy. This prohibits external search engines' crawlers
from downloading content from the domain, and an external
search engine must instead rely on outside links and anchor
text to index these restricted pages.
By restricting itself to only a specific domain of the internet
, a localized search engine can provide more precise
search results.
Consider the canonical ambiguous search
query, `jaguar', which can refer to either the car manufacturer
or the animal. A scientist trying to research the habitat
and evolutionary history of a jaguar may have better
success using a finely tuned zoology-specific search engine
than querying Google with multiple keyword searches and
wading through irrelevant results. A method to learn better
ranking functions for retrieval was recently proposed by
Radlinski and Joachims [19] and has been applied to various
local domains, including Cornell University's website [8].
PAGERANK OVERVIEW
The PageRank algorithm defines the importance of web
pages by analyzing the underlying hyperlink structure of a
web graph. The algorithm works by building a Markov chain
from the link structure of the web graph and computing its
stationary distribution. One way to compute the stationary
distribution of a Markov chain is to find the limiting
distribution of a random walk over the chain. Thus, the
PageRank algorithm uses what is sometimes referred to as
the `random surfer' model. In each step of the random walk,
the `surfer' either follows an outlink from the current page
(i.e. the current node in the chain), or jumps to a random
page on the web.
We now precisely define the PageRank problem. Let U
be an m
× m adjacency matrix for a given web graph such
that U
ji
= 1 if page i links to page j and U
ji
= 0 otherwise.
We define the PageRank matrix P
U
to be:
P
U
= U D
-1
U
+ (1
- )ve
T
,
(1)
where D
U
is the (unique) diagonal matrix such that U D
-1
U
is column stochastic,  is a given scalar such that 0
1,
e is the vector of all ones, and v is a non-negative, L
1
normalized
vector, sometimes called the `random surfer' vector
. Note that the matrix D
-1
U
is well-defined only if each
column of U has at least one non-zero entry--i.e., each page
in the webgraph has at least one outlink. In the presence of
such `dangling nodes' that have no outlinks, one commonly
used solution, proposed by Brin et al. [3], is to replace each
zero column of U by a non-negative, L
1
-normalized vector.
The PageRank vector r is the dominant eigenvector of the
PageRank matrix, r = P
U
r. We will assume, without loss of
generality, that r has an L
1
-norm of one. Computationally,
r can be computed using the power method. This method
first chooses a random starting vector r
(0)
, and iteratively
multiplies the current vector by the PageRank matrix P
U
;
see Algorithm 1. In general, each iteration of the power
method can take O(m
2
) operations when P
U
is a dense matrix
. However, in practice, the number of links in a web
graph will be of the order of the number of pages. By exploiting
the sparsity of the PageRank matrix, the work per
iteration can be reduced to O(km), where k is the average
number of links per web page. It has also been shown that
the total number of iterations needed for convergence is proportional
to  and does not depend on the size of the web
graph [11, 7]. Finally, the total space needed is also O(km),
mainly to store the matrix U .
117
Research Track Paper
Algorithm 1:
A linear time (per iteration) algorithm for
computing PageRank.
ComputePR
(U )
Input:
U : Adjacency matrix.
Output:
r: PageRank vector.
Choose (randomly) an initial non-negative vector r
(0)
such that r
(0) 1
= 1.
i
0
repeat
i
i + 1
U D
-1
U
r
(i-1)
{ is the random surfing probability
}r
(i)
+ (1 - )v {v is the random surfer vector.}
until
r
(i)
- r
(i-1)
&lt;
{ is the convergence threshold.}
r  r
(i)
PROBLEM DEFINITION
Given a local domain L, let G be an N
× N adjacency
matrix for the entire connected component of the web that
contains L, such that G
ji
= 1 if page i links to page j
and G
ji
= 0 otherwise. Without loss of generality, we will
partition G as:
G =
L
G
out
L
out
G
within
,
(2)
where L is the n
× n local subgraph corresponding to links
inside the local domain, L
out
is the subgraph that corresponds
to links from the local domain pointing out to the
global domain, G
out
is the subgraph containing links from
the global domain into the local domain, and G
within
contains
links within the global domain. We assume that when
building a localized search engine, only pages inside the local
domain are crawled, and the links between these pages
are represented by the subgraph L. The links in L
out
are
also known, as these point from crawled pages in the local
domain to uncrawled pages in the global domain.
As defined in equation (1), P
G
is the PageRank matrix
formed from the global graph G, and we define the global
PageRank vector of this graph to be g. Let the n-length
vector p

be the L
1
-normalized vector corresponding to the
global PageRank of the pages in the local domain L:
p

=
E
L
g
E
L
g
1
,
where E
L
= [ I
| 0 ] is the restriction matrix that selects
the components from g corresponding to nodes in L. Let p
denote the PageRank vector constructed from the local domain
subgraph L. In practice, the observed local PageRank
p and the global PageRank p

will be quite different. One
would expect that as the size of local matrix L approaches
the size of global matrix G, the global PageRank and the observed
local PageRank will become more similar. Thus, one
approach to estimating the global PageRank is to crawl the
entire global domain, compute its PageRank, and extract
the PageRanks of the local domain.
Typically, however, n
N , i.e., the number of global
pages is much larger than the number of local pages. Therefore
, crawling all global pages will quickly exhaust all local
resources (computational, storage, and bandwidth) available
to create the local search engine. We instead seek a supergraph
^
F of our local subgraph L with size O(n). Our goal
Algorithm 2:
The FindGlobalPR algorithm.
FindGlobalPR
(L, L
out
, T , k)
Input:
L: zero-one adjacency matrix for the local domain
, L
out
: zero-one outlink matrix from L to global
subgraph as in (2), T : number of iterations, k: number of
pages to crawl per iteration.
Output: ^
p: an improved estimate of the global PageRank
of L.
F  L
F
out
L
out
f  ComputePR(F )
for (i = 1 to T )
{Determine which pages to crawl next}
pages
SelectNodes(F , F
out
, f , k)
Crawl pages, augment F and modify F
out
{Update PageRanks for new local domain}
f  ComputePR(F )
end
{Extract PageRanks of original local domain & normalize}
^
p
E
L
f
E
L
f
1
is to find such a supergraph ^
F with PageRank ^
f , so that
^
f when restricted to L is close to p

. Formally, we seek to
minimize
GlobalDif f ( ^
f ) =
E
L
^
f
E
L
^
f
1
- p

1
.
(3)
We choose the L
1
norm for measuring the error as it does
not place excessive weight on outliers (as the L
2
norm does,
for example), and also because it is the most commonly used
distance measure in the literature for comparing PageRank
vectors, as well as for detecting convergence of the algorithm
[3].
We propose a greedy framework, given in Algorithm 2,
for constructing ^
F . Initially, F is set to the local subgraph
L, and the PageRank f of this graph is computed. The algorithm
then proceeds as follows. First, the SelectNodes
algorithm (which we discuss in the next section) is called
and it returns a set of k nodes to crawl next from the set
of nodes in the current crawl frontier, F
out
. These selected
nodes are then crawled to expand the local subgraph, F , and
the PageRanks of this expanded graph are then recomputed.
These steps are repeated for each of T iterations. Finally,
the PageRank vector ^
p, which is restricted to pages within
the original local domain, is returned. Given our computation
, bandwidth, and memory restrictions, we will assume
that the algorithm will crawl at most O(n) pages. Since the
PageRanks are computed in each iteration of the algorithm,
which is an O(n) operation, we will also assume that the
number of iterations T is a constant. Of course, the main
challenge here is in selecting which set of k nodes to crawl
next. In the next section, we formally define the problem
and give efficient algorithms.
NODE SELECTION
In this section, we present node selection algorithms that
operate within the greedy framework presented in the previous
section. We first give a well-defined criteria for the
page selection problem and provide experimental evidence
that this criteria can effectively identify pages that optimize
our problem objective (3). We then present our main al-118
Research Track Paper
gorithmic contribution of the paper, a method with linear
running time that is derived from this page selection criteria
. Finally, we give an intuitive analysis of our algorithm in
terms of `leaks' and `flows'. We show that if only the `flow'
is considered, then the resulting method is very similar to a
widely used page selection heuristic [6].
5.1
Formulation
For a given page j in the global domain, we define the
expanded local graph F
j
:
F
j
=
F
s
u
T
j
0
,
(4)
where u
j
is the zero-one vector containing the outlinks from
F into page j, and s contains the inlinks from page j into
the local domain. Note that we do not allow self-links in
this framework. In practice, self-links are often removed, as
they only serve to inflate a given page's PageRank.
Observe that the inlinks into F from node j are not known
until after node j is crawled. Therefore, we estimate this
inlink vector as the expectation over inlink counts among
the set of already crawled pages,
s =
F
T
e
F
T
e
1
.
(5)
In practice, for any given page, this estimate may not reflect
the true inlinks from that page. Furthermore, this expectation
is sampled from the set of links within the crawled
domain, whereas a better estimate would also use links from
the global domain. However, the latter distribution is not
known to a localized search engine, and we contend that the
above estimate will, on average, be a better estimate than
the uniform distribution, for example.
Let the PageRank of F be f . We express the PageRank
f
+
j
of the expanded local graph F
j
as
f
+
j
=
(1
- x
j
)f
j
x
j
,
(6)
where x
j
is the PageRank of the candidate global node j,
and f
j
is the L
1
-normalized PageRank vector restricted to
the pages in F .
Since directly optimizing our problem goal requires knowing
the global PageRank p

, we instead propose to crawl
those nodes that will have the greatest influence on the PageRanks
of pages in the original local domain L:
influence(j)
=
kL
|f
j
[k]
- f[k]|
(7)
=
E
L
(f
j
- f )
1
.
Experimentally, the influence score is a very good predictor
of our problem objective (3). For each candidate global node
j, figure 1(a) shows the objective function value Global Diff(f
j
)
as a function of the influence of page j. The local domain
used here is a crawl of conservative political pages (we will
provide more details about this dataset in section 6); we
observed similar results in other domains. The correlation
is quite strong, implying that the influence criteria can effectively
identify pages that improve the global PageRank
estimate. As a baseline, figure 1(b) compares our objective
with an alternative criteria, outlink count. The outlink
count is defined as the number of outlinks from the local
domain to page j. The correlation here is much weaker.
.00001
.0001
.001
.01
0.26
0.262
0.264
0.266
Influence
Objective
1
10
100
1000
0.266
0.264
0.262
0.26
Outlink Count
Objective
(a)
(b)
Figure 1: (a) The correlation between our
influence
page selection criteria (7) and the actual objective
function (3) value is quite strong. (b) This is in contrast
to other criteria, such as outlink count, which
exhibit a much weaker correlation.
5.2
Computation
As described, for each candidate global page j, the influence
score (7) must be computed.
If f
j
is computed
exactly for each global page j, then the PageRank algorithm
would need to be run for each of the O(n) such global
pages j we consider, resulting in an O(n
2
) computational
cost for the node selection method. Thus, computing the
exact value of f
j
will lead to a quadratic algorithm, and we
must instead turn to methods of approximating this vector.
The algorithm we present works by performing one power
method iteration used by the PageRank algorithm (Algorithm
1). The convergence rate for the PageRank algorithm
has been shown to equal the random surfer probability  [7,
11]. Given a starting vector x
(0)
, if k PageRank iterations
are performed, the current PageRank solution x
(k)
satisfies:
x
(k)
- x
1
= O(
k
x
(0)
- x
1
),
(8)
where x

is the desired PageRank vector. Therefore, if only
one iteration is performed, choosing a good starting vector
is necessary to achieve an accurate approximation.
We partition the PageRank matrix P
F
j
, corresponding to
the
× subgraph F
j
as:
P
F
j
=
~
F
~
s
~
u
T
j
w
,
(9)
where
~
F
=
F (D
F
+ diag(u
j
))
-1
+ (1
- ) e
+ 1 e
T
,
~
s = s + (1 - ) e
+ 1 ,
~
u
j
=
(D
F
+ diag(u
j
))
-1
u
j
+ (1
- ) e
+ 1 ,
w
=
1
- 
+ 1 ,
and diag(u
j
) is the diagonal matrix with the (i, i)
th
entry
equal to one if the i
th
element of u
j
equals one, and is zero
otherwise. We have assumed here that the random surfer
vector is the uniform vector, and that L has no `dangling
links'. These assumptions are not necessary and serve only
to simplify discussion and analysis.
A simple approach for estimating f
j
is the following. First,
estimate the PageRank f
+
j
of F
j
by computing one PageRank
iteration over the matrix P
F
j
, using the starting vector
 =
f
0
. Then, estimate f
j
by removing the last
119
Research Track Paper
component from our estimate of f
+
j
(i.e., the component
corresponding to the added node j), and renormalizing.
The problem with this approach is in the starting vector.
Recall from (6) that x
j
is the PageRank of the added node
j. The difference between the actual PageRank f
+
j
of P
F
j
and the starting vector  is
- f
+
j
1
=
x
j
+ f
- (1 - x
j
)f
j 1
x
j
+
| f
1
- (1 - x
j
) f
j 1
|
=
x
j
+
|x
j
|
=
2x
j
.
Thus, by (8), after one PageRank iteration, we expect our
estimate of f
+
j
to still have an error of about 2x
j
. In particular
, for candidate nodes j with relatively high PageRank
x
j
, this method will yield more inaccurate results. We will
next present a method that eliminates this bias and runs in
O(n) time.
5.2.1
Stochastic Complementation
Since f
+
j
, as given in (6) is the PageRank of the matrix
P
F
j
, we have:
f
j
(1
- x
j
)
x
j
=
~
F
~
s
~
u
T
j
w
f
j
(1
- x
j
)
x
j
=
~
F f
j
(1
- x
j
) + ~
sx
j
~
u
T
j
f
j
(1
- x
j
) + wx
j
.
Solving the above system for f
j
can be shown to yield
f
j
= ( ~
F + (1 - w)
-1
~
s ~
u
T
j
)f
j
.
(10)
The matrix S = ~
F +(1-w)
-1
~
s ~
u
T
j
is known as the stochastic
complement of the column stochastic matrix P
F
j
with respect
to the sub matrix ~
F . The theory of stochastic complementation
is well studied, and it can be shown the stochastic
complement of an irreducible matrix (such as the PageRank
matrix) is unique. Furthermore, the stochastic complement
is also irreducible and therefore has a unique stationary distribution
as well. For an extensive study, see [15].
It can be easily shown that the sub-dominant eigenvalue
of S is at most
+1
, where
is the size of F . For sufficiently
large , this value will be very close to . This is important,
as other properties of the PageRank algorithm, notably the
algorithm's sensitivity, are dependent on this value [11].
In this method, we estimate the length
vector f
j
by
computing one PageRank iteration over the
× stochastic
complement S, starting at the vector f :
f
j
Sf.
(11)
This is in contrast to the simple method outlined in the previous
section, which first iterates over the ( + 1)
× ( + 1)
matrix P
F
j
to estimate f
+
j
, and then removes the last component
from the estimate and renormalizes to approximate
f
j
. The problem with the latter method is in the choice
of the ( + 1) length starting vector, . Consequently, the
PageRank estimate given by the simple method differs from
the true PageRank by at least 2x
j
, where x
j
is the PageRank
of page j. By using the stochastic complement, we
can establish a tight lower bound of zero for this difference.
To see this, consider the case in which a node k is added
to F to form the augmented local subgraph F
k
, and that
the PageRank of this new graph is
(1
- x
k
)f
x
k
. Specifi-cally
, the addition of page k does not change the PageRanks
of the pages in F , and thus f
k
= f . By construction of
the stochastic complement, f
k
= Sf
k
, so the approximation
given in equation (11) will yield the exact solution.
Next, we present the computational details needed to efficiently
compute the quantity f
j
-f
1
over all known global
pages j. We begin by expanding the difference f
j
-f , where
the vector f
j
is estimated as in (11),
f
j
- f  Sf - f
=
F (D
F
+ diag(u
j
))
-1
f + (1 - ) e
+ 1 e
T
f
+(1
- w)
-1
( ~
u
T
j
f )~
s - f.
(12)
Note that the matrix (D
F
+diag(u
j
))
-1
is diagonal. Letting
o[k] be the outlink count for page k in F , we can express
the k
th
diagonal element as:
(D
F
+ diag(u
j
))
-1
[k, k] =
1
o[k]+1
if u
j
[k] = 1
1
o[k]
if u
j
[k] = 0
Noting that (o[k] + 1)
-1
= o[k]
-1
- (o[k](o[k] + 1))
-1
and
rewriting this in matrix form yields
(D
F
+diag(u
j
))
-1
= D
-1
F
-D
-1
F
(D
F
+diag(u
j
))
-1
diag(u
j
).
(13)
We use the same identity to express
e
+ 1 =
e - e
( + 1) .
(14)
Recall that, by definition, we have P
F
= F D
-1
F
+(1
-)
e
.
Substituting (13) and (14) in (12) yields
f
j
- f  (P
F
f - f)
-F D
-1
F
(D
F
+ diag(u
j
))
-1
diag(u
j
)f
-(1 - )
e
( + 1) + (1 - w)
-1
( ~
u
T
j
f )~
s
=
x + y + ( ~
u
T
j
f )z,
(15)
noting that by definition, f = P
F
f , and defining the vectors
x, y, and z to be
x = -F D
-1
F
(D
F
+ diag(u
j
))
-1
diag(u
j
)f
(16)
y = -(1 - )
e
( + 1)
(17)
z = (1 - w)
-1
~
s.
(18)
The first term x is a sparse vector, and takes non-zero values
only for local pages k that are siblings of the global page
j. We define (i, j)
F if and only if F [j, i] = 1 (equiva-lently
, page i links to page j) and express the value of the
component x[k ] as:
x[k ] =
k
:(k,k )F ,u
j
[k]=1
f [k]
o[k](o[k] + 1) ,
(19)
where o[k], as before, is the number of outlinks from page k
in the local domain. Note that the last two terms, y and z
are not dependent on the current global node j. Given the
function h
j
(f ) =
y + ( ~
u
T
j
f )z
1
, the quantity
f
j
- f
1
120
Research Track Paper
can be expressed as
f
j
- f
1
=
k
x[k] + y[k] + ( ~
u
T
j
f )z[k]
=
k:x[k]=0
y[k] + ( ~
u
T
j
f )z[k]
+
k:x[k]=0
x[k] + y[k] + ( ~
u
T
j
f )z[k]
=
h
j
(f )
k
:x[k]=0
y[k] + ( ~
u
T
j
f )z[k]
+
k:x[k]=0
x[k] + y[k] + ( ~
u
T
j
f )z[k] .(20)
If we can compute the function h
j
in linear time, then we
can compute each value of
f
j
- f
1
using an additional
amount of time that is proportional to the number of non-zero
components in x. These optimizations are carried out
in Algorithm 3. Note that (20) computes the difference between
all components of f and f
j
, whereas our node selection
criteria, given in (7), is restricted to the components
corresponding to nodes in the original local domain L.
Let us examine Algorithm 3 in more detail. First, the
algorithm computes the outlink counts for each page in the
local domain. The algorithm then computes the quantity
~
u
T
j
f for each known global page j. This inner product can
be written as
(1
- ) 1
+ 1 +
k:(k,j)F
out
f [k]
o[k] + 1 ,
where the second term sums over the set of local pages that
link to page j. Since the total number of edges in F
out
was
assumed to have size O( ) (recall that
is the number of
pages in F ), the running time of this step is also O( ).
The algorithm then computes the vectors y and z, as
given in (17) and (18), respectively.
The L
1
NormDiff
method is called on the components of these vectors which
correspond to the pages in L, and it estimates the value of
E
L
(y + ( ~
u
T
j
f )z)
1
for each page j. The estimation works
as follows. First, the values of ~
u
T
j
f are discretized uniformly
into c values
{a
1
, ..., a
c
}. The quantity E
L
(y + a
i
z)
1
is
then computed for each discretized value of a
i
and stored in
a table. To evaluate
E
L
(y + az)
1
for some a
[a
1
, a
c
],
the closest discretized value a
i
is determined, and the corresponding
entry in the table is used. The total running time
for this method is linear in
and the discretization parameter
c (which we take to be a constant). We note that if exact
values are desired, we have also developed an algorithm that
runs in O( log ) time that is not described here.
In the main loop, we compute the vector x, as defined
in equation (16). The nested loops iterate over the set of
pages in F that are siblings of page j. Typically, the size
of this set is bounded by a constant. Finally, for each page
j, the scores vector is updated over the set of non-zero
components k of the vector x with k
L. This set has
size equal to the number of local siblings of page j, and is
a subset of the total number of siblings of page j. Thus,
each iteration of the main loop takes constant time, and the
total running time of the main loop is O( ). Since we have
assumed that the size of F will not grow larger than O(n),
the total running time for the algorithm is O(n).
Algorithm 3:
Node Selection via Stochastic
Complementation.
SC-Select
(F , F
out
, f , k)
Input:
F : zero-one adjacency matrix of size corresponding
to the current local subgraph, F
out
: zero-one
outlink matrix from F to global subgraph, f : PageRank
of F , k: number of pages to return
Output: pages: set of k pages to crawl next
{Compute outlink sums for local subgraph}
foreach (page j
F )
o[j]

k:(j,k)F
F [j, k]
end
{Compute scalar ~u
T
j
f for each global node j }
foreach (page j
F
out
)
g[j]
(1 - )
1
+1
foreach (page k : (k, j)
F
out
)
g[j]
g[j] +
f[k]
o[k]+1
end
end
{Compute vectors y and z as in (17) and (18) }
y  -(1 - )
e
( +1)
z  (1 - w)
-1
~
s
{Approximate y + g[j]  z
1
for all values g[j]
}
norm diffs
L
1
NormDiffs
(g, E
L
y, E
L
z)
foreach (page j
F
out
)
{Compute sparse vector x as in (19)}
x  0
foreach (page k : (k, j)
F
out
)
foreach (page k : (k, k )
F ))
x[k ]
x[k ] f
[k]
o[k](o[k]+1)
end
end
x  x
scores[j]
norm diffs[j]
foreach (k : x[k] &gt; 0 and page k
L)
scores[j]
scores[j] - |y[k] + g[j]  z[k]|
+
|x[k]+y[k]+g[j]z[k])|
end
end
Return k pages with highest scores
5.2.2
PageRank Flows
We now present an intuitive analysis of the stochastic
complementation method by decomposing the change in PageRank
in terms of `leaks' and `flows'. This analysis is motivated
by the decomposition given in (15). PageRank `flow' is
the increase in the local PageRanks originating from global
page j. The flows are represented by the non-negative vector
( ~
u
T
j
f )z (equations (15) and (18)). The scalar ~
u
T
j
f can be
thought of as the total amount of PageRank flow that page
j has available to distribute. The vector z dictates how the
flow is allocated to the local domain; the flow that local
page k receives is proportional to (within a constant factor
due to the random surfer vector) the expected number of its
inlinks.
The PageRank `leaks' represent the decrease in PageRank
resulting from the addition of page j.
The leakage can
be quantified in terms of the non-positive vectors x and
y (equations (16) and (17)). For vector x, we can see from
equation (19) that the amount of PageRank leaked by a
local page is proportional to the weighted sum of the Page-121
Research Track Paper
Ranks of its siblings. Thus, pages that have siblings with
higher PageRanks (and low outlink counts) will experience
more leakage. The leakage caused by y is an artifact of the
random surfer vector.
We will next show that if only the `flow' term, ( ~
u
T
j
f )z,
is considered, then the resulting method is very similar to
a heuristic proposed by Cho et al. [6] that has been widely
used for the "Crawling Through URL Ordering" problem.
This heuristic is computationally cheaper, but as we will see
later, not as effective as the Stochastic Complementation
method.
Our node selection strategy chooses global nodes that
have the largest influence (equation (7)). If this influence is
approximated using only `flows', the optimal node j

is:
j

=
argmax
j
E
L
~
u
T
j
f z
1
=
argmax
j
~
u
T
j
f E
L
z
1
=
argmax
j
~
u
T
j
f
=
argmax
j
(D
F
+ diag(u
j
))
-1
u
j
+ (1
- ) e
+ 1 , f
=
argmax
j
f
T
(D
F
+ diag(u
j
))
-1
u
j
.
The resulting page selection score can be expressed as a sum
of the PageRanks of each local page k that links to j, where
ea
A Dependability Perspective on Emerging Technologies
ABSTRACT
Emerging technologies are set to provide further provisions for 
computing in times when the limits of current technology of 
microelectronics become an ever closer presence. A technology 
roadmap document lists biologically-inspired computing and 
quantum computing as two emerging technology vectors for novel 
computing architectures <A href="5.html#12">[43]. But the potential benefits that will 
come from entering the nanoelectronics era and from exploring 
novel nanotechnologies are foreseen to come at the cost of 
increased sensitivity to influences from the surrounding 
environment. This paper elaborates on a dependability perspective 
over these two emerging technology vectors from a designer's 
standpoint. Maintaining or increasing the dependability of 
unconventional computational processes is discussed in two 
different contexts: one of a bio-inspired computing architecture 
(the Embryonics project) and another of a quantum computational 
architecture (the QUERIST project).

Categories and Subject Descriptors
B.8.1 [Performance and Reliability]: Reliability, Testing, and 
Fault-Tolerance. 
C.4 [Performance of Systems]: Fault-Tolerance, Reliability, 
Availability, and Serviceability.

General Terms
Design, Reliability, Theory.

INTRODUCTION
High-end computing has reached nearly every corner of our 
present day life, in a variety of forms taylored to accommodate 
either general purpose or specialized applications. Computers
may be considerred as fine exponents of the present days' 
technological wave ­ if not their finest, they certainly do count as 
solid, indispensable support for the finest.
From the very beginning of the computing advent, the main target 
was squeezing out any additional performance. The inception 
period was not always trouble-free, accurate computation results 
being required at an ever faster pace on a road that has become 
manifold: some applications do require computational speed as a 
top priority; others are set for the highest possible dependability, 
while still delivering sufficient performance levels.
Several definitions for dependability have been proposed: "the 
ability of a system to avoid service failures that are more frequent 
or more severe than is acceptable"  <A href="5.html#11">[2], or "the property of a 
computer system such that reliance can justifiably be placed on 
the service it delivers"  <A href="5.html#11">[9]<A href="5.html#12">[45]. Dependability is therefore a 
synthetic term specifying a qualitative system descriptor that can 
generally be quantified through a list of attributes including 
reliability, fault tolerance, availability, and others. 
In real world, a dependable system would have to operate 
normally over extended periods of time before experiencing any 
fail (reliability, availability) and to recover quickly from errors 
(fault tolerance, self-test and self-repair). The term "acceptable" 
has an essential meaning within the dependability's definition, 
setting the upper limits of the damages that can be supported by 
the system while still remaining functional or computationally 
accurate. A dependability analysis should take into consideration 
if not quantitative figures for the acceptable damage limit, at least 
a qualitative parameter representation for its attributes. 
Dependable systems are therefore crucial for applications that 
prohibit or limit human interventions, such as long-term exposure 
to aggressive (or even hostile) environments. The best examples 
are long term operating machines as required by managing deep-underwater/nuclear
activities and outer space exploration. 
There are three main concerns that should be posed through a 
system's design in order to achieve high dependability <A href="5.html#12">[42]: 
1.

Specifying the dependability requirements: selecting the 
dependability requirements that have to be pursued in 
building the computing system, based on known or assumed 
goals for the part of the world that is directly affected by the 
computing system;

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee.
2.

Designing and implementing the computing system so as to 
achieve the dependability required. However, this step is hard 
to implement since the system reliability cannot be satisfied
CF'06, May 3­5, 2006, Ischia, Italy. 
Copyright 2006 ACM 1-59593-302-6/06/0005...$5.00.

187
simply from careful design. Some techniques can be used to 
help to achieve this goal, such as using fault injection to 
evaluate the design process.
3.

Validating a system: gaining confidence that a certain 
dependability requirement/goal has been attained.
This paper will address these main concerns through an attempt to 
provide an in-depth view over modern computing directions and 
paradigms, which we consider to be representative for the efforts 
involved in improving overall dependability.
1.1

Motivations
We have listed some of the applications of dependable computing 
systems as linked to activities that take place in special 
environments, such as deep underwater or outer space. At a very 
first sight, these applications would appear specific enough to not 
encourage a specific design for dependability approach in 
computing. However, evidence suggest this is hardly the case; on 
the contrary, it is difficult to imagine a domain left unconquered 
by computer systems during times when industrial, transport, 
financial services and others do rely heavily on accurate computer 
operation at any given moment. If computer innacuracies could be 
more easily overlooked at home, professional environments 
cannot accept such missbehaviors. 
Yet the recent history of computing provides evidence that 
dependability is not a sine qua non feature. During their life 
cycle, electronic devices constantly suffer a number of influences 
that manifest predominantly over transient regimes, which in turn 
introduce a variety of errors unified in the literature under the 
name of transient faults, soft errors or single event upsets (SEUs). 
The rate electronic devices are affected with is known under the 
term of soft error rate or simply SER and is measured in fails per 
unit time. Because it relies on transient phenomena due to 
changing states and logical values, digital electronics makes up 
for a special category that is also affected by soft errors. No 
matter the name they are referred under, these errors affect the 
computing processes and are due to electromagnetic noise and/or 
<A href="5.html#12">external radiations rather than design or manufacturing flaws [28]. 
One cause at the origin of soft fails affecting digital devices is 
known to be due to radioactive decay processes. Radioactive 
isotopes, widely used for a range of purposes, might contaminate 
semiconductor materials leading to soft errors; evidence is 
available throughout the literature, both by empirical observations 
and experimental results <A href="5.html#12">[20]. Consequently, cosmic rays, 
containing a broad range of energized atomic/subatomic particles 
may lead to the appearance of soft fails. 
Computers therefore are susceptive to soft errors, an issue that 
will potentially become essential with the advent of emerging 
technologies. As acknowledged by the International Technology 
Roadmap for Semiconductors (ITRS), issued at the end of 2004 
<A href="5.html#12">[43], the microelectronics industry faces a challenging task in 
going to and beyond 45nm scale in order to address "beyond 
CMOS" applications. Scaling down the technology will enable an 
extremely large number of devices to be integrated onto the same 
chip. However, the great challenge will be to ensure the new 
<A href="5.html#11">devices will be operational at this scale [6], since they will exhibit 
a sensitive behavior to soft fails. In order to address the negative 
effects brought by technology scaling, it is to be expected that 
significant control resources will need to be implem<A href="5.html#11">ented [3].
Another challenging aspect concerning emerging technologies is 
to match the newly developed device technologies with new 
system architectures, a synergistic/collaborative development of 
the two being seen as likely to be very rewarding. The potential of 
biologically-inspired and quantum computing architectures is 
acknowledged by the ITRS report on emerging technologies <A href="5.html#12">[43] 
(see  <A href="5.html#2">Figure 1). This paper will investigate the relevance of soft 
fails and attempt to provide means of harnessing their negative 
effects on modern computing in the context of biologically-inspired
and quantum computing architectures.

Figure 1: Bio-inspired and quantum computing are
acknowledged as architectural technology vectors in emerging
technologies <A href="5.html#12">[43]
1.2

Paper Outline
This paper is structured as follows. Section <A href="5.html#3">2 will address the first 
main concern, that is, specifying and selecting dependability 
requirements that will have to be pursued when building a 
computational platform. Parameters that describe and quantify 
dependability attributes, such as reliability, will be introduced, 
with a highlight on their accepted models and their issues. A 
particular consideration will be given to the failure rate parameter, 
which is the basis of all reliability analyses. 
Section  <A href="5.html#7">3 will approach some of the means for design for 
dependability; it will therefore elaborate upon two emerging 
technology vectors, as seen by the ITRS report <A href="5.html#12">[43], which define 
two novel architectures, namely biologically-inspired (or bio-inspired
) and quantum computing. We will introduce two projects 
and their corresponding architectures, called Embryonics (as a 
biologically-inspired computing platform) and QUERIST (as a 
quantum computing platform designed to allow and study error 
injection). These two architectures are representative for the 
coming age of nano-computing, where computational processes 
take place as encoded at the very inner core level of matter, be it 
semiconductor material (for nanoelectronics, targetted here by the 
Embryonics project) or atomic scale dynamics (for quantum 
computing, targetted here by the QUERIST project). This section 
will then introduce dependability aspects within bio-inspired 
computing (the Embryonics project being investigated in 
SubSection  <A href="5.html#7">3.1) and within quantum computing (the QUERIST 
<A href="5.html#10">project being investigated in SubSection 3.2). 
Finally<A href="5.html#10">, Section 4 will present the conclusions and prospects for 
designing emerging technology dependable computing systems, 
as we see them.
188
DEPENDABILITY ATTRIBUTES
An important dependability attribute for any given system lies in 
its capacity to operate reliably for a given time interval, knowing 
that normal operation was delivered at initial time <A href="5.html#11">[8]. Reliability 
functions are modelled as exponential functions of parameter , 
which is the failure rate. The reliability of a system is the 
consequence of the reliability of all of its subsystems. The 
heterogeneity of the system leads to a difficult quantitative 
assessment of its overall reliability; moreover, estimating the 
reliability functions is further made difficult because formal 
rigour is not commercially available, this being kept under 
military mandate <A href="5.html#12">[44].
The failure rate for a given system can be modelled as a function 
of the failure rates of its individual subsystems, suggestions being 
present in the MIL-HDBC-217 document, which is publicly 
available  <A href="5.html#12">[44]. However, this document has been strongly 
criticized for its failure rate estimations based on the Arrhenius 
model, which relates the failure rate to the operating temperature:

B
E K T
Ke

=
(1)
where  K is a constant, K
B
is Boltzmann's constant, T is the
absolute temperature and E is the "activation energy" for the 
process
<A href="5.html#11">. Quantitative values for failure rates show significant
differences between those predicted using MIL-HDBC-217 and 
those from testing real devices (see
). There are two
conclusions that can be drawn from this:
B
<A href="5.html#11">[18]
<A href="5.html#3">Figure 2
1.

quantitative estimations for failure rate values are strongly 
dependant on the quality of information used; unfortunately, 
current reliable information about electronic devices is known 
to be lacking <A href="5.html#12">[44];
2.

despite differences between predicted and real values, the 
MIL-HDBC-217 methodology can be useful for qualitative 
analyses in order to take decisions regarding sub-system parts 
that should benefit from improved designs.

Figure 2. Predicted vs real failure rates plotted against
temperature <A href="5.html#11">[18]
So far the failure rate of digital devices has been considerred as 
due to internal causes. However, this is not always the case, soft 
fails being equally present due to the aggressive influences of the 
external environment, which <A href="5.html#12">also have to be modelled [22]. The 
external envirnment features highly dynamic changes in its 
parameters, which will eventually affect the normal operation of 
digital devices that lack sufficient protection or ability to adapt.
Ideally, computing devices would behave in a consistent and 
accurate manner regardless of fluctuations in environmental 
parameters. This is either a consequence of soft error mitigation 
techniques or due to flexible hardware/software functionality that 
allow the system as a whole to adapt to environamental changes 
and tolerate induced faults.
While certain soft error mitigation techniques are available, the 
technology scaling towards nanoelectronics affects their 
efficiency by integrating a larger number of devices per chip 
(which requires a larger amount of redundant/control logic or 
other measures), which feature, at the same time, smaller 
dimensions (which renders an electronic device much more 
senzitive to the influence of stray energetic particles that reach it 
as part of cosmic rays). Both aspects are involved in the 
development of the two emerging technology vectors mentioned 
<A href="5.html#2">in SubSection 1.1, although having slightly different motivations: 
while the nature of the quantum environment prohibits precise 
computation in the absence of fault tolerance techniques, such 
techniques are targetted by bio-inspired computing as means of 
improving the dependability of a computing platform.
2.1

Bio-Inspired Computing
If living beings may be considered to fulfill computational tasks, 
then Nature is the ultimate engineer: each of the living beings 
exhibit solutions that were successfully tested and refined in such 
ways human engineers will never afford. One reason is time: the 
testing period coinciding with the very existence of life itself. 
Another reason is variety and complexity: Nature has found and 
adapted a variety of solutions to address complex survivability 
issues in a dynamically changing environment. No matter how 
Nature approached the process of evolution, engineering could 
perhaps benefit most from drawing inspiration from its 
mechanisms rather from trying to develop particular techniques.
Bio-inspired computing is not a new idea. John von Neumann was 
preoccupied to design a machine that could replicate itself and 
was quite interested in the study of how the behavior of the 
human brain could be implemented by a computer <A href="5.html#11">[13][14]. He 
also pioneered the field of dependable computing by studying the 
possibility of building reliable machines out of unreliable 
components  <A href="5.html#11">[15]. Unfortunately, the dream of implementing his 
self-reproducing automata could not become true until the 1990s, 
when massively programmable logic opened the new era of 
reconfigurable computing.
But when trying to adapt nature's mechanisms in digital devices, 
it becomes most evident that biological organisms are rightfully 
the most intricate structures known to man. They continuously 
demonstrate a highly complex behavior due to massive, parallel 
cooperation between huge numbers of relatively simple elements, 
the cells. And considering uncountable variety of living beings, 
with a life span up to several hundreds (for the animal regnum) or 
even thousands (for the vegetal regnum) of years, it seems nature 
is the closest spring of inspiration for designing dependable, fault 
tolerant systems.
Investigating the particularities of natural systems, a taxonomy of 
three categories of processes<A href="5.html#12"> can be identified [32]:
1.

Phylogenetic processes constitute the first level of 
organization of the living matter. They are concerned with the 
temporal evolution of the genetic heritage of all individuals,
189
therefore mastering the evolution of all species. The 
phylogenetic processes rely on mechanisms such as 
recombination and mutation, which are essentially 
nondeterministic; the error rate ensures here nature's 
diversity.
2.

Ontogenetic processes represent the second level of 
organization of the living matter. They are also concerned 
with the temporal evolution of the genetic heritage of, in this 
case, a single, multicellular individual, therefore mastering an 
individual's development from the stage of a single cell, the 
zygote, through succesive cellular division and specialization, 
to the adult stage. These processes rely on deterministic 
mechanisms; any error at this level results in malformations.
3.

Epigenetic processes represent the third level of organization 
of the living matter. They are concerned with the integration 
of interactions with the surrounding environment therefore 
resulting in what we call learning systems.
This taxonomy is important in that it provides a model called POE 
(from Phylogeny, Ontogeny and Epigenesis) that inspires the 
combination of processes in order to create novel bio-inspired 
<A href="5.html#4">hardware (see Figure 3). We believe this is also important from a 
dependability engineering perspective, for the following reasons:
1.

Phylogenetic processes were assimilated by modern 
computing as evolutionary computation, including genetic 
algorithms and genetic programming. The essence of any 
genetic algorithm is the derivation of a solution space based 
on recombination, crossover and mutation processes that 
spawn a population of individuals, each encoding a possible 
solution. One may consider that each such step, with the 
exception of discovering the solution, is equivalent to a 
process of error injection, which in turn leads to wandering 
from the optimal solution (or class of solutions). However, 
genetic algorithms prove to be successful despite this error 
injection, the fitness function being responsible for the 
successful quantification of the significance of the "error". 
Therefore genetic computation is intrinsicaly resilient to 
faults and errors, largely due to the fact that they are part of 
the very process that generates the solutions.
2.

Ontogenetic processes have been implemented in digital 
hardware with modular and uniform architectures. Such an 
architecture enables the implementation of mechanisms 
similar to the cellular division and cellular differentiation that 
take place in living beings<A href="5.html#12"> [31]. These mechanisms bring the 
advantage of distributed and hierarchical fault tolerance 
strategies: the uniformity of the architecture also makes any 
module to be universal, that is, to be able to take over the role 
of any other damaged module.
3.

Epigenetic processes were assimilated by modern computing 
mainly as artificial neural networks (or ANNs) as inspired by 
the nervous system, and much less as inspired by the immune 
or endocrine systems from superior multicellular living 
beings. ANNs are known to have a generalization capacity, 
that is, to respond well even if the input patterns are not part 
of the patterns used during the learning phase. This means that 
ANNs possess a certain ability to tolerante faults, whether 
they manifest at the inputs or inside their intenal architecture.
With the advent of field programmable logic (of which the most 
salient representative are the FPGAs) it is now possible to change 
hardware functionality through software, thus allowing 
information to govern matter in digital electronics. This is not 
dissimilar to what happens in nature: information coded in DNA 
affects the development of an organism. A special kind of such 
digital devices that change dynamically their behavior are known 
as evolvable or adaptive hardware; they are bio-inspired 
computing systems whose behaviors may change according to 
computational targets, or, if harsh or unknown environments are 
to be explored, for the purpose of maximizing dependability.

Figure 3. The POE model of bio-inspired systems<A href="5.html#12"> [32]
2.2

Quantum Computing
Error detection and correction techniques are vital in quantum 
computing due to the destructive effect of the environment, which 
therefore acts as an omnipresent error generator. Error detection 
and correction must provide a safe recovery process within 
quantum computing processes through keeping error propagation 
under control. Without such dependability techniques there could 
be no realistic prospect of an operational quantum computational 
device <A href="5.html#11">[19].  
There are two main sources of errors: the first is due to the 
erroneous behavior of the quantum gate, producing the so-called 
processing errors; the second is due to the macroscopic 
environment that interacts with the quantum state, producing the 
storing and transmitting errors. 
The consistency of any quantum computation process can be 
destroyed by innacuracies and errors if the error probability in the 
basic components (qubits, quantum gates) excedes an accuracy 
threshold. This is a critical aspect since the microscopic quantum 
states are prone to frequent errors. 
The main error source is the decoherence<A href="5.html#11"> effect [16]. The 
environment is constantly attempting to measure the sensitive 
quantum superposition state, a phenomenon that cannot be 
avoided technologically since it is not (yet) possible to isolate 
them perfectly. The superposition state will decay through 
measuring and will therefore become a projection of the state 
vector onto a basis vector (or eigenstate). The most insidious 
error, however, appears when decoherence affects the quantum 
amplitudes without destroying them; this is similar to small 
analog errors. Issues stated above are solved, on one hand, 
through intrinsic fault tolerance by technological implementation 
<A href="5.html#11">(topological interactions [1]) and, on the other hand, by error 
correcting techniques at the unitary (gate network) level. We will 
focus on the error detecting and correcting techniques, which are 
difficult to approach due to quantum constraints: the useful state
190
can neither be observed (otherwise it will decohere), nor can it be 
cloned.
2.2.1

Background
<A href="5.html#11">As expressed in bra-ket notation [16], the qubit is a normalized 
vector in some Hilbert space
,
2
H
{
}
0 , 1 being the orthonormal
basis:
0
1
0
1
a
a

=
+
(
are the so-called quantum
amplitudes, representing the square root of the associated 
measurement probabilities for the eigenstates
0
1
,
a a
C
0 and  1
respectively, with
0
1
2
2
1
a
a
+
= ). Therefore, the qubit can be
affected by 3 types of errors: 
Bit flip errors are somewhat similar to classical bit flip errors. For 
a single qubit things are exactly the same as in classical 
computation: 0
1 , 1
0 . For 2 or more qubits, flip errors
affecting the state may modify it or leave it unchanged. For 
instance, if we consider the so-called cat state
(
1
00
11
2
Cat

=
+
)
<A href="5.html#11">[19], and the first qubit is affected by a
bit flip error, the resulting state will be
(
)
1 10 01
2
Cat

+
.
But, if both qubits are affected by bit flips, there will be no
change in the state:
(
)
1 11 00
2
Cat
Cat


+
=
.
Phase errors affect the phase of one of the qubit's amplitudes and 
is expressed as  0
0 , 1
- 1 . This type of error is very
dangerous, due to its propagation behavior but it only makes 
sense when dealing with superposition states. If we consider an 
equally weighted qubit superposition state and inject a phase
error, this results in
(
)
(
)
1
1
0
1
0
1
2
2
+

.
There is a strict correspondence between bit flip and phase error 
types due to the way they map onto Hilbert spaces with the same 
dimension but different basis. The bit flip is an error from the

space with basis
2
H
{
}
0 , 1 , whereas the phase error appears in the
same space with basis
(
)
(
)
1
1
0
1 ,
0
1
2
2


+

or
{
}
,
+ 1
a
.
The space basis conversion, in this case, is made by applying the 
Hadamard transform; <A href="5.html#5">Figure 4 shows an example of transforming 
a bit flip error into a phase error (A, and vice versa (B.
Small amplitude errors: amplitudes
of the quantum bit
can be affected by small errors, similar to analog errors. Even if 
such an error does not destroy the superposition and conserves the 
value of the superposed states, small amplitude errors could 
accumulate over time, eventually ruining the computation. In 
order to avoid this situation, specific methodologies for digitizing 
small errors are used to reduce them to a non-fault or a bit-flip
0
and
a
<A href="5.html#11">[19]. 
Due to the quantum physics laws, fault tolerance techniques have 
to comply with the following computational constraints: 
­

The observation destroys the state. Since observation is 
equivalent to measurement, this leads to destroying the 
useful state superposition.
­

Information copying is impossible. Quantum physics renders 
the cloning of a quantum state impossible, meaning that a 
quantum state cannot be copied correctly. Therefore 
quantum error correction must address the following 
problems:
Non-destructive measurement. Despite the first constraint it is 
necessary to find a way to measure the encoded information 
without destroying it. Because the encoded state cannot  be 
measured directly, one needs to properly prepare some scratch 
(ancilla) qubits, which can then be measured. 
 Fault-tolerant  recovery. Due to the high error rate in quantum 
computational devices, it is likely that the error recovery itself 
will be affected by errors. If the recovery process is not fault-tolerant
, then any error coding becomes useless. 
Phase error backward propagation. If we consider the XOR gate 
from  <A href="5.html#5">Figure 5(A, a flip error affecting the target qubit (b) will 
propagate backwards and also affect the source qubit. This is due 
to the gate network equivalence from <A href="5.html#5">Figure 5(B and the basis 
transformation described by<A href="5.html#5"> Figure 4.

Figure 4. Correspondence between bit flip and phase errors

Figure 5. (A The backward propagation of a phase error for
the XOR gate; (B Gate network equivalence
In order to deal with the problems described the next strategies 
have to be followed: 
Digitizing small errors. The presence of small errors is not a 
major concern, as they can be digitized using a special technique 
based on measuring auxiliary<A href="5.html#11"> (ancilla) qubits [19]. 
Ancilla usage. Since qubit cloning is impossible, a majority 
voting strategy is difficult to implement. However, by using 
ancilla qubits, the eigenstate information can be duplicated inside 
the existing superposition, resulting in the entanglement of the 
ancilla with the useful data. Because any measurement performed 
on the ancilla could have repercussions on the useful qubits, the 
appropriate strategy will employ special coding for both data 
qubits and ancilla (data errors only will be copied onto the 
ancilla), followed by the computation of an error syndrome, 
which has to be obtained through measuring the ancilla (see 
<A href="5.html#6">Figure 6). 
Avoiding massive spreading of phase errors. As shown 
previously, a phase error on the target qubit will propagate on all 
source qubits. The solution is to use more ancilla qubits as targets, 
so that no ancilla qubit is used more than once.
191

Figure 6. Fault-tolerant procedure with ancilla qubits
Ancilla and syndrome accuracy. Setting the ancilla code to some 
known quantum state could be an erroneous process. Computing 
the syndrome is also prone to errors. Hence, on one hand, one has 
to make sure that the ancilla qubits are in the right state by 
verifying and recovering them if needed; on the other hand, in 
order to have a reliable syndrome, it must be computed 
repeatedly. 
Error recovery. As the small errors can be digitized (therefore, 
they are either corrected or transformed into bit flip errors), the 
recovery must deal only with bit flip and phase errors. A state that 
needs to be recovered is described by:
0
1
1
0
0
1
0
1
1
0
0
1
if no error occurs
0
1
for a flip error
0
1
0
1
for a phase error
0
1
for both flip and phase errors
error
a
a
a
a
a
a
a
a
a
a

+

+

+




.


Correcting a bit flip error means applying the negation unitary
transformation
to the affected qubit. To
correct phase and combined errors, the following unitary 
operators will have to be applied respectively:
.
0 1
1 0
N
x
U


=
=
1
0
0
,
0
1
0
Z
Y
N
Z
i
U
U
U U
i



=
=

=






2.2.2

Quantum Error Correcting Codes
Quantum error coding and correcting (QECC) is performed with 
special coding techniques inspired from the classic Hamming 
codes. The classical error coding is adapted so that it becomes 
suitable for the quantum strategy, allowing only the ancilla qubits 
to be measured. 
The state-of-the-art in QECC is represented by the stabilizer 
encoding, a particular case being the Steane codes (the Shor codes 
may<A href="5.html#12"> also be used [29]). Steane's 7-qubit code is a single error 
correcting code inspired from classical Hamming coding and can 
be adapted for ancilla coding as well. Therefore it cannot recover 
from two identical qubit faults, but it can recover from a bit flip a 
phase flip. The Steane 7-qubit coding of  0  and  1  consists of 
an equally weighted superposition of all the valid Hamming 7-bit 
words with an even  and odd number of 1s, respectively:

(
)
,
0 1 2 3 0 1 2
0 1 2 3 0 1 2
32
32
1
0
2
1
0000000
0010111
0101110
0111001
2
1001011
1011100
1100101
1110010
u u u u c c c
S
even
u u u u c c c






=
=
+
+
+
+
+
+
+

(
)
,
0 1 2 3 0 1 2
0 1 2 3 0 1 2
32
32
1
1
2
1 1111111 1101000 1010001 1000110
2
0110100
0100011
0011010
0001101
u u u u c c c
S
odd
u u u u c c c






=
=
+
+
+
+
+
+
+

+
(3)
Applying the Steane coding on an arbitrary given quantum state
0
1
0
a
a

=
+
1  transforms it into
0
1
0
1
S
S
a
a

=
+
S
. This
code was designed to correct bit-flip errors, but by changing the 
basis (through a Hadamard transform) the phase error transforms 
into a bit flip error, which can then be corrected:
(
)
(
)
1
0
0
0
1
2
1
1
1
0
1
2
S
S
S
S
S
S
S
H
H
=

=
+
=

=
S
(4)
2.2.3

Fault Tolerance Methodologies
Quantum error-correcting codes exist for r errors,
,   1
r
r


N
.
Therefore a non-correctable error occurs if a number of
1
r
+
errors occur simultaneously before the recovery process. 
If the probability of a quantum gate error or storage error in the 
time unit is of order

, then the probability of an error affecting
the processed data block becomes of order
1
r

+
, which is
negligible if r is sufficiently large. However, by increasing r the 
safe recovery also becomes more complex and hence prone to 
errors: it is possible that
1
r
+  errors accumulate in the block
before the recovery is performed. 
Considering the relationship between r and the number of 
computational steps required for computing the syndrome is 
polynomial of the order
p
r
. It was proven  that in order to reduce
as much as possible the error probability r must be chosen so that
1
1
~
p
r
e


<A
href="5.html#11">[7][19]. By consequence, if attempting to execute N
cycles of error correction without any r+1 errors accumulating
before the recovery ends, then
1
~ exp
p
N





. Therefore the
accuracy degree will be of the form
(
)
~ log
p
N


, which is
better th
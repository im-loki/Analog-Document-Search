Beyond PageRank: Machine Learning for Static Ranking
ABSTRACT
Since  the  publication  of  Brin  and  Page's  paper  on  PageRank, 
many in the Web community have depended on PageRank for the 
static  (query-independent)  ordering  of  Web  pages.  We  show  that 
we can significantly outperform PageRank using features that are 
independent  of  the  link  structure  of  the  Web.  We  gain  a  further 
boost  in  accuracy  by  using  data  on  the  frequency  at  which  users 
visit  Web  pages.  We  use  RankNet,  a  ranking  machine  learning 
algorithm,  to  combine  these  and  other  static  features  based  on 
anchor  text  and  domain  characteristics.  The  resulting  model 
achieves  a  static  ranking  pairwise  accuracy  of  67.3%  (vs.  56.7% 
for PageRank or 50% for random).

Categories and Subject Descriptors
I.2.6  [Artificial  Intelligence]:  Learning.  H.3.3  [Information 
Storage and Retrieval
]: Information Search and Retrieval.

General Terms
Algorithms, Measurement, Performance, Experimentation.

INTRODUCTION
Over  the  past  decade,  the  Web  has  grown  exponentially  in  size. 
Unfortunately,  this  growth  has  not  been  isolated  to  good-quality 
pages.  The  number  of  incorrect,  spamming,  and  malicious  (e.g., 
phishing) sites has also grown rapidly. The sheer number of both 
good and bad pages on the Web has led to an increasing reliance 
on  search  engines  for  the  discovery  of  useful  information.  Users 
rely  on  search  engines  not  only  to  return  pages  related  to  their 
search  query,  but  also  to  separate  the  good  from  the  bad,  and 
order results so that the best pages are suggested first.
To  date,  most  work  on  Web  page  ranking  has  focused  on 
improving the ordering of the results returned to the user (query-dependent
ranking, or dynamic ranking). However, having a good 
query-independent  ranking  (static  ranking)  is  also  crucially 
important  for  a  search  engine.  A  good  static  ranking  algorithm 
provides numerous benefits:
·

Relevance
:  The  static  rank  of  a  page  provides  a  general
indicator  to  the  overall  quality  of  the  page.  This  is  a 
useful input to the dynamic ranking algorithm.
·

Efficiency
:  Typically,  the  search  engine's  index  is
ordered by static rank. By traversing the index from high-quality
 to  low-quality  pages,  the  dynamic  ranker  may 
abort  the  search  when  it  determines  that  no  later  page 
will  have  as  high  of  a  dynamic  rank  as  those  already 
found.  The  more  accurate  the  static  rank,  the  better  this 
early-stopping  ability,  and  hence  the  quicker  the  search 
engine may respond to queries.
·

Crawl Priority
: The Web grows and changes as quickly
as search engines can crawl it. Search engines need a way 
to prioritize their crawl--to determine which pages to re-crawl
,  how  frequently,  and  how  often  to  seek  out  new 
pages.  Among  other  factors,  the  static  rank  of  a  page  is 
used  to  determine  this  prioritization.  A better static rank 
thus provides the engine with a higher quality, more up-to
-date index.
Google  is  often  regarded  as  the  first  commercially  successful 
search  engine.  Their  ranking  was  originally  based  on  the 
PageRank  algorithm  [5][27].  Due  to  this  (and  possibly  due  to 
Google's  promotion  of  PageRank  to  the  public),  PageRank  is 
widely regarded as the best method for the static ranking of Web 
pages.
Though  PageRank  has  historically been thought to perform quite 
well,  there  has  yet  been  little  academic  evidence  to  support  this 
claim.  Even  worse,  there  has  recently  been  work  showing  that 
PageRank may not perform any better than other simple measures 
on  certain  tasks.  Upstill  et  al.  have  found  that  for  the  task  of 
finding home pages, the number of pages linking to a page and the 
type of URL were as, or more, effective than PageRank [32]. They 
found  similar  results  for  the  task  of  finding  high  quality 
companies  [31].  PageRank  has  also  been  used  in  systems  for 
TREC's  "very  large  collection"  and  "Web  track"  competitions, 
but with much less success than had been expected [17]. Finally, 
Amento  et  al.  [1]  found  that simple features, such as the number 
of pages on a site, performed as well as PageRank.
Despite  these,  the  general  belief  remains  among  many,  both 
academic  and  in  the  public,  that  PageRank  is  an  essential  factor 
for a good static rank. Failing this, it is still assumed that using the 
link structure is crucial, in the form of the number of inlinks or the 
amount of anchor text.
In this paper, we show there are a number of simple url- or page- 
based  features  that  significantly  outperform  PageRank  (for  the 
purposes  of  statically  ranking  Web  pages)  despite  ignoring  the
Copyright  is  held  by  the  International  World  Wide  Web  Conference 
Committee  (IW3C2).  Distribution  of  these  papers  is  limited  to 
classroom use, and personal use by others. 
WWW 2006, May 23­26, 2006, Edinburgh, Scotland. 
ACM 1-59593-323-9/06/0005.
707
structure  of the Web. We combine these and other static features 
using  machine  learning  to  achieve  a  ranking  system  that  is 
significantly  better  than  PageRank  (in  pairwise  agreement  with 
human labels).
A  machine  learning  approach  for  static  ranking  has  other 
advantages  besides  the  quality  of  the  ranking.  Because  the 
measure consists of many features, it is harder for malicious users 
to  manipulate  it  (i.e.,  to  raise  their  page's  static  rank  to  an 
undeserved level through questionable techniques, also known as 
Web  spamming).  This  is  particularly  true  if  the  feature  set  is  not 
known. In contrast, a single measure like PageRank can be easier 
to  manipulate  because  spammers  need  only  concentrate  on  one 
goal:  how  to  cause  more  pages  to  point  to  their  page.  With  an 
algorithm  that  learns,  a  feature  that  becomes  unusable  due  to 
spammer  manipulation  will  simply  be  reduced  or  removed  from 
the  final  computation  of  rank.  This  flexibility  allows  a  ranking 
system to rapidly react to new spamming techniques.
A machine learning approach to static ranking is also able to take 
advantage  of  any  advances  in  the  machine  learning  field.  For 
example,  recent  work  on  adversarial  classification  [12]  suggests 
that  it  may  be  possible  to  explicitly  model  the  Web  page 
spammer's (the adversary) actions, adjusting the ranking model in 
advance  of  the  spammer's  attempts  to  circumvent  it.  Another 
example  is  the  elimination  of  outliers  in  constructing  the  model, 
which  helps  reduce  the  effect  that  unique  sites  may  have  on  the 
overall  quality  of  the  static  rank.  By  moving  static  ranking  to  a 
machine  learning  framework,  we  not  only  gain  in  accuracy,  but 
also  gain  in  the  ability  to  react  to  spammer's  actions,  to  rapidly 
add  new  features  to  the  ranking  algorithm,  and  to  leverage 
advances in the rapidly growing field of machine learning.
Finally,  we  believe  there  will  be  significant  advantages  to  using 
this  technique  for  other  domains,  such  as  searching  a  local  hard 
drive  or  a  corporation's  intranet.  These  are  domains  where  the 
link  structure  is  particularly weak (or non-existent), but there are 
other domain-specific features that could be just as powerful. For 
example, the author of an intranet page and his/her position in the 
organization  (e.g.,  CEO,  manager,  or  developer)  could  provide 
significant  clues  as  to  the  importance  of  that  page.  A  machine 
learning approach thus allows rapid development of a good static 
algorithm in new domains.
This  paper's  contribution  is  a  systematic  study  of  static  features, 
including PageRank, for the purposes of (statically) ranking Web 
pages. Previous studies on PageRank typically used subsets of the 
Web that are significantly smaller (e.g., the TREC VLC2 corpus, 
used  by  many,  contains  only  19  million  pages).  Also,  the 
performance  of  PageRank  and  other  static  features  has  typically 
been  evaluated  in  the  context  of  a  complete  system  for  dynamic 
ranking, or for other tasks such as question answering. In contrast, 
we  explore  the  use  of  PageRank  and  other features for the direct 
task of statically ranking Web pages.
We first briefly describe the PageRank algorithm. In Section 3 we 
introduce  RankNet,  the  machine  learning  technique  used  to 
combine  static  features  into  a  final  ranking.  Section  4  describes 
the  static  features.  The  heart  of  the  paper  is  in  Section  5,  which 
presents  our  experiments  and  results.  We  conclude  with  a 
discussion of related and future work.
PAGERANK
The  basic  idea  behind  PageRank  is  simple:  a  link  from  a  Web 
page  to  another  can  be  seen  as  an  endorsement  of  that  page.  In
general, links are made by people. As such, they are indicative of 
the  quality  of  the  pages  to  which  they  point  ­  when  creating  a 
page, an author presumably chooses to link to pages deemed to be 
of  good  quality.  We  can  take  advantage  of  this  linkage 
information  to  order  Web  pages  according  to  their  perceived 
quality.
Imagine  a  Web  surfer  who  jumps  from  Web  page  to  Web  page, 
choosing  with  uniform  probability  which  link  to  follow  at  each 
step.  In  order  to  reduce  the  effect  of  dead-ends  or endless cycles 
the  surfer  will  occasionally  jump  to  a  random  page  with  some 
small  probability

,  or  when  on  a  page  with  no  out-links.  If
averaged  over  a  sufficient  number  of  steps,  the  probability  the 
surfer is on page j at some point in time is given by the formula:



+
=
j
i
i
i
P
N
j
P
B
F
)
(
)
1
(
)
(



(1)

Where F
i
is the set of pages that page i links to, and B
j
is the set of
pages that link to page j. The PageRank score for node j is defined 
as this probability: PR(j)=P(j). Because equation (1) is recursive, 
it must be iteratively evaluated until P(j) converges (typically, the 
initial distribution for P(j) is uniform). The intuition is, because a 
random  surfer  would  end  up  at  the  page  more  frequently,  it  is 
likely  a  better  page.  An  alternative  view  for  equation  (1)  is  that 
each  page  is  assigned  a  quality,  P(j).  A  page  "gives"  an  equal 
share of its quality to each page it points to.
PageRank  is  computationally  expensive.  Our  collection  of  5 
billion pages contains approximately 370 billion links. Computing 
PageRank  requires  iterating  over  these  billions  of  links  multiple 
times  (until  convergence).  It  requires  large  amounts  of  memory 
(or  very  smart  caching  schemes  that  slow  the  computation  down 
even  further),  and  if  spread  across  multiple  machines,  requires 
significant communication between them. Though much work has 
been  done  on  optimizing  the  PageRank  computation  (see  e.g., 
[25]  and  [6]),  it  remains  a  relatively  slow,  computationally 
expensive property to compute.
RANKNET
Much work in machine learning has been done on the problems of 
classification and regression. Let X={x
i
} be a collection of feature
vectors  (typically,  a  feature  is  any  real  valued  number),  and 
Y
={y
i
} be a collection of associated classes, where y
i
is the class
of  the  object  described  by  feature  vector  x
i
.  The  classification
problem is to learn a function f that maps y
i
=f(x
i
), for all i. When
y
i
is real-valued as well, this is called regression.
Static  ranking  can  be  seen  as  a  regression  problem.  If  we  let  x
i

represent  features  of  page  i,  and  y
i
be  a  value  (say,  the  rank)  for
each page, we could learn a regression function that mapped each 
page's  features  to  their  rank.  However,  this  over-constrains  the 
problem we wish to solve. All we really care about is the order of 
the pages, not the actual value assigned to them.
Recent  work  on  this  ranking  problem  [7][13][18]  directly 
attempts  to  optimize  the  ordering  of  the  objects,  rather  than  the 
value assigned to them. For these, let Z={&lt;i,j&gt;} be a collection of 
pairs of items, where item i should be assigned a higher value than 
item  j.  The  goal  of  the  ranking  problem,  then,  is  to  learn  a 
function f such that,
)
(
)
(
,
,
j
i
f
f
j
i
x
x
Z
&gt;



708
Note that, as with learning a regression function, the result of this 
process  is  a  function  (f)  that  maps  feature  vectors  to  real  values. 
This  function  can  still  be  applied  anywhere  that  a  regression-learned
 function  could  be  applied.  The  only  difference  is  the 
technique  used  to  learn  the  function.  By  directly  optimizing  the 
ordering of objects, these methods are able to learn a function that 
does a better job of ranking than do regression techniques.
We  used  RankNet  [7],  one  of  the  aforementioned  techniques  for 
learning  ranking  functions,  to  learn  our  static  rank  function. 
RankNet  is  a  straightforward  modification  to  the  standard  neural 
network  back-prop  algorithm.  As  with  back-prop,  RankNet 
attempts  to  minimize  the  value  of  a  cost  function  by  adjusting 
each  weight  in  the  network  according  to  the  gradient  of  the  cost 
function with respect to that weight. The difference is that, while a 
typical  neural  network  cost  function  is  based  on  the  difference 
between  the  network  output  and  the  desired  output, the RankNet 
cost function is based on the difference between a pair of network 
outputs.  That  is,  for  each  pair  of  feature  vectors  &lt;i,j&gt;  in  the 
training  set,  RankNet  computes  the  network  outputs  o
i
and  o
j
.
Since  vector  i  is  supposed  to  be  ranked  higher  than  vector  j,  the 
larger is o
j
-o
i
, the larger the cost.
RankNet  also  allows  the  pairs  in  Z  to  be  weighted  with  a 
confidence  (posed  as  the  probability  that  the  pair  satisfies  the 
ordering induced by the ranking function). In this paper, we used 
a  probability  of  one  for  all  pairs.  In  the  next  section,  we  will 
discuss the features used in our feature vectors, x
i
.
FEATURES
To  apply  RankNet  (or  other  machine  learning  techniques)  to  the 
ranking problem, we needed to extract a set of features from each 
page.  We  divided  our  feature  set  into  four,  mutually  exclusive, 
categories: page-level (Page), domain-level (Domain), anchor text 
and  inlinks  (Anchor),  and  popularity  (Popularity).  We  also 
optionally  used  the  PageRank  of  a  page  as  a  feature.  Below,  we 
describe each of these feature categories in more detail.
PageRank
We  computed  PageRank  on  a  Web  graph  of  5  billion  crawled 
pages  (and  20  billion  known  URLs  linked  to  by  these  pages). 
This  represents  a  significant  portion  of  the  Web,  and  is 
approximately  the  same  number  of  pages  as  are  used  by 
Google, Yahoo, and MSN for their search engines.
Because  PageRank  is  a  graph-based  algorithm,  it  is  important 
that it be run on as large a subset of the Web as possible. Most 
previous studies on PageRank used subsets of the Web that are 
significantly  smaller  (e.g.  the  TREC  VLC2  corpus,  used  by 
many, contains only 19 million pages)
We computed PageRank using the standard value of 0.85 for

.
Popularity
Another feature we used is the actual popularity of a Web page, 
measured  as  the  number  of  times  that  it  has  been  visited  by 
users  over  some  period  of  time.  We  have  access  to  such  data 
from users who have installed the MSN toolbar and have opted 
to  provide  it  to  MSN.  The  data  is  aggregated  into  a  count, for 
each Web page, of the number of users who viewed that page.
Though  popularity  data  is  generally  unavailable,  there  are  two 
other sources for it. The first is from proxy logs. For example, a 
university that requires its students to use a proxy has a record 
of  all  the  pages  they  have  visited  while  on  campus. 
Unfortunately, proxy data is quite biased and relatively small.
Another source, internal to search engines, are records of which 
results their users clicked on. Such data was used by the search 
engine  "Direct  Hit",  and  has  recently  been  explored  for 
dynamic  ranking  purposes  [20].  An  advantage  of  the  toolbar 
data  over  this  is  that  it  contains  information  about  URL  visits 
that are not just the result of a search.
The raw popularity is processed into a number of features such 
as  the  number  of  times  a  page  was  viewed  and  the  number  of 
times  any  page  in  the  domain  was  viewed.  More  details  are 
provided in section 5.5.
Anchor text and inlinks
These  features  are  based  on  the  information  associated  with 
links  to  the  page  in  question.  It  includes  features  such  as  the 
total  amount  of  text  in  links  pointing  to  the  page  ("anchor 
text"), the number of unique words in that text, etc.
Page
This category consists of features which may be determined by 
looking  at  the  page  (and  its  URL)  alone.  We  used  only  eight, 
simple  features  such  as  the  number  of  words  in  the  body,  the 
frequency of the most common term, etc.
Domain
This  category  contains  features  that  are  computed  as  averages 
across  all  pages  in  the  domain.  For  example,  the  average 
number of outlinks on any page and the average PageRank.
Many of these features have been used by others for ranking Web 
pages,  particularly  the  anchor  and  page  features.  As  mentioned, 
the  evaluation  is  typically  for  dynamic  ranking,  and  we  wish  to 
evaluate  the  use  of  them  for  static  ranking.  Also,  to  our 
knowledge,  this  is  the  first  study  on  the  use  of  actual  page 
visitation popularity for static ranking. The closest similar work is 
on  using  click-through  behavior  (that  is,  which  search  engine 
results  the  users  click  on)  to  affect  dynamic  ranking  (see  e.g., 
[20]).
Because we use a wide variety of features to come up with a static 
ranking,  we  refer  to  this  as  fRank  (for  feature-based  ranking). 
fRank  uses  RankNet  and  the  set  of  features  described  in  this 
section  to  learn  a  ranking  function  for  Web  pages.  Unless 
otherwise specified, fRank was trained with all of the features.

EXPERIMENTS
In  this  section,  we  will  demonstrate  that  we  can  out  perform 
PageRank  by  applying  machine  learning  to  a  straightforward  set 
of  features.  Before  the  results,  we  first  discuss  the  data,  the 
performance metric, and the training method.
5.1

Data
In  order  to  evaluate  the  quality  of  a  static  ranking,  we  needed  a 
"gold  standard"  defining  the  correct  ordering  for  a  set  of  pages. 
For this, we employed a dataset which contains human judgments 
for  28000  queries.  For  each  query,  a  number  of  results  are 
manually  assigned  a  rating,  from  0  to  4,  by  human  judges.  The 
rating  is  meant  to  be  a  measure  of  how  relevant  the  result  is  for 
the query, where 0 means "poor" and 4 means "excellent". There 
are  approximately  500k  judgments  in  all,  or  an  average  of  18 
ratings per query.
The  queries  are  selected  by  randomly  choosing  queries  from 
among  those  issued  to  the  MSN  search  engine.  The  probability 
that a query is selected is proportional to its frequency among all
709
of the queries. As a result, common queries are more likely to be 
judged  than  uncommon  queries.  As  an  example  of  how  diverse 
the  queries  are,  the  first  four  queries  in  the training set are "chef 
schools",  "chicagoland  speedway",  "eagles  fan  club",  and 
"Turkish  culture".  The  documents  selected  for  judging  are  those 
that  we  expected  would,  on  average,  be  reasonably  relevant  (for 
example,  the  top  ten  documents  returned  by  MSN's  search 
engine).  This  provides  significantly  more  information  than 
randomly  selecting  documents  on  the  Web,  the  vast  majority  of 
which would be irrelevant to a given query.
Because  of  this  process,  the  judged  pages  tend  to  be  of  higher 
quality  than  the  average  page  on  the  Web,  and  tend  to  be  pages 
that will be returned for common search queries. This bias is good 
when  evaluating  the  quality  of  static  ranking  for  the  purposes  of 
index ordering and returning relevant documents. This is because 
the  most  important  portion  of  the  index  to  be  well-ordered  and 
relevant  is  the  portion  that  is  frequently  returned  for  search 
queries. Because of this bias, however, the results in this paper are 
not  applicable  to  crawl  prioritization.  In  order  to  obtain 
experimental  results  on  crawl  prioritization,  we  would  need 
ratings on a random sample of Web pages.
To  convert  the  data  from  query-dependent  to  query-independent, 
we  simply  removed  the  query,  taking  the  maximum  over 
judgments  for  a  URL  that  appears  in  more  than  one  query.  The 
reasoning behind this is that a page that is relevant for some query 
and  irrelevant  for  another  is  probably  a  decent  page  and  should 
have  a  high  static  rank.  Because  we  evaluated  the  pages  on 
queries that occur frequently, our data indicates the correct index 
ordering,  and  assigns  high  value  to  pages  that  are  likely  to  be 
relevant to a common query.
We randomly assigned queries to a training, validation, or test set, 
such  that  they  contained  84%,  8%,  and  8%  of  the  queries, 
respectively. Each set contains all of the ratings for a given query, 
and  no  query  appears  in  more  than  one  set.  The  training  set  was 
used  to  train  fRank.  The  validation  set  was  used  to  select  the 
model that had the highest performance. The test set was used for 
the final results.
This  data  gives  us  a  query-independent  ordering  of  pages.  The 
goal  for  a  static  ranking  algorithm  will  be  to  reproduce  this 
ordering  as  closely  as  possible.  In  the  next  section,  we  describe 
the measure we used to evaluate this.
5.2

Measure
We  chose  to  use  pairwise  accuracy  to  evaluate  the  quality  of  a 
static  ranking.  The  pairwise  accuracy  is  the  fraction  of  time  that 
the ranking algorithm and human judges agree on the ordering of 
a pair of Web pages.
If  S(x)  is  the  static  ranking  assigned  to  page  x,  and  H(x)  is  the 
human  judgment  of  relevance  for  x,  then  consider  the  following 
sets:
)}
(
)
(
:
,
{
y
H
x
H
y
x
&gt;
=
p
H

and
)}
(
)
(
:
,
{
y
S
x
S
y
x
&gt;
=
p
S

The  pairwise  accuracy  is  the  portion  of  H
p
that is also contained
in S
p
:
p
p
p
H
S
H

=
accuracy

pairwise

This  measure  was  chosen  for  two  reasons.  First,  the  discrete 
human judgments provide only a partial ordering over Web pages,
making it difficult to apply a measure such as the Spearman rank 
order correlation coefficient (in the pairwise accuracy measure,  a 
pair of documents with the same human judgment does not affect 
the  score).  Second,  the  pairwise  accuracy  has  an  intuitive 
meaning:  it  is  the  fraction  of  pairs  of  documents  that,  when  the 
humans  claim  one  is  better  than  the  other,  the  static  rank 
algorithm orders them correctly.
5.3

Method
We  trained  fRank  (a  RankNet  based  neural  network)  using  the 
following parameters. We used a fully connected 2 layer network. 
The hidden layer had 10 hidden nodes. The input weights to this 
layer  were  all  initialized  to  be  zero.  The  output  "layer"  (just  a 
single  node)  weights  were  initialized  using  a  uniform  random 
distribution in the range [-0.1, 0.1]. We used tanh as the transfer 
function from the inputs to the hidden layer, and a linear function 
from  the  hidden  layer  to  the  output.  The  cost  function  is  the 
pairwise cross entropy cost function as discussed in section 3.
The features in the training set were normalized to have zero mean 
and  unit  standard  deviation.  The  same  linear  transformation  was 
then applied to the features in the validation and test sets.
For training, we presented the network with 5 million pairings of 
pages,  where  one  page  had  a  higher  rating  than  the  other.  The 
pairings  were  chosen  uniformly  at  random  (with  replacement) 
from all possible pairings. When forming the pairs, we ignored the 
magnitude of the difference between the ratings (the rating spread) 
for  the  two  URLs.  Hence,  the  weight  for  each  pair  was  constant 
(one),  and  the  probability  of  a  pair  being  selected  was 
independent of its rating spread.
We  trained  the  network  for  30  epochs.  On  each  epoch,  the 
training pairs were randomly shuffled. The initial training rate was 
0.001. At each epoch, we checked the error on the training set. If 
the error had increased, then we decreased the training rate, under 
the  hypothesis  that  the  network  had  probably  overshot.  The 
training rate at each epoch was thus set to:
Training rate =
1
+



Where

is  the  initial  rate  (0.001),  and

is  the  number  of  times
the  training  set  error  has  increased.  After  each  epoch,  we 
measured the performance of the neural network on the validation 
set,  using  1  million  pairs  (chosen  randomly  with  replacement). 
The network with the highest pairwise accuracy on the validation 
set  was  selected,  and  then  tested  on  the  test  set.  We  report  the 
pairwise  accuracy  on  the  test  set,  calculated  using  all  possible 
pairs.
These parameters were determined and fixed before the static rank 
experiments  in  this  paper.  In  particular,  the  choice  of  initial 
training  rate,  number  of  epochs,  and  training  rate  decay  function 
were taken directly from Burges et al [7].
Though  we  had  the  option  of  preprocessing  any  of  the  features 
before  they  were  input  to  the  neural  network,  we  refrained  from 
doing so on most of them. The only exception was the popularity 
features.  As  with  most  Web  phenomenon,  we  found  that  the 
distribution  of  site  popularity  is  Zipfian.  To  reduce  the  dynamic 
range,  and  hopefully  make  the  feature  more  useful, we presented 
the  network  with  both  the  unpreprocessed,  as  well  as  the 
logarithm,  of  the  popularity  features  (As  with  the  others,  the 
logarithmic  feature  values  were  also  normalized  to  have  zero 
mean and unit standard deviation).
710
Applying fRank to a document is computationally efficient, taking 
time  that  is  only  linear in the number of input features; it is thus 
within a constant factor of other simple machine learning methods 
such as naïve Bayes. In our experiments, computing the fRank for 
all  five  billion  Web  pages  was  approximately  100  times  faster 
than computing the PageRank for the same set.
5.4

Results
As Table 1 shows, fRank significantly outperforms PageRank for 
the purposes of static ranking. With a pairwise accuracy of 67.4%, 
fRank  more  than  doubles  the  accuracy  of  PageRank  (relative  to 
the baseline of 50%, which is the accuracy that would be achieved 
by  a  random  ordering  of  Web  pages).  Note  that  one  of  fRank's 
input features is the PageRank of the page, so we would expect it 
to  perform  no  worse  than  PageRank.  The  significant  increase  in 
accuracy  implies  that  the  other  features  (anchor,  popularity,  etc.) 
do in fact contain useful information regarding the overall quality 
of a page.

Table 1: Basic Results
Technique
Accuracy (%)
None (Baseline)
50.00
PageRank
56.70
fRank
67.43

There  are  a  number  of  decisions  that  go  into  the  computation  of 
PageRank, such as how to deal with pages that have no outlinks, 
the  choice  of

,  numeric  precision,  convergence  threshold,  etc.
We  were  able  to  obtain  a  computation  of  PageRank  from  a 
completely  independent  implementation  (provided  by  Marc 
Najork)  that  varied  somewhat  in  these  parameters.  It  achieved  a 
pairwise accuracy of 56.52%, nearly identical to that obtained by 
our  implementation.  We  thus  concluded  that  the  quality  of  the 
PageRank  is  not  sensitive  to  these  minor variations in algorithm, 
nor  was  PageRank's  low  accuracy  due  to  problems  with  our 
implementation of it.
We  also  wanted  to  find  how  well  each  feature  set performed. To 
answer  this,  for  each  feature  set,  we  trained  and  tested  fRank 
using  only  that  set  of  features.  The  results are shown in Table 2. 
As can be seen, every single feature set individually outperformed 
PageRank  on  this  test.  Perhaps  the  most  interesting  result  is  that 
the Page-level features had the highest performance out of all the 
feature  sets.  This  is  surprising  because  these  are  features  that  do 
not depend on the overall graph structure of the Web, nor even on 
what pages point to a given page. This is contrary to the common 
belief  that  the  Web  graph  structure  is  the  key  to  finding  a  good 
static ranking of Web pages.

Table 2: Results for individual feature sets.
Feature Set
Accuracy (%)
PageRank
56.70
Popularity
60.82
Anchor
59.09
Page
63.93
Domain
59.03
All Features
67.43

Because  we  are  using  a  two-layer neural network, the features in 
the  learned  network  can  interact  with  each  other  in  interesting, 
nonlinear  ways.  This  means  that  a  particular  feature  that  appears 
to  have  little  value  in  isolation  could  actually  be  very  important 
when  used  in  combination  with  other  features.  To  measure  the 
final  contribution  of  a  feature  set,  in  the  context  of  all  the  other 
features,  we  performed  an  ablation study. That is, for each set of 
features, we trained a network to contain all of the features except 
that  set.  We  then  compared  the  performance  of  the  resulting 
network to the performance of the network with all of the features. 
Table 3 shows the results of this experiment, where the "decrease 
in  accuracy"  is  the  difference  in  pairwise  accuracy  between  the 
network  trained  with all of the features, and the network missing 
the given feature set.

Table  3:  Ablation  study.  Shown  is  the  decrease  in  accuracy 
when  we  train  a  network  that  has  all  but  the  given  set  of 
features.  The  last  line  is  shows  the  effect  of  removing  the 
anchor,  PageRank,  and  domain  features,  hence  a  model 
containing no network or link-based information whatsoever.
Feature Set
Decrease in
Accuracy
PageRank
0.18
Popularity
0.78
Anchor
0.47
Page
5.42
Domain
Anchor, PageRank & Domain
0.10 
0.60

The results of the ablation study are consistent with the individual 
feature set study. Both show that the most important feature set is 
the  Page-level  feature  set,  and  the  second  most  important  is  the 
popularity feature set.
Finally,  we  wished  to  see  how  the  performance  of  fRank 
improved  as  we  added  features;  we  wanted  to  find  at  what  point 
adding  more  feature  sets  became  relatively  useless.  Beginning 
with no features, we greedily added the feature set that improved 
performance  the  most.  The  results  are  shown  in  Table  4.  For 
example,  the  fourth  line  of  the  table  shows  that  fRank  using  the 
page,  popularity,  and  anchor  features  outperformed  any  network 
that used the page, popularity, and some other feature set, and that 
the performance of this network was 67.25%.

Table 4: fRank performance as feature sets are added. At each 
row, the feature set that gave the greatest increase in accuracy 
was  added  to  the  list  of  features  (i.e.,  we  conducted  a  greedy 
search over feature sets).
Feature Set
Accuracy (%)
None
50.00
+Page
63.93
+Popularity
66.83
+Anchor
67.25
+PageRank
67.31
+Domain
67.43

711
Finally,  we  present  a  qualitative  comparison  of  PageRank  vs. 
fRank. In Table 5 are the top ten URLs returned for PageRank and 
for  fRank.  PageRank's  results  are  heavily  weighted  towards 
technology sites. It contains two QuickTime URLs (Apple's video 
playback  software),  as  well  as  Internet  Explorer  and  FireFox 
URLs  (both  of  which  are  Web  browsers).  fRank,  on  the  other 
hand,  contains  
Evolutionary Learning with Kernels: A Generic Solution for Large Margin Problems
ABSTRACT
In this paper we embed evolutionary computation into statistical
learning theory. First, we outline the connection between
large margin optimization and statistical learning and
see why this paradigm is successful for many pattern recognition
problems. We then embed evolutionary computation
into the most prominent representative of this class of learning
methods, namely into Support Vector Machines (SVM).
In contrast to former applications of evolutionary algorithms
to SVMs we do not only optimize the method or kernel parameters
. We rather use both evolution strategies and particle
swarm optimization in order to directly solve the posed
constrained optimization problem. Transforming the problem
into the Wolfe dual reduces the total runtime and allows
the usage of kernel functions. Exploiting the knowledge
about this optimization problem leads to a hybrid mutation
which further decreases convergence time while classification
accuracy is preserved. We will show that evolutionary
SVMs are at least as accurate as their quadratic programming
counterparts on six real-world benchmark data sets.
The evolutionary SVM variants frequently outperform their
quadratic programming competitors. Additionally, the proposed
algorithm is more generic than existing traditional
solutions since it will also work for non-positive semidefinite
kernel functions and for several, possibly competing, performance
criteria.
Track: Learning Classifier Systems and other Genetics-Based
Machine Learning
Categories and Subject Descriptors
Learning
General Terms
Algorithms, Theory, Experimentation

INTRODUCTION
In this paper we will discuss how evolutionary algorithms
can be used to solve large margin optimization problems.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
GECCO'06, July 8­12, 2006, Seattle, Washington, USA.
Copyright 2006 ACM 1-59593-186-4/06/0007 ...
$
5.00.
We explore the intersection of three highly active research
areas, namely machine learning, statistical learning theory,
and evolutionary algorithms. While the connection between
statistical learning and machine learning was analyzed before
, embedding evolutionary algorithms into this connection
will lead to a more generic algorithm which can deal
with problems today's learning schemes cannot cope with.
Supervised machine learning is often about classification
problems. A set of data points is divided into several classes
and the machine learning method should learn a decision
function in order to decide into which class an unseen data
point should be classified.
The maximization of a margin between data points of different
classes, i. e. the distance between a decision hyperplane
and the nearest data points, interferes with the ideas
of statistical learning theory. This allows the definition of an
error bound for the generalization error. Furthermore, the
usage of kernel functions allows the learning of non-linear
decision functions. We focus on Support Vector Machines
(SVM) as they are the most prominent representatives for
large margin problems. Since SVMs guarantee an optimal
solution for the given data set they are currently one of the
mostly used learning methods. Furthermore, many other
optimization problems can also be formulated as large margin
problem [26]. The relevance of large margin methods
can be measured by the number of submissions to the main
machine learning conferences over the past years
1
.
Usually, the optimization problem posed by SVMs is solved
with quadratic programming. However, there are some drawbacks
.
First, for kernel functions which are not positive
semidefinite no unique global optimum exists. In these cases
quadratic programming is not able to find satisfying solutions
at all. Moreover, most implementations do not even
terminate [8]. There exist several useful non-positive kernels
[15], among them the sigmoid kernel which simulates a
neural network [3, 23]. A more generic optimization scheme
should allow such non-positive kernels without the need for
omitting the more efficient dual optimization problem [17].
Second, SVMs should be able to optimize several performance
measures at the same time. Traditional SVMs try
to maximize the prediction accuracy alone. However, depending
on the application area other specific performance
criteria should be optimized instead of or additionally to
prediction accuracy. Although first attempts were made to
incorporate multivariate performance measures into SVMs
[13], the problem is not generally solved and no solution exist
1
More than 30% of all accepted papers for ICML 2005 dealt
with SVMs and other large margin methods.
1553
for competing criteria. This problem as well as the general
trade-off between training error and capacity could be easily
solved by an (multi-objective) evolutionary optimization
approach.
Former applications of evolutionary algorithms to SVMs
include the optimization of method and kernel parameters
[6, 19], the selection of optimal feature subsets [7], and the
creation of new kernel functions by means of genetic programming
[10]. The latter is particularly interesting since
it cannot be guaranteed that the resulting kernel functions
are again positive semi-definite.
Replacing the traditional optimization techniques by evolution
strategies or particle swarm optimization can tackle
both problems mentioned above. We will extract as much
information as possible from the optimization problem at
hand and develop and compare different search point operations
. We will show that the proposed implementation
leads to as good results as traditional SVMs on all real-world
benchmark data sets. Additionally, the optimization
is more generic since it also allows non-positive semi-definite
kernel functions and the simultaneous optimization of different
, maybe competing, criteria.
1.1
Outline
In Section 2 we give a short introduction into the concept
of structural risk minimization and the ideas of statistical
learning theory. We will also discuss an upper bound for
the generalization error. This allows us to formalize the optimization
problem of large margin methods in Section 3.
We will introduce SVMs for the classification of given data
points in Section 3.1 and extend the separation problem to
non-separable datasets (see Section 3.2) with non-linear hyperplanes
(see Section 3.3). This leads to a constrained optimization
problem for which we utilize evolution strategies
and particle swarm optimization in Section 4. We discuss
several enhancements and a new type of mutation before
we evaluate the proposed methods on real-world benchmark
datasets in Section 5.
STRUCTURAL RISK MINIMIZATION
In this section we discuss the idea of structural risk minimization
. Machine learning methods following this paradigm
have a solid theoretical foundation and it is possible to define
bounds for prediction errors.
Let X  IR
m
be a real-valued vector of random variables.
Let Y  IR be another random variable. X and Y obey a
fixed but unknown probability distribution P (X, Y ). Machine
Learning tries to find a function f(x, ) which predict
the value of Y for a given input x  X. The function class
f depends on a vector of parameters , e. g. if f is the
class of all polynomials,  might be the degree. We define
a loss function L(Y, f(X, )) in order to penalize errors
during prediction [9]. Every convex function with arity 2,
positive range, and L(x, x) = 0 can be used as loss function
[22]. This leads to a possible criterion for the selection of a
function f, the expected risk:
R() =
Z
L(y, f(x, ))dP (x, y).
(1)
Since the underlying distribution is not known we are not
able to calculate the expected risk.
However, instead of
estimating the probability distribution in order to allow this
calculation, we directly estimate the expected risk by using
a set of known data points T = {(x
1
, y
1
) , . . . , (x
n
, y
n
)
}
X × Y . T is usually called training data. Using this set of
data points we can calculate the empirical risk :
R
emp
() = 1
n
n
X
i
=1
L (y
i
, f (x
i
, )) .
(2)
If training data is sampled according to P (X, Y ), the empirical
risk approximates the expected risk if the number of
samples grows:
lim
n

R
emp
() = R().
(3)
It is, however, a well known problem that for a finite number
of samples the minimization of R
emp
() alone does not
lead to a good prediction model [27]. For each loss function
L, each candidate , and each set of tuples T  X ×
Y with T  T
=
exists another parameter vector
so that L(y, f(x, )) = L(y, f(x,  )) for all x  T and
L(y, f(x, )) &gt; L(y, f(x,  )) for all x  T . Therefore, the
minimization of R
emp
() alone does not guarantee the optimal
selection of a parameter vector  for other samples
according to the distribution P (X, Y ). This problem is often
referred to as overfitting.
At this point we use one of the main ideas of statistical
learning theory. Think of two different functions perfectly
approximating a given set of training points. The first function
is a linear function, i. e. a simple hyperplane in the considered
space IR
m
. The second function also hits all training
points but is strongly wriggling in between. Naturally, if we
had to choose between these two approximation functions,
we tend to select the more simple one, i. e. the linear hyperplane
in this example. This derives from the observation
that more simple functions behave better on unseen examples
than very complicated functions. Since the mere minimization
of the empirical risk according to the training data
is not appropriate to find a good generalization, we incorporate
the capacity
2
of the used function into the optimization
problem (see Figure 1). This leads to the minimization of
the structural risk
R
struct
() = R
emp
() + ().
(4)
is a function which measures the capacity of the function
class f depending on the parameter vector . Since the
empirical risk is usually a monotonically decreasing function
of , we use  to manage the trade-off between training error
and capacity. Methods minimizing this type of risk function
are known as shrinkage estimators [11].
2.1
Bound on the generalization performance
For certain functions  the structural risk is an upper
bound for the empirical risk.
The capacity of the function
f for a given  can for example be measured with help
of the Vapnik-Chervonenkis dimension (VC dimension) [27,
28]. The VC dimension is defined as the cardinality of the
biggest set of tuples which can separated with help of f in all
possible ways. For example, the VC dimension of linear hyperplanes
in an m-dimensional space is m+1. Using the VC
dimension as a measure for capacity leads to a probabilistic
bound for the structural risk [27]. Let f be a function class
with finite VC dimension h and f() the best solution for the
2
Although not the same, the capacity of a function resembles
a measurement of the function complexity. In our example
we measure the ability to "wriggle". More details in [27].
1554
X
Y
Figure 1: The simultaneous minimization of empirical
risk and model complexity gives a hint which
function should be used in order to generalize the
given data points.
empirical risk minimization for T with |T | = n. Now choose
some  such that 0    1. Then for losses smaller than
some number B, the following bound holds with probability
1
- :
R()  R
emp
() + B
s
h `log
2l
h
+ 1´
- log
4
l
.
(5)
Surprisingly, this bound is independent of P (X, Y ). It only
assumes that both the seen and the unseen data points are
independently sampled according to some P (X, Y ). Please
note that this bound also no longer contains a weighting
factor  or any other trade-off at all. The existence of a
guaranteed error bound is the reason for the great success of
structural risk minimization in a wide range of applications.
LARGE MARGIN METHODS
As discussed in the previous section we need to use a class
of functions whose capacity can be controlled. In this section
we will discuss a special form of structural risk minimization
, namely large margin approaches. All large margin
methods have one thing in common: they embed structural
risk minimization by maximizing a margin between a linear
function and the nearest data points. The most prominent
large margin method for classification tasks is the Support
Vector Machine (SVM).
3.1
Support Vector Machines
We constrain the number of possible values of Y to 2,
without loss of generality these values should be
-1 and
+1. In this case, finding a function f in order to decide
which of both predictions is correct for an unseen data point
is referred to as classification learning for the classes
-1
and +1. We start with the simplest case: learning a linear
function from perfectly separable data. As we shall see in
Section 3.2 and 3.3, the general case - non-linear functions
derived from non-separable data - leads to a very similar
problem.
If the data points are linearly separable, a linear hyperplane
must exist in the input space IR
m
which separates
both classes. This hyperplane is defined as
H = {x| w, x + b = 0} ,
(6)
H
w
Margin
Origin
-b
|w|
+1
-1
Figure 2: A simple binary classification problem for
two classes
-1 (empty bullets) and +1 (filled bullets).
The separating hyperplane is defined by the vector
w and the offset b. The distance between the nearest
data point(s) and the hyperplane is called
margin.
where w is normal to the hyperplane, |b|/||w|| is the perpendicular
distance of the hyperplane to the origin (offset
or bias), and
||w|| is the Euclidean norm of w. The vector w
and the offset b define the position and orientation of the hyperplane
in the input space. These parameters correspond
to the function parameters . After the optimal parameters
w and b were found, the prediction of new data points can
be calculated as
f(x, w, b) = sgn ( w, x + b) ,
(7)
which is one of the reasons why we constrained the classes
to
-1 and +1.
Figure 2 shows some data points and a separating hyperplane
. If all given data points are correctly classified by the
hyperplane at hand the following must hold:
i : y
i
( w, x
i
+ b)  0.
(8)
Of course, an infinite number of different hyperplanes exist
which perfectly separate the given data points. However,
one would intuitively choose the hyperplane which has the
biggest amount of safety margin to both sides of the data
points.
Normalizing w and b in a way that the point(s)
closest to the hyperplane satisfy
| w, x
i
+ b| = 1 we can
transform equation 8 into
i : y
i
( w, x
i
+ b)  1.
(9)
We can now define the margin as the perpendicular distance
of the nearest point(s) to the hyperplane.
Consider two
points x
1
and x
2
on opposite sides of the margin. That is
w, x
1
+b = +1 and w, x
2
+b = -1 and w, (x
1
-x
2
) = 2.
The margin is then given by 1/||w||.
It can be shown, that the capacity of the class of separating
hyperplanes decreases with increasing margin [21].
Maximizing the margin of a hyperplane therefore formalizes
the structural risk minimization discussed in the previous
section. Instead of maximizing 1/||w|| we could also minimize
1
2
||w||
2
which will result into more simple equations
later. This leads to the optimization problem
minimize
1
2
||w||
2
(10)
subject to
i : y
i
( w, x
i
+ b)  1.
(11)
1555
Function 10 is the objective function and the constraints
from equation 11 are called inequality constraints.
They
form a constrained optimization problem. We will use a Lagrangian
formulation of the problem. This allows us to replace
the inequality constraints by constraints on the Lagrange
multipliers which are easier to handle. The second
reason is that after the transformation of the optimization
problem, the training data will only appear in dot products.
This will allow us to generalize the optimization to the non-linear
case (see Section 3.3). We will now introduce positive
Lagrange multipliers
i
, i = 1, . . . , n, one for each of the
inequality constraints. The Lagrangian has the form
L
P
(w, b, ) = 12||w||
2
n
X
i
=1

i
y
i
( w, x
i
+ b) .
(12)
Finding a minimum of this function requires that the derivatives
L
P
(w,b,)
w
= w n
P
i
=1

i
y
i
x
i
(13)
L
P
(w,b,)
b
=
n
P
i
=1

i
y
i
(14)
are zero, i. e.
w =
n
P
i
=1

i
y
i
x
i
(15)
0 =
n
P
i
=1

i
y
i
.
(16)
The Wolfe dual, which has to be maximized, results from
the Lagrangian by substituting 15 and 16 into 12, thus
L
D
(w, b, ) =
n
X
i
=1

i
- 12
n
X
i
=1
n
X
j
=1
y
i
y
j

i

j
x
i
, x
j
.
(17)
This leads to the dual optimization problem which must
be solved in order to find a separating maximum margin
hyperplane for given set of data points:
maximize
n
P
i
=1

i
1
2
n
P
i
=1
n
P
j
=1
y
i
y
j

i

j
x
i
, x
j
(18)
subject to
i
0 for all i = 1, . . . , n
(19)
and
n
P
i
=1

i
y
i
= 0.
(20)
From an optimal vector

we can calculate the optimal
normal vector w

using equation 15. The optimal offset can
be calculated with help of equation 11. Please note, that w
is a linear combination of those data points x
i
with
i
= 0.
These data points are called support vectors, hence the name
support vector machine. Only support vectors determine the
position and orientation of the separating hyperplane, other
data points might as well be omitted during learning. In
Figure 2 the support vectors are marked with circles. The
number of support vectors is usually much smaller than the
total number of data points.
3.2
Non-separable data
We now consider the case that the given set of data points
is not linearly separable.
The optimization problem discussed
in the previous section would not have a solution
since in this case constraint 11 could not be fulfilled for all
i. We relax this constraint by introducing positive slack
variables
i
, i = 1, . . . , n. Constraint 11 becomes
i : y
i
( w, x
i
+ b)  1 i
.
(21)
In order to minimize the number of wrong classifications
we introduce a correction term C P
n
i
=1

i
into the objective
function. The optimization problems then becomes
minimize
1
2
||w||
2
+ C
n
P
i
=1

i
(22)
subject to
i : y
i
( w, x
i
+ b)  1 i
.
(23)
The factor C determines the weight of wrong predictions as
part of the objective function. As in the previous section
we create the dual form of the Lagrangian. The slacking
variables
i
vanish and we get the optimization problem
maximize
n
P
i
=1

i
1
2
n
P
i
=1
n
P
j
=1
y
i
y
j

i

j
x
i
, x
j
(24)
subject to 0

i
C for all i = 1, . . . , n
(25)
and
n
P
i
=1

i
y
i
= 0.
(26)
It can easily be seen that the only difference to the separable
case is the additional upper bound C for all
i
.
3.3
Non-linear learning with kernels
The optimization problem described with equations 24,
25, and 26 will deliver a linear separating hyperplane for
arbitrary datasets. The result is optimal in a sense that no
other linear function is expected to provide a better classification
function on unseen data according to P (X, Y ). However
, if the data is not linearly separable at all the question
arises how the described optimization problem can be gener-alized
to non-linear decision functions. Please note that the
data points only appear in the form of dot products x
i
, x
j
.
A possible interpretation of this dot product is the similarity
of these data points in the input space IR
m
. Now consider a
mapping  : IR
m
H into some other Euclidean space H
(called feature space) which might be performed before the
dot product is calculated. The optimization would depend
on dot products in this new space H, i. e. on functions of
the form  (x
i
) ,  (x
j
) . A function k : IR
m
× IR
m
IR
with the characteristic
k (x
i
, x
j
) =  (x
i
) ,  (x
j
)
(27)
is called kernel function or kernel. Figure 3 gives a rough
idea how transforming the data points can help to solve
non-linear problems with the optimization in a (higher dimensional
) space where the points can be linearly separated.
A fascinating property of kernels is that for some mappings
 a kernel k exists which can be calculated without
actually performing . Since often the dimension of H is
greater than the dimension m of the input space and H
sometimes is even infinite dimensional, the usage of such
kernels is a very efficient way to introduce non-linear decision
functions into large margin approaches. Prominent examples
for such efficient non-linear kernels are polynomial
kernels with degree d
k (x
i
, x
j
) = ( x
i
, x
j
+ )
d
,
(28)
radial basis function kernels (RBF kernels)
k (x
i
, x
j
) = e
||xi
-xj||2
22
(29)
1556
H
R
m
Figure 3: After the transformation of all data points
into the feature space H the non-linear separation
problem can be solved with a linear separation algorithm
. In this case a transformation in the space
of polynomials with degree 2 was chosen.
for a  &gt; 0, and the sigmoid kernel
k (x
i
, x
j
) = tanh ( x
i
, x
j
- )
(30)
which can be used to simulate a neural network.  and 
are scaling and shifting parameters. Since the RBF kernel
is easy interpretable and often yields good prediction performance
, it is used in a wide range of applications. We will
also use the RBF kernel for our experiments described in
section 5 in order to demonstrate the learning ability of the
proposed SVM.
We replace the dot product in the objective function by
kernel functions and achieve the final optimization problem
for finding a non-linear separation for non-separable data
points
maximize
n
P
i
=1

i
1
2
n
P
i
=1
n
P
j
=1
y
i
y
j

i

j
k (x
i
, x
j
)
(31)
subject to 0

i
C for all i = 1, . . . , n
(32)
and
n
P
i
=1

i
y
i
= 0.
(33)
It can be shown that if the kernel k, i. e. it's kernel matrix
, is positive definite, the objective function is concave
[2]. The optimization problem therefore has a global unique
maximum. However, in some cases a specialized kernel function
must be used to measure the similarity between data
points which is not positive definite, sometimes not even
positive semidefinite [21]. In these cases the usual quadratic
programming approaches might not be able to find a global
maximum in feasible time.
EVOLUTIONARY COMPUTATION FOR LARGE MARGIN OPTIMIZATION
Since traditional SVMs are not able to optimize for non-positive
semidefinite kernel function, it is a very appealing
idea to replace the usual quadratic programming approaches
by an evolution strategies (ES) approach [1] or by particle
swarm optimization (PSO) [14]. In this section we will describe
both a straightforward application of these techniques
and how we can exploit some information about our optimization
problem and incorporate that information into our
search operators.
4.1
Solving the dual problem and other sim-plifications
The used optimization problem is the dual problem for
non-linear separation of non-separable data developed in the
last sections (equations 31, 32, and 33). Of course it would
also be possible to directly optimize the original form of
our optimization problem depicted in equations 22 and 23.
That is, we could directly optimize the weight vectors and
the offset. As mentioned before, there are two drawbacks:
first, the costs of calculating the fitness function would be
much higher for the original optimization problem since the
fulfillment of all n constraints must be recalculated for each
new hyperplane. It is a lot easier to check if all 0

i

C apply. Second, it would not be possible to allow non-linear
learning with efficient kernel functions in the original
formulation of the problem. Furthermore, the kernel matrix
K with K
ij
= k (x
i
, x
j
) can be calculated beforehand and
the training data is never used during optimization again.
This further reduces the needed runtime for optimization
since the kernel matrix calculation is done only once.
This is a nice example for a case, where transforming the
objective function beforehand is both more efficient and allows
enhancements which would not have been possible before
. Transformations of the fitness functions became a very
interesting topic recently [25].
Another efficiency improvement can be achieved by formulating
the problem with b = 0. All solution hyperplanes
must then contain the origin and the constraint 33 will vanish
. This is a mild restriction for high-dimensional spaces
since the number of degrees of freedom is only decreased by
one. However, during optimization we do not have to cope
with this equality constraint which would take an additional
runtime of O(n).
4.2
EvoSVM and PsoSVM
We developed a support vector machine based on evolution
strategies optimization (EvoSVM). We utilized three
different types of mutation which will be described in this
section.
Furthermore, we developed another SVM based
on particle swarm optimization (PsoSVM) which is also described
.
The first approach (EvoSVM-G, G for Gaussian mutation
) merely utilizes a standard ES optimization. Individuals
are the real-valued vectors  and mutation is performed
by adding a Gaussian distributed random variable with standard
deviation C/10. In addition, a variance adaptation is
conducted during optimization (1/5 rule [18]). Crossover
probability is high (0.9). We use tournament selection with
a tournament size of 0.25 multiplied by the population size.
The initial individuals are random vectors with 0

i
C.
The maximum number of generations is 1000 and the optimization
is terminated if no improvement occurred during
the last 5 generations. The population size is 10.
The second version is called EvoSVM-S (S for switching
mutation). Here we utilize the fact that only a small amount
of input data points will become support vectors (sparsity ).
On the other hand, one can often observe that non-zero
alpha values are equal to the upper bound C and only a very
small amount of support vectors exists with 0 &lt;
i
&lt; C.
Therefore, we just use the well known mutation of genetic
algorithms and switch between 0 and C with probability
1/n for each
i
. The other parameters are equal to those
described for the EvoSVM-G.
1557
for i = 1 to n do {
if (random(0, 1) &lt; 1/n) do {
if (alpha_i &gt; 0) do {
alpha_i = 0;
} else do {
alpha_i = random(0, C);
}
}
}
Figure 4: A simple hybrid mutation which should
speed-up the search for sparser solutions.
It contains
elements from standard mutations from both
genetic algorithms and evolution strategies.
Using this switching mutation inspired by genetic algorithms
only allow
i
= 0 or
i
= C. Instead of a complete
switch between 0 and C or a smooth change of all values
i
like the Gaussian mutation does, we developed a hybrid
mutation combining both elements. That means that we
check for each
i
with probability 1/n if the value should be
mutated at all. If the current value
i
is greater than 0,
i
is
set to 0. If
i
is equal to 0,
i
is set to a random value with
0

i
C. Figure 4 gives an overview over this hybrid
mutation. The function random(a, b) returns an uniformly
distributed random number between a and b. The other parameters
are the same as described for the EvoSVM-G. We
call this version EvoSVM-H (H for hybrid).
As was mentioned before, the optimization problem usually
is concave and the risk for local extrema is small. Therefore
, we also applied a PSO technique. It should be inves-tigated
if PSO, which is similar to the usual quadratic programming
approaches for SVMs in a sense that the gradient
information is exploited, is able to find a global optimum in
shorter time. We call this last version PsoSVM and use a
standard PSO with inertia weight 0.1, local best weight 1.0,
and global best weight 1.0. The inertia weight is dynami-cally
adapted during optimization [14].
EXPERIMENTS AND RESULTS
In this section we try to evaluate the proposed evolutionary
optimization SVMs. We compare our implementation to
the quadratic programming approaches usually applied to
large margin problems. The experiments demonstrate the
competitiveness in terms of classification error minimization,
runtime, and robustness.
We apply the discussed EvoSVM variants as well as the
PsoSVM on six real-world benchmark datasets. We selected
these datasets from the UCI machine learning repository
[16] and the StatLib dataset library [24], because they already
define a binary classification task, consist of real-valued
numbers only and do not contain missing values.
Therefore, we did not need to perform additional prepro-cessing
steps which might introduce some bias. The properties
of all datasets are summarized in Table 1. T
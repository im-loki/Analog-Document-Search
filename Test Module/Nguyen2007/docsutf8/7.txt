A Fair and Traffic Dependent Scheduling Algorithm for Bluetooth Scatternets
Abstract
mechanisms and algorithms necessary to set up and maintain them. The operation of a scatternet requires some Bluetooth units to be inter-piconet
units (gateways), which need to time-division multiplex their presence among their piconets. This requires a scatternet-scheduling
algorithm that can schedule the presence of these units in an efficient manner. In this paper, we propose a distributed scatternet-scheduling
scheme that is implemented using the HOLD mode of Bluetooth and adapts to non-uniform and changing traffic. Another attribute of the
scheme is that it results in fair allocation of bandwidth to each Bluetooth unit. This scheme provides an integrated solution for both intra-and
inter-piconet scheduling, i.e., for polling of slaves and scheduling of gateways.
Introduction
The Bluetooth [10] technology was developed as a replacement
of cables between electronic devices and this is perhaps
its most obvious use. But, it is the ability of Bluetooth devices
to form small networks called piconets that opens up a
whole new arena for applications where information may be
exchanged seamlessly among the devices in the piconet. Typ-ically
, such a network, referred to as a PAN (Personal Area
Network), consists of a mobile phone, laptop, palmtop, headset
, and other electronic devices that a person carries around
in his every day life. The PAN may, from time to time, also
include devices that are not carried along with the user, e.g.,
an access point for Internet access or sensors located in a
room. Moreover, devices from other PANs can also be interconnected
to enable sharing of information.
The networking capabilities of Bluetooth can be further
enhanced by interconnecting piconets to form scatternets.
This requires that some units be present in more than one piconet
. These units, called gateways, need to time-division
their presence among the piconets. An important issue with
the gateways is that their presence in different piconets needs
to be scheduled in an efficient manner. Moreover, since the
gateway cannot receive information from more than one piconet
at a time, there is a need to co-ordinate the presence of
masters and gateways.
Some previous work has looked at scheduling in a piconet
[2,5] and also in a scatternet. In [4], the authors define a
Rendezvous-Point based architecture for scheduling in a scatternet
, which results in the gateway spending a fixed fraction
of its time in each piconet. Such a fixed time-division of the
gateway may clearly be inefficient since traffic is dynamic.
In [9], the authors propose the Pseudo-Random Coordinated
Scatternet Scheduling (PCSS) scheme in which Bluetooth
nodes assign meeting points with their peers. The sequence
of meeting points follows a pseudo-random process that leads
to unique meeting points for different peers of a node. The
intensity of these meeting points may be increased or decreased
according to the traffic intensity. This work presents
performance results for various cases. In [11], a scatternet-scheduling
algorithm based on the concept of a switch table,
which can be dynamically adjusted based on traffic load, is
presented. In [1], the authors present a credit-based scheduling
scheme based on the SNIFF mode of Bluetooth, where
credits may be reallocated to cater to changing traffic.
Our scheduling scheme addresses the issues of fairness and
utilization of bandwidth. Since Bluetooth is a low-bandwidth
environment, it is important that bandwidth should be effi-ciently
utilized. Also, since a low bandwidth can easily lead
to starvation of flows, another metric we focus on is fairness.
We propose a distributed scatternet-scheduling algorithm that
is implemented using the HOLD mode [10] of Bluetooth
and adapts to non-uniform and changing traffic. This algorithm
provides an integrated solution for both intra- and inter-piconet
scheduling, i.e., for polling of slaves and scheduling
of gateways. The algorithm leads to a high bandwidth utilization
and results in a fair division of (a) the piconet bandwidth
between the slaves of a piconet and (b) the gateway presence
among different piconets.
In section 2, we discuss the Bluetooth technology. In
section 3, we present a definition of fairness in the context
10
R. KAPOOR ET AL.
Figure 1. Gateway may be viewed as a virtual master and masters as virtual
slaves.
of Bluetooth scatternets, which takes into account intra- and
inter-piconet max-min fairness. Section 4 describes the algorithm
and proves its fairness property. Section 5 presents
simulation results and section 6 presents the conclusions.
Bluetooth technology
The Bluetooth system [3] operates in the worldwide unlicensed
2.4 GHz Industrial­Scientific­Medical (ISM) frequency
band. To make the link robust to interference, it uses
a Frequency Hopping (FH) technique with 79 radio carriers.
It allows a raw data transmission rate of 1 Mbit/s.
Two or more Bluetooth units sharing the same channel
form a piconet. Each piconet consists of a master unit and
up to seven active slave units. The master unit polls the slave
units according to a polling algorithm and a slave is only allowed
to transmit after the master has polled it. The piconet
capacity is thus, shared among the slave units according to the
polling algorithm.
Furthermore, two or more piconets can be interconnected,
forming a scatternet. This requires a unit, called an inter-piconet
unit (gateway), to be a part of more than one piconet.
Such a unit can simultaneously be a slave member of multiple
piconets, but a master in only one, and can transmit and
receive data in only one piconet at a time; so participation in
multiple piconets has to be on a time-division multiplex basis
. The time of the gateway is, thus, also shared among the
piconets it belongs to. In this work, we assume that the gateway
can only be a slave in its piconets. If a gateway were
to be a master in a piconet, it would lead to the stoppage of
all transmission in the piconet when the gateway visits some
other piconet. Thus, we believe that the use of the gateway as
a slave is the most efficient method of scatternetting.
Fair allocation of bandwidth
As introduced in the previous section, units belonging to a
piconet share the piconet capacity according to the polling
algorithm used by the master. In an analogous manner, gateways
in a scatternet divide their time among their different
piconets, according to the "master-listening" algorithm they
use. It can be noted that there is a duality in this architecture
. On the one hand, a master divides its capacity among
the units of its piconet by using a polling algorithm. On the
other hand, a gateway shares its capacity among the piconets
it belongs to, on the basis of a scheduling algorithm it uses
for listening to the masters. The gateway, can, then be viewed
as a "virtual master" and its masters can be viewed as "virtual
slaves" forming a "virtual piconet", in which the polling cycle
is, actually, the "listening cycle" of the gateway. A graphical
interpretation of this duality is given in figure 1, in which the
solid line shows the actual piconets, and the dotted line shows
the virtual piconet.
Due to this duality, we design our scheduling scheme such
that the same scheduling algorithm is used for fair sharing of
both (a) the piconet capacity among slaves and (b) the gateway
time among piconets.
We now give a definition of max-min fairness [7]. We then
go on to define max-min fairness in the context of Bluetooth
scatternets, by considering (a) intra-piconet fairness, i.e., fairness
in division of piconet bandwidth among slaves (both
gateway and non-gateway) of a piconet and (b) inter-piconet
fairness, i.e., fairness in division of the gateway's presence
among its piconets. We first define a `feasible' rate distribution
since this is used in the definition of max-min fairness.
Definition 1 (Feasible). A rate distribution is feasible if rates
are non-negative, the aggregate rate is not greater than one,
and no unit receives a higher rate than required.
Definition 2 (Max-min fairness). An allocation of rates
1
,

2
, . . . ,
s
among s units is max-min fair if it is feasible, and
for each unit i,
i
cannot be increased (while maintaining fea-sibility
) without decreasing
j
for some other unit j for which

j

i
.
The distribution of max-min fair rates depends upon the
set of rate demands (traffic generated) of the units. In the
following subsections, we discuss factors that determine the
max-min "fair share" of a slave (gateway or non-gateway).
We call these factors the Piconet Presence Fraction and the
Scatternet Presence Fraction and show how they may be used
to calculate the "fair share" for a slave in a scatternet.
3.1. Piconet presence fraction
Consider a piconet consisting of gateway and non-gateway
slaves in which the master has complete knowledge of the rate
demands of all slaves (an ideal master). Using this knowledge
, the master polls the slaves in a max-min fair manner
such that each slave gets its "fair share" of the master's
polling. We refer to the "fair share" received by a slave as the
"piconet presence fraction" (PPF) of the slave. The gateway
has a PPF for each piconet it belongs to.
Consider the piconets shown in figures 2(a) and 2(b), each
consisting of one gateway and two slaves, with the traffic rates
of each slave as shown. In figure 2(a) (Piconet I), the PPF of
each non-gateway slave is 0.2, while the PPF of the gateway
is 0.6. In figure 2(b) (Piconet II), the PPFs of the slaves are
0.2 and 0.4, while the PPF of the gateway is 0.4.
A FAIR AND TRAFFIC DEPENDENT SCHEDULING
11
Figure 2. Piconets with traffic rates between master and each slave shownn.
3.2. Scatternet presence fraction
A gateway will, in general, be a slave in multiple piconets and
may have different amounts of traffic to exchange with each
piconet. Consider an ideal gateway that has complete knowledge
of the rate demands of all its masters. The gateway can
then divide its presence among its piconets in a max-min fair
manner, giving each piconet a "fair share" of its presence. We
call this fair share the "scatternet presence fraction" (SPF) of
the gateway for the piconet. The importance of the SPF is that
a fair division of the gateway's presence among its piconets
can be achieved based on the SPF.
Consider the piconets of figure 2 again, but the gateway of
each of the piconets now connects them to form a scatternet,
as shown in figure 3. The traffic requirements are the same as
shown in figure 2. The SPF of the gateway is 0.5 in Piconet I
and 0.5 in Piconet II.
3.3. Fair share
We see that for a gateway to be fair, there are two kinds of
fairness it has to achieve: that dictated by the PPFs, which
achieves fairness between the gateway and the other slaves of
a piconet, and that of the SPFs, which distributes the presence
of the gateway between its piconets in a fair manner. Both
these kinds of fairness may not always be completely achiev-able
and this can lead to a change in the values of PPF and
SPF, as we now discuss.
We observe that an ideal master (as in section 3.1) does
not give a gateway more than the PPF of its polling. Thus,
if the SPF of a gateway is greater than its PPF for a piconet,
the gateway spends a fraction of its time equal to the PPF
in the piconet. The gateway cannot stay for a fraction equal
to its SPF in the piconet since it is limited by its PPF. Thus,
the extra scatternet presence fraction (the difference of the
SPF and the PPF) is redistributed in a fair manner among
the gateway's other piconets for which the SPF is less than
the PPF. This may increase the SPF of the gateway in the
other piconets. In other words, the gateway behaves as if
its SPF in a particular piconet is reduced to the PPF and
thus, its SPF in the other piconets increases. We refer to this
changed SPF as the "updated SPF" of the gateway in a piconet
.
Similarly, an ideal gateway does not stay a fraction of time
more than the SPF in a piconet. Thus, if the PPF of the gate-Table
1
Calculation of fair share of the gateway in the two piconets of figure 3.
Piconet I
Piconet II
Actual traffic rate
0.7
0.6
PPF
0.6
0.4
SPF
0.5
0.5
Updated PPF
0.6
0.4
Updated SPF
0.6
0.4
Fair share
0.6
0.4
Figure 3. Gateway shared between two piconets; traffic rates between slaves
and the master are shown.
way in the piconet is greater than the SPF, the gateway spends
a fraction of time equal to the SPF in the piconet. The remaining
PPF of the gateway (the difference of the PPF and the
SPF) is redistributed in a fair manner among the other slaves
of the piconet (if this other slave is a gateway, it is redistributed
to it if its SPF is greater than its PPF in the piconet). This
may increase the PPF of these slaves. We refer to this changed
PPF as the "updated PPF" of the slave in the piconet. In case
there is no such redistribution, the updated PPF is equal to the
PPF and the updated SPF is equal to the SPF.
The fair share can now be calculated from the "updated
PPF" and the "updated SPF" as the minimum of these two
quantities. Note that all these quantities ­ PPF, SPF, updated
PPF, updated SPF and fair share­are dependent on the traffic.
Any change in traffic demand of a unit may lead to a change
in some of these quantities. We explain the calculation of the
fair share using some examples.
An example is given in table 1, which shows the actual traffic
rate, PPF, SPF, Updated PPF, Updated SPF and fair share
of the gateway in the two piconets of figure 3. In Piconet II,
the gateway has a PPF of 0.4, which is less than the SPF. In
Piconet I, the gateway has a PPF of 0.6 and an SPF of 0.5.
Thus, the extra scatternet presence fraction of the gateway in
Piconet II (the difference between the SPF and the PPF) is
given to Piconet I, which has a higher traffic rate than may
be allowed by the SPF. This is reflected in the "updated SPF"
values. Thus, the "fair share" of the gateway in Piconet I is
0.6 and in Piconet II is 0.4. The fair shares of the non-gateway
slaves are equal to their PPF.
As another example, consider the scatternet consisting of
5 piconets with the traffic rates shown as in figure 4. As shown
in table 2, gateway G2 has a PPF of 0.5 and an SPF of 0.4 in
Piconet B. Thus, the "updated PPF" of G2 in Piconet B is 0.4.
The extra PPF (
= PPF - SPF) is added to the PPF of gateway
12
R. KAPOOR ET AL.
Figure 4. Scatternet with two gateways.
Table 2
Calculation of fair share of the gateways G1 and G2 in the scatternet of
figure 4.
Gateway G1
Piconet A
Piconet B
Piconet C
Actual traffic rate
0.4
0.6
0.1
PPF
0.25
0.5
0.1
SPF
0.4
0.5
0.1
Updated PPF
0.25
0.6
0.1
Updated SPF
0.25
0.65
0.1
Fair share
0.25
0.6
0.1
Gateway G2
Piconet B
Piconet D
Piconet E
Actual traffic rate
0.7
0.2
0.4
PPF
0.5
0.2
0.4
SPF
0.4
0.2
0.4
Updated PPF
0.4
0.2
0.4
Updated SPF
0.4
0.2
0.4
Fair share
0.4
0.2
0.4
G1 in Piconet B. The "updated PPF" of G1 in Piconet B is,
thus, 0.6.
Also, gateway G1 has a PPF of 0.25 and an SPF of 0.4
in Piconet A. Thus, the "updated SPF" of G1 in Piconet A is
0.25. The extra SPF (
= SPF-PPF) is added to the SPF of G1
in Piconet B. The "updated SPF" of G1 in Piconet B, is thus,
equal to 0.65. The fair shares can now be easily calculated.
A division of the master's polling and the gateway's presence
based on PPF and SPF as described in this section takes
into account the traffic demands of the slaves and the gateways
and leads to fairness in the scatternet. In the next section
, we introduce and describe an algorithm that aims to
achieve such a fair distribution of bandwidth.
Description of algorithm
We first explain how the algorithm works in the case of a single
piconet with no gateway. We then extend the algorithm
to the case of a scatternet and explain how the coordination
between the master and the gateways is achieved. We then
prove the fairness of the algorithm.
4.1. Single piconet with no gateways
The polling algorithm is based on the master estimating the
traffic rate between each slave and itself. This traffic rate is
the sum of the traffic rates from the master to a slave and in
the reverse direction. We assume, in order to simplify the explanation
of the algorithm, that traffic flows only from slaves
to master; masters generate no traffic to slaves. The same algorithm
also applies with little change when traffic flows in
both directions (explained later).
The master uses a Round Robin polling scheme, with the
modification that a slave is skipped if it does not belong to the
"active list" of the master. The slaves are moved in and out
of the active list on the basis of two variables that the master
maintains for each slave. These two variables are:
r
­ estimate of the rate of traffic generated by the slave;
N
­ estimate of the queue length of the slave.
When a slave is polled, the master­slave pair gets a chance
to exchange a maximum amount of data in each direction,
denoted by M. After each such polling phase, the master updates
the values of N and r in the following manner:
For the slave just polled:
N
= N + r - x,
(1)
r
=




r
+ (1 - ) xT ,
x &lt; M
,
r
+ (1 - ) xT + , x = M.
(2)
For other slaves:
N
= N + r,
(3)
where  is the time elapsed since the last update, x is the
amount of data exchanged during the poll phase, T is the total
time elapsed since the last poll of the same slave,  is a parameter
used to smooth the rate estimation and  is a parameter
used to probe for more bandwidth. Note that x is the actual
amount of data exchanged, which may be less than or equal
to M, depending upon the number of packets in the slave's
queue. Since N is an estimate of the slave's queue length and
r
is an estimate of the rate at which traffic is generated, N is
increased at the rate of r (as in equations (1) and (3)). Also,
when a slave is polled, N is decreased by the amount of data
exchanged ((equation 1)).
After updating these values, the master determines the
changes to be made to the active list. A slave is added or
deleted from the active list depending upon whether its value
of N is greater or smaller than a "threshold". The value of
this threshold is the minimum amount of data that the master
would like the slave to have in order to poll it. We choose
a value equal to a multiple of a DH5 packet for the threshold
since this packet incurs least overhead (the selection of
the value of the threshold is discussed further in the next subsection
). Thus, a slave is present in the active list if the master's
estimate of the value of N for the slave is greater than the
threshold. This makes the simple Round Robin polling strategy
adaptive to traffic and enables it to utilize bandwidth ef-ficiently
, even when slaves have different rates of traffic. The
maximum amount of data that can be exchanged at each poll,
M
, is also set equal to the threshold. Note that if the amount
of data, x, in the slave's queue is less than the threshold, the
polling of the slave ends after this data has been exchanged.
A FAIR AND TRAFFIC DEPENDENT SCHEDULING
13
If the value of N is less than the threshold for all the slaves,
then the slave whose value of N is estimated to take the smallest
time to reach the threshold is polled, i.e., the slave for
which the value of (Threshold
- N)/r is the smallest.
The master now goes to the next slave according to the
Round Robin ordering of slaves. If the slave is present in the
active list, it is polled. Else, the procedure is repeated for the
next slave in the Round Robin ordering.
Also, note that if the amount of data sent by the slave x
is equal to M, r is increased by a small amount, . This is
basically an attempt by the slave to probe for more bandwidth
if it is able to send data at the present rate. The usefulness
of this increase is evident in the proof of fairness in the next
section. The value of  chosen is 0.15 and that of  is 0.65.
We also discuss the rationale behind choosing these values in
the proof of fairness.
If traffic flows in both directions, i.e., from the slaves to
the master and in the reverse direction, x is the average of
the amount of data exchanged in the two directions, r refers
to the average of the rate-estimations of the two directions
and N refers to the average of the queue length estimates of
the two directions. Also, if the number of packets in either
direction is less than the threshold, the polling of the slave
continues till in both directions, (a) there is no more data to
send or (b) amount of data equal to the threshold has been
exchanged.
The initial value of N is set to the threshold (to ensure that
slaves get polled at the beginning) and that of r is set to 0.25
(as a reasonable value). Note that the algorithm converges to
the fair share, but a careful selection of initial values makes
the initial convergence faster.
Another advantage of such a scheme is that it may allow
the master to go into a power-saving mode if it realizes that no
slave has sufficient packets to send, i.e., if N is smaller than
the threshold for all slaves. Though we do not explore this
option in this paper, it may be useful since Bluetooth devices
are expected to work in power-constrained environments.
To improve the algorithm, we add a heuristic to it. The
maximum number of polling cycles that a slave is not polled
is bounded. If a slave generates a large burst of data occasionally
and then does not generate any data for a long time,
the value of r for the slave may be very low. This may cause
the value of N for the slave to be lower than the threshold
for a long time. By limiting the maximum number of cycles
missed by the slave, we make sure that such a behavior of the
slave does not lead to its starvation. In the experiments, this
value is taken to be equal to 5 cycles. We now explain how
the above algorithm works in a scatternet.
4.2. Scatternet
Scheduling of gateways using Rendezvous Points.
Before
describing how the algorithm works in a scatternet, we briefly
discuss the notion of Rendezvous Points (RPs) described
in [4]. A RP is a slot at which a master and a gateway have
agreed to meet, i.e., at this slot, the master will poll the gateway
and the gateway will listen to the master. In [4], RPs are
implemented using the SNIFF mode of Bluetooth, but we implement
RPs using the HOLD mode [10]. In the HOLD mode,
the slave does not have to listen to the master for a certain time
period and may use this time to visit other piconets. Prior to
entering the HOLD mode, the master and the slave agree on
the time duration the slave remains in the HOLD mode. We
implement our algorithm using RPs as described below.
The working of the algorithm in a scatternet is very similar
to its operation in a piconet. The master continues to poll the
non-gateway slaves in the same manner as described in the
previous section with the modification that a gateway is polled
at a Rendezvous Point. Each RP is a slot at which a particular
gateway is polled and a master has different RPs for each of its
gateways. These RPs are always unique (i.e., a master cannot
have the same RP with more than one gateway). Since the
gateway must be polled at the RP, this has implications in the
polling of the other slaves (discussed later). Once a gateway
has been polled, the master continues with the polling of the
other slaves in the same manner as described in the previous
section, i.e., it checks its active list to see if the next slave in
the polling cycle is to be polled and so on.
In order to divide its time among different piconets in a
fair manner, the gateway performs similar calculations as described
in the earlier section for the master. The gateway
maintains values of N and r for each piconet it belongs to and
these values are updated each time a gateway is polled (i.e.,
at each RP). Thus, the calculations performed by a gateway at
each RP are:
For the piconet in which the gateway was just polled:
N
= N + r - x,
(4)
r
=




r
+ (1 - ) xT ,
x &lt; M
,
r
+ (1 - ) xT + , x = M.
(5)
For other piconets:
N
= N + r,
(6)
where  is the time elapsed since the last update, x is the
amount of data exchanged during the poll phase, T is the total
time elapsed since the gateway was polled in the same piconet
, and  and  are as defined earlier.
Moreover, at each RP, the gateway and the master negotiate
the next RP between them. The assignment of this next
RP takes into account the fairness between (a) the gateway
and other slaves in a piconet and (b) the presence of the gateway
in different piconets. Also, we again employ a heuristic
that improves the algorithm. When the next RP is being nego-tiated
, we keep a bound on the maximum value this can take.
This prevents a piconet from not being visited by a gateway
for a long time. The maximum value of this next RP used in
our experiments is 400 slots.
We now see how the master and the gateway use the information
that they have to achieve fairness in the scatternet.
When a gateway is polled at a RP, the gateway and the master
do the following.
14
R. KAPOOR ET AL.
(i) Gateway. The gateway calculates the number of slots,
N
thresh
after which N for the piconet will become greater
than the threshold; N
thresh
= (threshold - N)/r, where
threshold is as explained in the previous section, N and
r
are values maintained by the gateway for the piconet.
The gateway makes use of this value and does not visit
a piconet till its estimate of N for the piconet becomes
greater than the threshold. This is similar to the algorithm
used by the master in which a slave is not polled till
the master's estimate of N for the slave becomes greater
than the threshold. Thus, the gateway tries to divide its
time between the piconets in a fair manner, i.e., according
to the SPFs. Note that N
thresh
may be negative if N
is greater than the threshold. Also, N
thresh
is allowed to
have a maximum value of 400.
Moreover, each time a gateway visits a piconet, it knows
the RPs for the other piconets it belongs to (except right
at the beginning or when the gateway is added to another
piconet).
(ii) Master. The master calculates the number of slots after
which the gateway can be polled such that the fairness
with other slaves is maintained. It adopts the following
procedure to achieve this:
It maintains a counter, num_slots (which is initialized
to 0) and checks the value of N for each slave, in a cyclic
order, starting from the slave after the current gateway in
the cyclic order to the slave before the current gateway.
The master checks if the value of N for the slave will be
greater than the threshold after num_slots slots. If this
condition is true, num_slots is incremented by twice the
value of the threshold. After incrementing num_slots, the
master also checks to see if it has a RP with any gateway
whose value is equal to num_slots and increments
num_slots by twice the value of the threshold if this is
true. This ensures that the master has a unique RP for
each of its gateways. Note that num_slots is incremented
by twice the value of the threshold since the master expects
to exchange threshold slots of data with a slave in
each direction.
The master uses the above procedure to estimate the number
of slaves who will have their value of N greater than
the threshold when the master polls the slaves in their
cyclic order starting from the gateway just polled. The
value of num_slots determines the number of slots which
the master expects to use in polling the other slaves in
one cycle before polling the gateway again and is thus,
used by the master to maintain fairness between the gateway
and the other slaves in the piconet. Again, note that
num_slots is allowed to have a maximum value of 400.
The master and the gateway now exchange the information
they have to calculate their next RP. This exchange takes
place using the LMP_hold_req PDU of the LMP (Link Manager
Protocol) layer. This PDU carries a hold instant and a
hold time, which are used to specify the instant at which the
hold will become effective and the hold time, respectively.
When the master is sending a packet to a gateway, the value
of num_slots can be sent after hold instant and hold time in
the packet. The master also sends the values of its RPs with
its other gateways in the packet. Similarly, the gateway sends
the master the values of its RPs with other piconets and the
value of N
thresh
also in an LMP_hold_req PDU. The master
now knows all the RPs of the gateway; similarly, the gateway
knows all the RPs of the master.
Note that the above information exchange requires a minimal
change in the Bluetooth specifications that the contents
of the LMP_hold_req PDU need to be enhanced. This PDU is
1-slot in length; thus, some bandwidth of the master is wasted
in sending these PDUs. This wasted bandwidth can be reduced
by increasing the value of threshold, i.e., the maximum
data that a slave and a master may exchange in each direction
during one poll of the slave. On the other hand, a large
value of the threshold will lead to larger delays for packets.
Thus, we have a tradeoff here. We choose a threshold value
equal to three times a DH5 packet. The effect of this wasted
bandwidth can be seen in the experiments section where the
piconet capacity used is slightly less than 1. Note that we
pay a small price here to get perfect coordination between the
master and the gateway and also to get a high degree of fairness
in the system, as the experiments later demonstrate.
Now, the master and the gateway both have complete information
. So, each of them calculates the next RP in the
following manner:
They take the maximum value out of num_slots and N
thresh
and as long as this value is the same as one of the RPs (note
that all relevant RPs are known to both the master and the
gateway), the value is incremented by 2
· threshold. The value
at the end of this small procedure is the next RP between the
gateway and the master. Since this value takes into account
both N
thresh
and num_slots, it incorporates both the fairness
of the master's polling and the gateway's presence.
Note that the value of num_slots calculated by the master is
just an estimate (the master assumes that each slave included
in the calculation of num_slots will exchange threshold slots
of data with the master in each direction, but this may not be
true). Thus, the master may have polled all the slaves that had
to be polled before the RP of the gateway (according to the
estimate in the calculation of num_slots) and still be left with
some slots before the RP. In this case, the master just continues
polling the slaves in their cyclic order and polls the gateway
when the time for the RP arrives. Note that this means
that the master may have to force a slave to send a packet
smaller than a certain length. For example, if two slots are
left for the RP, then the master will send a 1-slot packet and
ask the slave being polled to do the same. Note that the Bluetooth
header has 4 bits to represent the packet type and these
can represent 16 packet types. For ACL links, 10 (7 data,
3 control packets) of the packet types are defined. We use 2
of the remaining bit sequences to send packets that force the
slave to send packets smaller than or equal to a certain length.
This is shown in table 3.
From table 3, we see that this procedure is adopted if the
number of slots left for the RP is less than 10 (if the number
of slots left for the RP is greater than or equal to 10, then the
A FAIR AND TRAFFIC DEPENDENT SCHEDULING
15
Table 3
Procedure adopted by the master if slots left for the RP is less than 10.
Slots left for RP
Maximum
Maximum
length of packet
length of packet
sent by master
sent by slave
2
1
1
4
1
1
6
3
3
8
3
3
slave's packet length does not have to be restricted). Thus,
if the slots left for the RP is 2, the master can send a packet
of maximum length
= 1 and the gateway can send a packet
of maximum length
= 1 and so on. Note that for reasons of
fairness, the maximum packet length for the master and the
gateway is the same. Since the master needs to restrict the
maximum length of the gateway's packet to either 1 or 3 (as
shown in table 3), we need 2 packet types to achieve this. This
procedure effectively suspends the polling of a slave to honor
a RP with a gateway. The polling of the slave continues after
the gateway has been polled.
In addition, a gateway may lose a slot in switching from
one piconet to another. This loss is unavoidable since piconets
are in general, not synchronized in time. In the experiments in
the paper, we set the value of the threshold to three times the
payload of a DH5 packet, which can give a switching loss of
about 3% at heavy loads (every 2
·threshold slots, the gateway
loses about one slot in switching). At light loads, this switching
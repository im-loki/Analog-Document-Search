Machine Learning in Low-level Microarray Analysis
ABSTRACT
Machine learning and data mining have found a multitude
of successful applications in microarray analysis, with gene
clustering and classification of tissue samples being widely
cited examples. Low-level microarray analysis ­ often associated
with the pre-processing stage within the microarray
life-cycle ­ has increasingly become an area of active
research, traditionally involving techniques from classical
statistics. This paper explores opportunities for the application
of machine learning and data mining methods to several
important low-level microarray analysis problems: monitoring
gene expression, transcript discovery, genotyping and resequencing
. Relevant methods and ideas from the machine
learning community include semi-supervised learning, learning
from heterogeneous data, and incremental learning.
INTRODUCTION
DNA microarrays have revolutionized biological research over
the short time since their inception [2; 27; 28; 29]. Although
most widely used for parallel measurement of gene
expression [27; 28], microarrays are starting to find common
application in other areas of genomics and transcriptomics,
including genomic re-sequencing [30; 31], genotyping [32;
33], and transcript discovery [34].
Research labs armed with microarrays have been able to
partake in a range of studies, including finding gene function
[35; 36; 37]; correcting mistaken database annotations
[36; 7]; performing linkage analyses; determining specific
genes involved in biological pathways; identifying genes that
are important at certain times of development (or that are
turned on/off over a course of treatment); elucidating gene
regulatory networks [13]; diagnosing disease in tissue sam-Figure
1: The relationship between low-level and high-level
microarray analysis.
ples [38; 39; 40; 41];
tioners' misdiagnoses [38]. The common thread among these
high-level microarray analysis problems is that they answer
sophisticated questions of direct biological interest to medical
researchers (such as "which genes are being co-expressed
under treatment X?"), where the raw data used are estimates
of biologically meaningful parameters (such as the
expression level estimates for thousands of genes).
In contrast to these so-called high-level problems, low-level
microarray analysis [19] is concerned with the preceding step
in the microarray assay cycle (Figure 1) ­ given raw data
straight from a scanner which has no direct biological interpretation
, clean and summarize this data to produce the
biologically meaningful parameter estimates (such as expression
level estimates) that are later used in high-level analyses
.
In low-level analysis, more consideration is generally given to
the behavior of the underlying molecular biology, microarray
technology, and experimental design than in high-level analysis
. This makes generative methods readily applicable in
low-level problems, facilitating the formulation of confidence
SIGKDD Explorations.
and even identifying medical practi-Volume
5,Issue 2 - Page 130
statements such as p-values in gene expression calls. Hence,
while high-level problems have been tackled with discriminative
approaches, such as those found in machine learning and
data mining, in addition to classical statistical methods, the
low-level analysis community has traditionally called upon
only the latter.
In this paper we argue that low-level microarray analysis
poses a number of interesting problems for the data mining
and machine learning community, distinct to the traditional
high-level microarray problems. These problems are relevant
to the long-term success of DNA microarrays and are
already topics of active research in the low-level microarray
analysis community. It is our hope that this position paper
motivates and enables further machine learning research in
the area. Although we will focus on high density oligonucleotide
microarrays, particularly those of the Affymetrix
GeneChip variety, the underlying concepts and opportunities
remain the same for related technologies. Throughout
the paper, we distinguish machine learning from statistics.
While these disciplines are closely related and serve as foundations
for inference in microarray analysis, the distinction
does have content. In our view, classical statistics is generative
, dealing with relatively low-dimensional data and parameter
spaces, while machine learning is often discriminative
in nature and explicitly addresses computational issues
in high-dimensional data analysis.
Section 2 reviews relevant background ideas from machine
learning. For an overview of the background molecular biology
and microarray technology, see the guest editorial elsewhere
in this issue. The low-level problems of absolute and
differential expression level summarization, expression detection
, and transcript discovery are reviewed in Section 3,
along with suggested applications of machine learning approaches
to these problems. Sections 4 and 5 similarly cover
microarray-based genotyping and re-sequencing.
Finally,
Section 6 concludes the paper.
BACKGROUND MACHINE LEARNING
We assume familiarity with the notions of unsupervised learning
(clustering) and supervised learning (classification and
regression). As many of the low-level analysis problems discussed
below are amenable to learning from partially labeled
data, learning from heterogeneous data, and incremental
learning, we briefly review these paradigms here.
2.1
Learning from Partially Labeled Data
Given an i.i.d. labeled sample {(x
i
, y
i
)}
n
i=1
drawn from the
unknown and fixed joint distribution F (x, y), and an i.i.d.
unlabeled sample {x
i
}
m
i=n+1
drawn from the marginal distribution
F (x), the problem of learning from partially labeled
data [22; 20] is to use the data in choosing a function ^
g
m
(X)
approximating E(Y |X) where (X, Y )  F . This problem
has been motivated by a number of applications where only
limited labeled data is present, say due to expense, while unlabeled
data is plentiful [16]. This is particularly the case in
the areas of text classification, medical research, and computer
vision [42], within which much of the research into
learning from partially labeled data has occurred.
This problem, also called the labeled-unlabeled data problem
[42], has been explored under a number of closely-related
guises. Some of the earliest approaches used so-called hybrid
learners [6], where an unsupervised learning algorithm assigns
labels to the unlabeled data, thereby expanding the labeled
dataset for subsequent supervised learning. The term
multimodal learning is sometimes used to refer to partially
labeled learning in the computer vision literature [17]. Co-training
is a form of partially labeled learning where the two
datasets may be of different types and one proceeds by using
the unlabeled data to bootstrap weak learners trained
on the labeled data [16].
More recently, semi-supervised learning [25] and transductive
learning [26] have gained popularity. Equivalent to partially
labeled learning, semi-supervised learning includes a
number of successful algorithms, such as those based on the
support vector machine (SVM) [25; 8]. Transductive learners
, on the other hand, aim to predict labels for just the
unlabeled data at hand, without producing the inductive
approximation ^
g
m
. This approach can be used to generalize
the aforementioned hybrid learners, whose unsupervised
step typically ignores the labeled data. In particular
, it is shown in [26] that direct transduction is more effective
than the traditional two-step approach of induction
followed by deduction. A number of transductive schemes
have been proposed, such as those based on the SVM [4; 25],
a graph-based transductive learner [9], and a leave-one-out
error ridge regression method [26]. Joachims [25] describes
an approximate solver for the semi-supervised SVM which
utilizes a fast SVM optimizer as an inner loop.
The story is not all good. [10] tells us that while unlabeled
data may be useful, labeled examples are exponentially more
valuable in a suitable sense. [43] tells us that unlabeled data
may lead the transductive SVM to maximize the wrong margin
, and in [42] it is shown that unlabeled data may in fact
degrade classifier performance under certain conditions relating
the risk and empirical risk. Nonetheless, learning from
partially labeled data has enjoyed great success in many theoretical
and empirical studies [16; 42; 44; 43].
We are especially interested in partially labeled learning as
an approach to the low-level microarray analysis problems
discussed in Sections 3­5, where we have relatively few labeled
examples but an abundant source of unlabeled data.
[45] is a recent example of partially labeled learning applied
to high-level microarray analysis.
There, the problem of
predicting gene function is tackled using a semi-supervised
scheme trained on a two-component dataset of DNA microarray
expression profiles and phylogenetic profiles from
whole-genome sequence comparisons. This leads us to the
next relevant idea from machine learning.
2.2
Learning from Heterogeneous Data
Learning from heterogeneous data is the process of learning
from training data, labeled or not, that can be partitioned
into subsets, each of which contains a different type of data
structure or originates from a different source. This notion
is equivalent to the methods of data fusion [5].
Research into learning from heterogeneous data tends to
be quite domain-specific and has enjoyed increasing interest
from the bioinformatics community in particular (e.g., [18]).
[46] presents a kernel-based framework for learning from heterogeneous
descriptions of a collection of genes, proteins or
other entities. The authors demonstrate the method's superiority
to the homogeneous case on the problem of predicting
yeast protein function using knowledge of amino acid
sequence, protein complex data, gene expression data, and
known protein-protein interactions.
SIGKDD Explorations.
Volume 5,Issue 2 - Page 131
[37] proposes an SVM method for classifying gene function
from microarray expression estimates and phylogenetic profiles
. This is achieved through the construction of an explicitly
heterogeneous kernel: first separate kernels are constructed
for each data type, taking into account high-order
within-type correlations, then these kernels are combined,
ignoring high-order across-type correlations.
Our interest in learning from heterogeneous data arises because
several sources of knowledge relevant to low-level microarray
analysis are available, and incorporating such problem
domain knowledge has been shown to improve the performance
of learning algorithms in the past.
2.3
Incremental Learning
Incremental learning is focused on learning from data presented
sequentially, where the model may be required to
make predictions on unseen data during training. This is in
contrast to cases where all training occurs before any predictions
are made (batch learning ), and is similar to online
learning [24].
A number of incremental learning algorithms have been proposed
and applied in the literature. For example, several
incremental support vector machines have been studied [24;
21; 47]. In [48], incremental learning is applied to distributed
video surveillance. SVM algorithm parameter selection is
investigated in [47]. [21] applies an incremental SVM to detecting
concept drift ­ the problem of varying distributions
over long periods of data gathering ­ and to adaptive classification
of documents with respect to user interest. An exact
incremental SVM is proposed in [24], where decremental unlearning
of incremental training data is possible. This can
be used to efficiently evaluate the computationally-expensive
leave-one-out error measure.
Due to the relatively small sizes of datasets typically available
in low-level microarray analysis, there is great potential
for learners that can incrementally incorporate new data
gathered in the lab, thereby improving estimator performance
specific to that lab's patterns of microarray assay.
EXPRESSION ANALYSIS
The most successful application of DNA microarray technology
to date has been to gene expression analysis. Tra-ditionally
, this has involved estimating gene expression levels
(Section 3.1), an area that is being addressed through
successful statistical methods and active statistics research.
However, the task of determining transcription activity over
entire chromosomes (Section 3.2) is less well developed and
offers serious opportunities for machine learning.
3.1
Gene Expression Monitoring
3.1.1
The Problem
Traditional microarrays measure mRNA target abundance
using the scanned intensities of fluorescence from tagged
molecules hybridized to substrate-attached probes [29]. The
brighter the intensity within a cell of identical probes, the
more hybridization there has been to those probes (Figure
2a). The scanned intensity, then, roughly corresponds
to target abundance.
Since probes are limited in length while targets may be thousands
of bases long, the GeneChip uses a set of probes to
detect each target nucleic acid. The probes are spread out
Figure 2: Probe-level features for expression level summarization
: (a) a cell of probes; (b) target transcript, perfect
match probe and mis-match probe sequences; and (c)
scanned and image-analyzed probe-level intensities.
along a 600 base pair region close to the 3' end of the transcript
. To measure the effects of cross-hybridization, or un-intended
hybridization of target A to the probes intended for
target B, a system of probe pairs is used. In each pair, a perfect
match (PM) probe contains the target's exact complementary
sequence, while a mismatch (MM) probe replaces
the middle base of the perfect match probe with its Watson-Crick
complement. In this way, a target is probed by a probe
set of 11-20 PM-MM probe pairs. The aim is roughly for
the PMs to measure signal plus noise and for the MMs to
measure just noise, so that the signal is revealed using some
function of (PM - MM). Figure 2b depicts the probe set arrangement
, while Figure 2c gives an example of the scanned
intensities. We may now define the expression level summarization
problem.
Low-level Problem 1. Given a probe set's intensities (possibly
after background correction and normalization), the
expression level summarization problem is to estimate the
amount of target transcript present in the sample.
While the expression level summary aims to estimate gene
expression level from the features of Figure 2, expression
detection is concerned with determining the presence of any
gene expression at all.
Low-level Problem 2. Given a probe set's intensities, possibly
normalized, the expression detection problem is to predict
whether the target transcript is present (P) or absent
(A) in the sample, or otherwise call marginal (M) if it is too
difficult to tell. In addition to the P/M/A detection call, we
wish to state a confidence level in the call, such as a p-value.
Detection calls are not as widely utilized as expression level
estimates. They are often used, for example, to filter out
genes with negligible expression before performing computationally
-expensive high-level analyses, such as clustering on
gene expression profiles.
SIGKDD Explorations.
Volume 5,Issue 2 - Page 132
The previous two problems dealt with estimates based on a
single probe-set read from a single array. Comparative studies
, on the other hand, involve assaying two arrays, one the
baseline and the other the experiment, followed by computation
of a single comparative estimate.
Low-level Problem 3. Given two sets of intensities, possibly
normalized, for the same probe set on two arrays:
a. The differential expression level summarization problem
is to estimate the relative abundance of target transcript
on each array.
b. The comparison call problem is to predict whether the
expression of the target has increased, not changed, or
decreased from one chip to the other. As in Low-level
Problem 2, a statement of confidence in the call should
be supplied.
The log-ratio of expression levels for a target is sometimes
known as the relative expression level [3] and is closely related
to the notion of fold change (which is sign(log-ratio) ×
2log-ratio). Comparison calls are sometimes referred to as
change calls. An advantage of working with these comparative
estimates is that probe-specific affinities (one cause
of undesired variation) are approximately cancelled out by
taking ratios [3].
All of these problems are complicated by exogenous sources
of variation which cloud the quantities we are interested in.
[49] proposes a breakdown of the sources of variation in microarray
experiments into intrinsic noise (variation inherent
in the experiment's subjects), intermediate noise (arising
for example from laboratory procedures), and measurement
error (variation due to the instrumentation, such as array
manufacture, scanning, or in silico processing).
3.1.2
Current Approaches
At the level of microarray design, sophisticated probe modeling
and combinatorial techniques are used to reduce probe-specific
effects and cross-hybridization. However, much of
the unwanted variation identified above must still be tackled
during low-level analysis. This means that care must
be taken with the relevant statistical issues. For example,
in experimental design, we must trade off between biological
replicates (across samples) and technical replicates (one
sample across chips). Background correction and normalization
, for reducing systematic variation within and across
replicate arrays, also surface as major considerations [19;
11].
Three popular approaches to Low-level Problem 1 [11] are
the Affymetrix microarray suite (MAS) 5.0 signal measure
[14; 3; 1], the robust multi-array average (RMA) [50; 11]
and the model-based expression index (MBEI) [51].
MAS5 first performs background correction by subtracting a
background estimate for each cell, computed by partitioning
the array into rectangular zones and setting the background
of each zone to that zone's second-percentile intensity. Next
MAS5 subtracts an "ideal mismatch value" from each PM
intensity and log-transforms the adjusted PMs to stabilize
the variance. A robust mean is computed for the resulting
values using a biweight estimator, and finally this value is
scaled using a trimmed mean to produce the signal estimate.
RMA proceeds by first performing quantile normalization
[52], which puts the probe intensity distributions across replicate
arrays on the same scale. RMA then models the PMs
Figure 3: An ROC curve: (0, 0) and (1, 1) correspond to
the "always negative" and "always positive" classifiers respectively
. The closer to the ideal point (0, 1) the better.
Neither of the two families A or B dominates the other. Instead
, one or the other is better according to the desired
trade-off between FP and TP.
as background plus signal, where the signal is exponentially
and the background normally distributed ­ MM intensities
are not used in RMA. A robust additive model is used to
model the PM signal (in log-space) as the sum of the log
scale expression level, a probe affinity effect, and an i.i.d.
error term. Finally, median polish estimates the model parameters
and produces the log-scale expression level summary
.
MBEI fits P M
i,j
-M M
i,j
=
i

j
+
i,j
, using maximum likelihood
to estimate the per-gene expression levels
i
. Here the

j
are probe-specific affinities and the
i,j
are i.i.d. normal
errors.
Although it may seem that expression detection is just a
matter of thresholding expression level estimates, this has
proven not to be the case [53]. It is known that expression
level estimators often have difficulty at low levels of expression
, while detection algorithms are designed with this
setting in mind.
The most widely used detection algorithm for the GeneChip
is a method based on a Wilcoxon signed-rank test [54; 3;
55]. This algorithm corresponds to a hypothesis test of H
0
:
median(
P M
i
-M M
i
P M
i
+M M
i
) =  versus H
1
: median(
P M
i
-M M
i
P M
i
+M M
i
) &gt;
, where  is a small positive constant. These hypotheses
correspond to absence and presence of expression, respectively
. The test is conducted using a p-value for a sum of
signed ranks R
i
=
P M
i
-M M
i
P M
i
+M M
i
-  . The p-value is thresholded
so that values in [0,
1
), [
1
,
2
), and [
2
, 1] result
in present, marginal, and absent calls, respectively. Here
0 &lt;
1
&lt;
2
&lt; 0.5 control the trade-off between false positives
(FP) and true positives (TP).
Recently, a number of alternate rank sum-based algorithms
have been proposed [53]. One in particular ­ a variant on
the MAS5 method where scores are set to R
i
= log
P M
i
M M
i
­
has been shown to outperform MAS5 detection in a range
of real-world situations.
One aspect of the study in [53]
of particular interest is the use of the Receiver Operating
Characteristic (ROC) Convex Hull method [56] for comparing
competing classifiers on a spike-in test set.
ROC curves (see Figure 3) characterize the classification
performance of a family of classifiers parameterized by a tun-SIGKDD
Explorations.
Volume 5,Issue 2 - Page 133
able parameter that controls the FP-TP trade-off. For example
, as the level of a hypothesis test is decreased, the rate
of false positive rejections decreases (by definition), while
the rate of false negative acceptances will typically go up.
An ROC curve encodes this trade-off, extending the notion
of contingency table to an entire curve. It is a more expressive
object than accuracy, which boils performance down to
one number [56; 57].
Comparing ROC curves has traditionally been achieved by
either choosing the "clear winner" (in the rare case of domination
[57]), or choosing the maximizer of the Area Under
Curve (AUC). Although AUC works in some cases, it gives
equal credit to performance over all misclassification cost
and class size settings ­ usually an undesirable strategy if
any domain knowledge is available. The ROC Convex Hull
method, on the other hand, relates expected-cost optimality
to conditions on relative misclassification cost and class size,
so that the typical case of semi-dominance (as in Figure 3)
can be handled in a principled way ­ rather than selecting
p-value thresholds by hand, end-users are provided with the
right classifier and thresholds by the method. This use of
the ROCCH method demonstrates a surprising application
of machine learning to low-level microarray analysis.
Many of these absolute expression algorithms have their
comparative analogues. For example, MAS5 produces the
signal log ratio with an associated confidence interval, using
a biweight algorithm [14; 3]. MAS5 also implements a
comparison call based on the Wilcoxon signed-rank sum test,
just as in the absolute MAS5 detection algorithm above [55].
While the Affymetrix microarray suite is the software package
bundled with the GeneChip, the Bioconductor project
[15] ­ an open-source set of R [12] packages for bioinformatics
data analysis ­ has been gaining popularity and implements
most of the methods discussed here.
3.1.3
Open Problems
While Low-level Problem 1 involves prediction of continuous
expression levels (non-negative real values) given a vector of
(non-negative real) perfect match and mismatch intensities,
with total length between 22 and 40, Low-level Problem 2 is
a 3-class classification problem with call confidence levels.
Open Problem 1. In the respective settings of Low-level
Problems 1­3:
a. What machine learning techniques are competitive with
algorithms based on classical statistical methods for expression
level estimation?
b. Which machine learning classifiers are competitive for expression
detection?
c. What machine learning methods achieve high performance
on the comparative analogues of the previous two problems
, posed on the appropriate product space of microarray
measurements?
Comparisons for expression level estimators might be made
based on bias and variance, computational efficiency, and biological
relevance of learned models. The ROCCH method
is ideal for detector comparison. Issues of background correction
and normalization across multiple arrays must likely
also be addressed to enable competitiveness with the state
of the art.
Research into applying semi-supervised, heterogeneous data
and incremental learners to gene expression monitoring is directly
motivated by the proportion of labeled to unlabeled
data available, the existence of GeneChip domain knowledge
, and the endemic nature of microarray assays that are
continually performed in individual research labs. Biologists
could augment the limited labeled probe-level data available
with relatively abundant unlabeled data. Labeled data can
be procured, for example, from bacterial control experiments
with known concentrations, called spike-in assays, and bacterial
control probe sets that are present in some GeneChips
for calibration purposes. The former source of labeled data
is the more useful for this problem, as it provides examples
with a range of labels. Unfortunately, spike-in studies are
rare because they are not of independent scientific interest:
they are only performed for low-level microarray research.
For the few spike-in assays that are available, only a small
number of targets are spiked in at an equally small number of
concentrations (typically 10). Unlabeled data, in contrast,
could be taken from the large collection of available biologically
relevant assays; each one providing tens of thousands
of data points. Beyond probe intensities, other data sources
could include probe sequences and probe-affinity information
derived from probe models. Such information is closely
related to the hybridization process and might be of use
in expression level estimation: both target and non-specific
hybridization are known to be probe-dependent. Although
labeled data from spike-in studies are of greatest utility for
learning [10], the quantity of unlabeled data produced by
a series of biologically interesting microarray assays in any
given lab suggests a semi-supervised incremental approach.
Since the ROCCH involves taking a pointwise maximum
over the individual noisy ROC curves, it incorporates a possibly
large degree of uncertainty. It should be possible to
extend the results of [53] to quantify this property.
Open Problem 2. Can the ROC Convex Hull method
of [56] be extended to provide confidence intervals for its
conditions on expected-cost optimality?
3.2
Transcript Discovery
3.2.1
The Problem
The applications to expression monitoring described above
are all related to addressing questions about pre-defined
transcripts.
More precisely, the vast majority of expression
analysis is performed using probes interrogating only
a small sub-sequence of each transcript. This has clearly
been a useful approach, but there are at least two potential
drawbacks. One is that we can only monitor the expression
of genes known to exist at the time of the array's design.
Even in a genome as well-studied as that of the human, new
transcripts are routinely discovered. Another is that in directly
monitoring only a sub-sequence of the transcript, it
will often be impossible to distinguish between alternatively
spliced forms of the same gene (which may have very different
functional roles).
An alternative approach is to use arrays with probes tiled
uniformly across genomic sequence, without regard to current
knowledge of transcription.
Such genome tiling arrays
have been used to monitor expression in all the non-repetitive
sequence of human chromosomes 21 and 22 [34],
and more widespread use is underway.
SIGKDD Explorations.
Volume 5,Issue 2 - Page 134
The problems arising in the analysis of data from genome
tiling arrays are essentially the same as those for the expression
monitoring arrays described above: estimation of
expression level, detection of presence, and detection of differential
expression. There is, however, the additional challenge
of determining the number of distinct transcripts and
their location within the tiled genomic region.
Low-level Problem 4. The problem of transcript discovery
can be viewed in two steps:
a. Determining the exon structure of genes within a tiled
region; and
b. Determining which exons should be classified together as
part of a single gene's transcript.
3.2.2
Current Approaches
A simple heuristic approach is taken in [34], in which PM-MM
probe pairs are classified as positive or negative based
on thresholds applied to the difference and ratio of the PM
and MM values. Positions classified as positive and located
close to other positive positions are grouped together to form
predicted exons.
A more effective approach [58] is based on the application of
a Wilcoxon signed-rank test in a sliding window along the
genomic sequence, using the associated Hodges-Lehmann estimator
for estimation of expression level. Grouping into
exons is achieved by thresholding on present call p-values or
estimated expression level, then defining groups of probes
exceeding the threshold to be exons.
3.2.3
Open Problems
The problem of detecting exons based on probe intensities
(Low-level Problem 4a) is very similar to the problem of
absolute expression detection (Low-level Problem 2). For
example, the exon detection method of [58] and the MAS5
expression detection algorithm [55] are both built around
the Wilcoxon signed-rank test. The problem of finding exons
has been addressed as described, but the methods are
heuristic and there is plenty of room for improvement. Associating
exons to form transcripts (Low-level Problem 4b) has
been addressed in a large experiment across almost 70 experimental
pairs using a heuristic correlation-based method;
again, this presents an opportunity for research into more
effective methods.
Open Problem 3. Are there machine learning methods
that are able to out-perform current classical statistical methods
in transcript discovery as defined in Low-Level Problem
4?
One possibility which appears well suited to the problem is
the use of hidden Markov models where the underlying un-observed
Markov chain is over states representing expressed
versus non-expressed sequence. The distribution of the observed
probe intensities would depend on the underlying hidden
state. Another possible approach, considering the success
which has been demonstrated in predicting genes from
sequence data alone, would also be to integrate array-derived
data with sequence information in prediction of transcripts.
GENOTYPING
Descriptions of genome sequencing efforts such as the human
genome project often lend the impression that there
is a unique genomic sequence associated with each species.
This is a useful and approximately correct abstraction. But
in fact, any two individuals picked at random from a species
population will have differing nucleotides at a small fraction
of the corresponding positions in their genomes. Such single-nucleotide
polymorphisms, or SNPs, help form the basis of
genetically-determined variation across individuals. Biologists
estimate that about one position in 1,000 in the human
genome is a SNP. With over 3 billion bases of genomic DNA,
we see that SNPs number in the several millions. Although
there are other kinds of individual genomic variation, such
as insertions, deletions, and duplications of DNA segments,
our focus here is SNPs.
Further complicating the picture is the fact that humans are
diploid organisms--each person possesses two complete but
different copies of the human genome, one inherited from the
mother and one from the father. Now consider a polymorphic
position, or locus, at which two different bases occur
in the population, say G and T. These variants are called
the alleles at the locus, so in this case we are describing a
biallelic SNP. A given individual will have inherited either a
G or T in the paternal genome, and the same is true of the
maternal genome. Thus there are three possible genotypes,
or individual genetic signatures, at this SNP: they are de-noted
GG, TT, and GT. We do not distinguish the last case
from TG, since there is no inherent ordering of the paternal
and maternal genomes at a given polymorphic position.
We refer generically to the alleles of a biallelic SNP as A and
B. Biological evidence suggests that essentially all SNPs are
biallelic in humans. The genotyping problem, then, is to establish
an individual's genotype as AA, BB, or AB for as
many SNPs as possible in the human genome. The completion
of the human genome project means that one has
recourse to the full genomic sequence surrounding a SNP
to help solve the genotyping problem. Furthermore, various
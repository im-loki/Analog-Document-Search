On the Discovery of Significant Statistical Quantitative Rules
ABSTRACT
In this paper we study market share rules, rules that have a certain 
market share statistic associated with them. Such rules are 
particularly relevant for decision making from a business 
perspective. Motivated by market share rules, in this paper we 
consider statistical quantitative rules (SQ rules) that are 
quantitative rules in which the RHS can be any statistic that is 
computed for the segment satisfying the LHS of the rule. Building 
on prior work, we present a statistical approach for learning all 
significant SQ rules, i.e., SQ rules for which a desired statistic lies 
outside a confidence interval computed for this rule. In particular 
we show how resampling techniques can be effectively used to 
learn significant rules. Since our method considers the 
significance of a large number of rules in parallel, it is susceptible 
to learning a certain number of &quot;false&quot; rules. To address this, we 
present a technique that can determine the number of significant 
SQ rules that can be expected by chance alone, and suggest that 
this number can be used to determine a &quot;false discovery rate&quot; for 
the learning procedure. We apply our methods to online consumer 
purchase data and report the results.

Categories and Subject Descriptors
H.2.8 [Database Management]: Database Applications - Data 
Mining.

General Terms
Algorithms, Management.

INTRODUCTION
Rule discovery is widely used in data mining for learning 
interesting patterns. Some of the early approaches for rule
learning were in the machine learning literature [11, 12, 21]. More 
recently there have been many algorithms [1, 25, 28, 31] proposed 
in the data mining literature, most of which are based on the 
concept of association rules [1]. While all these various 
approaches have been successfully used in many applications [8, 
22, 24], there are still situations that these types of rules do not 
capture. The problem studied in this paper is motivated by market 
share rules, a specific type of rule that cannot be represented as 
association rules. Informally, a market share rule is a rule that 
specifies the market share of a product or a firm under some 
conditions.
The results we report in this paper are from real user-level Web 
browsing data provided to us by comScore Networks. The data 
consists of browsing behavior of 100,000 users over 6 months. In 
addition to customer specific attributes, two attributes in a 
transaction that are used to compute the market share are the site 
at which a purchase was made and the purchase amount. Consider 
the example rules below that we discovered from the data:
(1)  Household Size = 3

35K &lt; Income &lt; 50K

ISP =
Dialup

marketshare
Expedia
= 27.76%, support = 2.1%
(2)  Region = North East

Household Size = 1


marketshare
Expedia
= 25.15%, support = 2.2%
(3) Education

=

College

Region = West

50 &lt; Household
Eldest

Age

&lt;

55



marketshare
Expedia
=

2.92%,

support=2.2%
(4)  18 &lt; Household Eldest Age &lt; 20

marketshare
Expedia
=
8.16%, support = 2.4%
The market share for a specific site, e.g. Expedia.com, is 
computed as the dollar value of flight ticket purchases (satisfying 
the LHS of the rule) made at Expedia.com, divided by the total 
dollar value of all flight ticket purchases satisfying the LHS. The 
discovered rules suggest that Expedia seems to be doing 
particularly well among the single households in the North East 
region (rule 2), while it cedes some market in the segment of 
teenagers (rule 4). Rules such as these are particularly relevant for 
business since they suggest natural actions that may be taken. For 
example, it may be worth investigating the higher market share 
segments to study if there is something particularly good that is 
being done, which is not being done in the lower market share 
segments.
More generally, "market share" is an example of a statistic that is 
computed based on the segment satisfying the antecedent of the 
rule. Besides market share, various other quantitative statistics on 
the set of transactions satisfying the LHS of a rule can be 
computed, including mean and variance of an attribute. Prior
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
KDD'04, August 22Â­25, 2004, Seattle, Washington, USA. 
Copyright 2004 ACM 1-58113-888-1/04/0008...$5.00.

374
Research Track Paper
work on learning quantitative association rules [2, 33] studied the 
discovery of rules with statistics such as the mean, variance, or 
minimum/maximum of a single attribute on the RHS of a rule. In 
this paper we generalize the structure of the rules considered in 
[2] to rules in which the RHS can be any quantitative statistic that 
can be computed for the subset of data satisfying the LHS. This 
statistic can even be computed based on multiple attributes. We 
term such rules as statistical quantitative rules (SQ rules).
With respect to learning SQ rules from data, we formulate the 
problem as learning significant SQ rules that have adequate 
support. We define an SQ rule to be significant if the specific 
statistic computed for the rule lies outside a certain confidence 
interval. This confidence interval represents a range in which the 
statistic can be expected by chance alone. This is an important 
range to identify if the rules discovered are to be interpreted as 
suggesting fundamental relationships between the LHS and the 
market share. For example, by chance alone if it is highly likely 
that the market share of Expedia is between 25% and 30% for any 
subset of data, then it is not at all clear that the rule relating 
income and Expedia's market share (rule 1 in the example) is 
identifying a fundamental relationship between income and the 
market share.
While prior work [6, 9] has used confidence intervals to identify 
significant rules, most of these approaches are either parametric 
or specific for binary data. Building on prior work in this paper 
we present a statistical approach for learning significant SQ rules 
that is entirely non-parametric. In particular we show how 
resampling techniques, such as permutation, can be effectively 
used to learn confidence intervals for rules. Based on these 
confidence intervals, significant rules can be identified. However, 
since our method considers the significance of a large number of 
rules in parallel, for a given significance level it is susceptible to 
learning a certain number of false rules. To address this we 
present an intuitive resampling technique that can determine the 
number of false rules, and argue that this number can be used to 
determine a &quot;false discovery rate&quot; for the learning procedure. The 
practical significance of this approach is that we learn significant 
SQ rules from data and specify what the false discovery rate 
exactly is.
The paper is organized as follows. We first define SQ rules in the 
next section. Section 3 presents an algorithm for computing 
confidence intervals and Section 4 presents an algorithm for 
learning significant SQ rules. In Section 5 we explain how the 
false discovery rate for our approach can be computed. We 
present detailed experimental results on real web browsing data in 
Section 6 followed by a literature review and conclusions.

STATISTICAL QUANTITATIVE RULES
In this section we define SQ rules and significant SQ rules. Let 
A= {A
1
, A
2
,..., A
n
} be a set of attributes that will be used to
describe segments and B = {B
1
, B
2
,..., B
m
} be another set of
attributes that will be used to compute various statistics that 
describe the segment. Let dom(A
i
) and dom(B
j
) represent the set
of values that can be taken by attribute A
i
and B
j
respectively, for
any A
i

A and B
j

B. Let D be a dataset of N transactions where
each transaction is of the form {A
1
= a
1
, A
2
= a
2
,..., A
n
= a
n
, B
1
=
b
1
, B
2
= b
2
,..., B
m
= b
m
} where a
i

dom(A
i
) and b
j

dom(B
j
). Let
an atomic condition be a proposition of the form value
1


A
i



value
2

for ordered attributes and A
i
= value for unordered
attributes where value, value
1
, value
2
belong to the finite set of
discrete values taken by A
i
in D. Finally, let an itemset represent a
conjunction of atomic conditions.
Definition 2.1 (SQ rule). Given (i) sets of attributes A and B, (ii) 
a dataset D and (iii) a function f that computes a desired statistic 
of interest on any subset of data, an SQ rule is a rule of the form:

X
f(D
X
) = statistic, support = sup
1
(2.1)
where  X is an itemset involving attributes in A only, D
X
is the
subset of D satisfying X, the function f computes some statistic 
from the values of the B attributes in the subset D
X
, and support is
the number of transactions in D satisfying X.


Note that the statistic on the RHS of the rule can be computed 
using the values of multiple attributes. The following examples 
are listed to demonstrate different types of rules that an SQ rule 
can represent. For ease of exposition we use the name of the 
desired statistic in the RHS instead of referring to it as f(D
X
).
1.

Quantitative association rules [2]:
population-subset
mean or variance values for the subset (2.2)
Quantitative association rules are a popular representation for 
rules in the data mining literature in which the RHS of a rule 
represents the mean or variance of some attribute. Example: 
Education = graduate
Mean(purchase) = $15.00. (2.2) is a
special case of (2.1), where f(subset) is the mean of some attribute 
B
j
in the subset of data.
2.

Market share rules:
Let {A
1
, A
2
,..., A
n
, MSV, P} be a set of attributes in a dataset D.
MSV (Market Share Variable) is a special categorical attribute for 
which the market share values are to be computed. P is a special 
continuous variable that is the basis for the market share 
computation for MSV. For example, each transaction T
k
may
represent a book
2
purchased online. A
1
through A
n
may represent
attributes of the customer who makes the purchase, such as 
income, region of residence and household size. For each 
transaction, MSV is the variable indicating the online book retailer 
where the purchase was made. dom(MSV) may be {Amazon, 
Barnes&Noble, Ebay} and P is the price of the book purchased. 
For a specific v
dom(MSV) a market share statistic can be
computed as described below. Market share rules have the 
following form:

X
marketshare
v

= msh, support = sup
(2.3)
where  X is an itemset consisting of attributes in {A
1
, A
2
,..., A
n
}
and marketshare
v
is a statistic that represents the market share of a
specific  v
dom(MSV). This is computed as follows. Let D
X

represent the subset of transactions satisfying X and D
X, MSV=v


1

In association rules, support is the number of transactions satisfying both
LHS and RHS of a rule. In SQ rules, since the RHS is not an itemset, we 
define support as the number of transactions satisfying the LHS of a rule 
only.
2
The provider, comScore Networks categorizes each purchase into 
categories such as "book", "travel" and "consumer electronics". Hence 
we can generate datasets in which all transactions represent purchases in 
a single category, and this helps in the generation of market share rules 
representing specific categories.
375
Research Track Paper
represent the subset of transactions  satisfying (X
MSV  =  v).
Then marketshare
v
is computed as sum(P, D
X, MSV=v
) / sum(P, D
X
),
where sum(P, D) is the sum of all the values of attribute P in the 
transactions in D.
Market share rules naturally occur in various applications, 
including online purchases at various Web sites, sales 
applications, and knowledge management applications. The 
examples presented in the introduction are real market share rules 
discovered in online purchase data. The following additional 
examples illustrate the versatility and usefulness of market share 
rules.
Â·

Within a specific product category (e.g. shoes) Footlocker 
sells competing brands of shoes. In their transaction data, the 
brand of the shoe can be the MSV and the purchase price is 
P.
Â·

Consider a dataset of patents associated with some area (e.g. 
hard disks). Each record may consist of several attributes 
describing a patent, including one attribute (MSV) which 
represents the organization to which the patent belongs and 
another attribute that is always 1 (representing P and 
indicating a granted patent) in the data. For a specific 
organization, e.g. IBM, market share rules will represent the 
percentage of patents that belong to IBM under some 
conditions involving other attributes of the patent.
Definition 2.1 differs from the definition of quantitative rule [2, 
33] as follows. First, it is not limited to mean and variance 
statistics and assumes a much broader class of statistics, including 
the market share statistics. Second, unlike quantitative rules, the 
statistic of interest in the RHS of a rule can be computed based on 
multiple attributes.
Definition 2.2 (Significant SQ rule). For a given significance 
level


(0, 1), let (stat
L
, stat
H
) be the (1 Â­
) confidence interval
for a desired statistic, where this confidence interval represents 
the range in which the statistic can be expected by chance alone. 
An SQ rule X

f(D
X
) = statistic, support = sup is significant if
statistic lies outside the range (stat
L
, stat
H
).

The main objective of this paper is to discover all significant SQ 
rules. The first challenge in learning significant SQ rules is in 
constructing a confidence interval for the desired statistic such 
that this interval represents a range of values for the RHS statistic 
that can be expected by chance alone. In the next section we 
present an algorithm for learning these confidence intervals.

COMPUTING CONF INTERVALS
The first question that needs to be addressed is what is meant by 
"a range for the statistic that can be expected by chance alone". In 
this section we start by addressing this question and outline a 
procedure by which such a range can be computed. Next we will 
point out the computational challenge in implementing such a 
procedure for learning these intervals for several SQ rules and 
then outline three observations that will substantially help address 
the computational problems. Based on these observations we 
present a resampling-based algorithm for computing the 
confidence intervals.
3.1

Motivation and outline of a procedure
For a given SQ rule, the desired confidence interval theoretically 
represents the range in which the statistic can be expected when 
there is no fundamental relationship between the LHS of the rule 
and the statistic. More precisely, since the statistic is computed 
from the values of the B attributes, the confidence interval 
represents the range in which the statistic can be expected when 
the A attributes are truly independent of the B attributes.
Without making any parametric distributional assumptions, such a 
confidence interval can be generated using the classical nonparametric
technique of permutation. Indeed permutation-based 
approaches have been commonly used to generate confidence 
intervals in the statistics literature [16]. If R is the set of all 
attributes in a dataset, the basic idea in permutation is to create 
multiple datasets by randomly permuting the values of some 
attributes  R
i

R. Such a permutation would create a dataset in
which  R
i
is independent of (R  Â­  R
i
), but would maintain the
distributions of R
i
and (R Â­ R
i
) in the permutation dataset to be the
same as the distributions of these attributes in the original dataset.
Table 3.1 illustrates one example of a permutation dataset D
in
which the B attributes are randomly permuted. Since a desired 
statistic can be computed on each permutation dataset, a 
distribution for the statistic can be computed based on its values 
from the multiple permutation datasets. A confidence interval can 
then be computed from this distribution.
Table 3.1  Dataset permutation
Original dataset D:

Permutation dataset D
:
A
1

A
2

B
1

B
2

A
1

A
2

B
1

B
2

1 2 3 8
1 2 5 6
1 3 5 6
1 3 7 4
2 3 7 4


2 3 3 8
As mentioned above, this is a commonly used procedure in nonparametric
statistics. The reason this procedure makes sense is as 
follows. Even if there is a relationship between the LHS of an SQ 
rule and the statistic on the RHS, by holding the A attributes fixed 
and randomly re-ordering the values of the B attributes the 
relationship is destroyed and the A attributes and B attributes are 
now independent of each other. Repeating this procedure many 
times provides many datasets in which the A attributes and B 
attributes are independent of each other, while maintaining the 
distributions of the A and B attributes to be the same as their 
distributions in the original dataset. The values for the statistic 
computed from the many permutation datasets is used to construct 
a distribution for the statistic that can be expected when the A 
attributes are truly independent of the B attributes.
Specifically, for the same itemset X, compare the following two 
SQ rules in D and D
,

D: X
f(
X
D
) = stat
D
, support = sup
D
(3.1)

D
: X  f(
X
D ) = stat
D
, support = sup
D
(3.2)
First note that the supports of the rules are the same since the 
number of records satisfying X in the permutation dataset is the 
same as the original dataset. We will use this observation to build 
a more efficient method for computing confidence intervals
376
Research Track Paper
shortly. A confidence interval for the rule in (3.1) can be 
computed using the following naÃ¯ve procedure.
1.

Create permutation dataset D
from the original dataset D
and compute stat
D

(as mentioned earlier in Section 2, the
function  f computes this number based on the records 
satisfying X).
2.

Repeat step 1 N
perm
&gt; 1000 times
3
, sort all the N
perm
stat
D


values in an ascending order (stat
D
-1
,  stat
D
-2
,..., stat
D
-Nperm
)
and let the
/2
th
and (1 Â­
/2)
th
percentiles
4
from this list be
stat
D
-L
and stat
D
-H
. The N
perm
values computed above
represents a distribution for the statistic that can be expected 
by chance alone, while the percentile values from this 
distribution determine a specific confidence interval. (Below 
we use the terms "distribution" and "confidence interval" 
frequently.)
3.

The (1 Â­
) confidence interval for the SQ rule in Equation
(3.1) is (stat
D
-L
, stat
D
-H
).
3.2

Computational challenges and solutions
Computing these confidence intervals for multiple candidate SQ 
rules creates several computational problems which we will 
address in this section. For example, if we need to test 10,000 
potential significant rules (which is a reasonably small number for 
data mining tasks), then we would need to repeat the above steps 
10,000 times, and this means generating permutation datasets 
10,000
Ã N
perm
&gt; 10
7
times and to compute the desired statistic in
each permutation dataset.
The following observations substantially reduce the 
computational complexity of the procedure. 
1. Sampling can be used instead of creating permutation datasets. 
For the SQ rule in Equation (3.1), computing stat
D

on a
permutation dataset is really equivalent to computing stat
D

based
on a random sample of sup
D
records in D. This is the case since
none of the A attributes play a role in the computation of the 
statistic. Permuting the dataset, identifying the (sup
D
) records
where  X holds, and then computing the statistic on this subset 
achieves the same effect as picking a random sample of sup
D

records from D and computing the statistic on this random subset. 
Hence to determine the confidence interval for the SQ rule in 
Equation (3.1), instead of permuting the dataset N
perm
times, it is
enough to sample sup
D
records from D for N
perm
times.
2. Some of the candidate SQ rules have the same support values 
as other rules. Based on this observation, confidence intervals for 
two SQ rules with the same support can be approximated by the 
same interval. This is the case since for a given rule the interval is 
generated by sampling sup
D
records many times and if another
rule has support = sup
D
then the interval for that rule will be
similar if the same procedure is repeated (it will not be exactly the
3

N
perm
is typically a big number. If we let N
perm
= N!, which is the number
of all possible permutations, we will be implementing a Monte Carlo 
test. On large datasets, such a test is impractical. For a statistic like 
market share whose value is limited by 0 and 1, N
perm
&gt; 1000 makes the
distribution precise to the third decimal place. In our experiments, N
perm

= 1999.
4

Since we do not have any prior assumption about the expected value of
the statistic we use a two sided p-value.
same because of randomization). Therefore, fewer confidence 
intervals need to be generated.
3.  It is adequate to generate a fixed number of intervals, 
independent of the number of rules considered. We observe that 
the interval for an SQ rule with support = sup
D
can be
approximated by an interval computed by sampling sup
E
records
where sup
E
is "reasonably close" to sup
D
. This is a heuristic that
we use to considerably reduce the complexity of the procedure. 
Denote N
Rule
as the number of rules to be tested. If all rules have
different support values, we need to construct N
Rule
distributions.
Instead, we would construct a fixed number N
dist
distributions,
such that for rule "X
f(D
X
) = statistic, support = sup", statistic
is compared with the distribution that is constructed by sampling 
the closest number of transactions to sup. This heuristic is more 
meaningful when we consider support in terms of percentage of 
transactions satisfying LHS of a rule, which is a number between 
0 and 1.
3.3

Algorithm CIComp
Based on the above observations, we present in Figure 3.1 
algorithm  CIComp for constructing N
dist
distributions and
determining the (1 Â­
) confidence intervals for a given
significance level.
Input: dataset
D
with
N
transactions, the number of
distributions
N
dist
, the number of points in each
distribution
N
perm
, a function
f
that computes the desired
statistic, and significance level

.
Output:
N
dist
distributions and significance thresholds.
1
for (
dist
= 1;
dist



N
dist
;
dist
++ ) {
2

N
sample
=
dist
/
N
dist

Ã

N
;
3
for (
perm
= 1;
perm
&lt;
N
perm
;
perm
++ ) {
4

S
=
N
sample
transactions from
D
sampled without
replacements
5
;
5
stat[
dist
][
perm
] =
f
(
S
);
6
}
7
sort(stat[
dist
]);
8
LowerCI[
dist
] = stat[
dist
][(
N
perm
+ 1)
Ã


/2];
9
UpperCI[
dist
] = stat[
dist
][(
N
perm
+ 1)
Ã
(1 Â­

/2)];
10 }
11  Output stat[][], LowerCI[], UpperCI[]
Figure 3.1  Algorithm CIComp
In the above algorithm, N
dist
,  N
perm
, and
are user-defined
parameters.
is usually chosen to be 5%, 2.5% or 1%. For N
dist

and  N
perm
, the larger they are, the more precise the distributions
will be. Let N = 1000, N
dist
= 100, N
perm
= 999, and
= 5%. We
use these numbers as an example to explain the algorithm. For 
step 2, the first distribution corresponds to N
sample
= dist/N
dist

Ã
N
= 1/100
Ã 1000 = 10 transactions. Step 3 to 6 computes N
perm
=
999 statistics for 10 randomly sampled transactions from dataset 
D. Then we sort these 999 statistics and pick
/2 and 1 Â­ /2
percentiles, which are the 25
th
and 975
th
numbers in the
distribution, as the lower and upper thresholds for the (1 Â­
)
confidence interval. Steps 2 through 9 are repeated N
dist
= 100
times to get the desired number of distributions and confidence 
intervals.
5
If the sampling is done with replacement then the interval will be the
bootstrap confidence interval. The two intervals will essentially be the 
same when the support of the itemset is small.
377
Research Track Paper
The computation complexity of the algorithm in Figure 3.1 is O(N 
Ã N
perm

Ã N
dist
), whereas the complexity of naÃ¯ve method is O(N
Ã
N
perm

Ã  N
rule
). Note that N
dist
can be fixed to a reasonable small
number, e.g. 100, whereas N
rule
is the number of rules that are
being tested and can easily be orders of magnitude more than 
N
dist
.
DISCOVERING SQ RULES
Given the distributions and confidence intervals, discovering all 
significant statistical rules is straightforward. Algorithm 
SigSQrules is presented in Figure 4.1.
Input: dataset
D
with
N
transactions, sets of attributes
A and B,  N
dist
,  stat[][],  LowerCI[], and UpperCI[] from
algorithm
CIComp
, a function
f
that computes the desired
statistic, minimum support
minsup
and a large itemset
generation procedure
largeitemsets
.
Output: set of

Significant rules, sigrules.
1
L =
largeitemsets
(
D
,
A
,
minsup
) # generates large
itemsets involving attributes in A
2
sigrules = {}
3
forall (itemsets
x


L) {
4

x.stat
=
f
(
D
x
) // statistic computed on
transactions satisfying
x

5

dist
= round(
support(x)
/
N

Ã

N
dist
)
6
if
x
.stat


(LowerCI[
dist
], UpperCI[
dist
]) {
//
x



f
(
D
x
)

=
x
.
stat
is significant
7

x.pvalue
= 2
Ã
percentile of
x.stat
in
stat[
dist
][1..
N
perm
]
8
sigrules = sigrules

{
x



f
(
D
x
)

=
x
.
stat
,
support
=
support
(
x
)}
9
}
10 }
Figure 4.1  Algorithm SigSQrules
Given N
dist
distributions constructed from the algorithm CIComp,
we use the above algorithm to discover all significant SQ rules. 
We continue to use the example N = 1000, N
dist
= 100, and N
perm
=
999 to describe the steps in Figure 4.1. Note that the attributes in 
A represent the attributes in the dataset that are used to describe 
segments for which statistics can be computed. Step 1 uses any 
large itemset generation procedure in rule discovery literature to 
generate all large itemsets involving attributes in A. The exact 
procedure used will depend on whether the attributes in A are all 
categorical or not. If they are, then Apriori algorithm can be used 
to learn all large itemsets. If some of them are continuous then 
other methods such as the ones described in [31] can be used.
Step 4 computes the statistic function for each large itemset, x. In 
step 5, we find out which distribution is to be used for 
significance test. For example, if support(x) = 23, then 
support(x)/N
Ã N
dist
= (23/1000)
Ã 100 = 2.3, and hence dist will
be  round(2.3) = 2. We would compare x.stat with its 
corresponding confidence interval (LowerCI[2],  UpperCI[2]) in 
step 6. If x.stat is outside of the confidence interval, the rule is 
significant, and we use step 7 to calculate its 2-side p-value. If 
x.stat is the qth percentile, the 2-side p-value is 2
Ã min(q%, 1Â­
q%). The p-value is not only a value to understand how 
significant a rule is, but is also useful for determining the false 
discovery rate in Section 5. Note that the confidence interval used 
to test significance of a rule is approximate since we do not 
compute this interval for the exact value of the support of this 
rule. Instead we use the closest interval (which was pre-computed 
as described in Section 3.2) corresponding to this support value.
In future research we will quantify the effects of this 
approximation.
We would also like to point out that in many cases (see below) the 
computation of the statistic can be done efficiently within the 
itemset generation procedure (largeitems) itself.  This can be used 
to modify the algorithm to make it more efficient once a specific 
itemset generation procedure is used. This is the case if the 
function f that computes the statistic on transactions T
1
, T
2
,..., T
s

is a recursive function on s, that is,

f(T
1
, T
2
,..., T
s
) = g(f(T
1
, T
2
,..., T
s-1
), f(T
s
), s)
(4.1)
Many statistics, such as mean and market share, are recursive. For 
example, Mean(T
1
, T
2
,..., T
s
) = [Mean(T
1
, T
2
,..., T
s Â­ 1
)
Ã (s Â­ 1) +
Mean(T
s
)] / s.
In this section we presented an algorithm SigSQrules for 
generating significant SQ rules. However, as mentioned in the 
introduction, for any given level of significance for a rule, the fact 
that thousands of rules are evaluated for their significance makes 
it possible to discover a certain number of false rules. This is the 
well known multiple hypothesis testing problem [4]. While it is 
difficult to eliminate this problem, it is possible to quantify this 
effect. In the next section we discuss the problem of false 
discovery in detail and present an algorithm for determining the 
false discovery rate associated with the discovery of significant 
SQ rules.

FALSE DISCOVERY OF SQ RULES
As mentioned above, when multiple rules are tested in parallel for 
significance, it is possible to learn a number of "false" rules by 
chance alone. Indeed, this is a problem for many rule discovery 
methods in the data mining literature. The false discovery rate 
(FDR) is the expected percentage of false rules among all the 
discovered rules. Prior work in statistics has taken two approaches 
to deal with the multiple hypothesis testing problem [4, 17, 34]. 
One approach attempts to lower the false discovery rate by 
adjusting the significance level at which each rule is tested. As we 
will describe below, this approach is not suitable for data mining 
since it will result in very few rules being discovered. The second 
approach assumes that a given number of false discoveries should 
be expected, and focuses on estimating what the false discovery 
rate (FDR) exactly is. This is more useful for data mining, since it 
permits the discovery of a reasonable number of rules, but at the 
same time computes a FDR that can give users an idea of what 
percentage of the discovered rules are spurious. In this section, we 
first review key ideas related to the multiple hypotheses testing 
problem and then present a nonparametric method to determine 
false discovery rate for our procedure.
For significance tests for a single rule, the significance level
is
defined as the probability of discovering a significant rule when 
the LHS and RHS of the rule are actually independent of each 
other; in other words,
is the probability of a false (spurious)
discovery. For example, on a random dataset where all attributes 
are independent, if we test 10,000 rules, then by definition of
,
we expect 10,000
Ã 5% = 500 false discoveries by pure chance
alone. When some of the attributes are dependent on each other, 
as is the case for most datasets on which rule discovery methods 
are used, the above approach cannot be used to get an expectation 
for the number of false rules. In such cases, two approaches are
378
Research Track Paper
possible. In statistics, a measure called familywise error rate 
(FWER) is defined as the probability of getting at least one false 
rule output. Most conventional approaches in statistics that deals 
with the multiple hypotheses testing problem use different 
methods to control FWER by lowering significance level for 
individual rule,

ind
. For example, Bonferroni-type procedures
would have

ind
=
/ the number of rules tested, which is 5% /
10,000 = 5
Ã 10
-6
. However, when the number of hypotheses
tested is large (as is the case in data mining algorithms), extreme 
low
value, e.g. 5 Ã 10
-6
, will result in very few rules discovered.
The other type of approach, as taken recently in [4] estimates the 
false discovery rate (FDR), the expectation of the proportion of 
false discoveries in all discoveries.
Table 5.1  Confusion matrix of the number of rules
Non-Significant
Rules
Significant
Rules
LHS independent of RHS
a b
LHS dependent on RHS
c d
In Table 5.1, the number of rules tested is (a + b + c + d), out of 
which (a + b) is the number of rules where the LHS of the rules is 
truly independent of the RHS, and (c + d) is the number of rules 
where there is a real relationship between the LHS and the RHS 
of the rules. The columns determine how many tested rules are 
output as significant or non-significant. The two terms FDR and 
FWER can be defined precisely as FDR = Exp(b / b + d) and 
FWER = Prob(b &gt;0).
We adopt FDR estimation in this section because it effectively 
estimates false discoveries without rejecting too many discovered 
rules. However, the method proposed in the literature [4, 7, 35] 
for FDR cannot be used for large scale rule discovery because of 
the following two reasons: first, the assumption that statistics of 
the rules tested are independent fr
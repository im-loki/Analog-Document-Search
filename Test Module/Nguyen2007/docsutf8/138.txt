Modeling and Predicting Personal Information Dissemination Behavior
ABSTRACT
In this paper, we propose a new way to automatically model and 
predict human behavior of receiving and disseminating 
information by analyzing the contact and content of personal 
communications. A personal profile, called CommunityNet, is 
established for each individual based on a novel algorithm 
incorporating contact, content, and time information 
simultaneously. It can be used for personal social capital 
management. Clusters of CommunityNets provide a view of 
informal networks for organization management. Our new 
algorithm is developed based on the combination of dynamic 
algorithms in the social network field and the semantic content 
classification methods in the natural language processing and 
machine learning literatures. We tested CommunityNets on the 
Enron Email corpus and report experimental results including 
filtering, prediction, and recommendation capabilities. We show 
that the personal behavior and intention are somewhat predictable 
based on these models. For instance, &quot;to whom a person is going 
to send a specific email&quot; can be predicted by one's personal social 
network and content analysis. Experimental results show the 
prediction accuracy of the proposed adaptive algorithm is 58% 
better than the social network-based predictions, and is 75% better 
than an aggregated model based on Latent Dirichlet Allocation 
with social network enhancement. Two online demo systems we 
developed that allow interactive exploration of CommunityNet are 
also discussed.

Categories and Subject Descriptors
I.2.6 [Artificial Intelligence]: Learning
General Terms:
algorithms, experimentation



INTRODUCTION
Working in the information age, the most important is not
what you know, but who you know [1]. A social network, the 
graph of relationships and interactions within a group of 
individuals, plays a fundamental role as a medium for the spread
of information, ideas, and influence. At the organizational level, 
personal social networks are activated for recruitment, partnering, 
and information access. At the individual level, people exploit 
their networks to advance careers and gather information.
Informal network within formal organizations is a major, but
hard to acquire, factor affecting companies' performance. 
Krackhardt [2] showed that companies with strong informal 
networks perform five or six times better than those with weak 
networks, especially on the long-term performance. Friend and 
advice networks drive enterprise operations in a way that, if the 
real organization structure does not match the informal networks, 
then a company tends to fail [3]. Since Max Weber first studied 
modern bureaucracy structures in the 1920s, decades of related 
social scientific researches have been mainly relying on 
questionnaires and interviews to understand individuals' thoughts 
and behaviors for sensing informal networks. However, data 
collection is time consuming and seldom provides timely, 
continuous, and dynamic information. This is usually the biggest 
hurdle in social studies.
Personal Social Network (PSN) could provide an organizing
principle for advanced user interfaces that offer information 
management and communication services in a single integrated 
system. One of the most pronounced examples is the
networking

study by Nardi et al. [4], who coined the term
intensional
networks to describe personal social networks. They presented a 
visual model of user's PSN to organize personal communications 
in terms of a social network of contacts. From this perspective, 
many tools were built such as LinkedIn [5], Orkut [6], and 
Friendster [7]. However, all of them only provide tools for 
visually managing personal social networks. Users need to 
manually input, update, and manage these networks. This results 
in serious drawbacks. For instance, people may not be able to 
invest necessary efforts in creating rich information, or they may 
not keep the information up-to-date as their interests, 
responsibilities, and network change. They need a way to organize 
the relationship and remember who have the resources to help 
them. We coin the terminology of managing these goals as 
personal social capital management
1
.
In this paper, we develop a user-centric modeling
technology, which can dynamically describe and update a

1

Social capital infers to the accumulated contacts' human capital (asset,
power, resources) that a person can explore through social network [8].

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. 
KDD'05, August 21­24, 2005, Chicago, Illinois, USA. 
Copyright 2005 ACM 1-59593-135-X/05/0008...$5.00.
479
Industry/Government Track Paper
person's personal social network with context-dependent and 
temporal evolution information from personal communications. 
We refer to the model as a CommunityNet. Senders and receivers, 
time stamps, subject and content of emails contribute three key 
components ­ content semantics,  temporal information,  and 
social relationship. We propose a novel Content-Time-Relation 
(CTR) algorithm to capture dynamic and context-dependent 
information in an unsupervised way. Based on the CommunityNet 
models, many questions can be addressed by inference, prediction 
and filtering. For instance, 1) Who are semantically related to 
each other? 2) Who will be involved in a special topic? Who are 
the important (central) people in this topic? 3) How does the 
information flow? and 4) If we want to publicize a message, 
whom should we inform?
Figure 1 shows the procedure of our proposed scheme. First,
topic detection and clustering is conducted on training emails in 
order to define topic-communities. Then, for each individual, 
CommunityNet  is built based on the detected topics, the sender 
and receiver information, and the time stamps. Afterwards, these 
personal  CommunityNets can be applied for inferring 
organizational informal networks and predicting personal 
behaviors to help users manage their social capitals. We 
incorporate the following innovative steps: 
1)  Incorporate content analysis into social network in an
unsupervised way
2)  Build a CommunityNet for each user to capture the context-dependent
, temporal evolutionary personal social network 
based on email communication records

3)  Analyze people's behaviors based on CommunityNet,
including predicting people's information sending and 
receiving behaviors

4)  Show the potential of using automatically acquired personal
social network for organization and personal social capital 
management
Input: Emails
From: sally.beck@enron.com
To: shona.wilson@enron.com
Subject: Re: timing of submitting
information to Risk Controls
Good memo - let me know if you see results.
......
Topic Detection,
Content Analysis
Topics
Meeting schedule
Agreement
California Energy
Game
Holiday celebration
CommunityNet
CommunityNet
Modeling
Applications
Recommendation system
Prediction,
Filtering
Input: Emails
From: sally.beck@enron.com
To: shona.wilson@enron.com
Subject: Re: timing of submitting
information to Risk Controls
Good memo - let me know if you see results.
......
Topic Detection,
Content Analysis
Topics
Meeting schedule
Agreement
California Energy
Game
Holiday celebration
CommunityNet
CommunityNet
Modeling
Applications
Recommendation system
Prediction,
Filtering

Figure 1. An Overview of CommunityNet

We tested the CommunityNet model on the Enron email
corpus comprising the communication records of 154 Enron 
employees dating from Jan. 1999 to Aug. 2002. The Enron email 
dataset was originally made available to public by the Federal 
Energy Regulatory Commission during the investigation [9]. It 
was later collected and prepared by Melinda Gervasio at SRI for 
the CALO (A Cognitive Assistant that Learns and Organizes) 
project. William Cohen from CMU has put up the dataset on the 
web for research purpose [9]. This version of the dataset contains
around 517,432 emails within 150 folders. We clean the data and 
extract 154 users from those 150 folders with 166,653 unique 
messages from 1999 to 2002.  In the experiments, we use 16,873 
intra-organizational emails which connect these 154 people.
The primary contributions of this paper are three-fold. First
we develop an algorithm incorporating content-time-relation 
detection.  Second, we generate an application model which 
describes personal dynamic community network. Third, we show 
how this model can be applied to organization and social capital 
management. To the best of our knowledge, this is among the first 
reported technologies on fusing research in the social network 
analysis field and the content analysis field for information 
management.

We propose the CTR algorithm and the
CommunityNet  based on the Latent Dirichlet Allocation 
algorithm.

In our experiments, we observed clear benefit of
discovering knowledge based on multi-modality information 
rather than using only single type of data.
The rest of the paper is organized as follows. In Section 2,
we present an overview of related work. In Section 3, we present 
our model. We discuss how to use CommunityNet to analyze 
communities and individuals in section 4 and 5, respectively. In 
Section 6, we show two demo systems for query, visualization 
and contact recommendation. Finally, conclusions and future 
work are addressed in Section 7.
RELATED WORK
To capture relationships between entities, social network has
been a subject of study for more than 50 years. An early sign of 
the potential of social network was perhaps the classic paper by 
Milgram [10] estimating that on average, every person in the 
world is only six edges away from each other, if an edge between 
i and j means &quot;i knows j&quot;. Lately, introducing social network 
analysis into information mining is becoming an important 
research area. Schwartz and Wood [11] mined social relationships 
from email logs by using a set of heuristic graph algorithms. The 
Referral Web project [12] mined a social network from a wide 
variety of publicly-available online information, and used it to 
help individuals find experts who could answer their questions 
based on geographical proximity. Flake et al. [13] used graph 
algorithms to mine communities from the Web (defined as sets of 
sites that have more links to each other than to non-members). 
Tyler  et al. [14] use a betweenness centrality algorithm for the 
automatic identification of communities of practice from email 
logs within an organization. The Google search engine [15] and 
Kleinberg's HITS algorithm of finding hubs and authorities on the 
Web [16] are also based on social network concepts. The success 
of these approaches, and the discovery of widespread network 
topologies with nontrivial properties, have led to a recent flurry of 
research on applying link analysis for information mining.
A promising class of statistical models for expressing
structural properties of social networks is the class of Exponential 
Random Graph Models (ERGMs) (or p* model) [17]. This 
statistical model can represent structural properties that define 
complicated dependence patterns that cannot be easily modeled 
by deterministic models.  Let Y denote a random graph on a set of 
n nodes and let y denote a particular graph on those nodes. Then, 
the probability of Y equals to y is
(
)
( )
(
)
( )
exp
T
s y
P Y
y
c



=
=
(1)
480
Industry/Government Track Paper
where
( )
s y
is a known vector of graph statistics (Density,
Reciprocity, Transitivity, etc) on y,

is a vector of coefficients to
model the influence of each statistics for the whole graph, T 
means "transpose",
( )
c

is a normalization term to
satisfy
(
)
1
y
P Y
y

=
=

. The parameters

are estimated based on
the observed graph
obs
y
by maximum likelihood estimation.
All the research discussed above has focused on using static
properties in a network to represent the complex structure. 
However, social networks evolve over time. Evolution property 
has a great deal of influence; e.g., it affects the rate of information 
diffusion, the ability to acquire and use information, and the 
quality and accuracy of organizational decisions.
Dynamics of social networks have attracted many
researchers' attentions recently. Given a snapshot of a social 
network, [19] tries to infer which new interactions among its 
members are likely to occur in the near future. In [20], Kubica et 
al.
are interested in tracking changes in large-scale data by
periodically creating an agglomerative clustering and examining 
the evolution of clusters over time. Among the known dynamical 
social networks in literature, Snijder's dynamic actor-oriented 
social network [18] is one of the most successful algorithms. 
Changes in the network are modeled as the stochastic result of 
network effects (density, reciprocity, etc.). Evolution is modeled 
by continuous-time Markov chains, whose parameters are 
estimated by the Markov chain Monte Carlo procedures. In [21], 
Handcock et al. proposed a curved ERGM model and applied it to 
the new specifications of ERGMs This latest model uses nonlinear 
parameters to represent structural properties of networks.
The above mentioned dynamic analyses show some success
in analyzing longitudinal stream data.  However, most of them are 
only based on pure network properties, without knowing what 
people are talking about and why they have close relationships.
2.2 Content Analysis
In statistical Natural Language processing, one common way
of modeling the contributions of different topics to a document is 
to treat each topic as a probability distribution over words, 
viewing a document as a probability distribution over words, and 
thus viewing a document as a probabilistic mixture over these 
topics. Given T topics, the probability of the ith word in a given 
document is formalized as:

( )
(
) (
)
1
|
T
i
i
i
i
j
P w
P w z
j P z
j
=
=
=
=

(2)
where
i
z
is a latent variable indicating the topic from which the
i
th word was drawn and
(
)
|
i
i
P w z
j
=
is the probability of the word
i
w
under the jth topic.
(
)
i
P z
j
=
gives the probability of choosing
a word from topics j in the current document, which varies across 
different documents.
Hofmann [22] introduced the aspect model Probabilistic
Latent Semantic Analysis (PLSA), in which, topics are modeled 
as multinomial distributions over words, and documents are 
assumed to be generated by the activation of multiple topics. Blei 
et al.
[23] proposed Latent Dirichlet Allocation (LDA) to address
the problems of PLSA that parameterization was susceptible to 
overfitting and did not provide a straightforward way to infer 
testing documents. A distribution over topics is sampled from a
Dirichlet distribution for each document. Each word is sampled 
from a multinomial distribution over words specific to the 
sampled topic. Following the notations in [24], in LDA, D 
documents containing T topics expressed over W unique words, 
we can represent  ( | )
P w z
with a set of T multinomial
distributions

over the W words, such that
( )
( |
)
w
j
P w z
j

=
=
,
and  P(z) with a set of D multinomial distribution

over the T
topics, such that for a word in document d,
( )
(
)
d
j
P z
j

=
=
.
Recently, the Author-Topic (AT) model [25] extends LDA to 
include authorship information, trying to recognize which part of 
the document is contributed by which co-author. In a recent 
unpublished work, McCallum et al. [26] further extend the AT 
model to the Author-Recipient-Topic model by regarding the 
sender-receiver pair as an additional author variable for topic 
classification.  Their goal is role discovery, which is similar to one 
of our goals as discussed in Sec. 4.1.2 without taking the temporal 
nature of emails into consideration.
Using LDA,

and

are parameters that need to be estimated
by using sophisticated approximation either with variational 
Bayes or expectation propagation. To solve this problem, Griffiths 
and Steyvers [24] extended LDA by considering the posterior 
distribution over the assignments of words to topics and showed 
how Gibbs sampling could be applied to build models. 
Specifically,

(
)
( )
( )
( )
( )
,
,
,
,
|
,
i
i
i
w
d
i j
i j
i
i
d
i j
i
n
n
P z
j z w
n
W
n
T








+
+
=

+
+
(3)
where
( )
i
n
is
a count that does not include the current assignment,
( )
w
j
n is the number of times word w has been assigned to topic j in
the vector of assignments  z,
( )
d
j
n is the number of times a word
from document d has been assigned to topic j,
( )
j
n is a sum of
( )
w
j
n ,
( )
d
n  is a sum of
( )
d
j
n . Further, one can estimate
( )
w
j

, the
probability of using word w in topic j, and
( )
d
j

, the probability of
topic j in document d as follows:

( )
( )
( )
^
w
w
j
j
j
n
n
W



+
=
+

(4)


( )
( )
( )
^
d
d
j
j
d
n
n
T



+
=
+

(5)
In [24], experiments show that topics can be recovered by
their algorithm and show meaningful aspects of the structure and 
relationships between scientific papers.
Contextual, relational, and temporal information are three
key factors for current data mining and knowledge management 
models. However, there are few papers addressing these three 
components simultaneously. .In our recent paper, we built user 
models to explicitly describe a person's expertise by a relational 
and evolutionary graph representation called ExpertisetNet  [27]. 
In this paper, we continue exploring this thread, and build a 
CommunityNet model which incorporates these three components 
together for data mining and knowledge management.
COMMUNITYNET
In this section, we first define terminologies. Then, we
propose a Content-Time-Relation (CTR) algorithm to build the
481
Industry/Government Track Paper
personal  CommunityNet. We also specifically address the 
prediction of the user's behaviors as a classification problem and 
solve it based on the CommunityNet models.
3.1 Terminology
Definition 1. Topic-Community: A topic community is a group 
of people who participate in one specific topic.

Definition 2: Personal Topic-Community Network (PTCN): A 
personal topic-community network is a group of people directly 
connected to one person about a specific topic. 
Definition 3. Evolutionary Personal Social Network: An 
evolutionary personal social network illustrates how a personal 
social network changes over time.

Definition 4. Evolutionary Personal Topic-Community 
Network:  An evolutionary network illustrates how a person's 
personal topic-community network changes over time.

Definition 5. Personal Social Network Information Flow: A 
personal social network information flow illustrates how the 
information flows over a person's personal social network to 
other people's personal social networks 
Definition 6: Personal Topic-Community Information Flow: A 
personal Topic-CommunityNet information flow illustrates how 
the information about one topic flows over a person's personal 
social network to other people's personal social networks.
3.2  Personal Social Network
We build people's personal social networks by collecting
their communication records. The nodes of a network represent 
whom this person contacts with. The weights of the links measure 
the probabilities of the emails he sends to the other people: A 
basic form of the probability that an user u sending email to a 
recipient r is:

( )
number of times   sends emails to
|
total number of emails sent out by
u
r
P r u
u
=
(6)
We build evolutionary personal social networks to explore the 
dynamics and the evolution. The ERGM in Eq. (1) can be used to 
replace Eq. (6) for probabilistic graph modeling.  A big challenge 
of automatically building evolutionary personal social network is 
the evolutionary segmentation, which is to detect changes 
between personal social network cohesive sections. Here we apply 
the same algorithm as we proposed in [27]. For each personal 
social network in one time period t, we use the exponential 
random graph model [17] to estimate an underlying distribution to 
describe the social network.

An ERGM is estimated from the data
in each temporal sliding window. With these operations, we 
obtain a series of parameters which indicates the graph 
configurations.
3.3 Content-Time-Relation Algorithm
We begin with email content, sender and receiver
information, and time stamps, and use these sources of knowledge 
to create a joint probabilistic model. An observation is (u, r, d, w, 
t) corresponds to an event of a user u sending to receivers r an 
email  d containing words w during a particular time period t. 
Conceptually, users choose latent topics z, which in turn generate 
receivers  r, documents d, and their content words w during time 
period t.
(
)
(
)
(
)
, | ,
, | ,
| ,
z
P u r d t
P u r z t P z d t
=

(7)
where
,
u r  is a sender-receiver pair during time period t
.
,
u r
can be replaced by any variable to indicate the user's
behavior, as long as it is also assumed to be dependent on latent 
topics of emails.
In order to model the PTCN, one challenge is how to detect
latent topics dynamically and at the same time track the emails 
related to the old topics. This is a problem similar to topic 
detection tracking [28]. We propose an incremental LDA (ILDA) 
algorithm to solve it, in which the number of topics is 
dynamically updated based on the Bayesian model selection 
principle [24]. The procedures of the algorithm are illustrated as 
follows: 
Incremental Latent Dirichlet Allocation (ILDA) algorithm: 
Input: Email streams with timestamp t 
Output:
( )
,
w
j t

,
( )
,
d
j t

for different time period t
Steps: 
1)  Apply LDA on a data set with currently observed emails in a
time period t to generate latent topics
j
z and estimate
(
)
( )
0
0
,
| ,
w
j
j t
P w z t

=
and
(
)
( )
0
0
,
| ,
d
j
j t
P z d t

=
by equation (4)
and (5). The number of topics is determined by the Bayesian 
model selection principle.
2)  When new emails arrive during time period k, use Bayesian
model selection principle to determine the number of topics 
and apply
(
)
(
)
( )
( )
,
1
,
|
, ,
|
,
i
i
d
i j
i
i
k
i
k
d
i
n
P z
j z w t
P w z
j t
n
T





+
=

=
+
to
estimate
(
)
| ,
k
P z d t ,
(
)
| ,
k
P w z t , and
(
)
| ,
k
P z w t .
3)  Repeat step 2) until no data arrive.
Based on this ILDA algorithm, we propose a Content-Time-Relation
(CTR)
algorithm. It consists of two phases, the training
phase and the testing phase. In the training phase, emails as well 
as the senders, receivers and time stamps are available.
(
)
| ,
old
P w z t
and
(
)
, | ,
old
P u r z t
are learnt from the observed
data. In the testing phase, we apply ILDA to learn
(
)
| ,
new
P z d t
.
Based on
(
)
, | ,
old
P u r z t
,  which is learnt from the training
phase,
,
u r can be inferred. Again,  ,
u r  represents a sender-receiver
pair or any variable to indicate the user's behavior, as 
long as it is dependent on the latent topics of emails.  
Content-Time-Relation (CTR) algorithm: 
1) Training
phase
Input: Old emails with content, sender and receiver information, 
and time stamps
old
t
Output:
(
) (
)
(
)
| ,
,   | ,
, and  , | ,
old
old
old
P w z t
P z d t
P u r z t

Steps: 
a)  Apply Gibbs Sampling on the data according to equation (3).   
b) Estimate
(
)
( )
,
| ,
old
w
j
old
j t
P w z t

=
and
(
)
( )
,
| ,
old
d
j
old
j t
P z d t

=
by
equation (4), and (5).
c) Estimate


(
)
(
)
(
)
(
)
(
)
, | ,
, | ,
| ,
, | ,
| ,
old
old
old
d
old
old
d
P u r z t
P u r d t
P d z t
P u r d t
P z d t
=



(8)
2) Testing
phase
Input: New emails with content and time stamps
new
t

482
Industry/Government Track Paper
Output:
(
)
(
)
(
)
, | ,
,  | ,
,
and  | ,
new
new
new
P u r d t
P w z t
P z d t

Steps: 
a)  Apply incremental LDA by Gibbs Sampling based on
(
)
(
)
( )
( )
,
,
|
, ,
,
|
i
i
d
i j
i
i
new
i
old
d
i
n
P z
j z w t
P w z
j t
n
T




+
=

=
+
to
estimate
(
)
| ,
j
new
P w z t
, and
(
)
| ,
new
P z d t
by equation (4) and
(5).
b)  If the topics are within the training set, estimate
(
)
(
)
(
)
^
, | ,
, | ,
| ,
new
old
new
z
P u r d t
P u r z t
P z d t
=

, else if the
sender and receivers are within the training set, estimate
(
)
^
, | ,
new
P u r d t

by topic-independent social network
(
)
, |
old
P u r t
.
c)  If there are new topics detected, update the model by
incorporating the new topics.  
Inference, filtering, and prediction can be conducted based
on this model. For the CTR algorithm, sender variable u or 
receiver variable r is fixed. For instance, if we are interested 
in
(
)
| , ,
P r u d t , which is to answer a question of whom we should
send the message d to during the time period t. The answer will be
(
)
(
) (
)
(
)
(
)
/
^
argmax
| , ,
argmax
| ,
,
| , ,
| ,
| , ,
old
old
new
t
t
t
old
new
old
new
r
t
old
t
new
old
t
new
r
z
z
z
P r u d t
P r u z t
P z
u d t
P r u t
P z
u d t
=


+








(9)
where z
/
new
old
t
t
z
represents the new topics emerging during the
time period t. Another question is if we receive an email, who will 
be possibly the sender?

(
)
(
) (
)
(
)
(
)
/
^
argmax
| , ,
argmax
| ,
,
| , ,
| ,
| , ,
old
old
old
t
t
t
old
new
old
new
u
t
old
t
new
old
t
new
u
z
z
z
P u r d t
P u r z t
P z
r d t
P u r t
P z
r d t
=


+








(10)
Eq. (9) and Eq. (10) integrate the PSN, content and temporal 
analysis. Social network models such as ERGM in Eq. (1) or the 
model in Sec. 3.2 can be applied to the
(
)
, | ,
P u r d t terms.
Figure 2 illustrates the CTR model and compares to the
LDA, AT and ART models. In CTR, the observed variables not 
only include the words w in an email but also the sender u and the 
timestamp on each email d.
3.4 Predictive Algorithms
For the sake of easier evaluation, we focus on prediction
schemes in details. Specifically, we address the problem of 
predicting receivers and senders of emails as a classification 
problem, in which we train classifiers to predict the senders or 
receivers and other behavior patterns given the observed people's 
communication records. The trained classifier represents a 
function in the form of:

:
(
, )
f Comm t i t
Y

(11)
where
(
)
,
Comm t i t
is
the observed communication record
during the interval from time t-i to t,  Y is a set of receivers or 
senders or other user behavior patterns to be discriminated, and 
the value of
(
)
(
)
,
f Comm t i t
is
the classifier prediction
regarding which user behavior patterns gave rise to the observed 
communication records. The classifier is trained by providing the 
history of the communication records with known user behaviors.
3.4.1  Using Personal Social Network Model
We aggregate all the communication records in the history of
a given user, and build his/her personal social network. We 
choose those people with the highest communication frequency 
with this person as the prediction result.
3.4.2  Using LDA combined with PSN Model
We use the LDA model and combine it with PSN to do the
prediction, which is referred as LDA-PSN in the paper. Latent 
topics are detected by applying original LDA on the training set 
and LDA is used for inference in testing data without 
incorporating new topics when time passes by. The possible 
senders and receivers when new emails arrive,
(
)
, | ,
new
P u r d t
is
estimated as
(
)
(
)
(
)
^
, | ,
, | ,
| ,
new
old
new
z
P u r d t
P u r z t
P z d t
=

.
People are ranked by this probability as the prediction results.
3.4.3 Using CTR Model
People tend to send emails to different group of people under
different topics during different time periods. This is the 
assumption we made for our predictive model based on CTR.
LDA
AT
ART
u
u
LDA
AT
ART
u
u
LDA
AT
ART
u
u

: observations

A
N
D

T
u
z
w
r
CTR
S

Tm
t



: observations

A
N
D

T
u
z
w
r
CTR
S

Tm
t




Figure 2. The graphical model for the CTR model comparing to LDA, AT and ART models, where u: sender, t: time, r: receivers, w: 
words, z: latent topics, S: social network, D: number of emails, N: number of words in one email, T: number of topics, Tm: size of the 
time sliding window, A: number of authors,  ,

and

are the parameters we want to estimate with the hyperparameters
, ,

483
Industry/Government Track Paper
(
)
, | ,
new
P u r d t
is estimated by applying the CTR model
discussed in section 3.3. The prediction results are people with 
highest scores calculated by equation (9) and (10).
3.4.4  Using an Adaptive CTR Model
Both the personal social network and the CTR model ignore
a key piece of information from communication records -- the 
dynamical nature of emails. Both personal social network and 
Topic-Community dynamically change and evolve. Only based on 
the training data which are collected in history will not get the 
optimal performance for the prediction task. Adaptive prediction 
by updating the model with newest user behavior information is 
necessary. We apply several strategies for the adaptive prediction. 
The first strategy is aggregative updating the model by adding 
new user behavior information including the senders and receivers 
into the model. Then the model becomes:
(
)
(
)
(
)
(
)
(
)
1
/
^ , | ,
, | ,
| ,
, |
| ,
i
t
t
i
old
K
i
k old
k
i
old
t
i
k
z z
P u r d t
P u r z t P z d t
P u r t P z d t
=
=
+


(12)
where K is the number of old topics. Here, we always use the data 
from
old
t
, including
0
t  to
1
i
t
to
predict the user behavior
during
i
t .
In the second strategy, we assume the correlation between
current data and the previous data decays over time. The more 
recent data are more important.  Thus, a sliding window of size n 
is used to choose the data for building the prediction model, in 
which the prediction is only dependent on the recent data, with the 
influence of old data ignored. Here in equation (12),
old
t consists
of
i n
t
to
1
i
t

.
3.5 CommunityNet Model
We then build a CommunityNet model based on the CTR
algorithm. The CommunityNet model, which refers to the personal 
Topic-Community Network, draws upon the strengths of the topic 
model and the social network as well as the dynamic model, using 
a topic-based representation to model the content of the 
document, the interests of the users, the correlation of the users 
and the receivers and all these relationship changing over time. 
For prediction, CommunityNet incorporates the adaptive CTR 
model as described in Section 3.4.4.
COMMUNITY ANALYSIS
The first part of our analysis focuses on identifying clusters
of topics, and the senders and receivers who participated in those 
topics. First, we analyze the topics detected from the Enron 
Corpus. Then, we study the topic-community patterns.
4.1 Topic Analysis
In the experiment, we applied Bayesian model selection [24]
to choose the number of topics. In the Enron intra-organization 
emails, there are 26,178

word-terms involved after we apply stop-words
removal and stemming, We computed
(
)
|
P w T
for T values
of 30, 50, 70, 100, 110, 150 topics and chose T = 100 with the 
maximum value of
(
)
(
)
log
|
P w T
for the experiment.
4.1.1 Topic Distribution
After topic clustering based on words, for each document, we
have P(z|d), which indicates how likely each document belongs to 
each topic. By summing up this probability for all the documents,
we get the topic distribution of how likely each topic occurs in 
this corpus. We define this summed likelihood as "Popularity" of 
the topic in the dataset. From this topic distribution, we can see 
that some topics are hot - people frequently communicate with 
each other about them, while some others are cold, with only few 
emails related to them. Table 1 illustrates the top 5 topics in Enron 
corpus. We can see that most of them are talking about regular 
issues in the company like meeting, deal, and document. Table 2 
illustrates the bottom 5 topics in Enron corpus. Most of them are 
specific and sensitive topics, like "Stock" or "Market". People 
may feel less comfortable to talk about them broadly.
Table 1. Hot Topics
meeting deal Petroleum
Texas
document
meeting
plan
conference
balance
presentation
discussion
deal
desk
book
bill
group
explore
Petroleum
research
dear
photo
Enron
station
Houston
Texas
Enron
north
America
street
letter
draft
attach
comment
review
mark
Table 2. Cold Topics
Trade stock network
Project
Market
trade
London
bank
name
Mexico
conserve
Stock
earn
company
share
price
new
network
world
user
save
secure
system
Court
state
India
server
project 
govern
call
market
